[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Jon. Welcome to my blog."
  },
  {
    "objectID": "posts/lms-are-glms-part-01/index.html",
    "href": "posts/lms-are-glms-part-01/index.html",
    "title": "Linear Models are General Linear Models",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/lms-are-glms-part-01/index.html#tldr",
    "href": "posts/lms-are-glms-part-01/index.html#tldr",
    "title": "Linear Models are General Linear Models",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/lms-are-glms-part-01/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "href": "posts/lms-are-glms-part-01/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "title": "Linear Models are General Linear Models",
    "section": "Part 1: What are statistical models and how are they fit?",
    "text": "Part 1: What are statistical models and how are they fit?\nIt’s common for different statistical methods to be taught as if they’re completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\nSomething I’ve found immensely helpful over the years is the following pair of equations:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nIn words, the above is saying something like:\n\nThe predicted response \\(Y_i\\) for a set of predictors \\(X_i\\) is assumed to be drawn from (the \\(\\sim\\) symbol) a stochastic distribution (\\(f(.,.)\\))\nThe stochastic distribution contains both parameters we’re interested in, and which are determined by the data \\(\\theta_i\\), and parameters we’re not interested in and might just have to assume, \\(\\alpha\\).\nThe parameters we’re interested in determining from the data \\(\\theta_i\\) are themselves determined by a systematic component \\(g(.,.)\\) which take and transform two inputs: The observed predictor data \\(X_i\\), and a set of coefficients \\(\\beta\\)\n\nAnd graphically this looks something like:\n\n\n\n\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --&gt; g\n  beta --&gt; g\n  g --&gt; theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt; Y\n\n\n\n\n\n\n\nTo understand how this fits into the ‘whole game’ of modelling, it’s worth introducing another term, \\(D\\), for the data we’re using, and to say that \\(D\\) is partitioned into observed predictors \\(X_i\\), and observed responses, \\(y_i\\).\nFor each observation, \\(i\\), we therefore have a predicted response, \\(Y_i\\), and an observed response, \\(y_i\\). We can compare \\(Y_i\\) with \\(y_i\\) to get the difference between the two, \\(\\delta_i\\).\nNow, obviously can’t change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the \\(\\beta\\) parameters that feed into the systematic component \\(g\\). Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n\n\n\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D --&gt;|partition| X\n  D --&gt;|partition| y\n  X --&gt; g\n  beta --&gt;|rerun| g\n  g --&gt;|transform| theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt;|predict| Y\n  \n  Y --&gt;|compare| diff\n  y --&gt;|compare| diff\n  \n  diff --&gt;|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n\n\n\n\n\nPretty much all statistical model fitting involves iterating along this \\(g \\to \\beta\\) and \\(\\beta \\to g\\) feedback loop until some kind of condition is met involving minimising \\(\\delta\\).\nI’ll expand on this idea further in some later posts :)"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "First Post",
    "section": "",
    "text": "Hi, this is my first blog post. I’m making this using Quarto, starting off by slavishly following the tutorial, then incrementally adapting it to suit my preferences.\n\nI’m even keeping the default image of the first blog post. It’s not dissimilar to what I’m actually looking at!"
  },
  {
    "objectID": "posts/lms-are-glms-part-02/index.html",
    "href": "posts/lms-are-glms-part-02/index.html",
    "title": "Linear Models are General Linear Models",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/lms-are-glms-part-02/index.html#tldr",
    "href": "posts/lms-are-glms-part-02/index.html#tldr",
    "title": "Linear Models are General Linear Models",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/lms-are-glms-part-02/index.html#part-2-systematic-components-and-link-functions",
    "href": "posts/lms-are-glms-part-02/index.html#part-2-systematic-components-and-link-functions",
    "title": "Linear Models are General Linear Models",
    "section": "Part 2: Systematic components and link functions",
    "text": "Part 2: Systematic components and link functions\nIn part 1 of this series we introduced the following general framework for thinking about statistical models and what they contain.\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\] The terminology are as described previously.\nThese equations are too broad and abstract to be implemented directly. Instead, specific choices about the \\(f(.)\\) and \\(g(.)\\) need to be made. King, Tomz, and Wittenberg (2000) gives the following examples:\nLogistic Regression\n\\[\nY_i \\sim Bernoulli(\\pi_i)\n\\]\n\\[\n\\pi_i = \\frac{1}{1 + e^{-X_i\\beta}}\n\\]\nLinear Regression\n\\[\nY_i \\sim N(\\mu_i, \\sigma^2)\n\\] \\[\n\\mu_i = X_i\\beta\n\\]\nSo, what’s so special about linear regression, in this framework?\nIn one sense, not so much. It’s got a systematic component, and it’s got a stochastic component. But so do other models. But in another sense, quite a lot. It’s a rare case where the systematic component, \\(g(.)\\), doesn’t transform its inputs in some weird and wonderful way. We can say that \\(g(.)\\) is the identity transform, \\(I(.)\\), which in words means take what you’re given, do nothing to it, and pass it on.\nBy contrast, the systematic component for logistic regression .."
  },
  {
    "objectID": "posts/talk-at-edinbr/index.html",
    "href": "posts/talk-at-edinbr/index.html",
    "title": "EdinbR talk on modelling economic (in)activity transitions",
    "section": "",
    "text": "Yesterday I had the great privilege of being one of two speakers at the Edinburgh R Users group, called EdinbR. (Difficult to say without sounding like a pirate.)\nI spoke through some of the modelling and conceptual challenges involved in trying to model the effect that various drivers/factors/exposures have on how many people in the UK become economically inactive, especially economically inactive for reasons of long-term sickness.\nThe talk seemed to go well (though perhaps the speaker’s always the last person qualified to judge), even though some of the algebra didn’t render correctly. (Which unfortunately means I also used algebra.)\nLike this blog, the presentation also made use of Quarto, but in the presentation’s case using reveal.js.\nThe presentation is available, for those intrepid souls interested in seeing something with R code and algebra, here."
  },
  {
    "objectID": "posts/unattended-deaths/index.html",
    "href": "posts/unattended-deaths/index.html",
    "title": "A Deathly Silence",
    "section": "",
    "text": "Trends in R98/R99 deaths since 1990\n\n\nWhat does it mean when someone dies, and no one notices for days, weeks, or months on end?\nThe bodies, once found, will be decomposed to such an extent that no effective autopsy can be performed, and so no cause of death can be identified. Such deaths are then likely to be coded either as R98 (‘Unattended death’) or R99 (‘Other ill-defined and unknown causes of mortality’). Far from being ‘junk codes’, wouldn’t a sudden and sustained change in deaths coded this way (absent an obvious explanation, such as a change in coding practice) signal that something broader is afoot?\nWorking with Lu Hiam, an Oxford PhD student and former GP, and Theodore Estrin-Serlui, a histopathologist, I analysed trends in deaths with these codes, as compared with mortality trends overall in England & Wales.\nSuch codes are rarely used, but in England & Wales they sadly became many times more common over the 1990s and 2000s. Standardised mortality rates in the R98/R99 category became more than three and a half times a common between 1990 and 2010, even as general standardised mortality rates fell by around a third.\nFor every body found so decomposed that the R98/R99 category had to be used, there are usually many more that have been unattended for a few days, have started to decompose, but for which autopsy can still be successfully performed. If these deaths are the tip of the iceberg, the base of this iceberg may be a growing epidemic of loneliness and social isolation, of ever more people with connections to friends and family, with no one to turn to in times of crisis.\nOur paper, A Deathly Silence, has been published in the Journal of the Royal Society of Medicine, and received press coverage from a number of outlets."
  },
  {
    "objectID": "posts/r-code/index.html",
    "href": "posts/r-code/index.html",
    "title": "Post with code",
    "section": "",
    "text": "This short post is intended to confirm that I can run and render R code within a Quarto blog post.\n\nVery simple example\nLet’s start off with some very simple base-R\n\n1 + 1\n\n[1] 2\n\n\nAnd of course let’s not forget the obligatory\n\nstatement &lt;- \"Hello World\"\n\nstatement\n\n[1] \"Hello World\"\n\n\n\n\nGraphs\nLet’s now look at a base-R graphic, again using a cliched example\n\nplot(mtcars$mpg ~ mtcars$wt)\n\n\n\n\n\n\nSome extensions\nLet’s now continue to be cliched, and load and use the tidyverse\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\nmtcars |&gt; \n  group_by(carb) |&gt; \n  summarise(\n    mean_mpg = mean(mpg)\n  ) |&gt; \n  ungroup()\n\n# A tibble: 6 × 2\n   carb mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     1     25.3\n2     2     22.4\n3     3     16.3\n4     4     15.8\n5     6     19.7\n6     8     15  \n\n\nAnd to visualise\n\nmtcars |&gt; \n  mutate(cyl = factor(cyl)) |&gt; \n  ggplot(aes(x = wt, y = mpg, colour = cyl, group= cyl)) + \n  geom_point(aes(shape = cyl)) + \n  stat_smooth(se = FALSE, method = \"lm\") + \n  labs(\n    x = \"Weight\", \n    y = \"Miles per gallon\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nConclusion\nSo far, so good…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jon Minton’s Blog",
    "section": "",
    "text": "Linear Models are General Linear Models\n\n\nPart Two: Systematic components and link functions\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPost with code\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nLinear Models are General Linear Models\n\n\nPart One: Model fitting as parameter calibration\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nEdinbR talk on modelling economic (in)activity transitions\n\n\n\n\n\n\n\nR\n\n\nmodelling\n\n\ntalks\n\n\neconomics\n\n\nhealth\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nA Deathly Silence\n\n\n\n\n\n\n\nMortality\n\n\nEpidemiology\n\n\nPapers\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nFirst Post\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\nNo matching items"
  }
]