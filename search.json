[
  {
    "objectID": "glms.html",
    "href": "glms.html",
    "title": "Statistical Modelling: Theory and Practice",
    "section": "",
    "text": "This page lists all posts I’ve written on statistical modelling and inference, originally framed around ‘generalised linear models’ (glm in R) but quite a bit broader than that. I consider the production of this material something of a public service. More on the background to the series, which includes my own background, is available in this post here.\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nNov 28, 2023\n\n\nPart One: Model fitting as parameter calibration\n\n\n3 min\n\n\n\n\nDec 1, 2023\n\n\nPart Two: Systematic components and link functions\n\n\n3 min\n\n\n\n\nDec 2, 2023\n\n\nPart Three: glm is just fancy lm\n\n\n4 min\n\n\n\n\nDec 30, 2023\n\n\nPart Four: why only betas just look at betas\n\n\n18 min\n\n\n\n\nJan 2, 2024\n\n\nPart Five: Traversing the Likelihood Landscape\n\n\n9 min\n\n\n\n\nJan 3, 2024\n\n\nPart Six: The Robo-Chauffeur\n\n\n7 min\n\n\n\n\nJan 4, 2024\n\n\nPart Seven: Feeling Uncertain\n\n\n10 min\n\n\n\n\nJan 16, 2024\n\n\nPart Eight: Guessing what a landscape looks like by feeling the curves beneath our feet\n\n\n10 min\n\n\n\n\nJan 20, 2024\n\n\nPart Nine: Answering questions with honest uncertainty: Expected values and Predicted values\n\n\n10 min\n\n\n\n\nJan 27, 2024\n\n\nPart Ten: Log Likelihood estimation for Logistic Regression\n\n\n8 min\n\n\n\n\nFeb 3, 2024\n\n\nPart Eleven: Honest Predictions the easier way\n\n\n14 min\n\n\n\n\nFeb 4, 2024\n\n\nPart Twelve: Honest Predictions the slightly-less easier way\n\n\n13 min\n\n\n\n\nFeb 10, 2024\n\n\nPart Thirteen: On Marbles and Jumping Beans\n\n\n21 min\n\n\n\n\nFeb 19, 2024\n\n\nPart Fourteen: A non-technical but challenging introduction to causal inference…\n\n\n5 min\n\n\n\n\nFeb 22, 2024\n\n\nPart Fifteen: Causal Inference: The platinum and gold standards\n\n\n10 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jon Minton’s Blog",
    "section": "",
    "text": "Why can’t we just get on with making and fixing stuff?\n\n\nSome thoughts on Andy Weir’s Eng-Fi and neurodiversity\n\n\n\n\nstories\n\n\nfiction\n\n\nengineering\n\n\nutopia\n\n\nneurodiversity\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nEdinbr Pair Programming\n\n\n\n\n\n\n\nEdinbr\n\n\nR\n\n\npair programming\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nJim Gardner, Jon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Fifteen: Causal Inference: The platinum and gold standards\n\n\n\n\n\n\n\nstatistics\n\n\ncausality\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: 20 Feb 2024 - R Grants\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nfunding\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nKennedy Owusu-Afriyie, Antony Clark, Brendan Clarke, Jon Minton, Nick Christofides, Steph Curtis, Gats Osorio, Andrew Saul, Myrian Scansetti\n\n\n\n\n\n\n  \n\n\n\n\nPart Fourteen: A non-technical but challenging introduction to causal inference…\n\n\n…and the heroism or villainy of Henry Dundas\n\n\n\n\nhistory\n\n\ncausality\n\n\nEdinburgh\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nOn the background to my statistical inference series\n\n\n\n\n\n\n\nstatistics\n\n\ntraining\n\n\neducation\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nJon Minton\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday on Valentine's Day\n\n\n\nR\ntidy tuesday\nValentine's Day\n\n\n\n\n\n\n\n`Feb 14, 2024`{=html}\nBrendan Clarke, Jon Minton, Gatz Osorio, Kennedy Owuso-Afrije\n\n\n\n\n  \n\n\n\n\nPart Thirteen: On Marbles and Jumping Beans\n\n\n…and why Bayesians have superior posteriors\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Twelve: Honest Predictions the slightly-less easier way\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Eleven: Honest Predictions the easier way\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTardy Tuesdays: My Second Series\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday 30 Jan 2024: Groundhogs\n\n\nEven tardier than usual…\n\n\n\n\nR\n\n\nTidy Tuesday\n\n\nNorth America\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\nBrendan Clarke, Jon Minton, Kennedy Owusu-Afriyie, Katie Pyper, Andrew Saul\n\n\n\n\n\n\n  \n\n\n\n\nOn Sweary Soap Operas: A Concealed Television Genre\n\n\n\n\n\n\n\ngenres\n\n\ntelevision\n\n\nstories\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Ten: Log Likelihood estimation for Logistic Regression\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidytuesday 2024-01-23\n\n\nEducational attainment and town size\n\n\n\n\nR\n\n\nEducation\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nBrendan Clarke, Andrew Saul, Nick Christofides, Kennedy Owusu-Afriyie, Jon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Other Left-Right Divide: Iain McGilchrist and the Battle of the Hemispheres\n\n\n\n\n\n\n\npodcasts\n\n\nstories\n\n\nsci-fi\n\n\nmyth\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nI got permanently banned from a politics forum for mentioning how circles work\n\n\n\n\n\n\n\npolitics\n\n\ngeometry\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Nine: Answering questions with honest uncertainty: Expected values and Predicted values\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Eight: Guessing what a landscape looks like by feeling the curves beneath our feet\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nDavid Sederis: Humourists as Unrepentent Observational Confessionals\n\n\n\n\n\n\n\nbooks\n\n\nmindfulness\n\n\ncomedy\n\n\nhumour\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Seven: Feeling Uncertain\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Six: The Robo-Chauffeur\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Five: Traversing the Likelihood Landscape\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nGLMs: My first series\n\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Four: why only betas just look at betas\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Christmas films\n\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2023\n\n\nTom Fowler, Nick Christofides, Andrew Saul, Jon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Eerie Familiarity of Frasier (2023)\n\n\n…And why it’s inverted Dr Who\n\n\n\n\nstories\n\n\nsitcoms\n\n\narchetypes\n\n\nmathematics\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nChanging tenure in Scotland\n\n\n\n\n\n\n\nhousing\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nNerdy Dialogues on Life and Death\n\n\nPart 1: Introduction; Life Expectancy\n\n\n\n\ndemography\n\n\npopulation health\n\n\nmethodology\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday on Life Expectancy - Part Two\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nLife Expectancy\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\nJon Minton, Andrew Saul, Nick Christofides, James McMahon, Kennedy Owusu-Afriyie, Sandra Nwobi\n\n\n\n\n\n\n  \n\n\n\n\nMy Economic Inactivity Modelling Package: Informative Readme File!\n\n\n\n\n\n\n\nR\n\n\nEconomic Inactivity\n\n\nNews\n\n\nPackages\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nOptimised for Twitter?\n\n\n\n\n\n\n\ntwitter\n\n\nX\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nWhy such pushback against 20 minute neighbourhoods?\n\n\n\n\n\n\n\nresearch\n\n\nwalkability\n\n\ndriving\n\n\nconspiracy theories\n\n\nhuman development\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday on Life Expectancy\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nLife Expectancy\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nNick Christofides, Jon Minton, Sandra Nwobi\n\n\n\n\n\n\n  \n\n\n\n\nBeavis and Butt-Head: When a physics graduate dons Dumbface\n\n\n\n\n\n\n\ncartoons\n\n\nstories\n\n\neugenics\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Effective Savings on Interest-free Credit\n\n\n\n\n\n\n\nstatistics\n\n\nfinances\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nHow to wrap presents\n\n\n\n\n\n\n\nChristmas\n\n\nBirthdays\n\n\nArts & Crafts\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nNew blog feature: comments\n\n\n\n\n\n\n\nquarto\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Three: glm is just fancy lm\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Two: Systematic components and link functions\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nScientific Illustrations: Annotating the unit circle\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\ngraphics\n\n\neconomic inactivity\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nInteractive Sliders with Crosstalk and Plotly\n\n\n\n\n\n\n\nR\n\n\nplotly\n\n\ncrosstalk\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nRobocop (1987) is wonderfully childish\n\n\n\n\n\n\n\nstories\n\n\nfilms\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday on Dr Who\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nDr Who\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJon Minton, Nick Christofides\n\n\n\n\n\n\n  \n\n\n\n\nPart One: Model fitting as parameter calibration\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPost with code\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nEdinbR talk on modelling economic (in)activity transitions\n\n\n\n\n\n\n\nR\n\n\nmodelling\n\n\ntalks\n\n\neconomics\n\n\nhealth\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nA Deathly Silence\n\n\n\n\n\n\n\nMortality\n\n\nEpidemiology\n\n\nPapers\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nFirst Post\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-christmas-films/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-christmas-films/index.html",
    "title": "Tidy Tuesday: Christmas films",
    "section": "",
    "text": "A shorter and even tardier Tidy Tuesday this week, given we gave ourselves only half an hour rather than the usual hour to look over the most recent dataset.\nThe dataset was about Christmas films.\nOur first question: is Die Hard a Christmas film?\nNot according to the methods used to produce the dataset. If a film doesn’t have Christmas or equivalent in its title, it’s not coming in!\n\nloading\n\n\ntt &lt;- tidytuesdayR::tt_load('2023-12-12')\n\nOnly 7 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\nOnly 7 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\nOnly 7 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\nOnly 7 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\nOnly 7 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\nOnly 6 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\n--- Compiling #TidyTuesday Information for 2023-12-12 ----\n\n\nOnly 5 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\n--- There are 2 files available ---\n\n\nOnly 4 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\n--- Starting Download ---\n\n\nOnly 4 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\n    Downloading file 1 of 2: `holiday_movies.csv`\n\n\nOnly 3 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\n    Downloading file 2 of 2: `holiday_movie_genres.csv`\n\n\nOnly 2 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\n--- Download complete ---\n\ndf1 &lt;- tt[[1]]\ndf2 &lt;- tt[[2]]\n\n\n\ncount of films by year\n\n\ndf1 %&gt;%\n  count(year, sort = TRUE)\n\n# A tibble: 91 × 2\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  2021   183\n 2  2022   173\n 3  2020   172\n 4  2019   143\n 5  2018   129\n 6  2023   107\n 7  2017   102\n 8  2015    76\n 9  2016    75\n10  2012    68\n# ℹ 81 more rows\n\n\n\nhow many films by year -plot with log on y axis\n\n\ndf1 %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n))+\n  geom_point()+\n  #stat_smooth()+\n  scale_y_log10()\n\n\n\n\n\nhow many films by year -plot with log on y axis\nfilter by 1960 onwards\n\n\ndf1 %&gt;%\n  filter(year &gt;= 1960) %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n))+\n  geom_point()+\n  #stat_smooth()+\n  scale_y_log10()\n\n\n\n\n\nhow many films by year -plot with log on y axis\nfilter by 1960 onwards\n\n\ndf1 %&gt;%\n  filter(year &gt;= 1960) %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n))+\n  geom_point()+\n  stat_smooth(method = \"lm\")+\n  scale_y_log10()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nquestions\n\nhow are they published? [cinema / streaming?]\nis it on imdb?\nfull inclusion of 2023?\nare more recent films rubbish?\n\n\ndf1 %&gt;%\n  \n  group_by(year) %&gt;%\n  summarise(avg_rating = mean(average_rating)) %&gt;%\n  ggplot(aes(x = year, y = avg_rating))+\n  geom_point()\n\n\n\n\n\nnumber of films vs avg rating\nfewer films may drive extreme values\nnumber of films vs avg rating\n\n\ndf1 %&gt;%\n  \n  group_by(year) %&gt;%\n  summarise(\n    avg_rating = mean(average_rating), \n    n_films = n() ) %&gt;%\n  ggplot(aes(x = n_films, y = avg_rating))+\n  geom_point()"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html",
    "title": "Tidy Tuesday: 20 Feb 2024 - R Grants",
    "section": "",
    "text": "This TidyTuesday session investigated the funding of intrastructure steering committee grants from the R consortium over time, and was led by Kennedy Owuso-Afriyie."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#introduction",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#introduction",
    "title": "Tidy Tuesday: 20 Feb 2024 - R Grants",
    "section": "",
    "text": "This TidyTuesday session investigated the funding of intrastructure steering committee grants from the R consortium over time, and was led by Kennedy Owuso-Afriyie."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#data-loading",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#data-loading",
    "title": "Tidy Tuesday: 20 Feb 2024 - R Grants",
    "section": "Data loading",
    "text": "Data loading\nWe looked at two options for loading the dataset: one using the tidytuesdayR package; the other linking to the url directly.\n\n\nCode\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n \nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n \n \n# tuesdata &lt;- tidytuesdayR::tt_load('2024-02-20')\n# ## OR\n# tuesdata &lt;- tidytuesdayR::tt_load(2024, week = 8)\n \n# isc_grants &lt;- tuesdata$isc_grants\n \n# Option 2: Read directly from GitHub\n \nisc_grants &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-20/isc_grants.csv')\n \nisc_grants\n\n\n# A tibble: 85 × 7\n    year group title                          funded proposed_by summary website\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;  \n 1  2023     1 The future of DBI (extension …  10000 \"Kirill Mü… \"This … &lt;NA&gt;   \n 2  2023     1 Secure TLS Communications for…  10000 \"Charlie G… \"The p… &lt;NA&gt;   \n 3  2023     1 volcalc: Calculate predicted …  12265 \"Kristina … \"This … &lt;NA&gt;   \n 4  2023     1 autotest: Automated testing o…   3000 \"Mark Padg… \"The p… &lt;NA&gt;   \n 5  2023     1 api2r: An R Package for Auto-…  15750 \"Jon Harmo… \"This … &lt;NA&gt;   \n 6  2022     2 D3po: R Package for Easy Inte…   8000 \"Mauricio … \"The D… &lt;NA&gt;   \n 7  2022     2 Tooling and Guidance for Tran…   8000 \"Maëlle Sa… \"Tooli… &lt;NA&gt;   \n 8  2022     2 Online Submission and Review …  22000 \"Simon Urb… \"The O… &lt;NA&gt;   \n 9  2022     2 Upgrading SatRdays Website Te…   6000 \"Ben Ubah\"  \"The U… &lt;NA&gt;   \n10  2022     2 Building the “Spatial Data Sc…  25000 \"Orhun Ayd… \"The B… &lt;NA&gt;   \n# ℹ 75 more rows\n\n\nSome questions we initially thought about asking:\n\nAre there any keywords that stand out in the titles or summaries of awarded grants?\nHave the funded amounts changed over time?\n\nAs a fairly new user to R, Kennedy focused on the second question, creating a bar plot of funding over time using ggplot. Meanwhile, Clarke and Clark investigated and proposed some approaches for addressing the first question."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#graph-of-funding-over-time",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#graph-of-funding-over-time",
    "title": "Tidy Tuesday: 20 Feb 2024 - R Grants",
    "section": "Graph of funding over time",
    "text": "Graph of funding over time\n\n\nCode\nfunding_by_year &lt;- isc_grants %&gt;% \n  group_by(year) %&gt;% \n  summarise(total_funded = sum(funded)) %&gt;% \n  ungroup()\n \nfunding_by_year %&gt;% \n  ggplot(aes(x=year, y=total_funded)) + \n  geom_col() + \n  labs(\n    x = \"Year\", \n    y = \"total funded in dollars\",\n    title = \"Total funding by year\",\n    caption = \"source: TidyTuesday\",\n    subtitle = \"2018 is a bit weird\" \n  )\n\n\n\n\n\nWe discussed piping with the %&gt;% operator, and the value this has for being able to develop code step-by-step in a way similar to human languages.\n\nWe said, when we see &lt;- or -&gt;, this should be read as ‘is assigned to’.\nAnd we said, when we see the %&gt;% (or |&gt;) operator in a script, this should be read as, and then.\nWe noted how R can tell when it encounters an incomplete expression, and so doesn’t evaluate, just as when someone hears a sentence that ends ‘and then’, they know it’s not really the end of the sentence.\n\nWe also discussed how when making a graph, we should consider how objective or how subjective we should be when presenting the image to the viewer. This will depend on the audience. In our example, the x axis, y axis, title and caption labels are all just objective information. However the subtitle is more subjective, and so more our opinion rather than something no one could reasonably disagree with."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#tidy-text-to-get-important-key-words",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#tidy-text-to-get-important-key-words",
    "title": "Tidy Tuesday: 20 Feb 2024 - R Grants",
    "section": "Tidy Text to get important key words",
    "text": "Tidy Text to get important key words\nBrendan offered the following code chunk to explore the content of the free text summary field in the dataset:\n\n\nCode\n#install.packages(\"tidytext\")\n#install.packages(\"SnowballC\")\nlibrary(tidytext)\nlibrary(SnowballC) # for wordStem\n \nisc_grants |&gt;\n  unnest_tokens(word, summary) |&gt;\n  anti_join(get_stopwords()) |&gt;\n  mutate(stem = wordStem(word))\n\n\n# A tibble: 6,242 × 8\n    year group title                      funded proposed_by website word  stem \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n 1  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    prop… prop…\n 2  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    most… most…\n 3  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    focu… focus\n 4  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    main… main…\n 5  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    supp… supp…\n 6  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    dbi   dbi  \n 7  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    dbit… dbit…\n 8  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    test  test \n 9  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    suite suit \n10  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    three three\n# ℹ 6,232 more rows\n\n\nThis pulled out words (other than stopwords1) from the summary field, and identified the stem of these words. This potentially means the number of unique stems can be compared, rather than the number of unique words.\nAntony suggested that, as the summaries are all about supporting a technical programing language, some additional words are also so common they should also be considered stopwords. He also produced a wordcloud visualisation showing the most common non-stopwords in the corpus of summary text”\n\n\nCode\n# tidytext with SnowballC ----\n\n# Tokenize the text\n\n\nmy_stop_words &lt;- \n  bind_rows(\n    get_stopwords(),\n    tibble(\n      word = c(\"r\",\"package\",\"data\",\"users\",\"project\",\"cran\",\"community\",\"use\",\n               \"development\",\"documentation\",\"can\",\"also\",\"system\",\"new\",\"code\",\n               \"available\",\"existing\",\"support\",\"make\",\"two\",\"build\"),\n      lexicon = \"tony's custom stop words\"\n    )\n  )\n\nmy_stop_words &lt;- \n  my_stop_words %&gt;% \n  mutate(stem = wordStem(word))\n\ntokens &lt;- \n  isc_grants %&gt;%\n  unnest_tokens(word, summary) %&gt;% \n  mutate(stem = wordStem(word)) %&gt;% \n  anti_join(my_stop_words, by = \"stem\")\n\n\ntoken_frequency &lt;- tokens %&gt;% count(word) %&gt;% arrange(-n)\n\n\n\n# View the processed stems\nwordcloud::wordcloud(words = token_frequency$word, \n          freq = token_frequency$n, min.freq = 1,\n          max.words = 20, random.order = FALSE, rot.per = 0.35, \n          colors = RColorBrewer::brewer.pal(8, \"Dark2\"))"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#footnotes",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#footnotes",
    "title": "Tidy Tuesday: 20 Feb 2024 - R Grants",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStop words are terms that are so common within sentences they don’t really add much unique information. They’re words like ‘and’, ‘the’, ‘an’, and so on.↩︎"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-dr-who/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-dr-who/index.html",
    "title": "Tidy Tuesday on Dr Who",
    "section": "",
    "text": "First we load the packages\nThe tidyverse equivalent of pacman is now pak.\nThe latest dataset is here, and the specific files to work.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndta_list &lt;- tidytuesdayR::tt_load(x = \"2023-11-28\")\n\n--- Compiling #TidyTuesday Information for 2023-11-28 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `drwho_episodes.csv`\n    Downloading file 2 of 3: `drwho_directors.csv`\n    Downloading file 3 of 3: `drwho_writers.csv`\n\n\n--- Download complete ---\n\ndta_eps &lt;- dta_list[[\"drwho_episodes\"]]\ndta_wrt &lt;- dta_list[[\"drwho_writers\"]]\n\nLet’s see how the viewship changed over time\n\ndta_eps_season &lt;- \n  dta_eps |&gt; \n  group_by(season_number) |&gt; \n  mutate(\n    mean_viewers = mean(uk_viewers),\n    mean_date = mean(first_aired)\n    ) |&gt; \n  ungroup()\n\ndta_eps_season |&gt; \n  ggplot(aes(x = first_aired, y = uk_viewers)) + \n  geom_point(colour = \"grey\") +\n  geom_point(aes(x = mean_date, y = mean_viewers), size = 2.5) + \n  scale_x_date(breaks = \"2 years\", labels = \\(x) format(x, \"%Y\")) +\n  labs(\n    x = \"First aired\",\n    y = \"UK Viewers (millions)\",\n    title = \"Viewers over time for Dr Who\",\n    subtitle = \"People don't watch TV like they used to...\"\n  ) +\n  annotate(\"text\", x = lubridate::make_date(2015), y = 10, label = \"What happened here?!\") +\n  annotate(\"text\", x = lubridate::make_date(2014), y= 8, label = \"Smartphone strangling the TV from now\", hjust = 0) + \n  stat_smooth(colour = \"blue\", se = FALSE) \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nLet’s now look at writers by season\n\ndta_eps_wrt &lt;- \n  dta_eps |&gt; \n    left_join(dta_wrt, by = \"story_number\") \n\nHow many episodes by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  summarise(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  arrange(desc(n_written))\n\n# A tibble: 40 × 2\n   writer           n_written\n   &lt;chr&gt;                &lt;int&gt;\n 1 Steven Moffat           45\n 2 Russell T Davies        31\n 3 Chris Chibnall          29\n 4 Mark Gatiss              9\n 5 Toby Whithouse           7\n 6 Gareth Roberts           5\n 7 Helen Raynor             4\n 8 Jamie Mathieson          4\n 9 Peter Harness            4\n10 Matthew Graham           3\n# ℹ 30 more rows\n\n\nSo Moffat wrote most episodes, then Davies, then Chibnall\nAnd what about popularity by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  mutate(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  filter(n_written &gt;= 5) |&gt; \n  ggplot(aes(x = fct_reorder(writer, rating), y= rating)) + \n  geom_boxplot() + \n  coord_flip() + \n  labs(\n    x = \"Distribution of ratings\",\n    y = \"Writer\", \n    title = \"Rating distribution by writer\",\n    subtitle = \"Writers who wrote at least five episodes\"\n  )\n\n\n\n\nWhen were the different writers active?\n\nmajor_writers_active &lt;- \n  dta_eps_wrt |&gt; \n    group_by(writer) |&gt;  \n    mutate(\n      n_written = n()\n    ) |&gt; \n    ungroup() |&gt; \n    filter(n_written &gt;= 5) |&gt; \n    group_by(writer) |&gt; \n    summarise(\n      started_writing = min(first_aired),\n      finished_writing = max(first_aired),\n      n_written = n_written[1]\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(\n      yr_start = year(started_writing),\n      yr_end = year(finished_writing)\n    )\n\nmajor_writers_active |&gt; \n  arrange(started_writing)\n\n# A tibble: 6 × 6\n  writer           started_writing finished_writing n_written yr_start yr_end\n  &lt;chr&gt;            &lt;date&gt;          &lt;date&gt;               &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Russell T Davies 2005-03-26      2010-01-01              31     2005   2010\n2 Mark Gatiss      2005-04-09      2017-06-10               9     2005   2017\n3 Steven Moffat    2005-05-21      2017-12-25              45     2005   2017\n4 Toby Whithouse   2006-04-29      2017-06-03               7     2006   2017\n5 Gareth Roberts   2007-04-07      2011-09-24               5     2007   2011\n6 Chris Chibnall   2007-05-19      2022-10-23              29     2007   2022\n\n\nHere we see the tenure of different major writers. Russell T Davies and Steven Moffatt are the major players."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-dr-who/index.html#tidy-tuesday-challenge",
    "href": "posts/tardy-tuesday/tidy-tuesday-dr-who/index.html#tidy-tuesday-challenge",
    "title": "Tidy Tuesday on Dr Who",
    "section": "",
    "text": "First we load the packages\nThe tidyverse equivalent of pacman is now pak.\nThe latest dataset is here, and the specific files to work.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndta_list &lt;- tidytuesdayR::tt_load(x = \"2023-11-28\")\n\n--- Compiling #TidyTuesday Information for 2023-11-28 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `drwho_episodes.csv`\n    Downloading file 2 of 3: `drwho_directors.csv`\n    Downloading file 3 of 3: `drwho_writers.csv`\n\n\n--- Download complete ---\n\ndta_eps &lt;- dta_list[[\"drwho_episodes\"]]\ndta_wrt &lt;- dta_list[[\"drwho_writers\"]]\n\nLet’s see how the viewship changed over time\n\ndta_eps_season &lt;- \n  dta_eps |&gt; \n  group_by(season_number) |&gt; \n  mutate(\n    mean_viewers = mean(uk_viewers),\n    mean_date = mean(first_aired)\n    ) |&gt; \n  ungroup()\n\ndta_eps_season |&gt; \n  ggplot(aes(x = first_aired, y = uk_viewers)) + \n  geom_point(colour = \"grey\") +\n  geom_point(aes(x = mean_date, y = mean_viewers), size = 2.5) + \n  scale_x_date(breaks = \"2 years\", labels = \\(x) format(x, \"%Y\")) +\n  labs(\n    x = \"First aired\",\n    y = \"UK Viewers (millions)\",\n    title = \"Viewers over time for Dr Who\",\n    subtitle = \"People don't watch TV like they used to...\"\n  ) +\n  annotate(\"text\", x = lubridate::make_date(2015), y = 10, label = \"What happened here?!\") +\n  annotate(\"text\", x = lubridate::make_date(2014), y= 8, label = \"Smartphone strangling the TV from now\", hjust = 0) + \n  stat_smooth(colour = \"blue\", se = FALSE) \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nLet’s now look at writers by season\n\ndta_eps_wrt &lt;- \n  dta_eps |&gt; \n    left_join(dta_wrt, by = \"story_number\") \n\nHow many episodes by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  summarise(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  arrange(desc(n_written))\n\n# A tibble: 40 × 2\n   writer           n_written\n   &lt;chr&gt;                &lt;int&gt;\n 1 Steven Moffat           45\n 2 Russell T Davies        31\n 3 Chris Chibnall          29\n 4 Mark Gatiss              9\n 5 Toby Whithouse           7\n 6 Gareth Roberts           5\n 7 Helen Raynor             4\n 8 Jamie Mathieson          4\n 9 Peter Harness            4\n10 Matthew Graham           3\n# ℹ 30 more rows\n\n\nSo Moffat wrote most episodes, then Davies, then Chibnall\nAnd what about popularity by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  mutate(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  filter(n_written &gt;= 5) |&gt; \n  ggplot(aes(x = fct_reorder(writer, rating), y= rating)) + \n  geom_boxplot() + \n  coord_flip() + \n  labs(\n    x = \"Distribution of ratings\",\n    y = \"Writer\", \n    title = \"Rating distribution by writer\",\n    subtitle = \"Writers who wrote at least five episodes\"\n  )\n\n\n\n\nWhen were the different writers active?\n\nmajor_writers_active &lt;- \n  dta_eps_wrt |&gt; \n    group_by(writer) |&gt;  \n    mutate(\n      n_written = n()\n    ) |&gt; \n    ungroup() |&gt; \n    filter(n_written &gt;= 5) |&gt; \n    group_by(writer) |&gt; \n    summarise(\n      started_writing = min(first_aired),\n      finished_writing = max(first_aired),\n      n_written = n_written[1]\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(\n      yr_start = year(started_writing),\n      yr_end = year(finished_writing)\n    )\n\nmajor_writers_active |&gt; \n  arrange(started_writing)\n\n# A tibble: 6 × 6\n  writer           started_writing finished_writing n_written yr_start yr_end\n  &lt;chr&gt;            &lt;date&gt;          &lt;date&gt;               &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Russell T Davies 2005-03-26      2010-01-01              31     2005   2010\n2 Mark Gatiss      2005-04-09      2017-06-10               9     2005   2017\n3 Steven Moffat    2005-05-21      2017-12-25              45     2005   2017\n4 Toby Whithouse   2006-04-29      2017-06-03               7     2006   2017\n5 Gareth Roberts   2007-04-07      2011-09-24               5     2007   2011\n6 Chris Chibnall   2007-05-19      2022-10-23              29     2007   2022\n\n\nHere we see the tenure of different major writers. Russell T Davies and Steven Moffatt are the major players."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-dr-who/index.html#coda",
    "href": "posts/tardy-tuesday/tidy-tuesday-dr-who/index.html#coda",
    "title": "Tidy Tuesday on Dr Who",
    "section": "Coda",
    "text": "Coda\nNeither of us know much about Dr Who!\nBut hopefully we now know a bit more!"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html",
    "title": "Tidy Tuesday on Valentine’s Day",
    "section": "",
    "text": "The most recent TidyTuesday dataset is on Valentine’s Day: sales and engagement in the United States\nWe were joined by Gatz Osorio, who has a lot of experience with data science in Python, so much of the discussion at the outset was about package management in R compared with Python. We then looked at some of the trends data in Valentine’s Day sales and spend."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#introduction",
    "href": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#introduction",
    "title": "Tidy Tuesday on Valentine’s Day",
    "section": "",
    "text": "The most recent TidyTuesday dataset is on Valentine’s Day: sales and engagement in the United States\nWe were joined by Gatz Osorio, who has a lot of experience with data science in Python, so much of the discussion at the outset was about package management in R compared with Python. We then looked at some of the trends data in Valentine’s Day sales and spend."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#packages-in-r",
    "href": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#packages-in-r",
    "title": "Tidy Tuesday on Valentine’s Day",
    "section": "Packages in R",
    "text": "Packages in R\n\nWe discussed tidyverse, which is a kind of meta-packages, loading a range of specific tidyverse package.\n\nWe said using tidyverse is like going into a shed.\n\nWe then talked about some of the specific packages loaded by the tidyverse package, like dplyr and readr.\n\nWe said each of these individual tidyverse packages is like a toolbox.\n\nWe then talked about the :: (scope) operator in R. This allows us to specify a specific function in a package to use, without loading the entire package.\n\nFor example, dplyr::mutate() accesses the mutate() function in the dplyr package, without loading the entire dplyr package.\nWe said this is like getting out a single tool from a toolbox, without emptying or opening the entire toolbox.\nAnother example where this is useful is where two packages have different functions with the same name, and we need to be clear which one. For example both the MASS package and the dplyr package have a function called select(). If we are using both packages we can use the scope/namespace operator to specify exactly which function we want to use. For example, dplyr::select() if we want to use the dplyr function, and MASS::select() if we want to use the MASS function."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#package-version-management-in-r",
    "href": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#package-version-management-in-r",
    "title": "Tidy Tuesday on Valentine’s Day",
    "section": "Package version management in R",
    "text": "Package version management in R\nWe briefly discussed how R is often less specific than Python and other languages as to exactly which version of a package we want to use. For example if we did some analysis in 2021, and run the script again in 2024, the script may not work as it did previously because some of the packages and functions used may have changed in the meantime.\nWe briefly discussed the renv package for helping to address such issues. renv makes a snapshot of the versions of the packages we used when first running some code, and allows these versions (rather than the latest versions) to be restored when running the script at a later date. We saw that renv has different ways of trying to do this, which involve different tradeoffs between file size and reliability:\n\nlowest file size, most scope for problems: renv takes snapshots of package versions etc. On restore() renv tries to download the package versions used at the time. This should work most of the time, but if a package or package version is no longer available on CRAN or similar this may fail.\n\nThis is like maintaining a detailed recipe of exactly what tools etc used when the script was first run.\n\nmedium file size, less scope for problems: renv uses packrat (a precursor to renv) to save all package versions alongside the project and scripts, rather than just the recipe. This means there could be hundreds of megabytes of package content to support a few kilobytes of script.\n\nThis is like carrying around the lab in which an experiment was conducted in order to be able to repeat the experiment in almost identical conditions to when the experiment was first conducted.\n\nlargest file size, least scope for problems: renv can create a docker image to house the scripts/analysis in. A docker image is a virtual environment/machine, which will be identical on everyone’s computer. This will help avoid issues associated with one user running the script on a PC, another on a linux server, and a third running the script on a Macbook.\n\nThis is like carrying around the building and street in which the lab is based, as well as the lab itself!"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#tidytuesday-challenge-itself",
    "href": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#tidytuesday-challenge-itself",
    "title": "Tidy Tuesday on Valentine’s Day",
    "section": "Tidytuesday challenge itself",
    "text": "Tidytuesday challenge itself\nThere were three files as part of the Tidytuesday dataset, one with a time breakdown, a second with an age breakdown, and a third with a gender breakdown. We only looked at the time breakdown file\nWe loaded the data using the tidytuesdayR package. However in the script below we will just load it directly to avoid the tidy tuesday API denying requests.\n\n\nCode\n# load packages \nlibrary(tidyverse)\nlibrary(RColorBrewer)\n\n# Brendan introduced `pacman`, and the following line of code to ensure pacman is always loaded\n# install.packages(setdiff(\"pacman\", rownames(installed.packages())))\n\n# The following loads the datasets using the tidytuesdayR package:\n# tidytuesdayR::tt_load('2024-02-13') \n# tuesdata &lt;- tidytuesdayR::tt_load('2024-02-13')\n# historical_spending &lt;- tuesdata$historical_spending\n# gifts_age &lt;- tuesdata$gifts_age\n# gifts_gender &lt;- tuesdata$gifts_gender\n\nhistorical_spending &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/historical_spending.csv')\n\n\nWe first looked at whether the percentage of people doing something involving Valentine’s Day had changed over time\n\n\nCode\nhistorical_spending |&gt;\n  glimpse() |&gt;\n  filter(!is.na(PercentCelebrating)) |&gt;\n  glimpse() |&gt;\n  ggplot(aes(x = Year, y = PercentCelebrating)) +\n  geom_line() +\n  geom_point() +\n  geom_smooth() +\n  # ylim(0, NA) + # ylim and expand_limits seem equivalent in this case\n  expand_limits(y = 0) +\n  scale_x_continuous(breaks = 2010:2022)\n\n\nRows: 13\nColumns: 10\n$ Year               &lt;dbl&gt; 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 201…\n$ PercentCelebrating &lt;dbl&gt; 60, 58, 59, 60, 54, 55, 55, 54, 55, 51, 55, 52, 53\n$ PerPerson          &lt;dbl&gt; 103.00, 116.21, 126.03, 130.97, 133.91, 142.31, 146…\n$ Candy              &lt;dbl&gt; 8.60, 10.75, 10.85, 11.64, 10.80, 12.70, 13.11, 12.…\n$ Flowers            &lt;dbl&gt; 12.33, 12.62, 13.49, 13.48, 15.00, 15.72, 14.78, 14…\n$ Jewelry            &lt;dbl&gt; 21.52, 26.18, 29.60, 30.94, 30.58, 36.30, 33.11, 32…\n$ GreetingCards      &lt;dbl&gt; 5.91, 8.09, 6.93, 8.32, 7.97, 7.87, 8.52, 7.36, 6.5…\n$ EveningOut         &lt;dbl&gt; 23.76, 24.86, 25.66, 27.93, 27.48, 27.27, 33.46, 28…\n$ Clothing           &lt;dbl&gt; 10.93, 12.00, 10.42, 11.46, 13.37, 14.72, 15.05, 13…\n$ GiftCards          &lt;dbl&gt; 8.42, 11.21, 8.43, 10.23, 9.00, 11.05, 12.52, 10.23…\nRows: 13\nColumns: 10\n$ Year               &lt;dbl&gt; 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 201…\n$ PercentCelebrating &lt;dbl&gt; 60, 58, 59, 60, 54, 55, 55, 54, 55, 51, 55, 52, 53\n$ PerPerson          &lt;dbl&gt; 103.00, 116.21, 126.03, 130.97, 133.91, 142.31, 146…\n$ Candy              &lt;dbl&gt; 8.60, 10.75, 10.85, 11.64, 10.80, 12.70, 13.11, 12.…\n$ Flowers            &lt;dbl&gt; 12.33, 12.62, 13.49, 13.48, 15.00, 15.72, 14.78, 14…\n$ Jewelry            &lt;dbl&gt; 21.52, 26.18, 29.60, 30.94, 30.58, 36.30, 33.11, 32…\n$ GreetingCards      &lt;dbl&gt; 5.91, 8.09, 6.93, 8.32, 7.97, 7.87, 8.52, 7.36, 6.5…\n$ EveningOut         &lt;dbl&gt; 23.76, 24.86, 25.66, 27.93, 27.48, 27.27, 33.46, 28…\n$ Clothing           &lt;dbl&gt; 10.93, 12.00, 10.42, 11.46, 13.37, 14.72, 15.05, 13…\n$ GiftCards          &lt;dbl&gt; 8.42, 11.21, 8.43, 10.23, 9.00, 11.05, 12.52, 10.23…\n\n\n\n\n\nIt looks like the share has decreased over time, from around 60% to 50%.\nWe next decided to look at how the relative share of spend on different item categories had changed over time.\nWe realised this involved: - Pivoting some of the columns (individual item spend) onto wide format - Seeing whether the individual item spend categories add up to the total spend reported - Creating an additional spend category for other items which are not part of the standard categories listed\nData before:\n\n\nCode\nhistorical_spending\n\n\n# A tibble: 13 × 10\n    Year PercentCelebrating PerPerson Candy Flowers Jewelry GreetingCards\n   &lt;dbl&gt;              &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n 1  2010                 60      103    8.6    12.3    21.5          5.91\n 2  2011                 58      116.  10.8    12.6    26.2          8.09\n 3  2012                 59      126.  10.8    13.5    29.6          6.93\n 4  2013                 60      131.  11.6    13.5    30.9          8.32\n 5  2014                 54      134.  10.8    15      30.6          7.97\n 6  2015                 55      142.  12.7    15.7    36.3          7.87\n 7  2016                 55      147.  13.1    14.8    33.1          8.52\n 8  2017                 54      137.  12.7    14.6    32.3          7.36\n 9  2018                 55      144.  13.1    14.8    34.1          6.55\n10  2019                 51      162.  14.1    15.1    30.3          7.31\n11  2020                 55      196.  17.3    16.5    41.6          9.01\n12  2021                 52      165.  15.3    15.4    30.7          8.48\n13  2022                 53      175.  15.9    16.7    45.8          7.47\n# ℹ 3 more variables: EveningOut &lt;dbl&gt;, Clothing &lt;dbl&gt;, GiftCards &lt;dbl&gt;\n\n\nAfter pivoting and tidying:\n\n\nCode\nhistorical_spending_pivoted &lt;- historical_spending |&gt;\n  pivot_longer(!c(Year, PercentCelebrating, PerPerson)) |&gt;\n  group_by(Year) |&gt;\n  mutate(sum = sum(value), \n         other_spend = PerPerson-sum\n  ) |&gt;\n  select(-sum) |&gt;\n  pivot_wider() |&gt;\n  pivot_longer(!c(Year, PercentCelebrating, PerPerson))\n\nhistorical_spending_pivoted\n\n\n# A tibble: 104 × 5\n# Groups:   Year [13]\n    Year PercentCelebrating PerPerson name          value\n   &lt;dbl&gt;              &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1  2010                 60      103  other_spend   11.5 \n 2  2010                 60      103  Candy          8.6 \n 3  2010                 60      103  Flowers       12.3 \n 4  2010                 60      103  Jewelry       21.5 \n 5  2010                 60      103  GreetingCards  5.91\n 6  2010                 60      103  EveningOut    23.8 \n 7  2010                 60      103  Clothing      10.9 \n 8  2010                 60      103  GiftCards      8.42\n 9  2011                 58      116. other_spend   10.5 \n10  2011                 58      116. Candy         10.8 \n# ℹ 94 more rows\n\n\nWe then used the tidied and pivoted dataset to produce an area chart with a nicer and more accessible colour scheme for fill colours:\n\n\nCode\nhistorical_spending_pivoted |&gt;\n  ggplot(aes(x = Year, y = value, fill = name)) +\n  geom_area(position = \"fill\") +\n  theme_dark() +\n  scale_x_continuous(breaks = 2010:2022) +\n  scale_fill_brewer(palette = \"Paired\") +\n  scale_y_continuous(labels = scales::percent) +\n  ylab(\"Cumulative percentage of annual total spend\") +\n  ggtitle(\"Valentine's day spending by category\")\n\n\n\n\n\nJon recommended the Paired colour scheme in ColorBrewer. Brendan checked this improved the accessibility of the colours using the contrastchecker website. This showed the ColorBrewer colour scheme was much more accessible than ggplot’s default colours."
  },
  {
    "objectID": "posts/beavis-and-butthead-is-dumbface/index.html",
    "href": "posts/beavis-and-butthead-is-dumbface/index.html",
    "title": "Beavis and Butt-Head: When a physics graduate dons Dumbface",
    "section": "",
    "text": "Mike Judge’s Beavis and Butt-Head\n\n\nA second season of “Mike Judge’s Beavis & Butt-Head” is now available on Paramount+, continuing a series that began on MTV in the early 1990s. I’ve been watching it, generally enjoying it, but feeling a gnawing sense of discomfort while doing so. Here’s why.\nMike Judge is a physics graduate, whose other credits include: Silicon Valley, a sitcom about tech startups; King of the Hill, a surprisingly gentle and sympathetic animated sitcom about a lower middle class social conservative family; and Idiocracy, a feature length science fiction comedy whose premise is that, “People are getting dumber; society’s getting dumber; at this rate someone who’s average now will be considered a genius a few generations from now.”\nIn Idiocracy, the proposed mechanism for the world’s dumbing down is a kind of dysgenic selective breeding. Whereas smarter people, with their careful planning and fantastic career opportunities, equivocate and defer the decision to have children, dumber and more feckless people, who don’t tend to do much thinking or planning, and wouldn’t be giving up on any great opportunities, continue to breed like rabbits, or even viruses. Dumber people have a higher R number, so will outbreed smarter people until almost everyone in society’s dumb. From an evolutionary perspective, dumb is the winning strategy.\nIf this sounds like the kind of plotline a eugenicist might come up with, I think you might be right. The alternative is that Judge is a black pilled cynic, a wannabe eugenicist, who just wishes, like Marxism, it would only work in practice. Watching a Judge film or TV show is being invited to judge, to find others inferior and wanting, and so feel superior. But that short-term feeling of superiority is fleeting; what lingers is the sense of loneliness, of being ‘the only adult in the room’, the hell of other people, when the other people are idiots.\nThe intellectual elitism, and sneering at the dumb, that finds most full expression in Idiocracy, has always been present in Beavis & Butt-Head. To an extent that’s the entire plot. Beavis & Butt-Head’s lack of intellect is extrapolated to such an extent it becomes grotesquely surreal. And they combine this lack of intelligence with a lack of almost any detectable virtues or redeeming qualities, with the possible exception of Beavis’ sense of loyalty to Butt-Head, a loyalty that is often presented as misplaced, enabling the pair’s co-dependence and Butt-Head’s constant physical and emotional abuse of Beavis, his only friend in the world.\nOther targets of Beavis & Butt-Head’s humour are those characters who overestimate the two title characters, treat them with kindness, and try to help them. This includes their hippy teacher, Mr Van Driessen, who is frequently seen to permit delinquency and disruption from the titular pair, apparently to the detriment both of the pair themselves, and the rest of the class. Other recent episodes feature a kindly middle-aged couple, who happily provide the pair with provisions with which their own home will be attacked; someone who heroically rescues them from a sewer (which the pair mistakenly believe is Hell, as in their illiteracy they misread ‘Department of Sanitation’ as ‘De Apartment of Satan’); and their ever forgiving and kindly neighbour (and Hank Hill prototype?) Mr Anderson, whose property the pair damage and steal without apparent repercussion (except of the karmic variety).\nPerhaps the most depressing segments in the recent Beavis and Butt-Head are those titled Old Beavis and Butt-Head. Breaking a forth wall in long-run cartoon series, the premise of these segments is that we might expect that someone who was a teenager in the early 1990s might be middle aged (or ‘old’, from their former teenage self’s perspective) in the 2020s. And so these age-appropriate versions of the characters are presented. By now, the segments suggest, Butt-Head is jobless, obese, and living off disability payments. Beavis is wrinkly and crag-toothed, but has at least managed, after decades of (not) trying, to get a job, working as Butt-Head’s full time (taxdollar funded) carer.\nMike Judge, as well as creating the series, also voices both characters. In doing so, and in the context of his own academic achievements, just how wretchedly they are drawn, and his other outputs, I think he does the vocal equivalent of ‘donning Dumbface’. Beavis and Butt-Head aren’t just incapable and inferior along narrowly intellectual or academic lines, but in every conceivable way. Every thing they say (with the exception of some of their commentary segments), every thing they do, every scrape and escapade they put themselves in, is yet more evidence of their incorrigible worthlessness, and every attempt to help them as coming from a well-intentioned but misplaced belief that they could ever be better than they are. If we can’t get rid of people like them, the show seems to be suggesting, the best we can do is laugh at them mercilessly. (Maybe behind their backs, just to be safe.)\nAs mentioned, I’ve been watching the new series, and against my better nature enjoying it. It’s a guilty pleasure. Hopefully the above goes to illustrate just how guilty."
  },
  {
    "objectID": "posts/tardy-tuesday-series/index.html",
    "href": "posts/tardy-tuesday-series/index.html",
    "title": "Tardy Tuesdays: My Second Series",
    "section": "",
    "text": "There’s now enough posts related to the Tidy Tuesdays sessions I run weekly to, I think, justify putting it in a separate collection. Just check out the ‘Tardy Tuesday’ tab near the top left of the page!"
  },
  {
    "objectID": "posts/background-to-my-stats-series/index.html",
    "href": "posts/background-to-my-stats-series/index.html",
    "title": "On the background to my statistical inference series",
    "section": "",
    "text": "My blog series on statistical inference and modelling has, at the time of writing, 13 parts, and a feature-length reading time. 1 I’ve been strongly motivated to write this because it covers what I consider the essential theory and practice necessary to be a competent user of statistical methods. In this post I’ll go a bit more into my own background, and how I came to pick up this knowledge.\nMy first degree was in the applied physical sciences: electronic engineering. From this I learned two things: firstly, not to be afraid of algebra and coding; secondly, that I didn’t want to do electronic engineering as a career. So I moved into the social sciences, and this move took me to the health sciences, demography and epidemiology.\nThe move from the applied physical to the social and health sciences made me realise I’d learned something else from the engineering course: a pair of expectations about methods training. The first expectation was that the methods taught should allow the substantive questions of interest in the field to be addressed. The second expectation was that methods should be taught with sufficient rigour and formalism to ensure students attending the same course, and being sufficiently attentive in that course, leave with a common understanding of what’s been taught and exactly how they are applied.\nI wish I could honestly say otherwise, but in my experience the methods taught in much of the social sciences in the UK fell short of the standards of rigour and application that are just taken as given in an engineering course. Qualitative methods courses tend to trade in abstract nouns and unfalsifiable declarations - how does one really know whether one’s employing a feminist methodology, or a critical realist epistemology, or a post-structuralist framing, when asking people why they’re so sad, or angry, or poor? And most of the quantitative methods training, at least when I first encountered them, took the form of telling people what buttons to click, in which order, after opening up a copy of SPSS. Press this button, then this button, then this button, then look at this number here, and check it’s under 0.05, and look for the number of stars in this row, and so on.\nWhen I started a PhD in the quantitative social sciences I was highly unskilled. I sat in on some general social science methods courses, some econometrics, some first year probability and statistics courses run by the maths departments, but still didn’t feel I knew how to use the methods of quantitative research with the same level of rigour and understanding that I’d been used to in the engineering course. So I kept searching.\nThe training course that finally changed this was Gov 2001, a course that’s been run annually by Harvard university for decades, and seems to have become something of an institution. The course teaches statistical inference from the ground up, from the first principles of likelihood and probability, but also doesn’t scrimp on the practicalities of application. It’s also highly applied, with students evaluated on whether they can, at the end of the course, replicate and improve upon an article that’s already been published. It also emphasises the family resemblances between statistical models, the way almost all specific models are just different versions of an underlying ‘mother model’ (my term) which comprises two linked equations.\nI took the course as a distance student over a decade ago, and still find its contents immensely valuable. The blog post series listed below is largely based on that course, though with my own idiosyncratic spin and emphasis. It’s quite technical in places, but the juice is worth the squeeze. If you follow along you will know and understand more about statistical models and their application than almost any UK graduate in a field other than statistics. 2"
  },
  {
    "objectID": "posts/background-to-my-stats-series/index.html#footnotes",
    "href": "posts/background-to-my-stats-series/index.html#footnotes",
    "title": "On the background to my statistical inference series",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMuch of this reading may be messages generated by R functions, however.↩︎\nA statement based on a great deal of personal experience, sadly. Statistical inference is still generally quite poorly explained, poorly understood, and poorly applied in much of the UK, especially when it comes to model building, comparision, interpretation and use for prediction.↩︎"
  },
  {
    "objectID": "posts/r-code/index.html",
    "href": "posts/r-code/index.html",
    "title": "Post with code",
    "section": "",
    "text": "This short post is intended to confirm that I can run and render R code within a Quarto blog post.\n\nVery simple example\nLet’s start off with some very simple base-R\n\n1 + 1\n\n[1] 2\n\n\nAnd of course let’s not forget the obligatory\n\nstatement &lt;- \"Hello World\"\n\nstatement\n\n[1] \"Hello World\"\n\n\n\n\nGraphs\nLet’s now look at a base-R graphic, again using a cliched example\n\nplot(mtcars$mpg ~ mtcars$wt)\n\n\n\n\n\n\nSome extensions\nLet’s now continue to be cliched, and load and use the tidyverse\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\nmtcars |&gt; \n  group_by(carb) |&gt; \n  summarise(\n    mean_mpg = mean(mpg)\n  ) |&gt; \n  ungroup()\n\n# A tibble: 6 × 2\n   carb mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     1     25.3\n2     2     22.4\n3     3     16.3\n4     4     15.8\n5     6     19.7\n6     8     15  \n\n\nAnd to visualise\n\nmtcars |&gt; \n  mutate(cyl = factor(cyl)) |&gt; \n  ggplot(aes(x = wt, y = mpg, colour = cyl, group= cyl)) + \n  geom_point(aes(shape = cyl)) + \n  stat_smooth(se = FALSE, method = \"lm\") + \n  labs(\n    x = \"Weight\", \n    y = \"Miles per gallon\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nConclusion\nSo far, so good…"
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html",
    "href": "posts/effective-saving-for-interest-free-credit/index.html",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "",
    "text": "I have a Monzo account, and as part of the overall Monzo package make use of Monzo Flex, an interest-free credit service which means the payment made in one month is spread over broadly equal payments over the following three months.\nHowever, I’ve always only bought something if I could afford to pay for it in full.\nThe reason for using Monzo Flex follows from an intuition: Deferring some of the payment for a good obtained in month \\(t=0\\) to months \\(\\{t=1, t=2, t=3\\}\\) should in effect offer some degree of saving on the cost of the good, as a pound in 1-3 months has a slightly lower value than a pound this month. This is because of inflation, and the higher the rate of inflation, the higher the effective interest-free credit discount should become.\nHowever, I’ve never tried to work out what this effective savings rate is expected to be. Let’s try to work that out.\nTo do this, we need to consider the following:\n\nThe relationship between annual inflation rates and monthly inflation rates.\nThe concept of net present value (NPV)."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#introduction",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#introduction",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "",
    "text": "I have a Monzo account, and as part of the overall Monzo package make use of Monzo Flex, an interest-free credit service which means the payment made in one month is spread over broadly equal payments over the following three months.\nHowever, I’ve always only bought something if I could afford to pay for it in full.\nThe reason for using Monzo Flex follows from an intuition: Deferring some of the payment for a good obtained in month \\(t=0\\) to months \\(\\{t=1, t=2, t=3\\}\\) should in effect offer some degree of saving on the cost of the good, as a pound in 1-3 months has a slightly lower value than a pound this month. This is because of inflation, and the higher the rate of inflation, the higher the effective interest-free credit discount should become.\nHowever, I’ve never tried to work out what this effective savings rate is expected to be. Let’s try to work that out.\nTo do this, we need to consider the following:\n\nThe relationship between annual inflation rates and monthly inflation rates.\nThe concept of net present value (NPV)."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#monthly-and-annual-inflation-rates",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#monthly-and-annual-inflation-rates",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "Monthly and annual inflation rates",
    "text": "Monthly and annual inflation rates\nIf prices go up 10% in 12 months, and go up the same % each month, how much do they go up each month?\nAn intuitive but wrong answer is that, as there are 12 months per year, the monthly inflation rate will be one twelfth of the annual inflation rate, which would imply a monthly inflation rate of \\(0.1/12\\) or around 0.83%. So,\n\\[\n(1 + r_m) = \\frac{1}{12}(1 + r_y)\n\\] Or equivalently\n\\[\n(1 + r_y) = 12 (1 + r_m)\n\\] Where \\(r_y\\) is the annual increase and \\(r_m\\) is the monthly increase.\nHowever this assumption, as mentioned, is wrong, because it ignores the way that each month’s increase is applied to the product of all increases that occurred in previous months. For example, for three months with different inflation rates the total increase over the the three months will be\n\\[\n(1 + r_{1,2,3}) = (1 + r_1)(1+r_2)(1+r_3)\n\\] If the monthly inflation rates for each of the three months are the same, \\(r_m\\), then this simplifies slightly to\n\\[\n(1 + r_{1,2,3}) = (1 + r_m)^3\n\\]\nBy extension, as there are twelve months in a year, where the monthly inflation rate is fixed the equation becomes:\n\\[\n(1 + r_y) = (1 + r_m)^{12}\n\\]\nThis, not \\((1 + r_y) = 12 (1 + r_m)\\), is the correct starting point. Solve for \\(r_m\\) …\n\\[\n(1 + r_y)^{\\frac{1}{12}} = 1 + r_m\n\\]\n\\[\nr_m = {(1 + r_y)}^{\\frac{1}{12}} - 1\n\\]\nPlugging in a 10% annual inflation rate, i.e. 0.1 for \\(r_y\\), we therefore get an \\(r_m\\) value of around 0.007974, so around 0.8%."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#net-present-value",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#net-present-value",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "Net Present Value",
    "text": "Net Present Value\nThe idea of Net Present Value (NPV) is to translate costs and benefits that occur at different points in time onto a single timeframe, the present. This makes it easier to compare options that take place over different timeframes.\nIn the Flex example we are comparing two options:\n\n\nPay all now\n\n\nPay interest free over three consecutive monthly installments\n\n\nLet’s say the cost of the good at month \\(t\\) is £150. Graphically, and with no interest and inflation, the two options look as follow:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- tribble(\n  ~option, ~month, ~amount,\n  \"A\", 0, 150,\n  \"A\", 1, 0,\n  \"A\", 2, 0,\n  \"A\", 3, 0,\n  \"B\", 0, 0,\n  \"B\", 1, 50,\n  \"B\", 2, 50,\n  \"B\", 3, 50\n)\n\ndf |&gt; \n  ggplot(aes(month, amount)) + \n  geom_col() + \n  facet_wrap(~ option, nrow = 2)\n\n\n\n\n\nIn the no interest / no inflation scenario, the sums for option A and option B are equal, £150.\nHowever, in scenarios with inflation, the value of money keeps decreasing. This means that a commitment to pay £50 month 3 is a commitment to pay less than in month 0. Using the 10% annual inflation rate example, we can estimate the cumulative devaluation by months 1, 2 and 3 by dividing the product of devaluations so far by the monthly inflation rate:\n\n\nCode\nannual_to_monthly &lt;- function(x) {(1 + x)^(1/12) -1}\n\nannual_inflation &lt;- 0.10\nmonthly_inflation &lt;- annual_to_monthly(annual_inflation)\n\nindex0 &lt;- 1\nindex1 &lt;- index0 / (1 + monthly_inflation)\nindex2 &lt;- index1 / (1 + monthly_inflation)\nindex3 &lt;- index2 / (1 + monthly_inflation)\n\ndf &lt;- tibble(\n  month = 0:3, \n  index = c(index0, index1, index2, index3)\n)\n\ndf\n\n\n# A tibble: 4 × 2\n  month index\n  &lt;int&gt; &lt;dbl&gt;\n1     0 1    \n2     1 0.992\n3     2 0.984\n4     3 0.976\n\n\nContinuing the example of a £150 item paid over months 1, 2 and 3, we can therefore convert to NPV by discounting each month’s costs by the index relative to month 0\n\n\nCode\ndf2 &lt;- df |&gt; \n  mutate(\n    amount = c(0, 50, 50, 50)\n  ) |&gt; \n  mutate(npv_amount = amount * index)\n\ndf2\n\n\n# A tibble: 4 × 4\n  month index amount npv_amount\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1     0 1          0        0  \n2     1 0.992     50       49.6\n3     2 0.984     50       49.2\n4     3 0.976     50       48.8\n\n\nThe sum of npv_amount is now less than the £150 in option A, pay upfront. In this example, with 10% inflation, this sum is £147.64, which represents a 1.6% discount on option A.\nLet’s now generalise to other inflation rates\n\n\nCode\ncalc_npv_discount &lt;- function(ry, total = 150) { \n  annual_to_monthly &lt;- function(x) {(1 + x)^(1/12) -1}\n  \n  rm &lt;- annual_to_monthly(ry)\n  index0 &lt;- 1\n  index1 &lt;- index0 / (1 + rm)\n  index2 &lt;- index1 / (1 + rm)\n  index3 &lt;- index2 / (1 + rm)\n\n  npv_amt1 &lt;- (total / 3) * index1\n  npv_amt2 &lt;- (total / 3) * index2\n  npv_amt3 &lt;- (total / 3) * index3\n  \n  \n  1 - sum(npv_amt1, npv_amt2, npv_amt3) / total\n}\n\ndf &lt;- \n  tibble(\n    annual_rate = seq(0, 0.15, by = 0.01)\n  ) |&gt; \n  mutate(\n    effective_discount = map_dbl(annual_rate, calc_npv_discount)\n  )\n\ngg &lt;- \n  df |&gt; \n    ggplot(aes(100 * annual_rate, 100 * effective_discount)) + \n    geom_line() + \n    labs(x = \"Annual inflation rate (%)\", \n         y = \"Effective discount on paying over 3 months (%)\",\n         title = \"Effective short-term discount rate against inflation rate\"\n         ) + \n    scale_y_continuous(breaks = seq(0, 15, by = 0.1)) +\n    annotate(\"segment\", x = 14.8, xend = 14.8, colour = \"lightblue\", y = 0, yend = 100 * calc_npv_discount(0.148)) +\n    annotate(\"segment\", x = 0, xend = 100 * 0.148, colour = \"lightblue\", y = 100 * calc_npv_discount(0.148), yend = 100 * calc_npv_discount(0.148)) +\n    annotate(\"segment\", x = 9.6, xend = 9.6, colour = \"darkblue\", y = 0, yend = 100 * calc_npv_discount(0.096)) +\n    annotate(\"segment\", x = 0, xend = 100 * 0.096, colour = \"darkblue\", y = 100 * calc_npv_discount(0.096), yend = 100 * calc_npv_discount(0.096)) +\n    annotate(\"segment\", x = 5.3, xend = 5.3, colour = \"darkgrey\", y = 0, yend = 100 * calc_npv_discount(0.053)) +\n    annotate(\"segment\", x = 0, xend = 100 * 0.053, colour = \"darkgrey\", y = 100 * calc_npv_discount(0.053), yend = 100 * calc_npv_discount(0.053)) \n    \ngg +\n    annotate(\"text\", \n             x = 2, y = 0.1 + 100 * calc_npv_discount(0.148),\n             label = \"Goods (Highest)\"\n    ) + \n    annotate(\"text\", \n             x = 2, y = 0.1 + 100 * calc_npv_discount(0.096),\n             label = \"CPIH (Highest)\"\n    ) + \n    annotate(\"text\", \n             x = 2, y = 0.1 + 100 * calc_npv_discount(0.053),\n             label = \"Services (Highest)\"\n    )  \n\n\n\n\n\nIn the above I’ve indicated the effective discount rates implied by different annual interest rates reported by the ONS in Figure 7 of this page These range from almost 2.3% for goods, to around 0.86% for services.\nHowever, fortunately, the current inflation rates are somewhat lower, with the most recent reported inflation rates being 2.9% for goods, 6.2% for services, and 2.7% for CPIH.\n\n\nCode\ngg + \n  annotate(\"segment\", x = 2.9, xend = 2.9, colour = \"lightblue\", linetype = \"dashed\", y = 0, yend = 100 * calc_npv_discount(0.029)) +\n  annotate(\"segment\", x = 0, xend = 100 * 0.029, colour = \"lightblue\", linetype = \"dashed\", y = 100 * calc_npv_discount(0.029), yend = 100 * calc_npv_discount(0.029)) +\n  annotate(\"text\", \n           x = 2, y = 0.1 + 100 * calc_npv_discount(0.029),\n           label = \"Goods (Current)\"\n  ) + \n  annotate(\"segment\", x = 4.7, xend = 4.7, colour = \"darkblue\", linetype = \"dashed\", y = 0, yend = 100 * calc_npv_discount(0.047)) +\n  annotate(\"segment\", x = 0, xend = 100 * 0.047, colour = \"darkblue\", linetype = \"dashed\", y = 100 * calc_npv_discount(0.047), yend = 100 * calc_npv_discount(0.047)) +\n  annotate(\"text\", \n           x = 2, y = 0.1 + 100 * calc_npv_discount(0.047),\n           label = \"CPIH (Current)\"\n  ) + \n  annotate(\"segment\", x = 6.2, xend = 6.2, colour = \"darkgrey\", linetype = \"dashed\", y = 0, yend = 100 * calc_npv_discount(0.062)) +\n  annotate(\"segment\", x = 0, xend = 100 * 0.062, colour = \"darkgrey\", linetype=\"dashed\", y = 100 * calc_npv_discount(0.062), yend = 100 * calc_npv_discount(0.062)) +\n  annotate(\"text\", \n           x = 2, y = 0.1 + 100 * calc_npv_discount(0.062),\n           label = \"Services (Current)\"\n  )  \n\n\n\n\n\nSo, the effective discount for deferring has fallen alongside inflation. However it’s still something."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#some-thoughts",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#some-thoughts",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "Some thoughts",
    "text": "Some thoughts\nThe immediate cost of deferring is by contrast the same. It involves clicking a couple of buttons, so a couple of seconds, in the same Monzo app.\nThere are some other consequences too: Using a higher proportion of one’s credit limit tends to lower one’s credit rating. This means the ability to acquire credit on more favourable terms can be adversely affected. Another issue is that, to avoid paying any effective interest rate on the credit, there always needs to be sufficient money in the current account to cover all upcoming payments. This requires either keeping more money in the current account than might be optimal from a savings perspective, or additional daily management of current account balances. As interest rates on savings are over 4% currently, having more money in the current account confers an opportunity cost in lost savings interest which might outweigh any benefits of using short-term interest free credit.\nHowever, for now, as a general principle, realising marginal savings by pressing a couple of buttons doesn’t seem too bad, and at some points of time, and for some items, the savings have been around 2%.\nI might return to looking at other scenarios, including how much might be lost by keeping more money in the current account, in later posts. For now I’ll leave things here, as I simply wanted to get some numbers behind the intuition that, in principle, paying later is better than paying now."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html",
    "href": "posts/glms/lms-are-glms-part-07/index.html",
    "title": "Part Seven: Feeling Uncertain",
    "section": "",
    "text": "In the previous post we managed to use numerical optimisation, with the optim() function, to good \\(\\beta\\) estimates for linear regression model fit to some toy data. In this post, we will explore how the optim() function can be used to produce estimates of uncertainty about these \\(\\beta\\) coefficients, and how these relates to measures of uncertainty presented in the standard lm and glm summary functions."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-07/index.html#aim",
    "title": "Part Seven: Feeling Uncertain",
    "section": "",
    "text": "In the previous post we managed to use numerical optimisation, with the optim() function, to good \\(\\beta\\) estimates for linear regression model fit to some toy data. In this post, we will explore how the optim() function can be used to produce estimates of uncertainty about these \\(\\beta\\) coefficients, and how these relates to measures of uncertainty presented in the standard lm and glm summary functions."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#prereqs",
    "href": "posts/glms/lms-are-glms-part-07/index.html#prereqs",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Prereqs",
    "text": "Prereqs\nAs before, we’ll be using the same toy dataset, and same log likelihood function, as in the last two posts in this series. Let’s create these again:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nLet’s also run and save our parameter estimates produced both ‘the hard way’ (using optim), and ‘the easier way’ (using ‘glm’)\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nCode\npars_optim &lt;- optim_results$par\n\nnames(pars_optim) &lt;- c(\"beta0\", \"beta1\", \"eta\")\n\npars_optim\n\n\n    beta0     beta1       eta \n 2.460571  1.375421 -1.336209 \n\n\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- tibble(x = x, y = y)\nmod_glm &lt;- glm(y ~ x, data = df, family = gaussian(link=\"identity\"))\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nSo, both optim and the summary to mod_glm report \\(\\{\\beta_0 = 2.36, \\beta_1 = 1.38\\}\\), so both approaches appear to arrive at the same point on the log likelihood surface.\nHowever, note that the glm summary reports not just the estimates themselves (in the Estimate column of coefficients), but also standard errors (the Std. Error column) and derived quantities (t value, Pr(&gt;|t|), and the damnable stars at the very right of the table). How can these measures of uncertainty about the true value of the \\(\\beta\\) coefficients be derived from optim?"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#barefoot-and-blind-a-weird-analogy-for-a-complicated-idea",
    "href": "posts/glms/lms-are-glms-part-07/index.html#barefoot-and-blind-a-weird-analogy-for-a-complicated-idea",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Barefoot and Blind: A weird analogy for a complicated idea",
    "text": "Barefoot and Blind: A weird analogy for a complicated idea\nImagine optim, your hill-finding robo-chauffeur, has taken you to the top of a likelihood surface. Then it leaves you there…\n… and you’re blind, and have no shoes. (You also have an uncanny sense of your orientation, whether north-south, east-west, or some other angle.)\nSo, you know you’re at the top of the hill, but you can’t see what the landscape around you looks like. However, you still want to get a sense of this landscape, and how it varies around the spot you’re standing on.\nWhat do you do?\nIf you’re playing along with this weird thought experiment, one approach would be to use your feet as depth sensors. You make sure you never stray from where you started, and to always keep one foot planted on this initial spot (which you understand to be the highest point on the landscape). Then you use your other foot to work out how much further down the surface is from the highest point as you venture away from the highest point in different directions.\nSay you keep your left foot planted on the highest point, and make sure your right foot is always positioned (say) 10 cm horizontally from your left foot. Initially your two feet are arranged east-west; let’s call this 0 degrees. When you put your right foot down, you notice it needs to travel 2 cm further down to reach terra ferma relative to your left foot.\n2cm at 0 degrees. You’ll remember that.\nNow you rotate yourself 45 degrees, and repeat the same right foot drop. This time it needs to travel 3cm down relative to your left foot.\n3cm at 45 degrees. You remember that too.\nNow you rotate another 45 degrees, north-south orientation, place your right foot down; now it falls 5cm down relative to your left foot.\n2cm at 0 degrees; 3cm at 45 degrees; 5cm at 90 degrees.\nNow with this information, you try to construct the landscape you’re on top of with your mind’s eye, making the assumption that the way it has to have curved from the peak you’re on to lead to the drops you’ve observed is consistent all around you; i.e. that there’s only one hill, you’re on top of it, and it’s smoothly curved in all directions.\nIf you could further entertain the idea that your feet are infinitely small, and the gap between feet is also infinitely small (rather than the 10cm above), then you have the intuition behind this scary-looking but very important formula from King (1998) (p. 89):\n\\[\n\\widehat{V(\\hat{\\theta})} = - \\frac{1}{n}[\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]^{-1}_{\\tilde{\\theta} = \\hat{\\theta}}\n\\]\nWhat this is saying, in something closer to humanese, is something like:\n\nOur best estimate of the amount of uncertainty we have in our estimates is a function of how much the likelihood surface curves at the highest point on the surface. (It also gets less uncertain, the more observations we have)."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#information-and-uncertainty",
    "href": "posts/glms/lms-are-glms-part-07/index.html#information-and-uncertainty",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Information and uncertainty",
    "text": "Information and uncertainty\nAmongst the various bells, whistles and decals in the previous formula is the superscript \\((.)^{-1}\\). This means invert, which for a single value means \\(\\frac{1}{.}\\) but for a matrix means something conceptually the same but technically not.\nAnd what’s being inverted in the last formula? A horrible-looking expression, \\([\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]_{\\tilde{\\theta} = \\hat{\\theta}}\\), that’s basically an answer to the question of how curvy is the log likelihood surface at its peak position?\nWithin King (1998) (p.89, eq. 4.18), this expression (or rather the negative of the term) is defined as \\(I(\\hat{\\theta} | y)\\), where \\(I(.)\\) stands for information.\nSo, the algebra are saying\n\nUncertainty is inversely related to information\n\nOr perhaps even more intuitively\n\nThe more information we have, the less uncertain we are\n\nOf course this makes sense. If you ask someone “How long will this task take?”, and they say “Between one hour and one month”, they likely have less information about how long the task will actually than if they had said “Between two and a half and three hours”. More generally:\n\nShallow gradients mean wide uncertainty intervals mean low information\nSharp gradients mean narrow uncertaintly intervals mean high information\n\nThis is, fundamentally, what the blind and barefoot person in the previous analogy is trying to achieve: by feeling out the local curvature around the highest point, they are trying to work out how much information they have about different pieces of the model. The curvature along any one dimension of the surface (equivalent to the 0 and 90 degree explorations) indicates how much information there is about any single coefficient, and the curvature along the equivalent of a 45 degree plane gives a measure of how associated any two coefficients tend to be.\nWith these many analogies and equations spinning in our heads, let’s now see how these concepts can be applied in practice."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#optimal-uncertainty",
    "href": "posts/glms/lms-are-glms-part-07/index.html#optimal-uncertainty",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Optimal uncertainty",
    "text": "Optimal uncertainty\nHaving reminded myself of the particular options for optim that are typically used to report parameter uncertainty, let’s run the follows:\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nWe have used a slightly different algorithm (‘BFGS’), and a different way of specifying the function to search over (using fnscale = -1 to invert the likelihood), but we have the same par estimates as before: \\(\\beta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\). So the changes we’ve made to the optim arguments haven’t changed what it estimates.\nOne new argument we’ve set in optim is hessian = TRUE. Hessian is a kind of coarse fabric made from vegetable waste, typically woven in a criss-crossing, grid-like pattern. Hessian matrices are matrices of second derivatives, as described in the wikipedia article. 1 If you can bear to recall the really complex expression above, for calculating the curvature around a point on a surface, you’ll recall it’s also about second derivatives.\nNone of this is a coincidence. The hessian component of the optim output above contains what we need.\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\nhess\n\n\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nYou might notice that the Hessian matrix is square, with as many columns as rows. And, that the number of columns (or rows) is equal to the number of parameters we have estimated, i.e. three in this case.\nYou might also notice that the values are symmetrical about the diagonal running from the top left to the bottom right.\nAgain, this is no accident.\nRemember that variation is inversely related to information, and that \\((.)^{-1}\\) is the inversion operator on \\(I(.)\\), the Information Matrix. Well, this Hessian is (pretty much) \\(I(.)\\). So let’s see what happens when we invert it (using the solve operator):\n\n\nCode\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\nAs with hess, inv_hess is symmetric around the top-left to bottom-right diagonal. For example, the value on row 2 and column 1 is the same as on row 1, column 2.\nWe’re mainly interested in the first two columns and rows, as these contain the values most comparable with the glm summary reports\n\n\nCode\ninv_hess_betas &lt;- inv_hess[1:2, 1:2]\n\ninv_hess_betas\n\n\n             [,1]         [,2]\n[1,]  0.033577455 -0.004379668\n[2,] -0.004379668  0.004379668\n\n\nWhat the elements of the above matrix provide are estimates of the variances of a single parameter \\(\\beta_j\\), and/or the covariances between any two parameters \\(\\{\\beta_0, \\beta_1\\}\\). In this example:\n\\[\n\\begin{bmatrix}\nvar(\\beta_0) & cov(\\beta_0, \\beta_1) \\\\\ncov(\\beta_1, \\beta_0) & var(\\beta_1)\n\\end{bmatrix}\n\\]\nIt’s because the on-diagonal terms are variances of uncertaintly for a single term, that it can be useful to take the square root of these terms to get estimates of the standard errors:\n\n\nCode\nsqrt(diag(inv_hess_betas))\n\n\n[1] 0.18324152 0.06617906\n\n\nCompare with the Std Err term in the following:\n\n\nCode\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe estimates from the Hessian in optim, of \\(\\{0.18, 0.07\\}\\), are not exactly the same as the \\(\\{0.21, 0.08\\}\\) reported for mod_glm; the methods employed are not identical. But they are hopefully similar enough to demonstrate they provide similar information about similar quantities of uncertainty."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#summary",
    "href": "posts/glms/lms-are-glms-part-07/index.html#summary",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Summary",
    "text": "Summary\nThis is probably the most difficult single section so far. Don’t worry: it’s likely to get easier from here on in."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#coming-up",
    "href": "posts/glms/lms-are-glms-part-07/index.html#coming-up",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Coming up",
    "text": "Coming up\nThe next part of the series goes into more detail about how numerical optimisation works."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-07/index.html#footnotes",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough I had assumed Hessian matrices are called Hessian matrices because they sort-of resemble the criss-crossing grids of Hessian bags, they’re actually named after Otto Hesse, who proposed them.↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-01/index.html",
    "href": "posts/glms/lms-are-glms-part-01/index.html",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-01/index.html#tldr",
    "href": "posts/glms/lms-are-glms-part-01/index.html#tldr",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-01/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "href": "posts/glms/lms-are-glms-part-01/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "Part 1: What are statistical models and how are they fit?",
    "text": "Part 1: What are statistical models and how are they fit?\nIt’s common for different statistical methods to be taught as if they’re completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\nSomething I’ve found immensely helpful over the years is the following pair of equations:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nIn words, the above is saying something like:\n\nThe predicted response \\(Y_i\\) for a set of predictors \\(X_i\\) is assumed to be drawn from (the \\(\\sim\\) symbol) a stochastic distribution (\\(f(.,.)\\))\nThe stochastic distribution contains both parameters we’re interested in, and which are determined by the data \\(\\theta_i\\), and parameters we’re not interested in and might just have to assume, \\(\\alpha\\).\nThe parameters we’re interested in determining from the data \\(\\theta_i\\) are themselves determined by a systematic component \\(g(.,.)\\) which take and transform two inputs: The observed predictor data \\(X_i\\), and a set of coefficients \\(\\beta\\)\n\nAnd graphically this looks something like:\n\n\n\n\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --&gt; g\n  beta --&gt; g\n  g --&gt; theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt; Y\n\n\n\n\n\n\n\nTo understand how this fits into the ‘whole game’ of modelling, it’s worth introducing another term, \\(D\\), for the data we’re using, and to say that \\(D\\) is partitioned into observed predictors \\(X_i\\), and observed responses, \\(y_i\\).\nFor each observation, \\(i\\), we therefore have a predicted response, \\(Y_i\\), and an observed response, \\(y_i\\). We can compare \\(Y_i\\) with \\(y_i\\) to get the difference between the two, \\(\\delta_i\\).\nNow, obviously can’t change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the \\(\\beta\\) parameters that feed into the systematic component \\(g\\). Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n\n\n\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D --&gt;|partition| X\n  D --&gt;|partition| y\n  X --&gt; g\n  beta --&gt;|rerun| g\n  g --&gt;|transform| theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt;|predict| Y\n  \n  Y --&gt;|compare| diff\n  y --&gt;|compare| diff\n  \n  diff --&gt;|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n\n\n\n\n\nPretty much all statistical model fitting involves iterating along this \\(g \\to \\beta\\) and \\(\\beta \\to g\\) feedback loop until some kind of condition is met involving minimising \\(\\delta\\).\nI’ll expand on this idea further in part 2."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-12/index.html",
    "href": "posts/glms/lms-are-glms-part-12/index.html",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "",
    "text": "The last post ended by showing how the predict function can be used to show point estimates and uncertainty intervals for expected values and predicted values for a model based on a toothsome dataset. In this post we will start with that model and look at other information that can be recovered from it, information that will allow the effects of joint parameter uncertainty to be propagated through to prediction."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-12/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-12/index.html#aim",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "",
    "text": "The last post ended by showing how the predict function can be used to show point estimates and uncertainty intervals for expected values and predicted values for a model based on a toothsome dataset. In this post we will start with that model and look at other information that can be recovered from it, information that will allow the effects of joint parameter uncertainty to be propagated through to prediction."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-12/index.html#recap-of-core-concepts",
    "href": "posts/glms/lms-are-glms-part-12/index.html#recap-of-core-concepts",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Recap of core concepts",
    "text": "Recap of core concepts\nBack in part 8 we stated that estimates of the cloud of uncertainty in model parameters, that results from having limited numbers of observations in the data, can be represented as:\n\\[\n\\tilde{\\theta} \\sim MVN(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere MVN means multivariate normal, and needs the two quantities \\(\\dot{\\theta}\\) and \\(\\Sigma\\) as parameters.\nPreviously we showed how to extract (estimates of) these two quantities from optim(), where the first quantity, \\(\\dot{\\theta}\\), was taken from the converged parameter point estimate slot par, and the second quantity, \\(\\Sigma\\), was derived from the hessian slot.\nBut we don’t need to use optim() directly in order to recover these quantities. Instead we can get them from the standard model objects produced by either lm() or glm(). Let’s check this out…"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-12/index.html#building-our-model",
    "href": "posts/glms/lms-are-glms-part-12/index.html#building-our-model",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Building our model",
    "text": "Building our model\nLet’s load the data and model we arrived at previously\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nCode\nbest_model &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s now look at some convenience functions, other than just summary, that work with lm() and glm() objects, and recover the quantities required from MVN to represent the uncertainty cloud."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-12/index.html#extracting-quantities-for-modelling-uncertainty",
    "href": "posts/glms/lms-are-glms-part-12/index.html#extracting-quantities-for-modelling-uncertainty",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Extracting quantities for modelling uncertainty",
    "text": "Extracting quantities for modelling uncertainty\nFirstly, for the point estimates \\(\\dot{\\theta}\\), we can use the coefficients() function\n\n\nCode\ncoef &lt;- coefficients(best_model)\n\ncoef\n\n\n     (Intercept)        log(dose)           suppVC log(dose):suppVC \n       20.663333         9.254889        -3.700000         3.844782 \n\n\nAnd for the variance-covariance matrix, for representing joint uncertainty about the above estimates, we can use the vcov function\n\n\nCode\nSig &lt;- vcov(best_model)\n\nSig\n\n\n                   (Intercept)     log(dose)        suppVC log(dose):suppVC\n(Intercept)       4.612422e-01 -8.768056e-17 -4.612422e-01    -7.224251e-17\nlog(dose)        -8.768056e-17  1.440023e+00  1.753611e-16    -1.440023e+00\nsuppVC           -4.612422e-01  1.753611e-16  9.224843e-01     1.748938e-16\nlog(dose):suppVC -7.224251e-17 -1.440023e+00  1.748938e-16     2.880045e+00\n\n\nFinally, we can extract the point estimate for stochastic variation in the model, i.e. variation assumed by the model even if parameter uncertainty were minimised, using the sigma function:\n\n\nCode\nsig &lt;- sigma(best_model)\n\nsig\n\n\n[1] 3.719847\n\n\nWe now have three quantities, coef, Sig and sig (note the upper and lower case s in the above). These provide something almost but not exactly equivalent to the contents of par and that derived from hessian when using optim() previously. The section below explains this distinction in more detail.\n\nBack to the weeds (potentially skippable)\nRecall the ‘grandmother formulae’, from King, Tomz, and Wittenberg (2000), which the first few posts in this series started with:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nFor standard linear regression this becomes:\nStochastic Component\n\\[\nY_i \\sim Norm(\\theta_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\theta_i =X_i \\beta\n\\]\nOur main parameters are \\(\\theta\\), which combined our predictors \\(X_i\\) and our model parameter estimates \\(\\beta\\). Of these two components we know the data - they are what they are - but are merely estimating our model parameters \\(\\beta\\). So, any estimation uncertainty in this part of the equation results from \\(\\beta\\) alone.\nOur ancillary parameter is \\(\\sigma^2\\). This is our estimate of how much fundamental variation there is in how the data (the response variables \\(Y\\)) is drawn from the stochastic data generating process.\nWhen we used optim() directly, we estimated \\(\\sigma^2\\) along with the other \\(\\beta\\) parameters, via the \\(\\eta\\) parameter eta, defined as \\(\\sigma^2 = e^{\\eta}\\) to allow optim() to search over an unbounded real number range. If there are k \\(\\beta\\) parameters, therefore, optim()’s par vector contained k + 1 values, with this last value being the point estimate for the eta parameter. Similarly, the number of rows, columns, and length of diagonal elements in the variance-covariance matrix recoverable through optim’s hessian slot was also k + 1 rather than k, with the last row, last column, and last diagonal element being measures of covariance between \\(\\eta\\) and the \\(\\beta\\) elements, and variance in \\(\\eta\\) itself.\nBy contrast, the length of coefficients returned by coefficients(best_model) is k, the number of \\(\\beta\\) parameters being estimated, and the dimensions of vcov(best_model) returned are also k by k.\nThis means there is one fewer piece/type of information about model parameters returned by coefficients(model), vcov(model) and sigma(model) than was potentially recoverable by optim()’s par and hessian parameter slots: namely, uncertainty about the true value of the ancillary parameter \\(\\sigma^2\\). The following table summarises this difference:\n\n\n\n\n\n\n\n\nInformation type\nvia optim\nvia lm and glm\n\n\n\n\nMain parameters: point\nfirst k elements of par\ncoefficients() function\n\n\nMain parameters: uncertainty\nfirst k rows and columns of hessian\nvcov() function\n\n\nAncillary parameters: point\nk+1th through to last element of par\nsigma() function or equivalent for glm()\n\n\nAncillary parameters: uncertainty\nlast columns and rows of hessian (after rows and columns k)\n—\n\n\n\nSo long as capturing uncertainty about the fundamental variability in the stochastic part of the model isn’t critical to our predictions then omission of a measure of uncertainty in the ancillary parameters \\(\\alpha\\) is likely a price worth paying for the additional convenience of being able to use the model objects directly. However we should be aware that, whereas with optim we potentially have both \\(\\tilde{\\beta}\\) and \\(\\tilde{\\alpha}\\) to represent model uncertainty, when using the three convenience functions coefficients(), vcov() and sigma() we technically ‘only’ have \\(\\tilde{\\beta}\\) and \\(\\dot{\\alpha}\\) (i.e. point estimates alone for the ancillary parameters).\nWith the above caveat in mind, let’s now look at using the results of coefficients(), vcov() and sigma() to generate (mostly) honest representations of expected values, predicted values, and first differences"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-12/index.html#model-predictions",
    "href": "posts/glms/lms-are-glms-part-12/index.html#model-predictions",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Model predictions",
    "text": "Model predictions\nAs covered in section two, we can use the mvrnorm function from the MASS package to create \\(\\tilde{\\beta}\\), our parameter estimates with uncertainty:\n\nParameter simulation\n\n\nCode\nbeta_tilde &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = coef, \n    Sigma = Sig\n)\n\nhead(beta_tilde)\n\n\n     (Intercept) log(dose)    suppVC log(dose):suppVC\n[1,]    20.94078 10.472272 -5.104424      -0.01286336\n[2,]    20.12848  8.379597 -2.890160       3.22001417\n[3,]    20.51167  8.533110 -4.551441       6.20179932\n[4,]    20.56742 11.252963 -3.646873       1.29973377\n[5,]    20.43586  7.624446 -2.979613       7.61304278\n[6,]    21.75989 10.271595 -4.676590       2.92165979\n\n\nLet’s first look at each of these parameters individually:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    pivot_longer(everything(), names_to = \"coefficient\", values_to = \"value\") |&gt; \n    ggplot(aes(x = value)) + \n    facet_grid(coefficient ~ .) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nNow let’s look at a couple of coefficients jointly, to see how they’re correlated. Firstly the association between the intercept and the log dosage:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = `log(dose)`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere the covariance between the two parameters appears very low. Now let’s look at how log dosage and Vitamin C supplement factor are associated:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nAgain, the covariance appears low. Finally, the association between log dose and the interaction term\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = `log(dose):suppVC`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere we have a much stronger negative covariance between the two coefficients. Let’s look at the variance-covariance extracted from the model previously to confirm this:\n\n\nCode\nknitr::kable(Sig)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nlog(dose)\nsuppVC\nlog(dose):suppVC\n\n\n\n\n(Intercept)\n0.4612422\n0.000000\n-0.4612422\n0.000000\n\n\nlog(dose)\n0.0000000\n1.440023\n0.0000000\n-1.440023\n\n\nsuppVC\n-0.4612422\n0.000000\n0.9224843\n0.000000\n\n\nlog(dose):suppVC\n0.0000000\n-1.440023\n0.0000000\n2.880045\n\n\n\n\n\nHere we can see that the covariance between intercept and log dose is effectively zero, as is the covariance between the intercept and the interaction term, and the covariance between the log(dose) and suppVC factor. However, there is a negative covariance between log dose and the interaction term, i.e. what we have plotted above, and also between the intercept and the VC factor. For completeness, let’s look at this last assocation, which we expect to show negative association:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nYes it is! The parameter estimates follow the covariance provided by Sigma, as we would expect.\n\n\nExpected values\nLet’s stay we are initially interested in the expected values for a dosage of 1.25mg, with the OJ (rather than VC) supplement:\n\n\nCode\n# first element is 1 due to intercept\npredictor &lt;- c(1, log(1.25), 0, 0) \n\npredictions_ev &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor %*% this_beta\n)\n\nhead(predictions_ev)\n\n\n[1] 23.27760 21.99833 22.41578 23.07845 22.13721 24.05193\n\n\nLet’s now get a 95% credible interval:\n\n\nCode\nquantile(predictions_ev, probs = c(0.025, 0.500, 0.975))\n\n\n    2.5%      50%    97.5% \n21.27924 22.72922 24.19010 \n\n\nSo, the 95% interval for the expected value is between 21.31 and 24.14, with a middle (median) estimate of 22.73.1 Let’s check this against estimates from the predict() function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'confidence')\n\n\n      fit      lwr      upr\n1 22.7285 21.26607 24.19093\n\n\nThe expected values using the predict function give a 95% confidence interval of 21.27 to 24.19, with a point estimate of 22.73. These are not identical, as the methods employed are not identical,2 but they are hopefully similar enough to demonstrate they are attempts at getting at the same quantities of interest.\n\n\nPredicted values\nPredicted values also include inherent stochastic variation from the ancillary parameters \\(\\alpha\\), which for linear regression is \\(\\sigma^2\\). We can simply add these only the expected values above to produce predicted values:\n\n\nCode\nn &lt;- length(predictions_ev)\n\nshoogliness &lt;- rnorm(n=n, mean = 0, sd = sig)\n\npredictions_pv &lt;- predictions_ev + shoogliness\n\n\nhead(predictions_pv)\n\n\n[1] 23.72120 23.13082 19.51765 25.10035 19.90430 23.09212\n\n\nLet’s get the 95% interval from the above using quantile\n\n\nCode\nquantile(predictions_pv, probs = c(0.025, 0.5000, 0.975))\n\n\n    2.5%      50%    97.5% \n15.12925 22.66189 30.03233 \n\n\nAs expected, the interval is now much wider, with a 95% interval from 15.34 to 30.11. The central estimate should in theory, with an infinite number of runs, be the same, however because of random variation it will never be exactly the same to an arbitrary number of decimal places. In this case, the middle estimate is 22.75, not identical to the central estimate from the expected values distribution of 22.72. The number of simulations can always be increased to produce greater precision if needed.\nLet’s now compare this with the prediction interval produce by the predict function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'prediction')\n\n\n      fit      lwr     upr\n1 22.7285 15.13461 30.3224\n\n\nAgain, the interval estimates are not exactly the same, but they are very similar.\n\n\nFirst differences\nIt’s in the production of estimates of first differences - this, compared to that, holding all else constant - that the simulation approach shines for producing estimates with credible uncertainty. In our case, let’s say we are interested in asking:\n\nWhat is the expected effect of using the VC supplement, rather than the OJ supplement, where the dose is 1.25mg?\n\nSo, the first difference is from switching from OJ to VC, holding the other factor constant.\nWe can answer this question by using the same selection of \\(\\tilde{\\beta}\\) draws, but passing two different scenarios:\n\n\nCode\n#scenario 0: supplement is OJ\npredictor_x0 &lt;- c(1, log(1.25), 0, 0) \n\n#scenario 1: supplement is VC\npredictor_x1 &lt;- c(1, log(1.25), 1, 1 * log(1.25)) \n\n\npredictions_ev_x0 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x0 %*% this_beta\n)\n\npredictions_ev_x1 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x1 %*% this_beta\n)\n\npredictions_df &lt;- \n    tibble(\n        x0 = predictions_ev_x0,\n        x1 = predictions_ev_x1\n    ) |&gt;\n    mutate(\n        fd = x1 - x0\n    )\n\npredictions_df\n\n\n# A tibble: 10,000 × 3\n      x0    x1    fd\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  23.3  18.2 -5.11\n 2  22.0  19.8 -2.17\n 3  22.4  19.2 -3.17\n 4  23.1  19.7 -3.36\n 5  22.1  20.9 -1.28\n 6  24.1  20.0 -4.02\n 7  22.5  20.7 -1.80\n 8  21.8  19.8 -2.05\n 9  22.1  19.8 -2.28\n10  22.9  19.6 -3.30\n# ℹ 9,990 more rows\n\n\nLet’s look at the distribution of both scenarios individually:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario != \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    facet_wrap(~scenario, ncol = 1) + \n    geom_histogram(bins = 100)\n\n\n\n\n\nAnd the distribution of the pairwise differences between them:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nIt’s this last distribution which shows our first differences, i.e. our answer, hedged with an appropriate dose of uncertainty, to the specific question shown above. We can get a 95% interval of the first difference as follows:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt; \n    pull('estimate') |&gt;\n    quantile(probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-4.8570543 -2.8593842 -0.8175495 \n\n\nSo, 95% of estimates of the first difference are between -4.85 and -0.81, with the middle of this distribution (on this occasion) being -2.83.\nUnlike with the expected values and predicted values, the predict() function does not return first differences with honest uncertainty in this way. What we have above is something new."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-12/index.html#summary",
    "href": "posts/glms/lms-are-glms-part-12/index.html#summary",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Summary",
    "text": "Summary\nIn this post we’ve finally combined all the learning we’ve developed over the 11 previous posts to answer three specific ‘what if?’ questions: one on expected values, one on predicted values, and one on first differences. These are what King, Tomz, and Wittenberg (2000) refer to as quantities of interest, and I hope you agree these are more organic and reasonable types of question to ask of data and statistical models than simply looking at coefficients and p-values and reporting which ones are ‘statistically significant’.\nIf you’ve been able to follow everything in these posts, and can generalise the approach shown above to other types of statistical model, then congratulations! You’ve learned the framework for answering meaningful questions using statistical models which is at the heart of one of the toughest methods courses for social scientists offered by one of the most prestigious universities in the world.3"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-12/index.html#coming-up",
    "href": "posts/glms/lms-are-glms-part-12/index.html#coming-up",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Coming up",
    "text": "Coming up\nThe next post uses the same dataset and model we’ve developed and applied, but shows how it can be implemented using a Bayesian rather than Frequentist modelling approach. In some ways it’s very familar, but in others it introduces a completely new paradigm to how models are fit and run."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-12/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-12/index.html#footnotes",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese are the values produced the first time I ran the simulation. They are likely to be a little different each time, so may not be identical to the number of decimal places reported when I next render this document. These estimates are approximations.↩︎\nBecause the simulation approach relies on random numbers, the draws will never be the same unless the same random number seed is using using set.seed(). However with more simulations, using the n parameter from mvrnorm, the distributions of estimates should become ever closer to each other.↩︎\nI took this course via the Harvard extension school while doing my PhD in York quite a few years ago. I took it as a non-credit option - as what’s the value of a fraction of a degree when I had two already? - but was told by the tutors that I’d completed it to a ‘Grade A-’ level. So, not perfect, but good enough…↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-14/index.html",
    "href": "posts/glms/lms-are-glms-part-14/index.html",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "",
    "text": "Henry Dundas, as observed\n\n\n\n\n\n\n\nHenry Dundas, the unobserved good counterfactual\n\n\n\n\n\n\n\nHenry Dundas, the unobserved bad counterfactual"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-14/index.html#henry-dundas-hero-or-villain",
    "href": "posts/glms/lms-are-glms-part-14/index.html#henry-dundas-hero-or-villain",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Henry Dundas: Hero or Villain?",
    "text": "Henry Dundas: Hero or Villain?\nA few minutes’ walk from where I live is St Andrew Square. And in the middle of St Andrew Square is the Melville Monument, a 40 metre tall column, on which stands a statue of Henry Dundas, 1st Viscount Melville.\nThough the Melville Monument was constructed in the 19th century to commemorate and celebrate this 18th century figure, in 2020 the City of Edimburgh Council chose to add more context to Dundas’ legacy by unveiling a plaque with the following message::\n\nAt the top of this neoclassial column stands a statue of Hentry Dundas, 1st Viscount Melville (1742-1811). He was the Scottish Lord Advocate, an MP for Edinburgh and Midlothian, and the First Lord of the Admiralty. Dundas was a contentious figure, provoking controversies that resonate to this day. While Home Secretary in 1792, and first Secretary of State for War in 1796 he was instrumental in deferring the abolition of the Atlantic slave trade. Slave trading by British ships was not abolished until 1807. As a result of this delay, more than half a million enslaved Africans crossed the Atlantic.\n\nSo, the claim of the council plaque was that Dundas caused the enslavement of hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nThe descendents of Dundas contested these claims, however, instead arguing:\n\nThe claim that Henry Dundas caused the enslavement of more than half a million Africans is patently false. The truth is: Dundas was the first MP to advocate in Parliament for the emancipation of slaves in the British territories along with the abolition of the slave trade. Dundas’s efforts resulted in the House of Commons voting in favour of ending the Atlantic slave trade for the first time in its history.\n\nSo, the claim of the descendents was that Dundas prevented the enslavement of (at least) hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nHow can the same agreed-upon historical facts lead to such diametrically opposing interpretations of the effects of Dundas and his actions?\nThe answer to this question is at the heart of causal inference, and an example of why, when trying to estimate causal effects, at least half of the data are always missing."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-14/index.html#the-unobserved-counterfactual",
    "href": "posts/glms/lms-are-glms-part-14/index.html#the-unobserved-counterfactual",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "The unobserved counterfactual",
    "text": "The unobserved counterfactual\nBoth parties in the Dundas debate have, as mentioned, access to the same historical facts. They agree on the same observed historical reality. And both are making bold claims about the impact of Dundas in relation to the Transatlantic slave trade. In doing this, they are both comparing this observed historical reality with something else: the unobserved counterfactual.\nThe unobserved counterfactual is the data that would have been observed if what had happened, hadn’t happened 1 However, what happened did happen, so this data isn’t observed. So, as it hasn’t been observed, it doesn’t exist in any historic facts. Instead, the unobserved counterfactual has to be imputed, or inferred… in effect, made up.\nCausal inference always involves some kind of comparison between an observed reality and an unobserved counterfactual. The issue at heart of the Dundas debate is that both parties have compared the observed reality with a different unobserved counterfactual, and from this different Dundas effects have been inferred.\nFor the council, the unobserved counterfactual appears to be something like the following:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. The more radical and rapid version of the bill passes, and slavery is abolished earlier, leading to fewer people becoming enslaved.\n\nWhereas for the descendents, the unobserved counterfactual appears to be something like this:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. Because of this, the more radical version of the bill doesn’t have enough support in parliament (perhaps because it would be acting too much against the financial interests of some parliamentarians and powerful business interests), and so is defeated. As a result of this, the abolition of slavery is delayed, leading to more people becoming enslaved.\n\nSo, by having the same observed historical facts, the observed Dundas, but radically different counterfactuals, the two parties have used the same methodology to derive near antithetical estimates of the ‘Dundas Effect’."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-14/index.html#coming-up",
    "href": "posts/glms/lms-are-glms-part-14/index.html#coming-up",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Coming up",
    "text": "Coming up\nThe next post offers more of a technical treatment of the key concept introduced here: namely that causal effect estimation depends on comparing observed with counterfactual data, and as the counterfactual is unobserved, causal effect estimation is fundamentally a missing data problem."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-14/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-14/index.html#footnotes",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data that would have been observed if what hadn’t happened, had happened, is the other type of unobserved counterfactual.↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-04/index.html",
    "href": "posts/glms/lms-are-glms-part-04/index.html",
    "title": "Part Four: why only betas just look at betas",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-04/index.html#tldr",
    "href": "posts/glms/lms-are-glms-part-04/index.html#tldr",
    "title": "Part Four: why only betas just look at betas",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-04/index.html#part-4-why-overuse-of-linear-regression-leads-people-to-look-at-models-in-the-wrong-way",
    "href": "posts/glms/lms-are-glms-part-04/index.html#part-4-why-overuse-of-linear-regression-leads-people-to-look-at-models-in-the-wrong-way",
    "title": "Part Four: why only betas just look at betas",
    "section": "Part 4: Why overuse of linear regression leads people to look at models in the wrong way",
    "text": "Part 4: Why overuse of linear regression leads people to look at models in the wrong way\nIn the last post in this series I’ve reintroduced standard linear regression and logistic regression as both being special versions of the same generalised model formula.\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nWith standard linear regression the link function \\(g(.)\\) is \\(I(.)\\), i.e. the identity function, meaning what goes in, is what comes out. By contrast for logistic regression \\(g(.)\\) is the logistic function, which squishes and squashes any real number as an input onto a value between 0 and 1 as an output.\nThough it’s not always phrased this way, a motivating question behind the construction of most statistical models is, “What influence does a single input to the model, \\(x_j\\), have on the output, \\(Y\\)?”1 For a single variable \\(x_j\\) which is either present (1) or absent (0), this is in effect asking what is \\(E(Y | x_j = 1) - E(Y | x_j = 0)\\) ?2\nLet’s look at a linear regression case, then a logistic regression case.\n\nLinear Regression example\nUsing the iris dataset, let’s try to predict Sepal Width (a continuous variable) on Sepal Length (a continuous variable) and whether the species is setosa or not (a discrete variable). As a reminder, the data relating these three variables look as follows:\n\n\nCode\nlibrary(ggplot2)\n\niris |&gt;\n    ggplot(aes(Sepal.Length, Sepal.Width, group = Species, colour = Species, shape = Species)) + \n    geom_point()\n\n\n\n\n\nLet’s now build the model:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- iris |&gt; mutate(is_setosa = Species == 'setosa')\n\nmod_lm &lt;- lm(Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nmod_lm\n\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nCoefficients:\n  (Intercept)   Sepal.Length  is_setosaTRUE  \n       0.7307         0.3420         0.9855  \n\n\nThe coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}\\) are \\(\\{0.73, 0.34, 0.99\\}\\), and refer to the intercept, Sepal Length and is_setosa respectively.\nIf we assume a Sepel Length of 6, for example, then the expected Sepal Width (the thing we are predicting) is 0.73 + 6 * 0.34 + 0.99 or about 3.77 in the case where is_setosa is true, and 0.73 + 6 * 0.34 or about 2.78 where is_setosa is false.\nThe difference between these two values, 3.77 and 2.78, i.e. the ‘influence of setosa’ on the outcome, is 0.99, i.e. the \\(\\beta_2\\) coefficient shown before. In fact, for any conceivable (and non-conceivable, i.e. negative) value of Sepal Length, the difference is still 0.99.\nThis is the \\(\\beta_2\\) coefficient, and the reason why, for linear regression, and almost exclusively linear regression, looking at the coefficients themselves provides substantively meaningful information (something King, Tomz, and Wittenberg (2000) calls a ‘quantity of interest’) about the size of influence that a predictor has on a response.\n\n\nLogistic Regression example\nNow let’s look at an example using logistic regression. We will use another tiresomely familiar dataset, mtcars. We are interested in estimating the effect that having a straight engine (vs=1) has on the probability of the car having a manual transmission (am=1). Our model also tries to control for the miles-per-gallon (mpg). The model specification is shown, the model is run, and the coefficeints are all shown below:\n\n\nCode\nmod_logistic &lt;- glm(\n    am ~ mpg + vs,\n    data = mtcars, \n    family = binomial()\n    )\n\nmod_logistic\n\n\n\nCall:  glm(formula = am ~ mpg + vs, family = binomial(), data = mtcars)\n\nCoefficients:\n(Intercept)          mpg           vs  \n    -9.9183       0.5359      -2.7957  \n\nDegrees of Freedom: 31 Total (i.e. Null);  29 Residual\nNull Deviance:      43.23 \nResidual Deviance: 24.94    AIC: 30.94\n\n\nHere the coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}\\) are \\(\\{-9.92, 0.54, -2.80\\}\\), and refer to the intercept, mpg, and vs respectively.\nBut what does this actually mean, substantively?\n\n\n(Don’t) Stargaze\nA very common approach to trying to answer this question is to look at the statistical significance of the coefficients, which we can do with the summary() function\n\n\nCode\nsummary(mod_logistic)\n\n\n\nCall:\nglm(formula = am ~ mpg + vs, family = binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -9.9183     3.4942  -2.839  0.00453 **\nmpg           0.5359     0.1967   2.724  0.00644 **\nvs           -2.7957     1.4723  -1.899  0.05758 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 24.944  on 29  degrees of freedom\nAIC: 30.944\n\nNumber of Fisher Scoring iterations: 6\n\n\nA common practice in many social and health sciences is to offer something like a narrative summary of the above, something like:\n\nOur logistic regression model indicates that manualness is positively and significantly associated with our measure of fuel efficiency (p &lt; 0.01). There is also an indication of a negative association with straight engine, but this effect does not quite meet conventional thresholds for statistical significance (p &lt; 0.10).\n\nThis above practice is known as ‘star-gazing’, because summary tables like those above tend to have one or more * symbols in the final row, if the value of the Pr(&gt;|z|) is below 0.05, and narrative summaries like those just above tend to involve looking at the number of stars in each row, alongside whether the Estimate values have a minus sign in front of them.\nStar gazing is a very common practice. It’s also a terrible practice, which - ironically - turns the final presented output of a quantitative model into the crudest of qualitative summaries (positive, negative; significant, not significant). Star gazing is what researchers tend to default to when presented with model outputs from the above because, unlike in the linear regression example, the extent to which the \\(\\beta\\) coefficients answer substantive ‘how-much’-ness questions, like “How much does having a straight engine change the probability of manual transmission?, is not easily apparent from the coefficients themselves.\n\n\nStandardisation\nSo, how can we do better?\nOne approach is to standardise the data that goes into the model before passing them to the model. Standardisation means attempting to make the distribution and range of different variables more similar, and is especially useful when comparing between different continuous variables.\nTo give an example of this, let’s look at a specification with weight (wt) and horsepower (hp) in place of mpg, but keeping engine-type indicator (vs):\n\n\nCode\nmod_logistic_2 &lt;- glm(\n    am ~ vs + wt + hp,\n    data = mtcars, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2)\n\n\n\nCall:\nglm(formula = am ~ vs + wt + hp, family = binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) 25.35510   11.24613   2.255   0.0242 *\nvs          -3.12906    2.92958  -1.068   0.2855  \nwt          -9.64982    4.05528  -2.380   0.0173 *\nhp           0.03242    0.01959   1.655   0.0979 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.5012  on 28  degrees of freedom\nAIC: 16.501\n\nNumber of Fisher Scoring iterations: 8\n\n\nHere both wt and hp are continuous variables.\nA star gazing zombie might say something like\n\nmanualness is negatively and significantly associated with weight (p &lt; 0.05); there is a positive association with horsepower but this does not meet standard thresholds of statistical significance (0.05 &lt; p &lt; 0.10).\n\nA slightly better approach would be to standardise the variables wt and hp before passing to the model. Standardising means trying to set the variables to a common scale, and giving the variables more similar statistical characteristics.\n\n\nCode\nstandardise &lt;- function(x){\n  (x - mean(x)) / sd(x)\n}\n\nmtcars_z &lt;- mtcars\nmtcars_z$wt_z = standardise(mtcars$wt)\nmtcars_z$hp_z = standardise(mtcars$hp)\n\nmod_logistic_2_z &lt;- glm(\n    am ~ vs + wt_z + hp_z,\n    data = mtcars_z, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2_z)\n\n\n\nCall:\nglm(formula = am ~ vs + wt_z + hp_z, family = binomial(), data = mtcars_z)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.9348     1.4500  -0.645   0.5191  \nvs           -3.1291     2.9296  -1.068   0.2855  \nwt_z         -9.4419     3.9679  -2.380   0.0173 *\nhp_z          2.2230     1.3431   1.655   0.0979 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.5012  on 28  degrees of freedom\nAIC: 16.501\n\nNumber of Fisher Scoring iterations: 8\n\n\nwt_z is the standardised version of wt, and hp_z is the standardised version of hp. By convention, whereas unstandardised coefficients are usually referred to as \\(\\beta\\) (‘beta’) coefficients, standardised coefficients are instead referred to as \\(b\\) coefficients. But really, it’s the same model.\nNote the p value of wt_z is the same as for wt, and the p value of hp_z is the same as that for hp. Note also the directions of effect are the same: the coefficients on wt and wt_z are both negative, and the coefficients of hp and hp_z are both positive.\nThis isn’t a coincidence. Of course standardising can’t really add any new information, can’t really change the relationship between a predictor and a response. It’s not really a new variable, it’s the same old variable, so the relationship between predictor and response that there used to be is still there now.\nSo why bother standardising?\nOne reason is it gives, subject to some assumptions and caveats, a way of gauging the relative importance of the two different continuous variables, by allowing a slightly more meaningful comparison between the two coefficients.\nIn this case, we have a standardised \\(b\\) coefficient of -9.44 for wt_z, and of 2.22 for hp_z. As with the unstandardised coefficients we can still assert that manualness is negatively associated with weight, and positively associated with horsepower. But now we can also compare the two numbers -9.44 and 2.22. The ratio of these two numbers is around 4.3. So, we might hazard to suggest something like:\n\na given increase in weight is around four times as important in negatively predicting manual transmission (i.e. in predicting an automatic transmission) as an equivalent increase in horsepower is in positively predicting manual transmission.\n\nThis isn’t a statement that’s easy to parse, but does at least allow slightly more information to be gleamed from the model. For example, it implies that, if a proposed change to a vehicle leads to similar relative (standardised) increases in both weight and horsepower then, as the weight effect is greater than the horsepower effect, the model will predict a decreased probability of manualness as a result.\nBut what about the motivating question, “What’s the effect of a straight engine (vs=1) on the probability of manual transmission (am=1)?”\nThe problem, unlike with the linear regression, is this is now a badly formulated question, based on an incorrect premise. The problem is with the word ‘the’, which implies there should be a single answer to this question, i.e. that the effect of vs on the probability of am=1 should always be the same. But, at least when it comes to absolute changes in the probability of am=1, this is no longer the case, as it depends on the values of the other variables in the model.\nInstead of assuming vs=1 has a single effect on P(am=1), we instead need to think about predictions of the marginal effects of vs on am in the context of other plausible values of the other predictors in the model, wt and hp. This involves asking the model a series of well formulated and specific questions.\n\n\nMaximum marginal effects: Divide-by-four\nBefore we do that, however, there’s a useful heuristic that can be employed when looking at discrete variables and using a logistic regression specification. The heuristic, which is based on the properties of the logistic function,3 is called divide-by-four. What this means is that, if we take the coefficient on vs of -3.13, and divide this value by four, we get a value of -0.78. Notice that the absolute value of -0.78 is between 0 and 1.4 What this value gives is the maximum possible effect that the discrete variable (the presence rather than absence of a straight engine) has on the probability of being a manual transmission. We can say, “a straight engine reduces the probability of a manual transmission by up to 78%”\nBut, as mentioned, this doesn’t quite answer the motivating question, it gives an upper bound to the answer, not the answer itself.5 We can instead start to get a sense of ‘the’ effect of the variable vs on P(am=1) by asking the model a series of questions.\n\n\nPredictions on a matrix\nWe can start by getting the range of observed values for the two continuous variables, hp and mpg:\n\n\nCode\nmin(mtcars$hp)\n\n\n[1] 52\n\n\nCode\nmax(mtcars$hp)\n\n\n[1] 335\n\n\nCode\nmin(mtcars$wt)\n\n\n[1] 1.513\n\n\nCode\nmax(mtcars$wt)\n\n\n[1] 5.424\n\n\nWe can then ask the model to make predictions of \\(P(am=1)\\) for a large number of values of hp and wt within the observed range, both in the condition in which vs=0 and in the condition in which vs=1. The expand_grid function6 can help us do this:\n\n\nCode\npredictors &lt;- expand_grid(\n  hp = seq(min(mtcars$hp), max(mtcars$hp), length.out = 100),\n  wt = seq(min(mtcars$wt), max(mtcars$wt), length.out = 100)\n)\n\npredictors_straight &lt;- predictors |&gt; \n  mutate(vs = 1)\n\npredictors_vshaped &lt;- predictors |&gt; \n  mutate(vs = 0)\n\n\nFor each of these permutations of inputs, we can use the model to get a conditional prediction. For convenience, we can also attach this as an additional column to the predictor data frame:\n\n\nCode\npredictions_predictors_straight &lt;- predictors_straight |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_straight)\n  )\n\npredictions_predictors_vshaped &lt;- predictors_vshaped |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_vshaped)\n  )\n\n\nWe can see how the predictions vary over hp and wt using a heat map or contour map:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  ggplot(aes(x = hp, y = wt, z = p_manual)) + \n  geom_contour_filled() + \n  facet_wrap(~vs) +\n  labs(\n    title = \"Predicted probability of manual transmission by wt, hp, and vs\"\n  )\n\n\n\n\n\nWe can also produce a contour map of the differences between these two contour maps, i.e. the effect of a straight (vs=1) compared with v-shaped (vs=0) engine, which gets us a bit closer to the answer:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled() + \n  labs(\n    title = \"Marginal effect of vs=1 given wt and hp on P(am=1)\"\n  )\n\n\n\n\n\nWe can see here that, for large ranges of wt and hp, the marginal effect of vs=1 is small. However, for particular combinations of hp and wt, such as where hp is around 200 and wt is slightly below 3, then the marginal effect of vs=1 becomes large, up to around a -70% reduction in the probability of manual transmission. (i.e. similar to the theoretical maximum marginal effect of around -78%).\nSo, what’s the effect of vs=1 on P(am=1)? i.e. how should we boil down all these 10,000 predicted effect sizes into a single effect size?\nI guess, if we have to try to answer this silly question, then we could take the average effect size…\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  summarise(\n    mean_diff_p_manual = mean(diff_p_manual)\n  )\n\n\n# A tibble: 1 × 1\n  mean_diff_p_manual\n               &lt;dbl&gt;\n1            -0.0821\n\n\nSo, we get an average difference of around -0.08, i.e. about an 8% reduction in probability of manual transmission.\n\n\nMarginal effects on observed data\nIs this a reasonable answer? Probably not, because although the permutations of wt and hp we looked at come from the observed range, most of these combinations are likely very ‘theoretical’. We can get a sense of this by plotting the observed values of wt and hp onto the above contour map:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled(alpha = 0.2, show.legend = FALSE) + \n  labs(\n    title = \"Observations from mtcars on the predicted probability surface\"\n  ) +\n  geom_point(\n    aes(x = hp, y = wt), inherit.aes = FALSE,\n    data = mtcars\n  )\n\n\n\n\n\nPerhaps a better option, then, would be to calculate an average marginal effect using the observed values, but switching the observations for vs to 1 in one scenario, and 0 in another scenario:\n\n\nCode\npredictions_predictors_observed_straight &lt;- mtcars |&gt; \n  select(hp, wt) |&gt; \n  mutate(vs = 1)\n\npredictions_predictors_observed_straight &lt;- predictions_predictors_observed_straight |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_straight)\n  )\n\npredictions_predictors_observed_vshaped &lt;- mtcars |&gt; \n  select(hp, wt) |&gt; \n  mutate(vs = 0) \n\npredictions_predictors_observed_vshaped &lt;- predictions_predictors_observed_vshaped |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_vshaped)\n  )\n  \n\npredictions_predictors_observed &lt;- \n  bind_rows(\n    predictions_predictors_observed_straight,\n    predictions_predictors_observed_vshaped\n  )\n\npredictions_marginal &lt;- \n  predictions_predictors_observed |&gt; \n    group_by(hp, wt) |&gt; \n    summarise(\n      diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n    )\n\npredictions_marginal |&gt; \n  ggplot(aes(x = diff_p_manual)) + \n  geom_histogram() +\n  geom_vline(aes(xintercept = mean(diff_p_manual)), colour = \"red\") + \n  geom_vline(aes(xintercept = median(diff_p_manual)), colour = \"green\")\n\n\n\n\n\nIn the above the red line indicates the mean value of these marginal differences, which is -0.12, and the green line the median value of these differences, which is around -0.02. So, even with just these two measures of central tendency, there’s around a six-fold difference in the estimate of ‘the effect’. We can also see there’s a lot of variation, from around nothing (right hand side), to around a 65% reduction (left hand side).\nIf forced to give a simple answer (to this overly simplistic question), we might plump for the mean for theoretical reasons, and say something like “The effect of a straight engine is to reduce the probability of a manual transmission by around an eighth”. But I’m sure, having seen how much variation there is in these marginal effects, we can agree this ‘around an eighth’ answer, or any single number answer, is likely to be overly reductive.\nHopefully, however, it is more informative than ‘statistically significant and negative’, (the stargazing approach) or ‘up to around 78%’ (the divide-by-four approach)."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-04/index.html#conclusion",
    "href": "posts/glms/lms-are-glms-part-04/index.html#conclusion",
    "title": "Part Four: why only betas just look at betas",
    "section": "Conclusion",
    "text": "Conclusion\nLinear regression tends to give a false impression about how straightforward it is to use a model to answer questions of the form “What is the effect of x on y?”. This is because, for linear regression, but few other model specifications, the answer to this question is in the \\(\\beta\\) coefficients themselves. For other model specifications, like the logistic regression example above, the correct-but-uninformative answer tends to be “it depends”, and potentially more informative answers tend to require a bit more work to derive and interpret."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-04/index.html#coming-up",
    "href": "posts/glms/lms-are-glms-part-04/index.html#coming-up",
    "title": "Part Four: why only betas just look at betas",
    "section": "Coming up",
    "text": "Coming up\nThis post concludes the first section of this blog series. We showed the importance of producing predictions from models, rather than just staring at tables of coefficients and producing qualitative ‘stargazing’ summaries of their statistical significance and direction of effect. Statistical significance of individual coefficients almost never answers questions of substantive significance, which instead come from model predictions.\nHowever in the predictions so far, we’ve accidentially pretended to know more than we do. For each prediction, despite imperfect knowledge, we’ve presented point estimates, a single prediction, implying our estimates are sometimes perfectly precise. To be more honest to the user, we should instead present a range of estimates that takes into account all the sources of uncertainty in our modelling which lead to uncertainty in our predictions.\nSection two of this blog series, starting with part five, takes us through the material necessary to go from presenting the kind of dishonest certainty of point estimates in predictions, to honest uncertainty in predictive intervals. This involves covering a lot of theoretical and methodological territory, and is fairly challenging. However we do this in order to make it easier for the end user of statistical models, decision makers, to get the kind of information they need and value most from models."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-04/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-04/index.html#footnotes",
    "title": "Part Four: why only betas just look at betas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote here I’m using \\(x_j\\), not \\(x_i\\), and that \\(X\\beta\\) is shorthand for \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) and so on. In using the \\(j\\) suffix, I’m referring to just one of the specific \\(x\\) values, \\(x_1\\), \\(x_2\\), \\(x_3\\), which is equivalent to selecting one of the columns in \\(X\\). By contrast \\(i\\) should be considered shorthand for selection of one of the rows of \\(X\\), i.e. one of the series of observations that goes into the dataset \\(D\\).↩︎\n\\(E(.)\\) is the expectation operator, and \\(|\\) indicates a condition. So, the two terms mean, respectively, what is the expected value of the outcome if the variable of interest is ‘switched on’?, and what is the expected value of the outcome if the variable of interest is ‘switched off’?↩︎\nThe logistic function maps any real number z onto the value range 0 to 1. z is \\(X\\beta\\), which in non-matrix notation is equivalent to a sum of products \\(\\sum_{k=0}^{K}x_k\\beta_k\\) (where, usually, \\(x_0\\) is 1, i.e. the intercept term). Another way of expressing this would be something like \\(\\sum_{k \\in S}x_k\\beta_k\\) where by default \\(S = \\{0, 1, 2, ..., K\\}\\). We can instead imagine partitioning out \\(S = \\{S^{-J}, S^{J}\\}\\) where the superscript \\(J\\) indicates the Jth variable, and \\(-J\\) indicates everything in \\(S\\) apart from the Jth variable. Where J is a discrete variable, the effect of J on \\(P(Y=1)\\) is \\(logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k + \\beta_J}) - logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k})\\), where \\(logistic(z) = \\frac{1}{1 + e^{-z}}\\). The marginal effect of the \\(\\beta_J\\) coefficient thus depends on the other term \\(\\sum_{k \\in S^{-J}}x_k\\beta_k\\). Where this other term is set to 0 the marginal effect of \\(\\beta_J\\) becomes \\(logistic(\\beta_J) - logistic(0)\\). According to p.82 of this chapter by Gelman we can equivalently ask the question ‘what is the first derivative of the logistic regression with respect to \\(\\beta\\)?’. Asking more about this to Wolfram Alpha we get this page of information, and scrolling down to the section on the global minimum we indeed get an absolute value of \\(\\frac{1}{4}\\), so the maximum change in \\(P(Y=1)\\) given a unit change in \\(\\beta\\) is indeed one quarter of the value of \\(\\beta\\), hence why the ‘divide-by-four’ heuristic ‘works’. This isn’t quite a full derivation, but more explanation than I was planning for a footnote! In general, it’s better just to remember ‘divide-by-four’ than go down the rabbit warren of derivation each time! (As I’ve just learned, to my cost, writing this footnote!)↩︎\nWe should always expect the absolute value of a coefficient for a discrete variable to be less than four, for this reason.↩︎\nThe lower bound for the marginal effect of a discrete variable, or any variable, is zero. This is when the absolute value of the sum of the product of the other variables is infinite.↩︎\nOr the base R expand.grid function↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-02/index.html",
    "href": "posts/glms/lms-are-glms-part-02/index.html",
    "title": "Part Two: Systematic components and link functions",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-02/index.html#tldr",
    "href": "posts/glms/lms-are-glms-part-02/index.html#tldr",
    "title": "Part Two: Systematic components and link functions",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-02/index.html#part-2-systematic-components-and-link-functions",
    "href": "posts/glms/lms-are-glms-part-02/index.html#part-2-systematic-components-and-link-functions",
    "title": "Part Two: Systematic components and link functions",
    "section": "Part 2: Systematic components and link functions",
    "text": "Part 2: Systematic components and link functions\nIn part 1 of this series we introduced the following general framework for thinking about statistical models and what they contain.\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\] The terminology are as described previously.\nThese equations are too broad and abstract to be implemented directly. Instead, specific choices about the \\(f(.)\\) and \\(g(.)\\) need to be made. King, Tomz, and Wittenberg (2000) gives the following examples:\nLogistic Regression\n\\[\nY_i \\sim Bernoulli(\\pi_i)\n\\]\n\\[\n\\pi_i = \\frac{1}{1 + e^{-X_i\\beta}}\n\\]\nLinear Regression\n\\[\nY_i \\sim N(\\mu_i, \\sigma^2)\n\\] \\[\n\\mu_i = X_i\\beta\n\\]\nSo, what’s so special about linear regression, in this framework?\nIn one sense, not so much. It’s got a systematic component, and it’s got a stochastic component. But so do other models. But in another sense, quite a lot. It’s a rare case where the systematic component, \\(g(.)\\), doesn’t transform its inputs in some weird and wonderful way. We can say that \\(g(.)\\) is the identity transform, \\(I(.)\\), which in words means take what you’re given, do nothing to it, and pass it on.\nBy contrast, the systematic component for logistic regression is known as the logistic function. \\(logistic(x) := \\frac{1}{1 + e^{-x}}\\) It transforms inputs that could be anywhere on the real number line to values that lay somewhere between 0 and 1. Why 0 to 1? Because what logistic regression models produce aren’t predicted values, but predicted probabilities, and nothing can be more probable than certain (1) or less probable than impossible (0).\nWe can compare the transformations used in linear and logistic regression as follows:1\n\n# Define transformations\nident &lt;- function(x) {x}\nlgt &lt;- function(x) {1 / (1 + exp(-x))}\n\n\n# Draw the associations\ncurve(ident, -6, 6,\n      xlab = \"x (before transform)\",\n      ylab = \"z (after transform)\",\n      main = \"The Identity 'Transformation'\"\n      )\ncurve(lgt, -6, 6, \n      xlab = \"x (before transform)\", \n      ylab = \"z (after transform)\",\n      main = \"The Logistic Transformation\"\n      )\n\n\n\n\n\n\nIdentity Transformation\n\n\n\n\n\n\n\nLogistic Transformation\n\n\n\n\n\n\nThe usual input to the transformation function \\(g(.)\\) is a sum of products. For three variables, for example, this could be \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). In matrix algebra this generalises to \\(\\boldsymbol{X\\beta}\\) , where \\(\\boldsymbol{X}\\) is the predictor data whose rows are observations, columns are variables, and whose first column is a vector of 1s (for the intercept term). The \\(\\boldsymbol{\\beta}\\) term is a row-wise vector comprising each specific \\(\\beta\\) term, such as \\(\\boldsymbol{\\beta} = \\{ \\beta_0, \\beta_1, \\beta_2 \\}\\) in the three variable example above.\nWhat’s special about the identity transformation, and so linear regression, is that there is a fairly clear correspondence between a \\(\\beta_j\\) term and the estimated influence of changing a predictor variable \\(x_j\\) on the predicted outcome \\(Y\\), i.e. the ‘effect of \\(x_j\\) on \\(Y\\)’. For other transformations this tends to not be the case.\nWe’ll delve into how this is implemented in practice in part 3."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-02/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-02/index.html#footnotes",
    "title": "Part Two: Systematic components and link functions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsing some base R graphics functions as I’m feeling masochistic↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-11/index.html",
    "href": "posts/glms/lms-are-glms-part-11/index.html",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "",
    "text": "The first four posts in this series formed ‘section one’, in which the focus was on framing the challenge of fitting (almost) all statistical models from the perspective of likelihood theory; and how to go beyond ‘star-gazing’ (just look at tables of coefficients), and using statistical models to make predictions and projections that answer substantively useful questions of interest. The section closed with post four, in which the challenges of describing and producing effect estimates as quantities of interest when using logistic regression models was illustrated in some detail.\nSection two, covering posts 5-10, then delved into more detail, about what likelihood theory is, and how R uses optimisation agorithms to try to solve loss functions based on likelihood to find the best combination of model parameters to represent the core relationships that exist between variables in a dataset. Within this, we identified how information and uncertainty (two sides of the same coin) about model parameters can be estimated using the Hessian, a measure of the instantaneous curvature around the position in the optimisation’s search space that maximises the log likelihood. It’s this measure of uncertainty, including joint uncertainty and correlation between predictor variables, that allows not just prediction and projection from models, but honest prediction and projection, allowing ‘what if?’ questions to be asked of a model that takes into account of parameter uncertainty."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-11/index.html#recap",
    "href": "posts/glms/lms-are-glms-part-11/index.html#recap",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "",
    "text": "The first four posts in this series formed ‘section one’, in which the focus was on framing the challenge of fitting (almost) all statistical models from the perspective of likelihood theory; and how to go beyond ‘star-gazing’ (just look at tables of coefficients), and using statistical models to make predictions and projections that answer substantively useful questions of interest. The section closed with post four, in which the challenges of describing and producing effect estimates as quantities of interest when using logistic regression models was illustrated in some detail.\nSection two, covering posts 5-10, then delved into more detail, about what likelihood theory is, and how R uses optimisation agorithms to try to solve loss functions based on likelihood to find the best combination of model parameters to represent the core relationships that exist between variables in a dataset. Within this, we identified how information and uncertainty (two sides of the same coin) about model parameters can be estimated using the Hessian, a measure of the instantaneous curvature around the position in the optimisation’s search space that maximises the log likelihood. It’s this measure of uncertainty, including joint uncertainty and correlation between predictor variables, that allows not just prediction and projection from models, but honest prediction and projection, allowing ‘what if?’ questions to be asked of a model that takes into account of parameter uncertainty."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-11/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-11/index.html#aim",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "Aim",
    "text": "Aim\nThe purpose of this post is to move onto a new section, section three, in which we will look at some of the ways that quantities of interest - expected values, predicted values, and first differences - can be answered with a fitted model honestly, i.e. accounting for parameter uncertainty."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-11/index.html#linear-regression-example",
    "href": "posts/glms/lms-are-glms-part-11/index.html#linear-regression-example",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "Linear regression example",
    "text": "Linear regression example\nLet’s start with one of the built-in datasets, ToothGrowth, which is described as follows:\n\nThe response is the length of odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid (a form of vitamin C and coded as VC).\n\nLet’s load the dataset and visualise\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nWhat does it look like?\n\n\nCode\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    expand_limits(x = 0, y = 0)\n\n\n\n\n\nSo, although this has just three variables, there is some complexity involved in thinking about how the two predictor variables, supp and dose, relate to the response variable len. These include:\n\nWhether the relationship between len and dose is linear in a straightforward sense, or associated in a more complicated wway\nWhether supp has the same effect on len regardless of dose, or whether there is an interaction between dose and supp.\n\n\nStage one: model fitting\nWe can address each of these questions in turn, but should probably start with a model which includes both predictors:\n\n\nCode\nmod_01 &lt;- lm(len ~ dose + supp, data = df)\n\nsummary(mod_01)\n\n\n\nCall:\nlm(formula = len ~ dose + supp, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.600 -3.700  0.373  2.116  8.800 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.2725     1.2824   7.231 1.31e-09 ***\ndose          9.7636     0.8768  11.135 6.31e-16 ***\nsuppVC       -3.7000     1.0936  -3.383   0.0013 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.236 on 57 degrees of freedom\nMultiple R-squared:  0.7038,    Adjusted R-squared:  0.6934 \nF-statistic: 67.72 on 2 and 57 DF,  p-value: 8.716e-16\n\n\nEach term is statistically significant at the conventional thresholds (P &lt; 0.05), with higher doses associated with greater lengths. Compared to OJ, the reference category, a vitamin C (VC) supplement is associated with lower lengths.\nTurning to the first question, about the type of relationship between len and dose, one possibility is that greater doses lead to greater lengths, but there are diminishing marginal returns: the first mg has the biggest marginal effect, then the second mg has a lower marginal effect. An easy way to model this would be to include the log of dose in the regression model, rather than the dose itself.1 We can get a sense of whether this log dose specification might be preferred by plotting the data with a log scale on the x axis, and seeing if the points look like they ‘line up’ better:\n\n\nCode\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    scale_x_log10() + \n    expand_limits(x = 0.250, y = 0)\n\n\n\n\n\nYes, with this scaling, the points associated with the three dosage regimes look like they line up better. Let’s now build this model specification:\n\n\nCode\nmod_02 &lt;- lm(len ~ log(dose) + supp, data = df)\n\nsummary(mod_02)\n\n\n\nCall:\nlm(formula = len ~ log(dose) + supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2108 -2.9896 -0.5633  2.2842  9.1892 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.6633     0.7033   29.38  &lt; 2e-16 ***\nlog(dose)    11.1773     0.8788   12.72  &lt; 2e-16 ***\nsuppVC       -3.7000     0.9947   -3.72 0.000457 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.852 on 57 degrees of freedom\nMultiple R-squared:  0.755, Adjusted R-squared:  0.7464 \nF-statistic: 87.81 on 2 and 57 DF,  p-value: &lt; 2.2e-16\n\n\nAgain, the same kind of relationship between variables is observed: higher log dose: greater length; and VC rather than OJ is associated with lower growth. But is this model actually any better? The model summary for the linear dose model gives an adjusted \\(R^2\\) of 0.69, whereas for the log dose model the adjusted \\(R^2\\) is 0.75. So, as the data are fundamentally the same,2 this suggests it is. However, as we know that linear regression models are really just another kind of generalised linear models, and that model fitting tends to involve trying to maximise the log likelihood, we can also compare the log likelihoods of the two models, using the logLik() function, and so which is higher:\n\n\nCode\nlogLik(mod_01)\n\n\n'log Lik.' -170.2078 (df=4)\n\n\nCode\nlogLik(mod_02)\n\n\n'log Lik.' -164.5183 (df=4)\n\n\nBoth report the same number of degrees of freedom (‘df’), which shouldn’t be suprising as they involve the same number of parameters. But the log likelihood for mod_02 is higher, which like the Adjusted R-squared metric suggests a better fit.\nAnother approach, which generalises better to other types of model, is to compare the AICs, which are metrics that try to show the trade off between model complexity (based on number of parameters), and model fit (based on the log likelihood). By this criterion, the lower the score, the better the model:\n\n\nCode\nAIC(mod_01, mod_02)\n\n\n       df      AIC\nmod_01  4 348.4155\nmod_02  4 337.0367\n\n\nAs both models have exactly the same number of parameters, it should be of no surprise that mod_02 is still preferred.\nLet’s now address the second question: is there an interaction between dose and supp. This interaction term can be specified in one of two ways:\n\n\nCode\n# add interaction term explicitly, using the : symbol\nmod_03a &lt;- lm(len ~ log(dose) + supp + log(dose) : supp, data = df)\n\n# add interaction term implicitly, using the * symbol \nmod_03b &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(mod_03a)\n\n\n\nCall:\nlm(formula = len ~ log(dose) + supp + log(dose):supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(mod_03b)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see from the summaries that both ways of specifying the models lead to exactly the same model, with exactly the same estimates, standared errors, adjusted \\(R^2\\)s, and so on. The adjusted \\(R^2\\) is now 0.76, a slight improvement on the 0.75 value for the model without the interaction term. As before, we can also compare the trade-off between additional complexity and improved fit using AIC\n\n\nCode\nAIC(mod_02, mod_03a)\n\n\n        df      AIC\nmod_02   4 337.0367\nmod_03a  5 333.7750\n\n\nSo, the AIC of the more complex model is lower, suggesting a better model, but the additional improvement in fit is small.\nWe can also compare the fit, and answer the question of whether the two models can be compared, in a couple of other ways. Firstly, we can use BIC, AIC’s (usually) stricter cousin, which tends to penalise model complexity more harshly:\n\n\nCode\nBIC(mod_02, mod_03a)\n\n\n        df      BIC\nmod_02   4 345.4140\nmod_03a  5 344.2467\n\n\nEven using BIC, the more complex model is still preferred, though the difference in values is now much smaller.\nThe other way we can compare the models is using an F-test using the anova (analysis of variance) function:\n\n\nCode\nanova(mod_02, mod_03a)\n\n\nAnalysis of Variance Table\n\nModel 1: len ~ log(dose) + supp\nModel 2: len ~ log(dose) + supp + log(dose):supp\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     57 845.91                              \n2     56 774.89  1    71.022 5.1327 0.02737 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere anova compares the two models, notes that the first model can be understood as a restricted variant of the second model,3 and compares the change in model fit between the two models against the change in number of parameters used to fit the model. The key parts of the summary to look at are the F test value, 5.13, and the associated P value, which is between 0.01 and 0.05. This, again, suggests the interaction term is worth keeping.\nSo, after all that, we finally have a fitted model. Let’s look now at making some predictions from it."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-11/index.html#stage-two-model-predictions",
    "href": "posts/glms/lms-are-glms-part-11/index.html#stage-two-model-predictions",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "Stage Two: Model predictions",
    "text": "Stage Two: Model predictions\nThe simplest approach to getting model predictions is to use the predict function, passing it a dataframe of values for which we want predictions:\n\n\nCode\npredictor_df &lt;- expand_grid(\n    supp = c('VC', 'OJ'), \n    dose = seq(0.25, 2.25, by = 0.01)\n)\npreds_predictors_df &lt;- predictor_df |&gt;\n    mutate(pred_len = predict(mod_03a, predictor_df))\n\npreds_predictors_df\n\n\n# A tibble: 402 × 3\n   supp   dose pred_len\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 VC     0.25   -1.20 \n 2 VC     0.26   -0.683\n 3 VC     0.27   -0.189\n 4 VC     0.28    0.288\n 5 VC     0.29    0.748\n 6 VC     0.3     1.19 \n 7 VC     0.31    1.62 \n 8 VC     0.32    2.04 \n 9 VC     0.33    2.44 \n10 VC     0.34    2.83 \n# ℹ 392 more rows\n\n\nWe can visualise these predictions as follows, with the predicted values as lines, and the observed values as points:\n\n\nCode\npreds_predictors_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, y = pred_len, colour = supp, linetype = interextrap)) + \n    geom_line() + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\"\n    )\n\n\n\n\n\nIn the above, I’ve shown the lines as solid when they represent interpolations of the data, i.e. are in the range of measured doses, and as dashed when they represent extrapolations from the data, meaning they are are predictions made outside the range of observed values. We can see an obvious issue when we extrapolate too far to the left: for low doses, and for the VC supplement, the model predicts negative tooth lengths. Extrapolation is dangerous! And gets more dangerous the further we extrapolate from available observations.\nWe can also use the predict function to produce uncertainty intervals, either of expected values, or predicted values. By default these are 95% intervals, meaning they are expected to contain 95% of the range of expected or predicted values from the model.\nLet’s first look at expected values, which include uncertainty about parameter estimates, but not observed variation in outcomes:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"confidence\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of expected values\"\n    )\n\n\n\n\n\nAnd the following shows the equivalent prediction intervals, which also incorporate known variance, as well as parameter uncertainty:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"prediction\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of predicted values\"\n    )\n\n\n\n\n\nAs should be clear from the above, and discussion of the difference between expected and predicted values in previous posts, predicted values and expected values are very different, and it is important to be aware of the difference between these two quantities of interest. Regardless, we can see once again how dangerous it is to use this particular model specification to extrapolate beyond the range of observations, expecially for lower doses."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-11/index.html#summary-and-coming-up",
    "href": "posts/glms/lms-are-glms-part-11/index.html#summary-and-coming-up",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "Summary and coming up",
    "text": "Summary and coming up\nIn this post, we’ve started and finished building the model, and started but not quite finished using the model to generate expected and predicted values. We’ve discussed some approaches to deciding on a model specification, by incrementally comparing a series of different specifications which test different ideas we have about how the predictor variables might be related to each other, and to the response variable.\nAs we’ve done quite a lot of work on building the model, we’ve not covered everything that I was planning to in terms of model prediction, and what we can do with a linear regression model (and generalised linear regression model), beyond (yawn) stargazing, for using models to get expected values, predicted values, and especially first differences. So, I guess that’s coming up in the next post!"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-11/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-11/index.html#footnotes",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe difference between 0.5mg and 1.0mg is 0.5mg, but it’s also one doubling. The difference between 1.0mg and 2.0mg is 1.0mg, but it’s also one doubling. The effect of logging dose is to space the values by the number of doublings, not the absolute difference.↩︎\nThese metrics can be used to compare different linear regression model specifications which use the same dataset, but should not be used to compare the same or different model specifications as applied to different datasets. Some data, and the data generating processes (i.e. reality) that produce them, are just inherently more variable than others, and no amount of model calibration or additional data will be able to address this.↩︎\nIn this example, our more complex model has coefficients fit from the data for the intercept, log(dose), supp and the interaction term log(dose):supp, whereas the less complex model has coefficients fit from the data for the intercept, log(dose), and supp. This means the less complex model can be specified as a restricted version of the more complex model, where the value of the coefficient on the interaction term log(dose):supp is set to be equal to zero, rather than determined from the data. An equivalent way of phrasing and thinking about this is that the two model specifications are nested, with the restricted model nested inside the unrestricted model, which includes the interaction term. It’s this requirement for models to be nested in this way which meant that mod_01 and mod_02 could not be compared using an F-test, as neither model could be described strictly as restricted variants of the other model: they’re siblings, not mothers and daughters. However, both mod_01 and mod_02 could be compared against a common ancestor model which only includes the intercept term.↩︎"
  },
  {
    "objectID": "posts/economic-inactivity-modelling-package-readme/index.html",
    "href": "posts/economic-inactivity-modelling-package-readme/index.html",
    "title": "My Economic Inactivity Modelling Package: Informative Readme File!",
    "section": "",
    "text": "A few weeks ago, as I wasn’t using any personal or sensitive data, I decided to make the main repository where I keep my economic inactivity modelling work public, meaning in theory anyone could take a look.\nHowever (much like this blog), I didn’t tell anyone about it.\nThe repo is a bit of a mess, but it works. It’s both an R package, containing various convenience functions and lookup files, and a series of notebooks, presentations and now draft papers which make use of such functions and files through quarto. In due course, it may be a good idea to separate the package side of things from the ‘working’ repo which makes use of the package. Any suggestions how best to do this are welcome.\nThe main thing I’ve changed recently is the readme.md file. The economic inactivity project makes extensive use of Understanding Society, in order to populate the models with information on transitions from one wave to the next between the seven mutually exclusive economic inactivity states. Now, the readme.md contains information about how and where to add the relevant Understanding Society1 dataset to a local clone of the repo in order to try out the functions and package.\nTo reiterate, caveat emptor, the repo is what it is. But if you’re interested in taking a look, creating your own fork of it, cloning it, and adding the requisite data, then it’s available from this link"
  },
  {
    "objectID": "posts/economic-inactivity-modelling-package-readme/index.html#footnotes",
    "href": "posts/economic-inactivity-modelling-package-readme/index.html#footnotes",
    "title": "My Economic Inactivity Modelling Package: Informative Readme File!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe particular version of the dataset I plumped for includes British Household Panel Survey data as well, the predecessor to Understanding Society, which began in 1991. So there’s the potential to use the package to explore changes in transition probabilities and drivers thereof between states for a much longer period than I’m using to calibrate the model and explore the trends.↩︎"
  },
  {
    "objectID": "posts/wrapping-guide/index.html",
    "href": "posts/wrapping-guide/index.html",
    "title": "How to wrap presents",
    "section": "",
    "text": "Here’s the key information I’ve learned from watching too many Youtube videos on how to wrap presents over the years.\n\n\n\n\nflowchart TB\n\nstart[Presents to Wrap]\ndecision{Are they cuboid?}\nbox(Put them in a cuboid)\naction(Wrap them)\nfinish[Presents are wrapped]\n\nstart --&gt; decision\ndecision --&gt;|yes| action\ndecision --&gt;|no| box\nbox --&gt; action\naction --&gt; finish\n\n\n\n\n\n\nYou’re welcome!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "First Post",
    "section": "",
    "text": "Hi, this is my first blog post. I’m making this using Quarto, starting off by slavishly following the tutorial, then incrementally adapting it to suit my preferences.\n\nI’m even keeping the default image of the first blog post. It’s not dissimilar to what I’m actually looking at!"
  },
  {
    "objectID": "posts/glm-series/index.html",
    "href": "posts/glm-series/index.html",
    "title": "GLMs: My first series",
    "section": "",
    "text": "I now have four fairly technical posts that form part of a series on understanding statistical modelling from a generalised linear regression (GLM) perspective. This series is far from complete, but is complete enough that it should be fairly useful to intrepid readers.\nTo make it easier to find this series, I’ve now created a page for links just to entries in this series. To see this, just look on the top left of this site, and click on ‘generalised linear models’."
  },
  {
    "objectID": "posts/circular-reasoning/index.html",
    "href": "posts/circular-reasoning/index.html",
    "title": "I got permanently banned from a politics forum for mentioning how circles work",
    "section": "",
    "text": "A month or so back, I was skimming a far-left-wing online politics forum, where a visual metaphor was presented to illustrate why it’s important that nominally left-wing parties and institutions be genuinely left-wing, and not the insidious pretenders to the left disparagingly known as ‘liberals’. Verbally, the argument was something as follows:\nSo, in this story, both the right and the liberal faux left are complicit in the horrible direction that modern society is taking. The liberals are at least as much to blame as the media, and so we hate them both.\nBut let’s first try to draw this visual metaphor, starting with just a unit circle:\nCode\nlibrary(tidyverse)\norigin &lt;- c(x= 0, y = 0)\n\nradius &lt;- 1\n\nangles &lt;- seq(0, 2*pi, length.out = 200)\n\nxpos &lt;- cos(angles)\nypos &lt;- sin(angles)\n\ndf &lt;- tibble(angle = angles, x = xpos, y = ypos)\n\ndf |&gt;\n    ggplot(aes(x=x, y = y)) + \n        geom_path(linewidth = 1) + \n        coord_equal() +\n        theme_minimal()\nNow let’s think about how to draw the teeth:\nCode\n# For a sawtooth circle, the radius will be made of two components: \n# r_main, for the inner part of the circle, which does not change\n# r_saw, the amount the circle extends beyond r_main at different angles\n\ngen_coords &lt;- function(angles, r_main = 1, r_saw_max = 1, n_teeth = 36) {\n\n    gen_saw_protrusion &lt;- function(r_saw_max, angles, n_teeth) {\n        r_saw_max  * angles %% (2*pi / n_teeth)\n    }\n\n    x &lt;- (r_main + gen_saw_protrusion(r_saw_max, angles, n_teeth) ) * cos(angles)\n    y &lt;- (r_main + gen_saw_protrusion(r_saw_max, angles, n_teeth) ) * sin(angles)\n\n    tibble(angle = angles, x = x, y = y)\n}\n\nangles &lt;- seq(0, 2*pi, length.out = 2000)\n\nsawcircle_coords &lt;- gen_coords(angles)\n\n\nsawcircle_coords |&gt;\n    ggplot(aes(x=x, y = y)) + \n        geom_path(linewidth = 1) + \n        coord_equal() +\n        theme_minimal()\nNow let’s label it:\nCode\nrw_angles &lt;- seq(20 * pi / 180, 70 * pi / 180, length.out = 100) \nrw_curve &lt;- tibble(\n    x = 1.1 * cos(rw_angles),\n    y = 1.1 * sin(rw_angles)\n)\n\nsawcircle_coords |&gt;\n    ggplot(aes(x=x, y = y)) + \n        geom_path(linewidth = 1) + \n        coord_equal() +\n        theme_minimal() + \n        annotate('rect', xmin = -1.2, xmax=0, ymin=0, ymax=1.2, fill = 'red', alpha = 0.1) + \n        annotate('rect', xmin=0, xmax = 1.2, ymin=0, ymax=1.2, fill = 'blue', alpha = 0.1) + \n        annotate('rect', xmin=0, xmax=1.2, ymin = -1.2, ymax = 0, fill = 'blue', alpha = 0.2) + \n        annotate('rect', xmin=-1.2, xmax=0, ymin=-1.2, ymax=0, fill = 'red', alpha = 0.2 ) + \n        annotate('text', x = -0.5, y = 0.25, label = \"centre left\") + \n        annotate('text', x = 0.5, y = 0.25, label = 'centre right') + \n        annotate('text', x = 0.5, y = -0.25, label = 'far right') + \n        annotate('text', x = -0.5, y = -0.25, label = 'far left') + \n        annotate('rect', xmin=-0.70, xmax=-0.95, ymin=0.70, ymax = 0.95, fill = 'red') + \n        annotate('text', x = -0.45, y = 0.58, colour = 'red', label = 'LIBERAL\\n BLOCKERS!') + \n        geomtextpath::geom_labelpath(aes(x = x, y = y), inherit.aes = FALSE, \n            data = rw_curve, \n            label = \"RW Media!\",\n            color = \"blue\", \n            arrow = arrow(ends = \"first\")\n\n        )\nYes. That’s pretty much how the image looked."
  },
  {
    "objectID": "posts/circular-reasoning/index.html#banning-offence",
    "href": "posts/circular-reasoning/index.html#banning-offence",
    "title": "I got permanently banned from a politics forum for mentioning how circles work",
    "section": "Banning offence…",
    "text": "Banning offence…\nSo, why did I get permanently banned from the forum that promoted this visual metaphor?\nWell, I tend to take metaphors very seriously. So I asked something like the following:\n\nDoesn’t this model suggest that some on the far right will become far left?\n\nWhich, if this political circle works like any other kind of circle, would seem to be the case…\n\n\nCode\nor_angles &lt;- seq(240 * pi / 180, 300 * pi / 180, length.out = 100) \nor_curve &lt;- tibble(\n    x = 0.95 * cos(or_angles),\n    y = 0.95 * sin(or_angles)\n)\n\n\nsawcircle_coords |&gt;\n    ggplot(aes(x=x, y = y)) + \n        geom_path(linewidth = 1) + \n        coord_equal() +\n        theme_minimal() + \n        annotate('rect', xmin = -1.2, xmax=0, ymin=0, ymax=1.2, fill = 'red', alpha = 0.1) + \n        annotate('rect', xmin=0, xmax = 1.2, ymin=0, ymax=1.2, fill = 'blue', alpha = 0.1) + \n        annotate('rect', xmin=0, xmax=1.2, ymin = -1.2, ymax = 0, fill = 'blue', alpha = 0.2) + \n        annotate('rect', xmin=-1.2, xmax=0, ymin=-1.2, ymax=0, fill = 'red', alpha = 0.2 ) + \n        annotate('text', x = -0.5, y = 0.25, label = \"centre left\") + \n        annotate('text', x = 0.5, y = 0.25, label = 'centre right') + \n        annotate('text', x = 0.5, y = -0.25, label = 'far right') + \n        annotate('text', x = -0.5, y = -0.25, label = 'far left') + \n        annotate('rect', xmin=-0.70, xmax=-0.95, ymin=0.70, ymax = 0.95, fill = 'red') + \n        annotate('text', x = -0.45, y = 0.58, colour = 'red', label = 'LIBERAL\\n BLOCKERS!') + \n        geomtextpath::geom_labelpath(aes(x = x, y = y), inherit.aes = FALSE, \n            data = or_curve, \n            label = \"Radicalised!\",\n            color = \"black\", \n            arrow = arrow(ends = \"first\")\n        ) + \n        geomtextpath::geom_labelpath(aes(x = x, y = y), inherit.aes = FALSE, \n            data = rw_curve, \n            label = \"RW Media!\",\n            color = \"blue\", \n            arrow = arrow(ends = \"first\")\n        ) + \n        geom_point(aes(x = x, y = y), inherit.aes= FALSE, \n        data = or_curve[1,], colour = \"darkred\", shape = 15, size = 3) + \n        geom_point(aes(x = x, y = y), inherit.aes= FALSE, \n        data = or_curve[nrow(or_curve),], colour = \"darkblue\", shape = 16, size = 3)\n\n\n\n\n\nYup. That’s how circles work.\nBut apparently mentioning this gets you banned!"
  },
  {
    "objectID": "posts/unrepentent-confessional/index.html",
    "href": "posts/unrepentent-confessional/index.html",
    "title": "David Sederis: Humourists as Unrepentent Observational Confessionals",
    "section": "",
    "text": "Whereas I devoted much of the 2022 Christmas break to (re?)learning Bayesian modelling, and over 2023 took four months off to learn software development, over the 2023 Christmas break I vowed to do nothing useful, nothing that involves thinking or effort or learning, in order to make the break as restful and uneventful as I could.\nHowever, as anyone who’s tried sitting with their own thoughts for more than a couple of minutes realises, it’s not really possible to avoid thinking about things, and attending to one’s own thoughts is often far from restful. Attempting to sit mindfully for more than a minute puts the lie to the very idea that I have thoughts. Instead the reverse is true: thoughts have us.\nAttempting to sit passively with one’s own thoughts is to attempt to recognise the eternal turbulence of what Rational Mystic Sam Harris keeps referring to as the ‘field of consciousness’. It’s like standing on a worn wet stone, surrounded by eddies and currents, and being invited to dip a toe into the waters without falling in. Failing at this task is the default state of cognition, it seems, especially (possibly uniquely?) for humans. Even if we stay mindful enough to notice that thoughts just appear to us, unbidden, without some kind of humuncular avatar of the self causing or driving them, the temptation of following a thought, of engaging with it, is something that is near impossible to resist. Drop into a thought, and it takes us to another thought, then another, then another, and before long we find ourselves thinking about cheese when we meant to think about spreadsheets, or Nazis when we meant to think about buying Christmas cards, or stuck in a ruminatory cycle where a complex of three or more thoughts cycle back onto each other in some kind of unstoppable mental vortex.\nInvariably, the thoughts we have, or rather the thoughts that have us, lead us into some dark, surreal or smelly places. We seldom report to others the mental journeys we went on to retrieve information that’s actionable or useful to others. We self-curate, and self-censor, displaying the pearl of wisdom we’ve found, without recounting the sadistic joy we felt ripping the oyster that contained it apart and ending its innocent life. We wipe off the stink and the gore accrued in the journeys we’ve been on inside our own heads, and instead try to present to others a pristine edit of these journeys, and we pretend these curated edits are our ‘true self’. And almost everyone else feels compelled to do the same, because we do, and we do, because everyone else does.\nAll this is a rambling, (mindful?) way of saying, I read David Sedaris’ latest book Happy-Go-Lucky over the Christmas break, and like his other books and stories found it humourous (as intended), engaging and refreshing as an example of how to tell stories that don’t aim to build and sell a pristine curated self to others. Though the term ‘humourist’ (or rather ‘humorist’) has been used to describe Sedaris’ outputs and the genre he dominates so effectively, a more accurate term may be “unrepentant observational confessional”.\nSedaris’ stories are finely crafted, polished, and I’m sure edited and reedited many times, often using, like most stand-up comedians, the involuntary and unvarnished feedback from audiences (such as whether people laugh) as a guide on how to improve his texts further. But they are not stories in service to the promotion of a pristine and virtuous self, a paragon to live by. No, they’re stylised records of ‘things that happen’, inside and out, and the relationship between occurrances, thoughts, feelings, and actions. And importantly, though stylised, they have a sense of verisimilitude to the inner world in which thoughts and associations are often far from linear or pristine, whether it be noting that crowds shouting “Black Lives Matter” do so with a quality and cadence of a fishmonger selling “Fresh-Caught Haddock”, the private vilification of those who move too slowly in queues, or Sedaris’ recognition that the deaths of his mentally ill sister, or homophobic father, did not evoke in him the quality of grief that a ‘good person’ ought to experience.\nHumour often comes from distance: the distance in time needed to sublimate tragedy into comedy (‘too soon?’), but also the distance between expectation and reality. The Pristine Self creates this expectation, whereas careful observation and honest accounting reveals a reality that near invariably falls short of this expectation. Sedaris’ stories work as humour because he knows how much most people edit the stories they tell themselves and others about themselves in order to maintain the Pristine Self, but he doesn’t. Instead he edits to make the blacks darker, the smells skinkier, and the circles his mind wanders in more eccentric."
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html",
    "href": "posts/edinbr-pair-programming/index.html",
    "title": "Edinbr Pair Programming",
    "section": "",
    "text": "This is a blog post where we’ve covered a pair programming exercise completed as part of the EdinbR group.\n\nThis session [was]… led by Dr Brittany Blankinship and Dr Kasia Banas, two academics from the Data Driven Innovation for Health and Social Care Talent Team, based at the Usher Institute. Brittany and Kasia are avid R programmers and data science educators.\n\nThe link to the dataset and exercise description is here\nThere are two datasets. We are looking at one related to Madrid (and maybe why we shouldn’t go there as it’s polluted?!)"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#intro",
    "href": "posts/edinbr-pair-programming/index.html#intro",
    "title": "Edinbr Pair Programming",
    "section": "",
    "text": "This is a blog post where we’ve covered a pair programming exercise completed as part of the EdinbR group.\n\nThis session [was]… led by Dr Brittany Blankinship and Dr Kasia Banas, two academics from the Data Driven Innovation for Health and Social Care Talent Team, based at the Usher Institute. Brittany and Kasia are avid R programmers and data science educators.\n\nThe link to the dataset and exercise description is here\nThere are two datasets. We are looking at one related to Madrid (and maybe why we shouldn’t go there as it’s polluted?!)"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#packages",
    "href": "posts/edinbr-pair-programming/index.html#packages",
    "title": "Edinbr Pair Programming",
    "section": "Packages",
    "text": "Packages\nWe will be using the tidyverse set of packages:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-1",
    "href": "posts/edinbr-pair-programming/index.html#task-1",
    "title": "Edinbr Pair Programming",
    "section": "Task 1",
    "text": "Task 1\nTo begin with, load the madrid_pollution.csv data set into your R environment. Assign the data to an object called madrid.\n\ndata_url &lt;- \"https://raw.githubusercontent.com/bblankinship/EdinbRTalk-2024-02-23/main/madrid_pollution.csv\"\n\ndta &lt;- read_tsv(data_url)\n\nRows: 51864 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr   (1): mnth\ndbl  (15): BEN, CO, EBE, MXY, NMHC, NO_2, NOx, OXY, O_3, PM10, PXY, SO_2, TC...\ndttm  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndta\n\n# A tibble: 51,864 × 17\n   date                  BEN     CO   EBE   MXY   NMHC  NO_2   NOx   OXY   O_3\n   &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2001-08-01 01:00:00 1.5   0.340  1.49   4.10 0.07    56.2  75.2 2.11  42.2 \n 2 2001-08-01 02:00:00 0.870 0.0600 0.880  2.41 0.01    29.7  31.4 1.20  56.5 \n 3 2001-08-01 03:00:00 0.660 0.02   0.610  1.60 0.01    22.8  22.5 0.800 64.1 \n 4 2001-08-01 04:00:00 0.470 0.0400 0.410  1    0.02    31.6  34.8 0.470 60.8 \n 5 2001-08-01 05:00:00 0.600 0.0400 0.670  1.68 0.01    30.9  32.5 0.740 65.6 \n 6 2001-08-01 06:00:00 0.520 0.0900 0.460  1.27 0.01    66.7  78.0 0.590 41.7 \n 7 2001-08-01 07:00:00 0.540 0.120  0.510  1.33 0.01    69.7  85.3 0.630 32.6 \n 8 2001-08-01 08:00:00 0.910 0.430  0.730  1.91 0.0600  97.8 139.  0.970 17.0 \n 9 2001-08-01 09:00:00 1.62  0.75   1.29   3.40 0.120  108   177   1.65   8.36\n10 2001-08-01 10:00:00 2.85  1.65   2.60   6.89 0.210  110.  202.  3.27   8.38\n# ℹ 51,854 more rows\n# ℹ 7 more variables: PM10 &lt;dbl&gt;, PXY &lt;dbl&gt;, SO_2 &lt;dbl&gt;, TCH &lt;dbl&gt;, TOL &lt;dbl&gt;,\n#   year &lt;dbl&gt;, mnth &lt;chr&gt;\n\n\nFirst thing we realised, it’s not really a CSV."
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-2",
    "href": "posts/edinbr-pair-programming/index.html#task-2",
    "title": "Edinbr Pair Programming",
    "section": "Task 2",
    "text": "Task 2\nNow that the data is loaded in R, create a scatter plot that compares ethylbenzene (EBE) values against the date they were recorded. This graph will showcase the concentration of ethylbenzene in Madrid over time. As usual, label your axes:\n\nx = Date\ny = Ethylbenzene (μg/m³)\n\nAssign your answer to an object called EBE_pollution.\n\nEBE_pollution &lt;- dta |&gt;\n  ggplot(aes(date, EBE)) +\n  geom_point(alpha = 0.4) +\n  scale_x_datetime() +\n  scale_y_log10() +\n  labs(x = \"Date\", y = \"Ethylbenzene (μg/m³)\")\n\nWhat is your conclusion about the level of EBE over time?\nLooks like it has a seasonal pattern."
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-3",
    "href": "posts/edinbr-pair-programming/index.html#task-3",
    "title": "Edinbr Pair Programming",
    "section": "Task 3",
    "text": "Task 3\nThe question above asks you to write out code that allows visualization of all EBE recordings in the dataset - which are taken every single hour of every day. Consequently the graph consists of many points and appears densely plotted. In this question, we are going to clean up the graph and focus on max EBE readings from each month. Create a new data set with maximum EBE reading from each month in each year. Save your new data set as madrid_pollution.\n\nmadrid_pollution &lt;-\n    dta |&gt;\n        mutate(\n            mnth = month(date, label = TRUE),\n            yr   = year(date)\n        ) |&gt;\n        group_by(\n            yr, mnth\n        ) |&gt;\n        summarise(\n            max_ebe = max(EBE, na.rm = TRUE)\n        ) |&gt;\n        ungroup()\n\n`summarise()` has grouped output by 'yr'. You can override using the `.groups`\nargument.\n\nmadrid_pollution\n\n# A tibble: 72 × 3\n      yr mnth  max_ebe\n   &lt;dbl&gt; &lt;ord&gt;   &lt;dbl&gt;\n 1  2001 Jan     11.7 \n 2  2001 Feb     18.9 \n 3  2001 Mar     15.6 \n 4  2001 Apr     12.5 \n 5  2001 May     14.7 \n 6  2001 Jun     12.0 \n 7  2001 Jul     73.0 \n 8  2001 Aug      8.39\n 9  2001 Sep     10.1 \n10  2001 Oct     39.8 \n# ℹ 62 more rows"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-4",
    "href": "posts/edinbr-pair-programming/index.html#task-4",
    "title": "Edinbr Pair Programming",
    "section": "Task 4",
    "text": "Task 4\nPlot the new maximum EBE values versus the month they were recorded, split into side-by-side plots for each year.\nAssign your answer to an object called madrid_plot.\n\nmadrid_plot &lt;- madrid_pollution |&gt;\n  ggplot(aes(mnth, max_ebe)) +\n  geom_col() +\n  scale_y_log10() +\n  labs(x = \"Month\", y = \"Maximum Ethylbenzene (μg/m³)\") +\n  facet_wrap(~ yr) + \n  coord_flip()\n\nmadrid_plot"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-5",
    "href": "posts/edinbr-pair-programming/index.html#task-5",
    "title": "Edinbr Pair Programming",
    "section": "Task 5",
    "text": "Task 5\nNow we want to see which of the pollutants has decreased the most in 2001.\nAssign your answer to an object called pollution_2001.\n\ndta |&gt;\n    filter(year(date) == 2001) |&gt;\n    select(-year, -mnth) |&gt;\n    pivot_longer(-date, names_to = \"pollutant\", values_to = \"value\") |&gt;\n    group_by(pollutant) |&gt;\n    summarise(\n        max_val = max(value, na.rm = TRUE),\n        min_val = min(value, na.rm = TRUE)\n    ) |&gt;\n    mutate(diff = max_val - min_val) |&gt; \n    arrange(desc(diff))\n\n# A tibble: 14 × 4\n   pollutant max_val min_val    diff\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 NOx       1416      5.13  1411.  \n 2 NO_2       271.     5.03   266.  \n 3 PM10       266.     0.820  265.  \n 4 TOL        243.     0.360  243.  \n 5 O_3        173.     4.98   168.  \n 6 SO_2       137.     0.190  137.  \n 7 PXY        103      0.150  103.  \n 8 OXY        103      0.190  103.  \n 9 MXY         93.1    0.230   92.9 \n10 EBE         77.3    0.140   77.1 \n11 BEN         49.9    0.100   49.8 \n12 CO          10.4    0.01    10.4 \n13 TCH          4.77   0.760    4.01\n14 NMHC         2.42   0        2.42\n\n\nThoughts:\nThis is an ambiguous question. We’ve chosen to answer it in a way that doesn’t make sense but is quick to solve!"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-6",
    "href": "posts/edinbr-pair-programming/index.html#task-6",
    "title": "Edinbr Pair Programming",
    "section": "Task 6",
    "text": "Task 6\nNow repeat what you did for Task 5, but filter for 2006 instead. Assign your answer to an object called pollution_2006.\n\npollution_2006 &lt;- dta |&gt;\n    filter(year(date) == 2006) |&gt;\n    select(-year, -mnth) |&gt;\n    pivot_longer(-date, names_to = \"pollutant\", values_to = \"value\") |&gt;\n    group_by(pollutant) |&gt;\n    summarise(\n        max_val = max(value, na.rm = TRUE),\n        min_val = min(value, na.rm = TRUE)\n    ) |&gt;\n    mutate(diff = max_val - min_val) |&gt; \n    arrange(desc(diff))\n\npollution_2006\n\n# A tibble: 14 × 4\n   pollutant  max_val min_val     diff\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 NOx       1274      6.68   1267.   \n 2 NO_2       287.     5.45    282.   \n 3 PM10       269.     0.660   268.   \n 4 O_3        132      3.20    129.   \n 5 TOL         64.8    0.160    64.7  \n 6 SO_2        66.2    5.59     60.6  \n 7 MXY         54.9    0.260    54.6  \n 8 OXY         31.4    0.310    31.1  \n 9 PXY         27.0    0.300    26.7  \n10 EBE         20.0    0.210    19.8  \n11 BEN         16.9    0.150    16.7  \n12 CO           3.48   0.140     3.34 \n13 TCH          2.84   1.08      1.76 \n14 NMHC         0.970  0.0800    0.890"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-7",
    "href": "posts/edinbr-pair-programming/index.html#task-7",
    "title": "Edinbr Pair Programming",
    "section": "Task 7",
    "text": "Task 7\nWhich pollutant decreased by the greatest magnitude between 2001 and 2006? Come up with a programmatic solution\nWe’ll interpret this as the average for the year for each pollutant in both years.\n\ndta |&gt; \n    filter(year %in% c(2001, 2006)) |&gt;\n    select(-date, -mnth) |&gt;\n    pivot_longer(-year, names_to = \"pollutant\", values_to = \"value\") |&gt;\n    group_by(year, pollutant) |&gt;\n    summarise(mean_value = mean(value, na.rm = TRUE)) |&gt;\n    ungroup() |&gt; \n    group_by(pollutant) |&gt;\n    summarise(\n        change = mean_value[year == 2006] - mean_value[year == 2001]\n    ) |&gt;\n    ungroup() |&gt;\n    arrange(change)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 14 × 2\n   pollutant   change\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 NOx       -52.4   \n 2 NO_2      -11.2   \n 3 TOL        -9.53  \n 4 SO_2       -4.18  \n 5 MXY        -3.70  \n 6 BEN        -1.76  \n 7 PXY        -1.63  \n 8 EBE        -1.59  \n 9 OXY        -1.55  \n10 CO         -0.445 \n11 TCH        -0.103 \n12 NMHC        0.0363\n13 O_3         1.66  \n14 PM10        3.14  \n\n\nNOTE: This worksheet has been adapted from Data Science: First Introduction Worksheets, by Tiffany Timbers, Trevor Campbell and Melissa Lee, available at https://worksheets.datasciencebook.ca/"
  },
  {
    "objectID": "posts/talk-at-edinbr/index.html",
    "href": "posts/talk-at-edinbr/index.html",
    "title": "EdinbR talk on modelling economic (in)activity transitions",
    "section": "",
    "text": "Yesterday I had the great privilege of being one of two speakers at the Edinburgh R Users group, called EdinbR. (Difficult to say without sounding like a pirate.)\nI spoke through some of the modelling and conceptual challenges involved in trying to model the effect that various drivers/factors/exposures have on how many people in the UK become economically inactive, especially economically inactive for reasons of long-term sickness.\nThe talk seemed to go well (though perhaps the speaker’s always the last person qualified to judge), even though some of the algebra didn’t render correctly. (Which unfortunately means I also used algebra.)\nLike this blog, the presentation also made use of Quarto, but in the presentation’s case using reveal.js.\nThe presentation is available, for those intrepid souls interested in seeing something with R code and algebra, here."
  },
  {
    "objectID": "tardy-tuesday.html",
    "href": "tardy-tuesday.html",
    "title": "Tardy Tuesdays",
    "section": "",
    "text": "For the last couple of years, I’ve run an informal weekly R training session to support people who want to develop and maintain their R skills in matters of data tidying and analysis. By default the sessions are based around the weekly TidyTuesday data challenges developed to support people using R through the tidyverse paradigm promoted in R for Data Science. Each week, TidyTuesday makes a new open source dataset available; we give ourselves one hour, with no preparation, to see what we can learn from the data.\nBecause, due to scheduling conflicts, Tuesdays tend to be especially busy, the sessions actually take place first thing on Wednesday mornings. Hence, Tardy Tuesdays! The scripts developed since I’ve been using Quarto for blogging, with attendees listed as authors (generally in alphabetical order) are shown below:"
  },
  {
    "objectID": "tardy-tuesday.html#introduction",
    "href": "tardy-tuesday.html#introduction",
    "title": "Tardy Tuesdays",
    "section": "",
    "text": "For the last couple of years, I’ve run an informal weekly R training session to support people who want to develop and maintain their R skills in matters of data tidying and analysis. By default the sessions are based around the weekly TidyTuesday data challenges developed to support people using R through the tidyverse paradigm promoted in R for Data Science. Each week, TidyTuesday makes a new open source dataset available; we give ourselves one hour, with no preparation, to see what we can learn from the data.\nBecause, due to scheduling conflicts, Tuesdays tend to be especially busy, the sessions actually take place first thing on Wednesday mornings. Hence, Tardy Tuesdays! The scripts developed since I’ve been using Quarto for blogging, with attendees listed as authors (generally in alphabetical order) are shown below:"
  },
  {
    "objectID": "posts/andy-weir-utopian-engfi/index.html",
    "href": "posts/andy-weir-utopian-engfi/index.html",
    "title": "Why can’t we just get on with making and fixing stuff?",
    "section": "",
    "text": "The Martian, 2015 film\n\n\nI tend to read non-fiction rather than fiction, but some of the novels I have consumed over the last few years are those by Andy Weir, whose 2011 debut novel The Martian was adapted into the 2015 film of the same name directed by Ridley Scott and starring Matt Damon. So far he’s published three print novels: The Martian (2011, 2014); Artemis (2017), and Project Hail Mary (2021)\nWeir’s stories are often categorised as science fiction, and as a first approximation that’s not inaccurate. However, I think science fiction doesn’t quite do justice to what’s distinct about Weir’s novels, or at least substantial parts of them. Instead, I think the genre of Weir’s novels are better understood as engineering fiction, or Eng-Fi.\nWhat are the key features of Eng-Fi? I think they include the follows:\n\nA strong focus on a protagonist operating in a universe governed by physical laws that closely approximate our own. 1\nA protagonist with a deep, broad and applied knowledge of how technologies and tools work, grounded in an understanding of the physical laws and scientific processes such technologies are known to utilise in order to work.\nThe narrative confabulation of situations in which the protagonist is required to solve a series of engineering challenges to avoid serious and adverse outcomes, with suboptimal time constraints and resources.2\nBodging, failure, and iteration: The interim solution to one engineering challenge has some flaws, which then leads to a problem, requiring another engineering solution.\nWorkings out: Very detailed and extensive discussions about how the protagonist went about trying to solve engineering challenges, including the many times they failed. So, the narrative equivalent of focusing at least as much on the methods and supplementary appendix sections of a scientific paper, as well as any scientific lab books and notes, as the final published results.\n\nThough grounded largely in physical reality, Weir’s novels often appear strangely utopian when it comes to social and psychological realities. This includes:\n\nValourisation of scientists and engineers: People who can ‘do stuff’ and ‘make stuff’, and understand the methods and techniques required to solve complex problems, are valued and valourised by society as a whole.\nPolitically effective technocracies: There are enough technically and scientifically capable people in positions of political power to be able to understand that something is a problem in need of a solution, and to devote sufficient resource and expertise to it being solved.\nProblem-solving and discovery as a universal human aspiration: The idea that people should try to understand the physical world well enough to solve complex problems, rather than just (say) write poetry about the human condition, is taken as a given.\nSpace-faring: Willingness to devote substantial public and private resource to space flight and colonisation, without (say) political pushback arguing that such resources should be better spent on either welfare or warfare, is also just taken as a given.\nCommon humanity: In situations of mutual peril and opportunity, the human race will come together to meet their collective challenges.\n\nSo, Weir’s Eng-Fi seems to combine physical realism with psycho-sociological utopianism. This might sound like a criticism, but in terms of compelling story-telling, it’s not! Not all of Weir’s novels are pure Eng-Fi. I’d categorise them as follows:\n\nThe Martian:3 90% Eng-Fi; 10% Sci-Fi\nArtemis: 35% Eng-Fi; 15% Sci-Fi; 25% Noir4; 25% Heist5\nProject Hail Mary: 55% Eng-Fi; 45% Sci-Fi\n\nAnd of the three, it’s the two with more Eng-Fi I’ve found the more compelling, both because of their attempted commitment to physical realism, but also because of their arguably less realistic portrayal of politics, psychology and sociology. In Weir’s stories, when push comes to shove, it’s the engineers and scientists, not the politicians and generals, who are called on to save the day. As compared with the dystopian sci-fi that predominates, in which humanity is both the catalyst - usually through its influence on the climate - and also accelerant - usually through its pettyminded tribal politicking - of its own demise, there’s something beautifully, poignantly hopeful about the collective sensemaking expressed in Weir’s stories."
  },
  {
    "objectID": "posts/andy-weir-utopian-engfi/index.html#andy-weir-and-eng-fi",
    "href": "posts/andy-weir-utopian-engfi/index.html#andy-weir-and-eng-fi",
    "title": "Why can’t we just get on with making and fixing stuff?",
    "section": "",
    "text": "The Martian, 2015 film\n\n\nI tend to read non-fiction rather than fiction, but some of the novels I have consumed over the last few years are those by Andy Weir, whose 2011 debut novel The Martian was adapted into the 2015 film of the same name directed by Ridley Scott and starring Matt Damon. So far he’s published three print novels: The Martian (2011, 2014); Artemis (2017), and Project Hail Mary (2021)\nWeir’s stories are often categorised as science fiction, and as a first approximation that’s not inaccurate. However, I think science fiction doesn’t quite do justice to what’s distinct about Weir’s novels, or at least substantial parts of them. Instead, I think the genre of Weir’s novels are better understood as engineering fiction, or Eng-Fi.\nWhat are the key features of Eng-Fi? I think they include the follows:\n\nA strong focus on a protagonist operating in a universe governed by physical laws that closely approximate our own. 1\nA protagonist with a deep, broad and applied knowledge of how technologies and tools work, grounded in an understanding of the physical laws and scientific processes such technologies are known to utilise in order to work.\nThe narrative confabulation of situations in which the protagonist is required to solve a series of engineering challenges to avoid serious and adverse outcomes, with suboptimal time constraints and resources.2\nBodging, failure, and iteration: The interim solution to one engineering challenge has some flaws, which then leads to a problem, requiring another engineering solution.\nWorkings out: Very detailed and extensive discussions about how the protagonist went about trying to solve engineering challenges, including the many times they failed. So, the narrative equivalent of focusing at least as much on the methods and supplementary appendix sections of a scientific paper, as well as any scientific lab books and notes, as the final published results.\n\nThough grounded largely in physical reality, Weir’s novels often appear strangely utopian when it comes to social and psychological realities. This includes:\n\nValourisation of scientists and engineers: People who can ‘do stuff’ and ‘make stuff’, and understand the methods and techniques required to solve complex problems, are valued and valourised by society as a whole.\nPolitically effective technocracies: There are enough technically and scientifically capable people in positions of political power to be able to understand that something is a problem in need of a solution, and to devote sufficient resource and expertise to it being solved.\nProblem-solving and discovery as a universal human aspiration: The idea that people should try to understand the physical world well enough to solve complex problems, rather than just (say) write poetry about the human condition, is taken as a given.\nSpace-faring: Willingness to devote substantial public and private resource to space flight and colonisation, without (say) political pushback arguing that such resources should be better spent on either welfare or warfare, is also just taken as a given.\nCommon humanity: In situations of mutual peril and opportunity, the human race will come together to meet their collective challenges.\n\nSo, Weir’s Eng-Fi seems to combine physical realism with psycho-sociological utopianism. This might sound like a criticism, but in terms of compelling story-telling, it’s not! Not all of Weir’s novels are pure Eng-Fi. I’d categorise them as follows:\n\nThe Martian:3 90% Eng-Fi; 10% Sci-Fi\nArtemis: 35% Eng-Fi; 15% Sci-Fi; 25% Noir4; 25% Heist5\nProject Hail Mary: 55% Eng-Fi; 45% Sci-Fi\n\nAnd of the three, it’s the two with more Eng-Fi I’ve found the more compelling, both because of their attempted commitment to physical realism, but also because of their arguably less realistic portrayal of politics, psychology and sociology. In Weir’s stories, when push comes to shove, it’s the engineers and scientists, not the politicians and generals, who are called on to save the day. As compared with the dystopian sci-fi that predominates, in which humanity is both the catalyst - usually through its influence on the climate - and also accelerant - usually through its pettyminded tribal politicking - of its own demise, there’s something beautifully, poignantly hopeful about the collective sensemaking expressed in Weir’s stories."
  },
  {
    "objectID": "posts/andy-weir-utopian-engfi/index.html#in-praise-of-neurodiversity",
    "href": "posts/andy-weir-utopian-engfi/index.html#in-praise-of-neurodiversity",
    "title": "Why can’t we just get on with making and fixing stuff?",
    "section": "In praise of neurodiversity",
    "text": "In praise of neurodiversity\nOn Weir’s wikipedia page, it’s stated that Weir’s parents worked as a physicist and an electrical engineer, and that Weir himself worked as a computer programmer. To the extent none of this is surprising, it’s because we implicitly understand that there’s something in common between these kinds of occupation, that they’re all about tractable rule-bound problem solving, that persons with similar psychological and neurological profiles are drawn disproportionately to work in such fields, and perhaps also that there may be a genetic component to such a predisposition.\nPerhaps it’s also not a stretch to assume that those factors which predispose people towards engineering, physics and programming also predispose them towards science fiction? Steve Silberman’s book Neurotribes, subtitled The legacy of Autism and how to think smarter about people who think differently, includes 47 references to ‘science fiction’, with an interest in science fiction being noted as relatively common amongst those more likely to be diagnosed with some form of autism or exhibit characteristics associated with this ‘disorder’.\nAs both title and subtitle of Silberman’s book makes clear, there are reasons both for considering the temperament and way of thinking, long associated and pathologised (and more recently disassociated but arguably still pathologised) by the likes of Hans Asperger and his followers, to have some kind of genetic component; and also to be thought of as something other than a neurological or developmental deficit or disorder, not so much as something ‘missing’ from those ‘with autism’ (or more recently ‘on the spectrum’) that’s present in everone else, but more as a distinct and valuable way of thinking that fits some environments and situations better than others."
  },
  {
    "objectID": "posts/andy-weir-utopian-engfi/index.html#evolutionary-origin-story",
    "href": "posts/andy-weir-utopian-engfi/index.html#evolutionary-origin-story",
    "title": "Why can’t we just get on with making and fixing stuff?",
    "section": "Evolutionary Origin Story",
    "text": "Evolutionary Origin Story\n\n\n\nPrefrontal Cortex (Source: Wikipedia)\n\n\nThe evolutionary origin story that makes sense to me - though it’s almost certainly too simple to count as ‘true’ - is the follows: Soon after the development of advanced communication in the human animal came the development of advanced mis-communication, and soon after the development of advanced mis-communication came the development of even more advanced counter-mis-communication. More simply: first we learned to tell the truth, then we learned to lie, then we learned to detect whether someone is being truthful, then we learned to make lies sound truthful, then we learned to better detect lies even when they are made to sound truthful… and so on, and so on.\nAnd by ‘and so on’, I really mean ‘and so on’ to the power of a lot. For millions of years humans, as social creatures, have been stuck in a cognitive arms race with ourselves. Our brains grew: larger, more energy hungry, and more physically unweildy. As infants, we are more vulnerable and underdeveloped than other mammals, with heads so large they can barely fit through birth canals (leading to much increased risk of complication during childbirth), and so heavy we can barely lift them for the first few weeks of life. But we also grew ever better able to communicate with others, and to miscommunicate with others, and to tell truth from lies.\nBut the end result, other than our massive brains - and in particular our massive pre-frontal cortexes - has been a stalemate. Because both lying and lie-detection co-evolved, buoyed by and attempting to outmaneauver each other, in the end we’re about as good at both, with the end result being that people - normally developed, neurotypical, well socialised people - tend to be about as good at lying as lie-detection, and so overall not especially good at using social cues to tell when someone’s telling the truth. 6\nSo, the legacy of complex human social co-evolution: massive brains, and an obsessive pre-occupation with other people, and trying to figure out if they mean what they say, or are saying what they mean."
  },
  {
    "objectID": "posts/andy-weir-utopian-engfi/index.html#what-does-it-really-mean-to-think-differently",
    "href": "posts/andy-weir-utopian-engfi/index.html#what-does-it-really-mean-to-think-differently",
    "title": "Why can’t we just get on with making and fixing stuff?",
    "section": "What does it really mean to think differently?",
    "text": "What does it really mean to think differently?\nThe origin story of neurodiversity then, or at least those aspects that lead to a love of Sci-Fi and Eng-Fi, comes from then asking: What would happen if, for just small proportion of people, just some of the evolved human capacity for advanced, complex reasoning were diverted away from the stalemate of social reasoning, and towards the pursuit of understanding more about the broader natural and physical world?\nThe short answer may well be, in its more extreme form, autism. But the longer and more interesting answer may be something like a neurological profile and disposition that both, at the individual level, leads to acute challenges and difficulties negotiating with complex social relationships and realities; but also, over the longer term, and at a societal level, is instrumental for causing those advancements that fundamentally change the material circumstances in which everyone lives.\nBut for this kind of neurodiversity, I suspect we’d still sitting around a fire, dressed in loincloths, lying to each other, in ever more sophisticated ways, about where the berries and wilderbeast are. But for this kind of neurodiversity, if someone in their twenties died, we’d still be looking for which sky or earth gods to appease, rather than to discover tetanus and antibiotics. Those with this kind of neurotype might struggle more than most to live in the world, but they’re also instrumental in making the world in which they struggle to live.\nIn a world without this kind of neurodiversity, I also suspect there’d be a lot less Eng-Fi, with their plucky problem-solvers and reasonable statesmen and politicans, as well as fewer means (other than orally, across a campfire) of reading about and listening to such stories. It would be a much poorer world, both materially and cognitively, that I’m grateful I don’t live in!"
  },
  {
    "objectID": "posts/andy-weir-utopian-engfi/index.html#footnotes",
    "href": "posts/andy-weir-utopian-engfi/index.html#footnotes",
    "title": "Why can’t we just get on with making and fixing stuff?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is one obvious point of departure from much of sci-fi, which can focus instead on asking “what if” one or more rules of the universe were different.↩︎\nIn The Martian the set-up is essentially “solve engineering challenges or die”; whereas in Project Hail Mary it’s “solve engineering challenges or humanity dies”.↩︎\nBasically Robinson Crusoe in space… so was Robinson Crusoe the first Eng-Fi novel? I guess it depends on how much detail it goes into about how exactly Crusoe makes fire and makes shelter!↩︎\nThink Chinatown… but on The Moon!↩︎\nThink Ocean’s Eleven… but on The Moon!↩︎\nTwo ways of improving lie-detection are: i) to focus on logical inconsistency and external verification for the statements others make, for which text transcripts of statements can be more effective than verbal or video records of those statements; ii) to look for ‘tells’ in someone’s behaviour in parts of their body they are less likely to seek to ‘lie with’. For FBI interrogator Joe Navarro, this leads to the suggestion to look at people’s feet more than their faces.↩︎"
  },
  {
    "objectID": "posts/on-sweary-soaps/index.html",
    "href": "posts/on-sweary-soaps/index.html",
    "title": "On Sweary Soap Operas: A Concealed Television Genre",
    "section": "",
    "text": "This is not a doll\nBack in the 1980s I used to play with He-Man dolls. My favourite came pre-decomposed: Modulok and Multi-bot. Both were collections of interchangeable heads, torsos and limbs - alien parts for Modulok; robot parts for Multi-bot - that could joined up in more ways that a young boy could count. The two characters could be combined, creating oddly cute orgosynthetic monstrosities: HR Giger by Mattel circling Lego’s IP, as it were.\nNevertheless, if my father asked me if I was enjoying playing with my He-Man dolls, I’d be quick to correct him. “They’re not dolls!”, I’d tell him. “They’re action figures!” If my father then asked, “What’s the difference?”, I think I’d be ready to confabulate a distinction, usually related to the functionality - “press this button and it can talk or punch, or talk to who it’s punching” - or its durability. Given MultiModulokbot starts off broken up, and is intended to be torn limb from torso from head from limb, I may have had a point on the latter, but more through luck than judgement.\nI was thinking about the doll/action figure distinction recently after realising that, over the last few weeks, I’d been tricked into watching a couple of soap operas, and worse still occasionally even enjoying doing so. The first of these was Billions, which I initially watched hoping it would be to Hedge Funds what The Wire was to Street Gangs. With a top investigative journalist as one of its executive producers, and an assault course of legal and financial jargon to contend with from the first minute, I thought maybe it might at least reach the navel of the Wire’s mantel. But by around the fourth series I realised fictionalised socioethnographic investigation was never what Billions wanted to be, and if it had ever played with the idea of saying something meaningful about the ultrawealthy,1 it had no interest in this any more. No. Billions, I’d realised, was quite happy being something like Dynasty with F-bombs, and C-bombs, and BDSM, strung together mainly around a kind of baroque storyline involving a preposterous love triangle, or maybe a love chevron, or maybe a love human centipede. In Billions, various ludicrous hypermasculine archetypes, sometimes played by women or non-binary actors, act as if every decision they make is life-or-death, that they’re one step away from destroying each other, vanquishing their foes, and achieving ultimate victory. Yet there they are, two years later, three years later, all in one piece, none-the-worse, still acting as if they’re still dancing on an existential precipice, and that maybe this scheme is the one that will finally seal the deal. (It doesn’t.)\nMore recently, I’ve found myself watching Loudermilk, about a former music journalist and recovering alcoholic who’s also a straight-talking foul-mouthed misanthropist with a heart of gold. Secondary characters call Loudermilk Loundermilk repeatedly, as if to remind viewers what show they’re watching, and attractive young women seem to find him appealing for no obvious reason. Much of the show involves men in recovery talking to each other in a room, hiding their love and concern for each other inside superficially cruel and callous insults. Plot twists abound - an affair here, a visit from a long-lost relative there - but ultimately it’s still the same set of characters, sitting in a room, loving to hate each other, and hating that they love each other.\nSeries like Billions and Loudermilk would never admit to being soap operas because, just as the young boy who played with action figures would never have played with dolls, so many viewers of sweary soap operas would never watch soap operas."
  },
  {
    "objectID": "posts/on-sweary-soaps/index.html#footnotes",
    "href": "posts/on-sweary-soaps/index.html#footnotes",
    "title": "On Sweary Soap Operas: A Concealed Television Genre",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI was thinking of shoehorning a reference to the characters of Billions thinking that they’re the “masters of the universe”, but are also ultimately no more realistic than those portrayed in He-Man, but decided against it!↩︎"
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html",
    "href": "posts/robocop-is-wonderfully-childish/index.html",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "",
    "text": "Robocop (1987): Wonderfully Childish"
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html#preamble",
    "href": "posts/robocop-is-wonderfully-childish/index.html#preamble",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Preamble",
    "text": "Preamble\nHere’s something that’s differently nerdy. Some thoughts on the enduring and childish appeal of Robocop as a character and concept, lifted largely from some notes I made on Obsidian (which also uses Markdown)."
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html#notes-on-robocop",
    "href": "posts/robocop-is-wonderfully-childish/index.html#notes-on-robocop",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Notes on Robocop",
    "text": "Notes on Robocop\nWent through a Robocop phase. First film is good. Second film is adequate fan service (by Frank Miller), though unpleasant in terms of how it makes a boy a main antagonist. Reboot is terrible.\nI think this was inspired by seeing footage for the Robocop game Rogue City. Also remembered discussion about how in the original film Robocop’s death and resurrection is modelled on Jesus. It’s definitely true his death is a ‘passion’ (modern equivalent: torture porn?), and that his suffering at the hands of sadistic tormenters fits this pattern.\nIn the first film there’s also the sense of the protagonist reclaiming his humanity. Murphy’s memory is wiped, but through force of will he brings himself to remember who he was, and to identify as Murphy. That’s the last line. He’s thanked and addressed by the Old Man as a person, not as property That’s the arc: becoming human again.\nBy contrast the reboot started with Murphy knowing who he was, though the scientists can modify the extent to which his feelings or programming are in charge. There’s a plot about police corruption and selling weapons illegally, and plenty of exposition from scientists where they try to shoehorn in cod-philosophy on personhood and free will, but this felt like ‘filler’ between a series of action sequences, which were much faster but also much less weighty and visceral than those in the original.\nIn the original Robocop is a bullet sponge. He was slow, apparently unfeeling. Though this might be partly a function of Peter Weller not having a great deal of mobility when wearing the suit, it also feeds into the central character arc: robots don’t feel; people do.1 Robots aren’t vulnerable as people are. As Robocop becomes damaged, he becomes more vulnerable, and his face becomes visible. Vulnerability is necessary for humanity to be restored. Robocop’s suit makes him a superhero, an adolescent boy fantasy of massive strength and power, but it also makes him a prisoner, trapped and entombed.\n\n\n\nRegaining Humanity: But at what cost?\n\n\nAgain, by contrast, in the remake Murphy has lost less. He still has a human hand, still has his memories and sense of self, and still has all the speed and mobility he had as a person, only more so. He doesn’t have an arc, he has perturbations and wobbles.\nLet’s think some more about why Robocop is an adolescent, or even pre-adolescent, fantasy. Firstly, it appeals to a kind of crude creativity of combinatorials: take two things that are familiar, combine them, and make something unfamiliar. With only a limited number of schemas, even a young child can wonder what happens when they are combined, and feel excited and proud about bootstrapping from everyday experience to pure fantasy. Like the distinction between animal, vegetable and mineral, ‘man’ and ‘machine’ are different primary colours, and seeing the concept of Robocop may be for a young child like seeing red and blue make purple for the first time.\n\n\n\nTeenage Mutant Ninja Turtles: Another contemporaneous example of the ‘primary colour chimera’ attractive to children\n\n\nSecond, there’s the power fantasy. Within this, there’s the sense of seeing someone who can play an action game and be allowed to ‘cheat’. The roles of an action game, involving shooting, must include that, once shot, the player must ‘play dead’. But in Robocop is a fantasy of a character who’s still allowed to shoot others whilst being able to ignore when others shoot him. Robocop presents a fantasy for a young child of playing a game where you can do things that no-one else can, because you’re more special than everyone else.\nI suspect that’s why, even though Robocop was clearly unsuitable for children, the concept of Robocop appeared to have so much appeal to children. There’s something inherently childish about seeing such a pure chimera rendered on screen, and the possibilities and affordances of this chimera are signalled so brightly from the images alone that the (adult) source material does not need to be consulted.\nAgain, this seems to be yet another reason why the remake did not work: 2014’s Murphy was not machine enough. The suit did not make him clearly ‘other’ enough. Even though the graphic violence was toned down so that in theory children could watch, the lack of schematic purity in the ideal types being mixed meant the pleasure of seeing primary colours being mixed, in the way the original so effectively managed, was muddied and diluted."
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html#footnotes",
    "href": "posts/robocop-is-wonderfully-childish/index.html#footnotes",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlistair Gray’s Lanark played with a similar idea, in the form of Dragonhide, a magical realist representation of dermatitis, emotional distance, and lack of outlet for artistic expression. Too much Dragonhide, the book warns, causes sufferers to become unable to move, and unable to vent, their heat and energy, without expression, eventually causing them to boil to death!↩︎"
  },
  {
    "objectID": "posts/unattended-deaths/index.html",
    "href": "posts/unattended-deaths/index.html",
    "title": "A Deathly Silence",
    "section": "",
    "text": "Trends in R98/R99 deaths since 1990\n\n\nWhat does it mean when someone dies, and no one notices for days, weeks, or months on end?\nThe bodies, once found, will be decomposed to such an extent that no effective autopsy can be performed, and so no cause of death can be identified. Such deaths are then likely to be coded either as R98 (‘Unattended death’) or R99 (‘Other ill-defined and unknown causes of mortality’). Far from being ‘junk codes’, wouldn’t a sudden and sustained change in deaths coded this way (absent an obvious explanation, such as a change in coding practice) signal that something broader is afoot?\nWorking with Lu Hiam, an Oxford PhD student and former GP, and Theodore Estrin-Serlui, a histopathologist, I analysed trends in deaths with these codes, as compared with mortality trends overall in England & Wales.\nSuch codes are rarely used, but in England & Wales they sadly became many times more common over the 1990s and 2000s. Standardised mortality rates in the R98/R99 category became more than three and a half times a common between 1990 and 2010, even as general standardised mortality rates fell by around a third.\nFor every body found so decomposed that the R98/R99 category had to be used, there are usually many more that have been unattended for a few days, have started to decompose, but for which autopsy can still be successfully performed. If these deaths are the tip of the iceberg, the base of this iceberg may be a growing epidemic of loneliness and social isolation, of ever more people with connections to friends and family, with no one to turn to in times of crisis.\nOur paper, A Deathly Silence, has been published in the Journal of the Royal Society of Medicine, and received press coverage from a number of outlets."
  },
  {
    "objectID": "posts/utterences-comments/index.html",
    "href": "posts/utterences-comments/index.html",
    "title": "New blog feature: comments",
    "section": "",
    "text": "I think I’ve managed to set up a blog comment feature for each of the posts.\nAs usual, the quarto documentation is great, though information on comments is in the HTML basics section rather than the website or blog section, so takes a bit of hunting. Albert Rapp’s very comprehensive blogpost is a great resource, covering this and much else.\nThe Quarto documentation gives three options for comments:\n\nHypothes.is, which allows comments and annotations to be provided line-by-line, a bit like non-editable tracked changes.\nutterances, which is a lightweight interface based on the discussion feature in github.\ngiscus, which seems to be built on utterances, but a bit more heavy-weight/opinionated.\n\nI’ve attempted, and think I’ve managed to implement, utterances.\nIn order to make a comment on a post, you need to have a Github username, and log in.\nFrom my end, I needed to do the following:\n\nSet up a public Github repo for blog comments. I unimaginately called this BlogComments\nInstall utterances on github and associate it with this repo\nWithin the file posts/_metadata.yml, add the following declaration\n\ncomments: \n  utterances: \n    repo:  JonMinton/BlogComments\nParameters in posts/_metadata.yml are applied to all posts within the posts subdirectory. This should mean that each post will now contain a comment box at the bottom.\nWhen a comment is added by a registered Github user, metadata from the specific post being commented on should be appended to an issue/discussion post within the JonMinton/BlogComments directory. And whenever a post is rendered, all associated discussion/issue items in the BlogComments repo should be fetched and shown at the bottom of the post.\nI’ve said should because I’ve only just set this up, and there are currently no comments.\nWhy not try to add a comment and see what happens?!"
  },
  {
    "objectID": "posts/socatic-dialogue-part-01/index.html",
    "href": "posts/socatic-dialogue-part-01/index.html",
    "title": "Nerdy Dialogues on Life and Death",
    "section": "",
    "text": "Two cats, Emu and Goose. Goose is invading Emu’s personal space slightly\nHere’s an attempt to think some more about how and why standardised rates are used to compare populations. I’m doing so via the medium of a Socratic Dialogue1."
  },
  {
    "objectID": "posts/socatic-dialogue-part-01/index.html#why-age-standardise-a-dialogue",
    "href": "posts/socatic-dialogue-part-01/index.html#why-age-standardise-a-dialogue",
    "title": "Nerdy Dialogues on Life and Death",
    "section": "Why age standardise? A dialogue",
    "text": "Why age standardise? A dialogue\nMore than twice as many deaths are reported in population A than population B\nFirst question: Does population A have about twice the population as population B?\nOkay. I’ve got the number of deaths in population A, \\(n_A\\), and the population size in population A, \\(N_A\\). So, I’ve calculated the rate \\(r_A = \\frac{n_A}{N_A}\\) for population A, and done the same for population B, \\(r_B = \\frac{n_B}{N_B}\\).\nOkay… so what’s the ratio of \\(r_A\\) to \\(r_B\\)?\nIt’s 1.4. So, the mortality rate in population A is 40% higher than in population B\nAnd what does that mean?\nPopulation A are exposed to more of something bad, or maybe less to something good, than those in population B.\nPossibly. What if I told you that the mortality rate in a care home was 40% higher than the mortality rate in a combat unit fighting on the front line? If the differences in rates is just due to differences in exposures, surely if we were to move the people in the care home into the combat unit, the differences in mortality rates between the two populations should disappear?\nThat doesn’t sound right. I think the mortality rates of the care home population would be even higher if they were in the combat unit.\nAnd what does that imply?\nDifferences in health outcomes between populations can be due to differences in the characteristics of the populations being compared, as well as differences in the exposures the two populations encounter\nExactly. And what are the main differences in characteristics between the care home population and the combat unit population likely to be?\nI’d expect the combat unit population to be much younger than the care home population. I’d also expect the combat unit population to be overwhelmingly male, whereas the care home population might be more mixed, but perhaps skewed more towards females than males.\nGood. So what does this mean for methodology?\nWe need to look to compare like-with-like when trying to work out how much of a difference in health outcome is due to differences in exposures. At the very least, we should try to compare like-with-like on age and sex, as these are very important determinants of mortality risk.\nGreat. So, instead of just a single ratio to compare between populations, we can compare a load of ratios, one for each combination of age and sex we’ve got common data for. So, the ratio of mortality rates in 25 year old females, 37 year old males, 60 year old females, 82 year old males, and so on…\nMaybe…\nIf we’ve got males and females, each by age in single year up to age 90, that means we have almost 200 such ratios to compare. Any difficulties with that?\nI guess that makes it hard to see the wood for the trees, one or two numbers is easier to convey than one hundred or two hundred.\n…\nSo I guess we need some way of summarising this further, making sure the summary measure presented is a reasonable summary of all of the like-with-like comparisons we’ve got?\nYes. What might be some ways of doing this?\nI guess we could do something like the mean or median value of these age-sex specific ratios??\nFrom first principles, that doesn’t seem like a terrible idea. However it would have some problems.\nSuch as?\nFor example, if a few ratios are based on small numbers of deaths and population counts, they could be very big or very small due to sample estimation issues alone. This would be more of an issue if using the mean than the median.\nAlso all subpopulations’ estimates would contribute equally to such a summary measure, even if some subpopulations contribute much more to the overall health outcome in the population than others.\nSo, what are some better alternatives?\nWell, I guess, for overall mortality, you could use life expectancy.\nAh, we’re all familiar with that.\nMaybe not as familiar as you think you are. It’s less straightforward to calculate and interpret than you might think. For example, imagine it’s 1890, and life expectancy is 51 years of age. You’re 31 years old. How long can you expect to live?\nTwenty more years? More if I’m careful\nProbably quite a bit longer. ‘Life expectancy’ \\(e\\) is usually used as shorthand for ‘period life expectancy from birth’, \\(e_0\\), or ‘unconditional period life expectancy’. Historically, the first year of life was one of the most dangerous ages to be alive.2 It’s a tall and weighty hurdle to cross. But as a 31 year old you’ve already crossed it. Your life expectancy isn’t ‘unconditional’ life expectancy from birth, \\(e_0\\), but ‘conditional’ life expectancy from age 31, \\(e_{31}\\).\nWhat does this mean?\nSay there were 10,000 people running the obstacle course of life, and they’re starting at the start of the course. The further you follow the course along, the fewer people reach each stage. Life expectancy at birth \\(e_0\\), heuristically, answers the question “at what stage in the course should you expect there’ll only be 5,000 of the original contestants remaining, on average?”\nAnd \\(e_{31}\\)?\nAlmost the same, except instead of the 10,000 people starting at the start of the course, they’re all allowed to start at stage 31 instead.\nAh! I think I see now why \\(e_{31}\\) should be greater than \\(e_0\\)!\nYes, and as mentioned it used to be a lot higher, because the first stage used to be one of the toughest.\nOkay. I think I understand why life expectancy isn’t a completely straightforward concept. Let’s go into the weeds even further. Why did you refer to life expectancy as period life expectancy? What’s the alternative?\nWell, let’s keep with the obstacle course analogy. Imagine two more things…\nOkay, but you’re hurting my brain.\n… Firstly, that the 10,000 contestants aren’t all the contestants. Instead, they’re just one of a series of cohorts of contestants. Every fifteen minutes, say, another 10,000 contestants are lined up at the starting block and, when their starting pistol goes, they start the course.\nSounds pretty crowded…\nYes. None of this is practically possible; we also have to assume everyone runs at the same rate for the analogy to work.\nAnyway, the second big thing to imagine is that the designers of the obstacle course are constantly redesigning it. They’re making some of the hurdles higher, and other hurdles lower, and they’re doing this all the time.\nAh, so the obstacle course is never the same for any two cohorts who traverse it?\nExactly! You’ve got it!3\nSo, it’s cohorts who traverse the course, but you said life expectancy is usually period life expectancy. What does this mean?\nA period life expectancy is like taking a snapshot, or just a few seconds-long clip, of people who are currently on the obstacle course, at all stages, and using this information to try to work out how far a cohort, starting at the start of the course, would likely get along the course, if the obstacle course never changed while the cohort is traversing the course.\nThat sounds like an important caveat, given you just said the course is always changing.\nIt certainly is. It’s for this reason a period life expectancy is sometimes also called a synthetic cohort life expectancy, because the cohort imagined to traverse the obstacle course doesn’t actually exist, but is made up of different pieces of different cohorts at different stages of the course.\nSo, why not use a real cohort?\nTwo reasons: Relevance, and data.\nGo on..\nEither you have a completed cohort, where there’s the data, but not the relevance. Or you have an incomplete cohort, where there’s the relevance, but not the data.\nI think I understand: The completed cohort completed the obstacle course, especially the start of it, when it was very different to how it is now, so although the data’s complete, much of it doesn’t speak to the current challenges on the course. And for the incomplete cohort, as they’ve not yet reached all the hurdles, we can’t yet know how many of them will reach each stage.\nCorrect, and correct. Two reasons why period life expectancies are usually used, even though they’re based on some pretty weird fictions.\nThis whole dialogue is a weird fiction. Shall we call it a night?\nIndeed we shall!"
  },
  {
    "objectID": "posts/socatic-dialogue-part-01/index.html#footnotes",
    "href": "posts/socatic-dialogue-part-01/index.html#footnotes",
    "title": "Nerdy Dialogues on Life and Death",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e. two people talking about an idea, one of whom thinks they know more than the other one, though, very annoyingly, insists they don’t.↩︎\nExisting starts with a boss fight.↩︎\nMy Socrates doesn’t intend this to sound condescending, but it still does a bit.↩︎"
  },
  {
    "objectID": "posts/x-minute-neighbourhoods/index.html",
    "href": "posts/x-minute-neighbourhoods/index.html",
    "title": "Why such pushback against 20 minute neighbourhoods?",
    "section": "",
    "text": "I had the privilege yesterday of hearing a series of talks by researchers at the University of Glasgow on 20 minute neighbourhoods.1 The talks covered areas like evidence surveys of health associations, GIS methods,2 and engagement with historical amenities.\nI was vaguely aware that this kind of initiative is sometimes conflated with (ultra) low emission zones, and in recent years sometimes receives a hostile response from some audiences. So, in the Q&A, I asked if this had been their experience, what they think the causes of the hositility were, and what (if anything) is best to do about it.\nThe researchers had encountered such responses, and the coordinator sent a link to a youtube video introducing the research they were involved in. Only around 2% of those who viewed the video decided to comment on it, but it surprised me that the vast majority did express the kind of hostility I was thinking about. The top few comments are indicative:\nOnly by the fifth ranked comment is there a response broadly supportive of the initiative, though is more lamenting than hopeful:\nOn the possible reasons for such responses, the researchers suggests that COVID may be a factor. On what to do about it, there was less clarity, except to be mindful that many people may not change their mind on such issues, so engaging with them might not be worth the time involved.\nThe COVID explanation definitely seems part of it, and is evident in some of the examples above. 2020 and 2021 was a confusing time, and the popularity of conspiracy theories which offered ‘answers’ seems to have grown as a result. Within the conspiracy theory linking Lockdown to ULEZ and walkable neighbourhoods, Lockdown was a dress rehearsal, an attempt to understand just how pliant and willing to give up on hard earned freedoms the populace at large would be when told such restrictions were necessary and temporary. Initiatives like Walkable Neighbourhoods are then framed as something like ‘the next phase’, initiatives which curtail freedom on a permanent rather than temporary basis, with the ultimate endpoint being something like ‘prison cities’, where everyone is controlled and monitored at all times in some kind of Orwellian nightmare.\nClearly, there seems to be a lot of imputation and extrapolation involved in getting from ‘being able to walk to school while passing some nice buildings’ to 1984. But perhaps having a preexisting set of assumptions, which link driving to freedom and so walking to tyranny, is something that makes people more susceptible to the conspiratorial way of thinking outlined above. Let’s consider this some more.\nIn surveys of household affluence from decades gone by, my understanding3 is that some surveys used to ask UK adults questions along the lines of: “How many cooked meals with meat did you eat in the last week?” The idea of such questions was that, if people could afford to eat more meat, they would do. Such questions were considered unobtrusive measures of individual and household means, because the individual wants, to eat as much meat as one could afford to do so, was simply taken as given.\nA few years ago Gapminder generalised something like this principle to international development, providing simple but graphic illustrations of how what people eat, drink, and use as transport varies across four very broad income levels. I’ve made this illustration the main image for this blog post.\nIf we look at income level 1, under $2 a day, people are obligate walkers, and they’re likely to be obligate vegans, relying on a simple grain to survive. As they reach higher levels, they start to be able to afford to augment their simple stable dish with vegetables, spices and meat. And they start to move from walking, to being able to afford a bicycle, then a motorcycle, then finally a car. The changing transport mode is presented as what people move onto when they can afford to do so, with each form presenting new found physical freedoms to go along with the new found financial freedoms their higher income level now affords them.\nMy suspicion is that many people who adopted the kind of conspiracy theory sketched above, which leads to the kind of hostile comments to the kind of walkability initiatives being discussed, did so because they internalised something like the Gapminder model of development both too deeply and too crudely. In particular, they conflate driving with money and freedom, and so not driving with poverty and restriction. I suspect it’s easier to subscribe to the car=freedom equation when you have direct experience of not being able to afford to own or run a car, of driving being a genuine hard-won freedom. In social epi parlance, I suspect there’s likely to be a socioeconomic gradient in hostility to walkability initiatives, as for many poorer people the idea of not being allowed to do something you can only just afford to do (and want to do in large part because you can only just afford to do), would seem inherently perverse.\nPersonally, as a city-dwelling vegetarian, my intuitions are all in support of Walkability initiatives. I’m just trying to be mindful of how those with different circumstances may look at the same things, but see something very different!"
  },
  {
    "objectID": "posts/x-minute-neighbourhoods/index.html#footnotes",
    "href": "posts/x-minute-neighbourhoods/index.html#footnotes",
    "title": "Why such pushback against 20 minute neighbourhoods?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis generalises to ‘x minute neighbourhoods’, e.g. 15 minute neighbourhoods, 30 minute neighbourhoods, 10 minute neighbourhoods.↩︎\nIn practice this seemed to be the identification of whether and how many common amenities are within 800 metres of someone’s home.↩︎\nPlease correct me if I’m wrong, or provide an example or two if I’m not. I wasn’t able to find an example dataset before, so might be confabulating this!↩︎"
  },
  {
    "objectID": "posts/changing-tenure-in-scotland/index.html",
    "href": "posts/changing-tenure-in-scotland/index.html",
    "title": "Changing tenure in Scotland",
    "section": "",
    "text": "As it’s something I’ve done already for work, but it’s all using public domain data, here’s a link to some charts showing how housing tenure in Scotland has changed over time. As Social Housing stock went down, private rental stock went up."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-10/index.html",
    "href": "posts/glms/lms-are-glms-part-10/index.html",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "",
    "text": "Within this series, parts 1-4 formed what we might call ‘section one’, and part 5-9 ‘section two’.\nSection one (re)introduced statistical models as siblings, children of a mother model which combines a systematic component (an equation with a \\(=\\) symbol in it) and a stochastic component (an equation with a \\(\\sim\\) in it, which can largely be read as ‘drawn from’). Part one provided a graphical representation of the challenge of model fitting from an algorithmic perspective, in which the parameters that go into the two component are tweaked and tweaked until some condition is met: usually that the discrepency between model predictions and observed outcomes are minimised some way. The two component mother model is largely equivalent to the concept of the generalised linear model: parts two and three explored this association a bit more. Part four demonstrated how, for statistical models other than standard linear regression, the kinds of answer one usually wants from a model are not readily apparent from the model coefficients themselves, and so careful use of model predictions, and calibration of the questions, are required to use models to answer substantivelly meaningful questions.\nSection two aimed to show how likelihood theory is used in practice in order to justify a loss function that algorithms can be used to try to ‘solve’.1 These loss functions and optimisation algorithms are usually called implicitly by statistical model functions, but we did things the hard way by building the loss function from scratch, and evoking the algorithms more directly, using R’s optim() function. As well as the pedagogical value (and bragging rights) of being able to create and fit statistical models directly, an additional benefit of using optim() (with some of its algorithms) is that it returns something called the Hessian. The Hessian is what allows us to be honest when making model predictions and projections, showing how our uncertainty about the true value of the model parameters (the multiple inputs that optim() algorithms try to tweak until they’re good enough) leads to uncertainty in what we’re predicting and projecting.\nUnfortunately, we’re still in section two. The material below aims to repeat the same kind of exercise performed for standard linear regression, but using logistic regression instead."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-10/index.html#recap",
    "href": "posts/glms/lms-are-glms-part-10/index.html#recap",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "",
    "text": "Within this series, parts 1-4 formed what we might call ‘section one’, and part 5-9 ‘section two’.\nSection one (re)introduced statistical models as siblings, children of a mother model which combines a systematic component (an equation with a \\(=\\) symbol in it) and a stochastic component (an equation with a \\(\\sim\\) in it, which can largely be read as ‘drawn from’). Part one provided a graphical representation of the challenge of model fitting from an algorithmic perspective, in which the parameters that go into the two component are tweaked and tweaked until some condition is met: usually that the discrepency between model predictions and observed outcomes are minimised some way. The two component mother model is largely equivalent to the concept of the generalised linear model: parts two and three explored this association a bit more. Part four demonstrated how, for statistical models other than standard linear regression, the kinds of answer one usually wants from a model are not readily apparent from the model coefficients themselves, and so careful use of model predictions, and calibration of the questions, are required to use models to answer substantivelly meaningful questions.\nSection two aimed to show how likelihood theory is used in practice in order to justify a loss function that algorithms can be used to try to ‘solve’.1 These loss functions and optimisation algorithms are usually called implicitly by statistical model functions, but we did things the hard way by building the loss function from scratch, and evoking the algorithms more directly, using R’s optim() function. As well as the pedagogical value (and bragging rights) of being able to create and fit statistical models directly, an additional benefit of using optim() (with some of its algorithms) is that it returns something called the Hessian. The Hessian is what allows us to be honest when making model predictions and projections, showing how our uncertainty about the true value of the model parameters (the multiple inputs that optim() algorithms try to tweak until they’re good enough) leads to uncertainty in what we’re predicting and projecting.\nUnfortunately, we’re still in section two. The material below aims to repeat the same kind of exercise performed for standard linear regression, but using logistic regression instead."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-10/index.html#log-likelihood-for-logistic-regression",
    "href": "posts/glms/lms-are-glms-part-10/index.html#log-likelihood-for-logistic-regression",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "Log likelihood for logistic regression",
    "text": "Log likelihood for logistic regression\nPreviously we focused on the log likelihood for standard linear regression. Let’s now do the same for logistic regression. According to the relevant section of the Zelig website:\nStochastic component \\[\nY_i \\sim Bernoulli(y_i | \\pi_i )\n\\]\n\\[\nY_i = \\pi_i^{y_i}(1 - \\pi_i)^{1-y_i}\n\\]\nwhere \\(\\pi_i = P(Y_i = 1)\\)\nAnd\nSystematic Component\n\\[\n\\pi_i = \\frac{1}{1 + \\exp{(-x_i \\beta)}}\n\\]\nThe likelihood is the product of the above for all observations in the dataset \\(i \\in N\\)\n\\[\nL(.) = \\prod{\\pi_i^{y_i}(1 - \\pi_i)^{1-y_i}}\n\\]\nThe effect of logging the above2:\n\\[\n\\log{L(.)} = \\sum{[y_i \\log{\\pi_i} + (1-y_i)\\log{(1-y_i)}]}\n\\]\nThis can now be implemented as a function:\n\n\nCode\nllogit &lt;- function(par, y, X){\n    xform &lt;- function(z) {1 / (1 + exp(-z))}\n    p &lt;- xform(X%*%par)\n    sum(y * log(p) + (1-y) * log(1 - p))\n}\n\n\nLet’s pick an appropriate dataset. How about… picking a Palmer Penguin!?\n\n\nCode\nlibrary(tidyverse)\npalmerpenguins::penguins\n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nLet’s say we want to predict whether a penguin is of the Chinstrap species\n\n\nCode\npalmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    ggplot(aes(x = bill_length_mm, y = bill_depth_mm, colour = is_chinstrap, shape = sex)) + \n    geom_point()\n\n\n\n\n\nNeither bill length nor bill depth alone appears to distinguish between chinstrap and other species. But perhaps the interaction (product) of the two terms would do:\n\n\nCode\npalmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    mutate(bill_size = bill_length_mm * bill_depth_mm) |&gt;\n    ggplot(aes(x = bill_size, fill = is_chinstrap)) + \n    facet_wrap(~sex) + \n    geom_histogram()\n\n\n\n\n\nThe interaction term isn’t great at separating the two classes, but seems to be better than either length or size alone. So I’ll include it in the model.\n\n\nCode\ndf &lt;- palmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    mutate(bill_size = bill_length_mm * bill_depth_mm) |&gt;\n    mutate(is_male = as.numeric(sex == \"male\"))\n\ny &lt;- df$is_chinstrap\n\nX &lt;- cbind(1, df[,c(\"bill_length_mm\", \"bill_depth_mm\", \"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\n\nSo, including the intercept term, our predictor matrix \\(X\\) contains 5 columns, including the interaction term bill_size. 3\nLet’s try now to use the above in optim()\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = rep(0, 5), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] 82.9075239 -2.4368673 -6.4311531  0.1787047 -6.4900678\n\n$value\n[1] -33.31473\n\n$counts\nfunction gradient \n     137       45 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n             [,1]         [,2]          [,3]         [,4]         [,5]\n[1,]   -12.103063    -550.0621    -209.30944    -9674.925    -3.700623\n[2,]  -550.062097  -25256.3082   -9500.55848  -443670.225  -184.360139\n[3,]  -209.309443   -9500.5585   -3650.65107  -168517.417   -68.158844\n[4,] -9674.924703 -443670.2251 -168517.41718 -7846293.352 -3464.964868\n[5,]    -3.700623    -184.3601     -68.15884    -3464.965    -3.700623\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n            [,1]         [,2]         [,3]          [,4]         [,5]\n[1,] 41.95816335 -0.156192235 -0.309892876 -4.036895e-02  9.329019450\n[2,] -0.15619224 -0.005017392 -0.024806420  1.070652e-03 -0.139430425\n[3,] -0.30989288 -0.024806420 -0.042869947  2.854565e-03 -0.337480429\n[4,] -0.04036895  0.001070652  0.002854565 -7.331214e-05  0.003098092\n[5,]  9.32901945 -0.139430425 -0.337480429  3.098092e-03  1.202424836\n\n\nNow let’s compare with glm()\n\n\nCode\nmod_glm &lt;- glm(is_chinstrap ~ bill_length_mm * bill_depth_mm +is_male, data = df, \nfamily = binomial())\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = is_chinstrap ~ bill_length_mm * bill_depth_mm + \n    is_male, family = binomial(), data = df)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  365.2924    88.3341   4.135 3.54e-05 ***\nbill_length_mm                -8.9312     2.0713  -4.312 1.62e-05 ***\nbill_depth_mm                -23.6184     5.5003  -4.294 1.75e-05 ***\nis_male                      -11.8725     2.6121  -4.545 5.49e-06 ***\nbill_length_mm:bill_depth_mm   0.5752     0.1292   4.452 8.53e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 337.113  on 332  degrees of freedom\nResidual deviance:  49.746  on 328  degrees of freedom\nAIC: 59.746\n\nNumber of Fisher Scoring iterations: 9\n\n\nUh oh! On this occasion it appears one or both approaches have become confused. A five dimensional search space might be too much for the algorithms to cope with, especially with collinearity 4 between some of the terms. Let’s simplify the task a bit, and just use intercept, bill size, and is_male as covariates. First with the standard package:\n\n\nCode\nmod_glm_simpler &lt;- glm(is_chinstrap ~ bill_size +is_male,   data = df, \nfamily = binomial())\nsummary(mod_glm_simpler)\n\n\n\nCall:\nglm(formula = is_chinstrap ~ bill_size + is_male, family = binomial(), \n    data = df)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -32.815339   4.325143  -7.587 3.27e-14 ***\nbill_size     0.043433   0.005869   7.400 1.36e-13 ***\nis_male      -7.038215   1.207740  -5.828 5.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 337.11  on 332  degrees of freedom\nResidual deviance:  90.60  on 330  degrees of freedom\nAIC: 96.6\n\nNumber of Fisher Scoring iterations: 7\n\n\nAnd now with the bespoke function and optim\n\n\nCode\nX &lt;- cbind(1, df[,c(\"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\nfuller_optim_output &lt;- optim(\n    par = rep(0, 3), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] -32.60343219   0.04314546  -6.98585077\n\n$value\n[1] -45.30114\n\n$counts\nfunction gradient \n      73       18 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]         [,2]         [,3]\n[1,]    -13.008605   -10662.078    -5.201308\n[2,] -10662.078251 -8846787.584 -4846.390833\n[3,]     -5.201308    -4846.391    -5.201308\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n             [,1]          [,2]         [,3]\n[1,] -536.7022079  0.7206703142 -134.7923170\n[2,]    0.7206703 -0.0009674672    0.1807806\n[3,] -134.7923170  0.1807806218  -33.4602664\n\n\nThe estimates from the two approaches are now much closer, even if they aren’t as close to each other as in the earlier examples. Using optim(), we have parameter estimates \\(\\beta = \\{\\beta_0 = -32.60, \\beta_1 = 0.04, \\beta_2 = -6.99\\}\\), and using glm(), we have estimates \\(\\beta = \\{\\beta_0 = -32.82, \\beta_1 = 0.04, \\beta_2 = -7.04 \\}\\)\nIf we cheat a bit, and give the five dimensional version starting values closer to the estimates from glm(), we can probably get similar estimates too.\n\n\nCode\nX &lt;- cbind(1, df[,c(\"bill_length_mm\", \"bill_depth_mm\", \"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\nfuller_optim_output &lt;- optim(\n    par = c(300, -10, -29, 0.5, -10), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] 299.5512512  -7.3684567 -19.3951742   0.4747209  -9.7521255\n\n$value\n[1] -25.33208\n\n$counts\nfunction gradient \n     153       22 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n             [,1]          [,2]          [,3]         [,4]         [,5]\n[1,]    -8.378918    -370.41592    -140.86865    -6342.301    -1.800406\n[2,]  -370.415921  -16580.87909   -6238.75358  -284403.350   -91.239716\n[3,]  -140.868648   -6238.75358   -2387.19776  -107598.410   -33.018551\n[4,] -6342.300809 -284403.34960 -107598.40987 -4906697.476 -1685.235507\n[5,]    -1.800406     -91.23972     -33.01855    -1685.236    -1.800406\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n            [,1]         [,2]        [,3]          [,4]         [,5]\n[1,] -59.5448267  2.316365876  5.14842594 -0.1737609491 10.383684649\n[2,]   2.3163659 -0.064512887 -0.16844980  0.0044962968 -0.166413655\n[3,]   5.1484259 -0.168449797 -0.33888931  0.0106735535 -0.387558164\n[4,]  -0.1737609  0.004496297  0.01067355 -0.0002712683  0.004068597\n[5,]  10.3836846 -0.166413655 -0.38755816  0.0040685965  1.904433768\n\n\nWell, they are closer, but they aren’t very close. As mentioned, the glm() model produced warnings, and some of the variables are likely to be collinear, so this initial specification may have been especially difficult to fit. Both approaches found an answer, but neither seem happy about it!"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-10/index.html#summary",
    "href": "posts/glms/lms-are-glms-part-10/index.html#summary",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "Summary",
    "text": "Summary\nIn the exercise above we did for logistic regression what the previous few posts in section two did for standard regression: i.e. we derived the log likelihood, applied it using optim, and compared with results from the glm() package. We saw in this case that fitting models isn’t always straightforward. We were - well, I was - overly ambitious in building and applying an overly parameterised model specification. But we eventually got to similar parameter values using both approaches.\nThough this wasn’t as straightforward as I was hoping for, I’m presenting it warts-and-all. In principle, the log-likelihood maximisation approach generalises to a great many model specifications, even if in practice some model structures aren’t as straightforward to fit as others."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-10/index.html#coming-up",
    "href": "posts/glms/lms-are-glms-part-10/index.html#coming-up",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "Coming up",
    "text": "Coming up\nIn the next post, I’ll finally be moving off ‘section two’, with its algebra and algorithms, and showing some tools that can be used to make honest prediction and projections with models, but without all the efforts undertaken here and in the last few posts."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-10/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-10/index.html#footnotes",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBy ‘loss function’ I mean a function that takes one or more numeric inputs and returns a single numeric output. The aim of the algorithm is to find the combination of inputs that minimises (or maximises) the function’s output.↩︎\nThanks to this post. My calculus is a bit rusty these days.↩︎\nAn important point to note is that, though bill_size is derived from other variables, it’s its own variable, and so has another distinct ‘slot’ in the vector of \\(\\beta\\) parameters. It’s just another dimension in the search space for optim to search through.↩︎\nThis is fancy-speak for when two terms aren’t independent, or both adding unique information. For example, length in mm, length in cm, and length in inches would all be perfectly collinear, so shouldn’t all be included in the model.↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html",
    "href": "posts/glms/lms-are-glms-part-05/index.html",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "",
    "text": "The last part of the series ended by showing how, except for linear regression, the kinds of answers users need from models aren’t in tables of model coefficients, but in model predictions. However, we only showed predictions as point estimates, giving the false and unreasonable impression that the model predicts with perfect accuracy.\nThis post, start of the the second section of the series, provides the theoretical and methodological background necessary to produce model predictions which are more honest, providing predictions with uncertainty intervals relating both to uncertainty about how the model has been fit, and fundamental variability in the stochastic processes that the statistical models aim to simulate. It’s likely to be the most challenging part of the blog series, but worth sticking with."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html#recap",
    "href": "posts/glms/lms-are-glms-part-05/index.html#recap",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "",
    "text": "The last part of the series ended by showing how, except for linear regression, the kinds of answers users need from models aren’t in tables of model coefficients, but in model predictions. However, we only showed predictions as point estimates, giving the false and unreasonable impression that the model predicts with perfect accuracy.\nThis post, start of the the second section of the series, provides the theoretical and methodological background necessary to produce model predictions which are more honest, providing predictions with uncertainty intervals relating both to uncertainty about how the model has been fit, and fundamental variability in the stochastic processes that the statistical models aim to simulate. It’s likely to be the most challenging part of the blog series, but worth sticking with."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-05/index.html#aim",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Aim",
    "text": "Aim\nIn the first part of this series, I stated that statistical model fitting, within the generalised model framework presented in King, Tomz, and Wittenberg (2000), involves adjusting candidate values for elements of \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_K \\}\\) such that the difference between what the model predicts given some predictor values, \\(Y_i | X_i\\), and what has been observed alongside the predictors, \\(y_i\\), is minimised on average1 in some way.\nThe aim of this post is to show how this process is typically implemented in GLMs, using likelihood theory."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html#bayes-rule-and-likelihood",
    "href": "posts/glms/lms-are-glms-part-05/index.html#bayes-rule-and-likelihood",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Bayes’ Rule and Likelihood",
    "text": "Bayes’ Rule and Likelihood\nStatisticians and more advanced users of statistical models often divide themselves into ‘frequentists’ and ‘Bayesians’. To some extent the distinction is really between ‘improper Bayesians’ and ‘proper Bayesians’, however, as Bayes’ Rule is at the root of both approaches. Bayes’ Rule is:\n\\[\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\\]\nNote in the above the left hand side of the equation is \\(P(A|B)\\) and the right hand side of the equation includes \\(P(B|A)\\). To write it out as awkward prose, therefore, Bayes’ Rule is a way of expressing that given this in terms of this given that.\nAs with much of algebra, \\(A\\) and \\(B\\) are just placeholders. We could instead use different symbols instead, such as:\n\\[\nP(\\tilde{\\theta} | y) = \\frac{P(y | \\tilde{\\theta})P(\\tilde{\\theta})}{P(y)}\n\\]\nLikelihood theory offers a way of thinking about how good a model is in terms of its relationship to the data. According to King (1998) (p. 59), it can be expressed as:\n\\[\nL(\\tilde{\\theta}| y) = k(y) P(y | \\tilde{\\theta})\n\\]\nOr\n\\[\nL(\\tilde{\\theta} | y) \\propto P(y | \\tilde{\\theta})\n\\]\nWhere \\(\\tilde{\\theta}\\) is a proposed parameter or parameter combination for the model, and \\(y\\) is the observed outcome.2\nThe important thing to note is that both Bayes’ Rule and Likelihood Theory are ways of expressing this given that as a function of that given this. Specifically, the model given the data, as a function of the data given the model. 3"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html#likelihood-for-linear-regression",
    "href": "posts/glms/lms-are-glms-part-05/index.html#likelihood-for-linear-regression",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Likelihood for linear regression",
    "text": "Likelihood for linear regression\nWhen, many years ago, I completed the course from this modelling framework is most associated, a hazing ritual employed near the start of the course was to require participants to derive the likelihood of different model specifications. However, I don’t feel like hazing myself right now, so instead we can use the derivation shown on slide 8 of these slides:\n\\[\nL(\\beta, \\sigma^2 | y) = \\prod{L(y_i | \\mu_i, \\sigma^2)}\n\\]\nWhere \\(\\mu = X \\beta\\), \\(i\\) indicates an observation in the data (a row of \\(X\\) when \\(X\\) is in matrix form), and \\(\\prod\\) indicates the likelihoods from each observation should be multiplied with each other to derive the overall likelihood for all observed data.\nIn practice the log Likelihood, rather than the likelihood itself, is used, because this allows calculation of a sum of terms (\\(\\sum\\)) rather than product of terms (\\(\\prod\\)), and the latter tends to be computationally easier to calculate.\nAs we are interested only in how likelihood varies as a function of those model parameters we wish to estimate, \\(\\theta = \\{\\beta, \\sigma^2\\}\\), some of the terms in the log likelihood expression can be omitted, leaving us with:\n\\[\n\\log{L(\\beta, \\sigma^2 | y)} \\doteq \\sum{-\\frac{1}{2}[\\log{\\sigma^2} + \\frac{(y_i - X_i\\beta)^2}{\\sigma^2}]}\n\\]\nFor all the complexity of the above expression, at heart it takes three inputs:\n\n\\(\\theta = \\{\\beta, \\sigma^2\\}\\) : The candidate parameters for the model.\n\\(y\\) : the observed response value from the dataset \\(D\\)\n\\(X\\) : the observed predictor values from the dataset \\(D\\)\n\nAnd returns one value, the log likelihood \\(\\log{L(.)}\\).\nTo reiterate, we can’t change the data, but we can keep changing the candidate parameters \\(\\theta\\). Each time we do so, \\(\\log{L(.)}\\) will change too.\nThe aim of model calibration, in the Likelihood framework, is to maximise the Likelihood. The parameter set that maximises the likelihood is also the parameter set that maximises the log likelihood.\nTo continue the example from the slides, we can write out a function for calculating the log likelihood of standard linear regression as follows:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\nIn the above, pars is (almost but not quite) \\(\\theta\\), the parameters to estimate. For standard linear regression \\(\\theta = \\{\\beta, \\sigma^2\\}\\), where \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_k\\}\\), i.e. a vector of beta parameters, one for each column (variable) in \\(X\\), the predictor matrix of observations; this is why \\(beta\\) is selected from the first K values in pars where K is the number of columns in \\(X\\).\nThe last value in pars is used to derive the proposed \\(\\sigma^2\\). If we call this last value eta (\\(\\eta\\)), then we can say \\(\\sigma^2 = e^{\\eta}\\). So, whereas \\(\\theta\\) is a vector that ‘packs’ \\(\\beta\\) and \\(\\sigma^2\\) into a single ordered series of values, pars packs eta in place of \\(\\sigma^2\\). This substitution of eta for \\(\\sigma^2\\) is done to make it easier for standard parameter fitting algorithms to work, as they tend to operate over the full real number range, rather than just over positive values.\nIn order to illustrate how the log likelihood function llNormal works in practice, let’s construct a simple toy dataset \\(D\\), and decompose \\(D = \\{y, X\\}\\), the two types of data input that go into the llNormal function.\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nIn the code above we have created \\(y\\), a vector of nine observed responses; and \\(X\\), a matrix of predictors with two columns (the number of variables for which \\(beta\\) terms need to be estimated) and nine rows (the number of observations).\nGraphically, the relationship between x and y looks as follows:\n\n\nCode\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()\n\n\n\n\n\nIn this toy example, but almost never in reality, we know the correct parameters for the model. These are \\({\\beta_0 = 2.5, \\beta_1 = 1.4}\\) and \\(\\sigma^2 = 0.25\\). 4 Soon, we will see how effectively we can use optimisation algorithms to recover these true model parameters. But first, let’s see how the log likelihood varies as a function jointly of different candidate values of \\(\\beta_0\\) (the intercept) and \\(\\beta_1\\) (the slope parameter), if we already set \\(\\sigma^2\\) to 0.25.\n\n\nCode\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-5, 5, by = 0.1),\n    beta_1 = seq(-5, 5, by = 0.1)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\n\n\n\nCode\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nLooking at this joint surface of values, we can see a ‘hotspot’ where \\(\\beta_0\\) is around 2.5, and \\(\\beta_1\\) is around 1.4, just as we should expect. We can check this further by filtering candidate_param_values on the highest observed values of ll.\n\n\nCode\ncandidate_param_values |&gt; \n    filter(ll == max(ll))\n\n\n# A tibble: 1 × 3\n  beta_0 beta_1    ll\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    2.4    1.4  1.41"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html#summary",
    "href": "posts/glms/lms-are-glms-part-05/index.html#summary",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Summary",
    "text": "Summary\nWe have now introduced the concepts of Bayes Rule, Likelihood, and log likelihood, then derived the log likelihood for standard linear regression. We then built a toy dataset where we know the true parameters, and looked at how the log likelihood varies as different \\(\\beta\\) parameters are proposed. We identified a ‘hot spot’ when the \\(\\beta\\) parameters proposed are close to the ‘true values’."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html#coming-up",
    "href": "posts/glms/lms-are-glms-part-05/index.html#coming-up",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Coming up",
    "text": "Coming up\nThe next part of this series shows how log likelihood functions tend to be used in practice, in conjunction with optimisation algorithms that (usally) arrive at good estimates of our coefficients in far fewer steps than we’ve used above."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-05/index.html#footnotes",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf \\(Y_i\\) is what the model predicts given observations \\(X_i\\), and \\(y_i\\) is the outcome observed to have occurred alongside \\(X_i\\), then we can call \\(\\delta_i = h(y_i, Y_i)\\) the difference, or error, between predicted and observed value. The function \\(h(.,.)\\) is typically the squared difference between predicted and observed values, \\((Y_i - y_i)^2\\), but could also in principle be the absolute difference \\(|Y_i - y_i|\\). Term-fitting algorithms usually compare not any individual \\(\\delta_i\\), but a sum of these error terms \\(\\delta\\). The aim of the algorithm is to find the set of \\(\\beta\\) terms that is least wrong for the whole dataset \\(D\\), rather than any specific row in the dataset \\(D_i\\).↩︎\nAs King (1998) (p. 59) describes it, “\\(k(y)\\) is an unknown fuction of the data. Whereas traditional probability is a measure of absolute uncertainty … the constant \\(k(y)\\) means that likelihood is only a relative measure of uncertainty”↩︎\nFrequentist approaches can thus be considered a kind of ‘improper Bayesian’ approach by considering \\(k(y)\\) in the Likelihood formula as a stand-in for \\(\\frac{P(\\tilde{\\theta})}{P(y)}\\) in Bayes’ Rule. Roughly speaking, it’s because of the improperness of treating the two terms as equivalent, and the relativeness of \\(k(y)\\), that mean frequentist probability statements can’t be interpreted as Bayesian probability statements. But thinking of the two terms as equivalent can be helpful for spotting the similarity between the two formulae.↩︎\ni.e. the square of the sd passed to rnorm() of 0.5↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-03/index.html",
    "href": "posts/glms/lms-are-glms-part-03/index.html",
    "title": "Part Three: glm is just fancy lm",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-03/index.html#tldr",
    "href": "posts/glms/lms-are-glms-part-03/index.html#tldr",
    "title": "Part Three: glm is just fancy lm",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-03/index.html#part-3-how-to-express-a-linear-model-as-a-generalised-linear-model",
    "href": "posts/glms/lms-are-glms-part-03/index.html#part-3-how-to-express-a-linear-model-as-a-generalised-linear-model",
    "title": "Part Three: glm is just fancy lm",
    "section": "Part 3: How to express a linear model as a generalised linear model",
    "text": "Part 3: How to express a linear model as a generalised linear model\nIn the last part, we introduced two types of generalised linear models, with two types of transformation for the systematic component of the model, g(.), the logit transformation, and the identity transformation. This post will show how this framework is implemented in practice in R.\nIn R, there’s the lm function for linear models, and the glm function for generalised linear models.\nI’ve argued previously that the standard linear regression is just a specific type of generalised linear model, one that makes use of an identity transformation I(.) for its systematic component g(.). Let’s now demonstrate that by producing the same model specification using both lm and glm.\nWe can start by being painfully unimaginative and picking using one of R’s standard datasets\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\niris |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\nIt looks like, where the petal length is over 2.5, the relationship with sepal length is fairly linear\n\niris |&gt; \n  filter(Petal.Length &gt; 2.5) |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\nSo, let’s make a linear regression just of this subset\n\niris_ss &lt;- \n  iris |&gt; \n  filter(Petal.Length &gt; 2.5) \n\nWe can produce the regression using lm as follows:\n\nmod_lm &lt;- lm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\nAnd we can use the summary function (which checks the type of mod_lm and evokes summary.lm implicitly) to get the following:\n\nsummary(mod_lm)\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09194 -0.26570  0.00761  0.21902  0.87502 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3731 on 98 degrees of freedom\nMultiple R-squared:  0.6864,    Adjusted R-squared:  0.6832 \nF-statistic: 214.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nWoohoo! Three stars next to the Petal.Length coefficient! Definitely publishable!\nTo do the same using glm.\n\nmod_glm &lt;- glm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\nAnd we can use the summary function for this data too. In this case, summary evokes summary.glm because it knows the class of mod_glm contains glm.\n\nsummary(mod_glm)\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nSo, the coefficients are exactly the same. But there’s also some additional information in the summary, including on the type of ‘family’ used. Why is this?\nIf we look at the help for glm we can see that, by default, the family argument is set to gaussian.\nAnd if we delve a bit further into the help file, in the details about the family argument, it links to the family help page. The usage statement of the family help file is as follows:\nfamily(object, ...)\n\nbinomial(link = \"logit\")\ngaussian(link = \"identity\")\nGamma(link = \"inverse\")\ninverse.gaussian(link = \"1/mu^2\")\npoisson(link = \"log\")\nquasi(link = \"identity\", variance = \"constant\")\nquasibinomial(link = \"logit\")\nquasipoisson(link = \"log\")\nEach family has a default link argument, and for this gaussian family, this link is the identity function.\nWe can also see that, for both the binomial and quasibinomial family, the default link is logit, which transforms all predictors onto a 0-1 scale, as shown in the last post.\nSo, by using the default family, the Gaussian family is selected, and by using the default Gaussian family member, the identity link is selected.\nWe can confirm this by setting the family and link explicitly, showing that we get the same results\n\nmod_glm2 &lt;- glm(Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), data = iris_ss)\nsummary(mod_glm2)\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), \n    data = iris_ss)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nIt’s the same!\nHow do these terms used in the glm function, family and link, relate to the general framework in King, Tomz, and Wittenberg (2000)?\n\nfamily is the stochastic component, f(.)\nlink is the systematic component, g(.)\n\nThey’re different terms, but it’s the same broad framework.\nLinear models are just one type of general linear model!\n\nComing up\nIn the next part of this series, we will delve into the differences between linear regression models and logistic regression models, with a focus on how to get meaningful effect estimates from both types of model."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-13/index.html",
    "href": "posts/glms/lms-are-glms-part-13/index.html",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "",
    "text": "In the last post we reached the end of a winding journey. This post will show how Bayesian approaches to model fitting, rather than the frequentist approaches more commonly used, can reach the intended destination of this journey more quickly, despite being a bit more conceptually challenging to start with."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-13/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-13/index.html#aim",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "",
    "text": "In the last post we reached the end of a winding journey. This post will show how Bayesian approaches to model fitting, rather than the frequentist approaches more commonly used, can reach the intended destination of this journey more quickly, despite being a bit more conceptually challenging to start with."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-13/index.html#recap",
    "href": "posts/glms/lms-are-glms-part-13/index.html#recap",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "Recap",
    "text": "Recap\nThe start of this blog series aimed to do two things:\n\nReintroduce statistical models via a generalised model formulae, comprising a systematic component and a stochastic component.\nReintroduce the fitting of statistical models from the perspective of algorithmic optimisation, in which the gap between what the model predicts and what’s observed is minimised in some way.\n\nThe rest of the first section of the series - posts two, three and four - added more context to the first post, and introduced the concept of using models for prediction - and the types of quantities of interest they can predict. The first section ended with post four, which illustrated some of the complexities of getting meaningful effect estimates - the overall effect of one specific predictor variable on the outcome being predicted - for model structures under than standard linear regression.\nThe second section - covering posts five to ten - delved into a lot more detail about how statistical models are fit. It introduced the concept of likelihood as a means of deciding what the target of a statistical optimisation algorithm should be. And it also showed - in sometimes excruciating detail - how to perform numeric optimisation based on likelihood in order to extract not just the best set of model parameters, but estimates of joint uncertainty in the best estimated set of model parameters. It’s this joint uncertainty in parameter estimates, estimated via the Hessian from the optim() function, which allowed uncertainty in model parameter estimates to be propagated and percolated through specific ‘what-if?’ questions - i.e. specific configurations of predictor variables passed through to the model - in order to produce honest answers to these ‘what-if?’ questions, which provide a range of answers, rather than a single answer, in order to show how model parameter estimation uncertainty leads to uncertainty in the answers the model provides.\nThe third section - posts 10-12 - completed the journey, showing how many of the concepts and ideas learned through considerable effort in sections one and (especially) two allow more intelligent and effective use of standard statistical model outputs - produced using R’s lm() and glm() functions - for honest prediction.\nThis post will extend the third section to show why the kind of honest prediction which we managed to produce using the kind of frequentist modelling framework used by lm() and glm() are, in fact, easier to produce using Bayesian models."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-13/index.html#on-marbles-and-jumping-beans",
    "href": "posts/glms/lms-are-glms-part-13/index.html#on-marbles-and-jumping-beans",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "On marbles and jumping beans",
    "text": "On marbles and jumping beans\nPost five introduced Bayes’ Rule and the Likelihood axiom. It pointed out that, at heart, Bayes’ Rule is a way of expressing that given this in terms of this given that; and that Likelihood is also a claim about how that given this relates to this given that. More specifically, the claim of Likelihood is:\n\nThe likelihood of the model given the data is proportional to the probability of the data given the model.\n\nThere are two aspects to the model: firstly its structure; secondly its parameters. The structure includes the type of statistical model - whether it is a standard linear regression, negative binomial regression, logistic regression, Poisson regression model and so on - and also the specific types of columns from the dataset selected as either predictor variables (\\(X\\)) or response variables (\\(Y\\)). It is only after both the higher level structure of the model family, and the lower level structure of the data inputs (what’s being regressed on what?) have been decided that the Likelihood theory is used.\nAnd how is Likelihood theory used? Well, it defines a landscape over which an algorithm searches. This landscape has as many dimensions as there are parameters to fit. Where there are just two parameters, \\(\\beta_0\\) and \\(\\beta_1\\) to fit, we can visualise this landscape using something like a contour plot, with \\(\\beta_0\\) as latitude, \\(\\beta_1\\) as longitude, and the likelihood at this position its elevation or depth. Each possible joint value \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which the algorithm might wish to propose leads to a different long-lat coordinate over the surface, and each coordinate has a different elevation or depth. Although we can’t see beyond three dimensions (latitude, longitude, and elevation/depth), mathematics has no problem extending the concept of multidimensional space into far more dimensions than we can see or meaningfully comprehenend. If a model has ten parameters to fit, for example, the likelihood search space really is ten dimensional, and so on.\nNoticed I used elevation and depth interchangably in the description above. Well, this is because it really doesn’t matter whether an optimisation algorithm is trying to find the greatest elevation over a surface, or the greatest depth over the surface. The aim of maximum likelihood estimation is to find the configuration of parameters that maximises the likelihood, i.e. finds the top of the surface. However we saw that when passing the likelihood function to optim() we often inverted the function by multiplying it by -1. This is because the optimisation algorithms themselves seek to minimise the objective function they’re passed, not maximise it. By multiplying the likelihood function by -1 we made what we were trying to seek compatible with what the optimisation algorithms seek to do: find the greatest depth over a surface, rather than the highest elevation over the surface.\nTo make this all a bit less abstract let’s develop the intuition of an algorithm that seeks to minimise a function by way of a(nother) weird little story:\n\nImagine there is a landscape made out of transparent perspex. It’s not just transparent, it’s invisible to the naked eye. And you want to know where the lowest point of this surface is. All you have to do this is a magical leaking marble. The marble is just like any other marble, except every few moments, at regular intervals (say every tenth of a second), it dribbles out a white dye that you can see. And this dye sticks on and stains the otherwise invisible landscape whose lowest point you wish to find.\n\n\nNow, you drop the marble somewhere on the surface. You see the first point it hits on the surface - a white blob appears. The second blob appears some distance away from the first blob; and the third blob slightly less far away from the second blob as the second was to the second. After a few seconds, a trail of white spots is visible, the first few of which form something like a straight line, each consecutive point slightly less closer to the previous one. A second or two later, and the rumbling sounds of the marble rolling over the surface cease; the marble has clearly run out of momentum. And as you look at the trail of dots it’s generated, and is still generating, and you see it keeps highlighting the same point on the otherwise invisible surface, again and again.\n\nPreviously I used the analogy of a magical robo-chauffer, taking you to the top of a landscape. But the falling marble is probably a closer analogy to how many of optim()’s algorithms actually work. Using gravity and its shape alone, it finds the lowest point on the surface, and with its magical leaking dye, it tells you where this lowest point is.\nNow let’s extend the story to convert the analogy of the barefoot-and-blind person from part sevencd as well:\n\nThe marble has now ‘told’ you where the lowest point on the invisible surface is. However you also want to know more about the shape of the depression it’s in. You want to know if it’s a steep depression, or a shallow depression. And you want to know if it’s as steep or shallow in every direction, or if it’s steeper in some ways than the other.\n\n\nSo you now have to do a bit more work. You move your hand to just above the marble, and with your forefinger ‘flick’ it in a particular direction (say east-west): you see it move in the direction you flick it briefly, before rolling back towards (and beyond, and then towards) the depression point. As it does so, it leaks dye onto the surface, revealing a bit more about the landscape’s steepness or shallowness in this dimension. Then you do the same, but along a different dimension (say, north-south). After you’ve done this enough times, you are left with a collection of dyed points on the part of the surface closest to its deepest depression. The spacing and shape of these points tells you something about the nature of the depression and the part of the landscape it’s surrounding.\n\nNotice in this analogy you had to do extra work to get the marble to reveal more information about the surface. By default, the marble tells you the specific location of the depression, but not what the surface is like around this point. Instead, you need to intervene twice: firstly by dropping the marble onto the surface; secondly by flicking it around once it’s reached the lowest point on the surface.\nNow, let’s imagine swapping out our magical leaking marble for something even weirder: a magical leaking jumping bean.\n\nThe magical jumping bean does two things: it leaks and it jumps. (Okay, it does three things: when it leaks it also sticks to the surface it’s dying). When the bean is first dropped onto the surface, it marks the location it lands on. Then, it jumps up and across in a random direction. After jumping, it drops onto another part of the surface, marks it, and the process starts again. Jumping, sticking, marking; jumping, sticking, marking; jumping, sticking, marking… potentially forever.\n\n\nBecause of the effect of gravity, though the jumping bean jumps in a random direction, after a few jump-stick-mark steps it’s still, like the marble, very likely to move towards the depression. However, unlike the marble, even when it gets towards the lowest point in the depression, it’s not going to just rest there. The magical jumping bean is never at rest. It’s forever jump-stick-marking, jump-stick-marking.\n\n\nHowever, once the magical bean has moved towards the depression, though it keeps moving, it’s likely never to move too far from the depression. Instead, it’s likely to bounce around the depression. And as it does so, it drops ever more marks on the surface, which keep showing what the surface looks like around the depression in ever more detail.\n\nSo, because of the behaviour of the jumping bean, you only have to act on it once, by choosing where to drop it, rather than twice as with the marble: first choosing where to drop it, then flicking it around once it’s reached the lowest point on the surface."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-13/index.html#so-what",
    "href": "posts/glms/lms-are-glms-part-13/index.html#so-what",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "So what?",
    "text": "So what?\nIn the analogies above, the marble is to frequentist statistics as the jumping bean is to Bayesian statistics. A technical distinction between the marble and the jumping bean is that the marble converges towards a point (meaning it reaches a point of rest on the surface) whereas the jumping bean converges towards a distribution (meaning it never rests).\nIt’s Bayesian statistics’ 1 property of converging to a distribution rather than a point that makes the converged posterior distribution of parameter estimates Bayesian models produce ideal for the kind of honest prediction so much of this blog series has been focused on.\nLet’s now do some Bayesian modelling to compare…"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-13/index.html#bayesian-modelling-now-significantly-less-terrifying-than-it-used-to-be",
    "href": "posts/glms/lms-are-glms-part-13/index.html#bayesian-modelling-now-significantly-less-terrifying-than-it-used-to-be",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "Bayesian modelling: now significantly less terrifying than it used to be",
    "text": "Bayesian modelling: now significantly less terrifying than it used to be\nThere are a lot of packages and approaches for building Bayesian models. In fact there are whole statistical programming languages - like JAGS, BUGS 2 and Stan - dedicated to precisely describing every assumption the statistician wants to make about how a Bayesian model should be built. For more complicated and bespoke models these are ideal.\nHowever there are also an increasingly large number of Bayesian modelling packages that abstract away some of the assumptions and complexity apparent in the above specialised Bayesian modelling languages, and allow Bayesian versions of the kinds of model we’re already familiar with to be specified using formulae interfaces almost identical to what we’ve already worked with. Let’s look at one of them, rstanarm, which allows us to use stan, a full Bayesian statistical programming language, without quite as much thinking and set-up being required on our part.\nLet’s try to use this to build a Bayesian equivalent of the hamster tooth model we worked on in the last couple of posts.\n\nData Preparation and Frequentist modelling\nLet’s start by getting the dataset and building the frequentist version of the model we’re already familiar with:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nCode\nbest_model_frequentist &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model_frequentist)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nBuilding the Bayesian equivalent\nNow how would we build a Bayesian equivalent of this? Firstly let’s load (and if necessary install3) rstanarm.\n\n\nCode\nlibrary(rstanarm)\n\n\nWhereas for the frequentist model we used the function lm(), rstanarm has what looks like a broadly equivalent function stan_lm(). However, as I’ve just discovered, it’s actually more straightforward with stan_glm instead:\n\n\nCode\nbest_model_bayesian &lt;- stan_glm(len ~ log(dose) * supp, data = df)\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000905 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 9.05 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.033 seconds (Warm-up)\nChain 1:                0.032 seconds (Sampling)\nChain 1:                0.065 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.034 seconds (Warm-up)\nChain 2:                0.034 seconds (Sampling)\nChain 2:                0.068 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 8e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.033 seconds (Warm-up)\nChain 3:                0.031 seconds (Sampling)\nChain 3:                0.064 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.031 seconds (Warm-up)\nChain 4:                0.033 seconds (Sampling)\nChain 4:                0.064 seconds (Total)\nChain 4: \n\n\nCode\nsummary(best_model_bayesian)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      len ~ log(dose) * supp\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 60\n predictors:   4\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      20.6    0.7 19.8  20.6  21.5 \nlog(dose)         9.2    1.2  7.7   9.2  10.8 \nsuppVC           -3.7    1.0 -4.9  -3.7  -2.5 \nlog(dose):suppVC  3.9    1.7  1.7   3.9   6.1 \nsigma             3.8    0.4  3.3   3.8   4.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 18.8    0.7 17.9  18.8  19.7 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  3618 \nlog(dose)        0.0  1.0  2635 \nsuppVC           0.0  1.0  3695 \nlog(dose):suppVC 0.0  1.0  2653 \nsigma            0.0  1.0  3019 \nmean_PPD         0.0  1.0  3889 \nlog-posterior    0.0  1.0  1571 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nSome parts of the summary for the Bayesian model look fairly familiar compared with the frequentist model summary; other bits a lot more exotic. We’ll skip over a detailed discussion of these outputs for now, though it is worth comparing the estimates section of the summary directly above, from the Bayesian approach, with the frequentist model produced earlier.\nThe frequentist model had point estimates of \\(\\{20.7, 9.3, -3.7, 3.8\\}\\). The analogous section of the Bayesian model summary is the mean column of the estimates section. These are reported to fewer decimal places by default - Bayesians are often more mindful of spurious precision - but are also \\(\\{20.7, 9.3, -3.7, 3.8\\}\\), so the same to this number of decimal places.\nNote also the Bayesian model reports an estimate for an additional parameter, sigma. This should be expected if we followed along with some of the examples using optim() for linear regression: the likelihood function required the ancillary parameters (referred to as \\(\\alpha\\) in the ‘mother model’ which this series started with, and part of the stochastic component \\(f(.)\\)) be estimated as well as the primary model parameters (referred to as \\(\\beta\\) in the ‘mother model’, and part of the systematic component \\(g(.)\\)). The Bayesian model’s coefficients (Intercept), log(dose), suppVC and the interaction term log(dose):suppVC are all part of \\(\\beta\\), whereas the sigma parameter is part of \\(\\alpha\\). The Bayesian model has just been more explicit about exactly which parameters it’s estimated from the data.\nFor the \\(\\beta\\) parameters, the Std. Error column in the Frequentist model summary is broadly comparable with the sd column in the Bayesian model summary. For the \\(\\beta\\) parameters these values are \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Frequentist model, and \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Bayesian model the summary. i.e. they’re the same to the degree of precision offered in the Bayesian model summary.\nBut let’s get to the crux of the argument: with Bayesian models honest predictions are easier.\nAnd they are, with the posterior_predict() function, passing what we want to predict on through the newdata argument, much as we did with the predict() function with frequentist models.\n\n\nScenario modelling\nLet’s recall the scenarios we looked at previously:\n\npredicted and expected values: length when dosage is 1.25mg and supplement is OJ\nfirst difference difference between OJ and VC supplement when dosage is 1.25mg\n\nLet’s start with the first question:\n\n\nCode\npredictors &lt;- data.frame(supp = \"OJ\", dose = 1.25)\n\npredictions &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nhead(predictions)\n\n\n            1\n[1,] 19.98504\n[2,] 21.22190\n[3,] 23.11970\n[4,] 25.59836\n[5,] 25.29067\n[6,] 19.73129\n\n\nCode\ndim(predictions)\n\n\n[1] 4000    1\n\n\nBy default posterior_predict() returns a matrix, which in this case has 4000 rows and just a single column. Let’s do a little work on this and visualise the distribution of estimates it produces:\n\n\nCode\npreds_df &lt;- tibble(estimate = predictions[,1])\n\n# lower, median, upper\nlmu &lt;- quantile(preds_df$estimate, c(0.025, 0.500, 0.975))\n\nlwr &lt;- lmu[1]\nmed &lt;- lmu[2]\nupr &lt;- lmu[3]\n\npreds_df |&gt;\n    mutate(\n        in_range = between(estimate, lwr, upr)\n    ) |&gt;\n    ggplot(aes(x = estimate, fill = in_range)) + \n    geom_histogram(bins = 100) + \n    scale_fill_manual(\n        values = c(`FALSE` = 'lightgray', `TRUE` = 'darkgray')\n    ) +\n    theme(legend.position = \"none\") + \n    geom_vline(xintercept = med, linewidth = 1.2, colour = \"steelblue\")\n\n\n\n\n\nThe darker-shaded parts of the histogram show the 95% uncertainty interval, and the blue vertical line the median estimate. This 95% interval range is 15.08 to 30.11.\nRemember we previously estimated both the expected values and the predicted values for this condition. Our 95% range for the expected values were 20.27 to 24.19 (or thereabouts), whereas our 95% range for the predicted values were (by design) wider, at 15.34 to 30.11. The 95% uncertainty interval above is therefore of predicted values, which include fundamental variation due to the ancillary parameters \\(\\sigma\\), rather than expected values, which result from parameter uncertainty alone.\nThere are a couple of other functions in rstanarm we can look at: predictive_error() and predictive_interval()\nFirst here’s predictive_interval. It is a convenience function that the posterior distribution generated previously, predictions, and returns an uncertainty interval:\n\n\nCode\npredictive_interval(\n    predictions\n)\n\n\n        5%     95%\n1 16.21975 28.9483\n\n\nWe can see by default the intervals returned are from 5% to 95%, i.e. are the 90% intervals rather than the 95% intervals considered previously. We can change the intervals requested with the prob argument:\n\n\nCode\npredictive_interval(\n    predictions, \n    prob = 0.95\n)\n\n\n      2.5%    97.5%\n1 15.08242 30.11371\n\n\nAs expected, this requested interval returns an interval closer to (but not identical to) the interval estimated using the quantile function.\nLet’s see if we can also use the model directly, specifying newdata directly to predictive_interval:\n\n\nCode\npredictive_interval(\n    best_model_bayesian,\n    newdata = predictors, \n    prob = 0.95\n)\n\n\n      2.5%    97.5%\n1 14.74887 30.66403\n\n\nYes. This approach works too. The values aren’t identical as, no doubt, a more sophisticated approach is used by predictive_interval to estimate the interval than simply arranging the posterior estimates in order using quantile.\nFor producing expected values we can use the function posterior_epred:\n\n\nCode\nepreds &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nexp_values &lt;- epreds[,1]\n\nquantile(exp_values, probs = c(0.025, 0.500, 0.975))\n\n\n    2.5%      50%    97.5% \n21.31601 22.69422 24.11516 \n\n\nFor comparison, the expected value 95% interval we obtained from the Frequentist model was 21.3 to 24.2 when drawing from the quasi-posterior distribution, and 22.7 to 24.2 when using the predict() function with the interval argument set to \"confidence\".\nNow, finally, let’s see if we can produce first differences: the estimated effect of using VC rather than OJ as a supplement when the dose is 1.25mg\n\n\nCode\npredictors_x0 &lt;- data.frame(supp = \"OJ\", dose = 1.25)\npredictors_x1 &lt;- data.frame(supp = \"VC\", dose = 1.25)\n\npredictors_fd &lt;- rbind(predictors_x0, predictors_x1)\n\npredictions_fd &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd)\n\n\n            1        2\n[1,] 22.80440 28.66365\n[2,] 28.38536 22.76590\n[3,] 16.91994 14.33873\n[4,] 22.52597 18.60893\n[5,] 26.48877 22.36600\n[6,] 20.95703 15.48210\n\n\nThe newdata argument to posterior_predict now has two rows, one for the OJ supplement and the other for the VC supplement scenario. And the predictions matrix returned by posterior_predict now has two columns: one for each scenario (row) in predictors_fd. We can look at the distribution of both of these columns, as well as the rowwise comparisions between columns, which will give our distribution of first differences for the predicted values:\n\n\nCode\npreds_fd_df &lt;- \n    predictions_fd |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\n\nTo reiterate, these are predicted values for the two scenarios, not the expected values shown in the first differences section of post 12. This explains why there is greater overlap between the two distributions. Let’s visualise and calculate the first differences in predicted values:\n\n\nCode\npreds_fd_df |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nWe can see that the average of the distribution is below 0, but as we are looking at predicted values the range of distributions is much higher. Let’s get 95% intervals:\n\n\nCode\nquantile(preds_fd_df$fd, probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-13.549970  -2.714121   8.115712 \n\n\nThe 95% intervals for first differences in predicted values is from -13.6 to +7.9, with the median estimate at -3.0. As expected, the median is similar to the equivalent value from using expected values (-2.9) but the range is wider.\nNow let’s use posterior_epred to produce estimates of first differences in expected values, which will be more directly comparable to our first differences estimates in part 12:\n\n\nCode\npredictions_fd_ev &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd_ev)\n\n\n          \niterations        1        2\n      [1,] 22.92632 19.24492\n      [2,] 23.22840 19.56456\n      [3,] 22.75610 18.91480\n      [4,] 23.04020 19.62760\n      [5,] 22.04188 20.24057\n      [6,] 22.89794 19.09881\n\n\n\n\nCode\npreds_fd_df_ev &lt;- \n    predictions_fd_ev |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df_ev |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\n\nThis time, as the stochastic variation related to the \\(\\sigma\\) term has been removed, the distributions of the expected values are more distinct, with less overlap. Let’s visualise and compare the first differences of the expected values:\n\n\nCode\npreds_fd_df_ev |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\n\n\nCode\nquantile(preds_fd_df_ev$fd, probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-4.8476031 -2.8100893 -0.6692995 \n\n\nWe now have a 95% interval for the first difference in expected values of -4.9 to -0.7. By contrast, the equivalent range estimated using the Frequentist model in part 12 was -4.8 to -0.8. So, although they’re not identical, they do seem to be very similar."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-13/index.html#summing-up",
    "href": "posts/glms/lms-are-glms-part-13/index.html#summing-up",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "Summing up",
    "text": "Summing up\nUp until now we’ve been using Frequentist approaches to modelling. However the simulation approach required to produce honest uncertainty depends on ‘tricking’ Frequentist models into producing something like the converged posterior distributions which, in Bayesian modelling approaches, come ‘for free’ from the way in which Bayesian frameworks estimate model parameters.\nAlthough Bayesian models are generally more technically and computationally demanding than Frequentist models, we have shown the folllowing:\n\nThat packages like rstanarm abstract away some of the challenges of building Bayesian models from scratch;\nThat the posterior distributions produced by Bayesian models produce estimates of expected values, predicted values, and first differences - our substantive quantities of interest - that are similar to those produced previously from Frequentist models\nThat for the estimation of these quantities of interest, the posterior distributions Bayesian models generate make it more straightforward, not less, to produce using Bayesian methods than using Frequentist methods.\n\nThanks for reading, and congratulations on getting this far through the series."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-13/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-13/index.html#footnotes",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr perhaps more accurately Bayesian statistical model estimation rather than Bayesian statistics more generally? Bayes’ Rule can be usefully applied to interpret results derived from frequentist models. But the term Bayesian Modelling generally implies that Bayes’ Rule is used as part of the model parameter estimation process, in which a prior distribution is updated according to some algorithm, and then crucially the posterior distribution produced then forms the prior distribution at the next step in the estimation. The specific algorithm that works as the ‘jumping bean’ is usually something like Hamiltonian Monte Carlo, HMC, and the general simulation framework in which a posterior distribution generated from applying Bayes’ Rule is repeatedly fed back into the Bayes’ Rule equation as the prior distribution is known as Markov Chain Monte Carlo, MCMC.↩︎\nOminously named.↩︎\nrstanarm has a lot of dependencies. It’s the friendly, cuddly face of a beast!↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-15/index.html",
    "href": "posts/glms/lms-are-glms-part-15/index.html",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "",
    "text": "This is the second post on a short mini-series on causal inference. The previous post provided a non-technical introduction to the core challenge of causal inference, namely that the counterfactual is always unobserved, meaning at least half of the data required to really know the causal effect of something is always missing. In the previous post different historians made different assumptions about what the counterfactual would have looked like - what would have happened if something that did happen, hadn’t happened - and based on this came to very different judgements about the effect that Henry Dundas, an 18th century Scottish politician, had on the transatlantic slave trade.\nThis post is more technical, aiming to show: how awkward phrases like “What would have happened if something that did happen, hadn’t happened” are expressed algebraically; how the core problem of causal inference is expressed in this framework; the technical impossibility of addressing the question of causal inference from the Platinum Standard of estimating causal effects on individuals; and describe the reason why randomised controlled trials (RCTs) provide the Gold Standard for trying to estimate these effects for populations."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-15/index.html#introduction",
    "href": "posts/glms/lms-are-glms-part-15/index.html#introduction",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "",
    "text": "This is the second post on a short mini-series on causal inference. The previous post provided a non-technical introduction to the core challenge of causal inference, namely that the counterfactual is always unobserved, meaning at least half of the data required to really know the causal effect of something is always missing. In the previous post different historians made different assumptions about what the counterfactual would have looked like - what would have happened if something that did happen, hadn’t happened - and based on this came to very different judgements about the effect that Henry Dundas, an 18th century Scottish politician, had on the transatlantic slave trade.\nThis post is more technical, aiming to show: how awkward phrases like “What would have happened if something that did happen, hadn’t happened” are expressed algebraically; how the core problem of causal inference is expressed in this framework; the technical impossibility of addressing the question of causal inference from the Platinum Standard of estimating causal effects on individuals; and describe the reason why randomised controlled trials (RCTs) provide the Gold Standard for trying to estimate these effects for populations."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-15/index.html#models-dont-care-about-causality-but-we-do",
    "href": "posts/glms/lms-are-glms-part-15/index.html#models-dont-care-about-causality-but-we-do",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Models don’t care about causality… but we do",
    "text": "Models don’t care about causality… but we do\nThe first stage when using a statistical model is to take a big rectangle of data, \\(D\\), and split the columns of the data into two types:\n\nPredictor variables, usually denoted \\(X\\)\nResponse variables, usually denoted \\(y\\)\n\nWith the predictor variables and the response variables defined, the challenge of model fitting is then to find some combination of model parameters \\(\\theta\\) that minimises in some way the gap between the observed response values \\(y\\), and the predicted response values from the model \\(Y\\).\nThe first point to note is that, from the perspective of the model, it does not matter which variable or variables from \\(D\\) we choose to put in the predictor side \\(X\\) or the response side \\(y\\). Even if we put a variable from the future in as a predictor of something in the past, the optimisation algorithms will still work in exactly the same way, working to minimise the gap between observed and predicted responses. The only problem is such a model would make no sense from a causal perspective.\nThe model also does not ‘care’ about how we think about and go about defining any of the variables that go into the predictor side of the equation, \\(X\\). But again, we do. In particular, when thinking about causality it can be immensely helpful to imagine splitting the predictor columns up into some conceptually different types. This will be helpful for thinking about causal inference using some algebra."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-15/index.html#the-impossible-platinum-standard",
    "href": "posts/glms/lms-are-glms-part-15/index.html#the-impossible-platinum-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "The (Impossible) Platinum Standard",
    "text": "The (Impossible) Platinum Standard\nIn some previous expressions of the data, \\(D\\), we used the subscript \\(i\\) to indicate the rows of the data which go into the model. Each of these rows is, by convention, a different observation. So, instead of saying the purpose of the model is to predict \\(y\\) on \\(X\\), it’s more precisely to predict \\(y_i\\) on \\(X_i\\), for all \\(i\\) in the data (i.e. all rows in \\(D\\)).\nNow let’s do some predictor variable fission and say, for our purposes, that:\n\\[\nX_i = \\{X_i^*, Z_i\\}\n\\]\nHere \\(Z_i\\) is an assignment variable, and takes either a value of 1, meaning ‘is assigned’, or 0, meaning ‘is not assigned’. The variable \\(X_i^*\\), by contrast, means ‘all other predictor variables’.\nFor individual observations \\(D_i\\) where \\(Z_i = 1\\), the individual is exposed (or treated) to something. And for individual observations \\(D_i\\) where \\(Z_i = 0\\), the individual is not exposed (or not treated) to that thing.\nThe causal effect of assignment, or treatment, for any individual observation is:\n\\[\nTE_i = y_i|(X_i^*, Z = 1) - y_i| (X_i^*, Z = 0)\n\\]\nThe fundamental problem of causal inference, however, is that for any individual observation \\(i\\), one of the two parts of this expression is always missing. If an individual \\(i\\) had been assigned, then \\(y_i|(X_i^*, Z=1)\\) is observed, but \\(y_i|(X_i^*, Z=0)\\) is unobserved. By contrast, if an individual \\(i\\) had not been assigned, then \\(y_i|(X_i^*, Z=0)\\) is observed, but \\(y_i|(X_i^*, Z=1)\\) is unobserved.\nAnother way to think about this is as a table, where the treatment effect for an individual involves comparing the outcomes reported in two columns of the same row, but the cells in one of these two columns is always missing:\n\n\n\n\n\n\n\n\n\nindividual\noutcome if treated\noutcome if not treated\ntreatment effect\n\n\n\n\n1\n4.8\n??\n??\n\n\n2\n3.7\n??\n??\n\n\n3\n??\n2.3\n??\n\n\n4\n3.1\n??\n??\n\n\n5\n??\n3.4\n??\n\n\n6\n??\n2.9\n??\n\n\n\nThe Platinum Standard of causal effect estimation would therefore be if the missing cells in the outcome columns could be accurately filled in, allowing the treatment effect for each individual to be calculated.\nHowever, this isn’t possible. It’s social science fiction, as we can’t split the universe and compare parallel realities: one in which what happened didn’t happen, and the other in which what didn’t happen happened.\nSo, what can be done?"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-15/index.html#the-everyday-fools-gold-standard",
    "href": "posts/glms/lms-are-glms-part-15/index.html#the-everyday-fools-gold-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "The Everyday Fool’s Gold Standard",
    "text": "The Everyday Fool’s Gold Standard\nThere’s one thing you might be tempted to do with the kind of data shown in the table above: compare the average outcome in the treated group with the average outcome in the untreated group, i.e.:\n\\[\nATE = E(y | Z = 1) - E(y | Z = 0)\n\\]\nLet’s do this with the example above:\n\n\nCode\ne_y_z1 &lt;- mean(c(4.8, 3.7, 3.1))\ne_y_z0 &lt;- mean(c(2.3, 3.4, 2.9))\n\n\n# And the difference?\ne_y_z1 - e_y_z0\n\n\n[1] 1\n\n\nIn this example, the difference in the averages between the two groups is 1.0.1 Based on this, we might imagine the first individual, who was treated, would have had a score of 3.8 rather than 4.8, and the third individual, who was not treated, would have received a score of 3.3 rather than 2.3 if they had been treated.\nSo, what’s the problem with just comparing the averages in this way? Potentially, nothing. But potentially, a lot. It depends on the data and the problem. More specifically, it depends on the relationship between the assignment variable, \\(Z\\), and the other characteristics of the individual, which includes but is not usually entirely captured by the known additional characteristics of the individual, \\(X_i^*\\).\nLet’s give a specific example: What if I were to tell you that the outcomes \\(y_i\\) were waiting times at public toilets/bathrooms, and the assignment variable, \\(Z\\), takes the value 1 if the individual has been assigned to a facility containing urinals, and 0 if the individual has been assigned to a facility containing no urinals? Would it be right to infer that the difference in the average is the average causal effect of urinals in public toilets/bathrooms?\nI’d suggest not, because there are characteristics of the individual which govern assignment to bathroom type. What this means is that \\(Z_i\\) and \\(X_i^*\\) are coupled or related to each other in some way. So, any difference in the average outcome between those assigned to (or ‘treated with’) urinals could be due to the urinals themselves; or could be due to other ways that ‘the treated’ and ‘the untreated’ differ from each other systematically. We may be able to observe a difference, and to report that it’s statistically significant. But we don’t know how much, if any, of that difference is due to the exposure or treatment of primary interest to us, and how much is due to other ways in the ‘treated’ and ‘untreated’ groups differ.\nSo, we need some way of breaking the link between \\(Z\\) and \\(X^*\\). How do we do this?"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-15/index.html#why-randomised-controlled-trials-are-the-real-gold-standard",
    "href": "posts/glms/lms-are-glms-part-15/index.html#why-randomised-controlled-trials-are-the-real-gold-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Why Randomised Controlled Trials are the real Gold Standard",
    "text": "Why Randomised Controlled Trials are the real Gold Standard\nThe clue’s in the subheading. Randomised Controlled Trials (RCTs) are known as the Gold Standard for scientific evaluation of effects for a reason, and the reason is this: they’re explicitly designed to break the link between \\(Z\\) and \\(X^*\\). And not just \\(X^*\\), but any unobserved or unincluded characteristics of the individuals, \\(W^*\\), which might also otherwise influence assignment or selection to \\(Z\\) but we either couldn’t measure or didn’t choose to include.\nThe key idea of an RCT is that assignment to either a treated or untreated group, or to any additional arms of the trial, has nothing to do with the characteristics of any individual in the trial. Instead, the allocation is random, determined by a figurature (or historically occasionally literal) coin toss. 2\nWhat this random assignment means is that assignment \\(Z\\) should be unrelated to the known characteristics \\(X^*\\), as well as unknown characteristics \\(W^*\\). The technical term for this (if I remember correctly) is that assignment is orthogonal to other characteristics, represented algebraically as \\(Z \\perp X^*\\) and \\(Z \\perp W^*\\).\nThis doesn’t mean that, for any particular trial, there will be zero correlation between \\(Z\\) and other characteristics. Nor does it mean that the characteristics of participants will be the same across trial arms. Because of random variation there are always going to be differences between arms in any specific RCT. However, we know that, because we are aware of the mechanism used to allocate participants to treated or non-treated groups (or more generally to trial arms), the expected difference in characteristics will be zero across many RCTs. Along with increased observations, this is the reason why, in principle, a meta-analysis of methodologically identical RCTs should offer even greater precision as to the causal effect of a treatment than just relying on a single RCT. 3"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-15/index.html#summing-up-and-coming-up",
    "href": "posts/glms/lms-are-glms-part-15/index.html#summing-up-and-coming-up",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Summing up and coming up",
    "text": "Summing up and coming up\nA key point to note is that, when analysing a properly conducted RCT to estimate a treatment effect, the ATE formula shown above, which is naive and likely to be biased when working with observational data, is likely to produce an unbiased estimate of the treatment effect. Because the trial design is sophisticated in the way it breaks the link between \\(Z\\) and everything else, the statistical analysis does not have to be sophisticated.\nThe flip side of this, however, is that when the data are observational, and it would be naive (as with the urinals and waiting times example) to assume that \\(Z\\) is unlinked to everything else known (\\(X^*\\)) and unknown (\\(W^*\\)), then more careful and bespoke statistical modelling approaches are likely to be required to recover non-biased causal effects. Such modelling approaches need to be mindful of both the platinum and gold standards presented above, and rely on modelling assumptions to try to simulate what the treatment effects would be if these unobtainable (platinum) and unobtained (gold) standards had been obtained.\nThe next post will start to delve into some of these modelling approaches."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-15/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-15/index.html#footnotes",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is pure fluke. I didn’t choose the values to get a difference of exactly 1, but there we go…↩︎\nIn the gold-plated gold standard of the double-blind RCT, not even the people running the trial and interacting with participants would be aware of which treatment a participant has been assigned. They would simply be given a participant ID, find a pack containing the participant’s treatment, and give this pack to the participant. Only a statistician, who has access to a random number cypher, would know which participants are assigned to which treatment, and they might not know until the trial has concluded. The idea of all of these layers of secrecy in assignment is to reduce the possibility that those running the experiment could intentionally or unintentially inform participants about which treatment they’re receiving, and so create expectations in participants about the effectiveness or otherwise of the treatments, which could have an additional effect on the outcomes.↩︎\nIn practice, issues like methodological variation, and publication bias, mean that meta-analyses of RCTs are unlikely to provide as accurate and unbiased an estimate of treatment effect as we would hope for.↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-08/index.html",
    "href": "posts/glms/lms-are-glms-part-08/index.html",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "",
    "text": "The previous post, perhaps the toughest of the series, showed how some special settings within R’s numerical optimisation optim() function can be used to estimate how much uncertainty there is in our estimates of the the model parameters \\(\\beta\\). We covered the concept that information and uncertainty are inversely related: the more information we have, the less uncertain we are, and vice versa. We estimated parameter uncertainty around the point that maximised (log) likelihood by asking the algorithm to take small steps from this highest point in different directions (dimensions, in effect variables), and report how steep the fall is in different directions. Steeper falls along a dimension imply less uncertainty and so more more information and narrower confidence intervals; as usual, the converse is also true. The component returned by optim() which reports the results of this ‘stepping out’ is a square matrix called the Hessian, which can be inverted to produce estimates of the variances and covarainces of each of the parameters being estimated in our model."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-08/index.html#recap",
    "href": "posts/glms/lms-are-glms-part-08/index.html#recap",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "",
    "text": "The previous post, perhaps the toughest of the series, showed how some special settings within R’s numerical optimisation optim() function can be used to estimate how much uncertainty there is in our estimates of the the model parameters \\(\\beta\\). We covered the concept that information and uncertainty are inversely related: the more information we have, the less uncertain we are, and vice versa. We estimated parameter uncertainty around the point that maximised (log) likelihood by asking the algorithm to take small steps from this highest point in different directions (dimensions, in effect variables), and report how steep the fall is in different directions. Steeper falls along a dimension imply less uncertainty and so more more information and narrower confidence intervals; as usual, the converse is also true. The component returned by optim() which reports the results of this ‘stepping out’ is a square matrix called the Hessian, which can be inverted to produce estimates of the variances and covarainces of each of the parameters being estimated in our model."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-08/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-08/index.html#aim",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Aim",
    "text": "Aim\nThe aims of this post are to show how estimates of uncertainty around the point estimates produced from the Hessian, based around the curvature measured around the point of maximum likelihood, are similar to those produced using a much more extensive (and computationally intensive) interrogation of the likelihood surface using a grid-search approach. It will also show how representations of joint uncertainty for parameter values can be generated using the multivariate normal distribution."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-08/index.html#comparing-inferred-and-observed-likelihood-surfaces",
    "href": "posts/glms/lms-are-glms-part-08/index.html#comparing-inferred-and-observed-likelihood-surfaces",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Comparing inferred and observed likelihood surfaces",
    "text": "Comparing inferred and observed likelihood surfaces\nLet’s return once again to the toy dataset used in the last two posts, whose true parameters we know because we made them up; and also the log likelihood function:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nTo estract estimates of uncertainty about the uncertainty of each of these parameters, we used optim() with the options shown below, and then inverted the matrix to go from information to uncertainty.\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\nWe were especially interested in the first two rows and columns of this matrix, as they correspond to uncertainty in \\(\\beta = \\{ \\beta_0, \\beta_1 \\}\\).\n\n\nCode\ninv_hess_betas &lt;- inv_hess[1:2, 1:2]\n\ninv_hess_betas\n\n\n             [,1]         [,2]\n[1,]  0.033577455 -0.004379668\n[2,] -0.004379668  0.004379668\n\n\nBack in part five, we used this same dataset to show how the log likelihood varies for various, equally spaced, candidate values for \\(\\beta_0\\) and \\(\\beta_1\\) (having fixed \\(\\eta = \\exp({\\sigma^2})\\) at its true value). This led to the followng map of the landscape1\n\n\nCode\nlibrary(tidyverse)\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-15, 15, by = 0.05),\n    beta_1 = seq(-15, 15, by = 0.05)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nWithin the above we can see that the log likelihood landscape for these two parameters looks like a bivariate normal distribution, we can also see a bit of a slant in this normal distribution. This implies a correlation between the two candidate values. The direction of the slant is downwards from left to right, implying the correlation is negative.\nFirstly let’s check that the correlation between \\(\\beta_0\\) and \\(\\beta_1\\) implied by the Hessian is negative. These are the off-diagonal elements, either first row, second column, or second row, first column:\n\n\nCode\ninv_hess_betas[1,2]\n\n\n[1] -0.004379668\n\n\nCode\ninv_hess_betas[2,1]\n\n\n[1] -0.004379668\n\n\nYes they are!\nAs mentioned previously, the likelihood surface produced by the gridsearch method involves a lot of computations, so a lot of steps, and likely a lot of trial and error, if it were to be used to try to find the maximum likelihood value for the parameters. By contrast, the optim() algorithm typically involves far fewer steps, ‘feeling’ its way up the hill until it reaches a point where there’s nowhere higher. 2 When it then reaches this highest point, it then ‘feels’ the curvature around this point in multiple directions, producing the Hessian. The algorithm doesn’t see the likelihood surface, because it hasn’t travelled along most of it. But the Hessian can be used to infer the likelihood surface, subject to subject (usually) reasonable assumptions.\nWhat are these (usually) reasonable assumptions? Well, that the likelihood surface can be approximated by a multivariate normal distribution, which is a generalisation of the standard Normal distribution over more than one dimensions.3\nWe can use the mvrnorm function from the MASS package, alongside the point estimates and Hessian from optim, in order to produce estimates of \\(\\theta = \\{ \\beta_0, \\beta_1, \\eta \\}\\) which represent reasonable uncertainty about the true values of each of these parameters. Algebraically, this can be expressed as something like the following:\n\\[\n\\tilde{\\theta} \\sim Multivariate Normal(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere \\(\\dot{\\theta}\\) are the point estimates from optim() and \\(\\Sigma\\) is the implied variance-covariance matrix recovered from the Hessian.\nLet’s create this MVN model and see what kinds of outputs it produces.\n\n\nCode\nlibrary(MASS)\n\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\nhead(param_draws)\n\n\n        beta0    beta1         eta\n[1,] 2.564978 1.375636 -0.30407255\n[2,] 2.440111 1.367774 -1.16815288\n[3,] 2.775332 1.338583 -0.05574937\n[4,] 2.283011 1.481799 -0.26095101\n[5,] 2.695635 1.228565 -1.18369341\n[6,] 2.686818 1.483601 -0.44262363\n\n\nWe can see that mvrnorm(), with these inputs from optim() produces three columns: one for each parameter being estimated \\(\\{ \\beta_0, \\beta_1, \\eta \\}\\). The n argumment indicates the number of draws to take; in this case, 10000. This number of draws makes it easier to see how much variation there is in each of the estimates.\n\n\nCode\ndf_param_draws &lt;- \nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    mutate(\n        sig2 = exp(eta)\n    ) |&gt;\n    pivot_longer(\n        -draw, \n        names_to = \"param\",\n        values_to = \"value\"\n    ) \n    \ndf_param_draws |&gt;\n    ggplot(aes(x = value)) + \n    geom_density() + \n    facet_grid(param ~ .) + \n    geom_vline(xintercept=0)\n\n\n\n\n\nThere are a number of things to note here: firstly, that the average of the \\(\\beta_0\\) and \\(\\beta_1\\) values appear close to their known ‘true’ values of 2.5 and 1.4 respectively. Secondly, that whereas the \\(\\eta\\) values are normally distributed, the \\(\\sigma^2\\) values derived from them are not, and are never below zero; this is the effect of the exponential link between quantities. Thirdly, that the implied values of \\(\\sigma^2\\) do appear to be centred around 0.25, as they should be as \\(\\sigma\\) was set to 0.50 in the model.\nAnd forthly, that the density around \\(\\beta_1\\) is more peaked than around \\(\\beta_0\\). This concords with what we saw previously in the filled contour map: both the horizontal beta0 axis and vertical beta1 axis are on the same scale, but the oval is broader along the horizontal axis than the vertical axis. This in effect implies that we have more information about the true value of \\(\\beta_1\\), the slope, than about the true value of \\(\\beta_0\\), the intercept.\nWe can also use these draws to reproduce something similar to, but not identical to, 4 the previous filled contour map:\n\n\nCode\n# param_draws |&gt;\n#     as_tibble(\n#         rownames = 'draw'\n#     ) |&gt;\n#     ggplot(aes(x = beta0, y = beta1)) + \n#     geom_point(alpha = 0.1) + \n#     coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 10))\n\nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    ggplot(aes(x = beta0, y = beta1)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nOnce again, we see the same qualities as the contour map produced by interrogating the likelihood surface exhaustively: the distribution appears bivariate normal; there is a greater range in the distribution along the beta0 than the beta1 axis; and there is evidence of some negative correlation between the two parameters."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-08/index.html#summary",
    "href": "posts/glms/lms-are-glms-part-08/index.html#summary",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Summary",
    "text": "Summary\nThis post has shown how optim(), which in its vanilla state only returns point estimates, can be configured to also calculater and report the Hessian, a record of instantaneous curvature around the point estimates. Even without a fine-grained and exhausive search throughout the likelihood surface, this measure of curvature can be used to produce similar measures of uncertainty to the more exhausive approach, in a fraction of the number of computations.\nMore importantly, it can be used to generate draws of plausible combinations of parameter values, something denoted as \\(\\tilde{\\theta}\\) earlier. This is something especially useful for producing honest quantities of interest, which both tell users of models something they want to know, while also representing how uncertain we are in this knowledge.\nWe’ll cover that in the next post… 5"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-08/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-08/index.html#footnotes",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’ve narrowed the space between values slightly, and increased the range of permutations of values to search through, for an even more precise recovery of the likelihood landscape.↩︎\nIn practice, the algorithm seeks to minimise the value returned by the function, not maximise it, hence the negative being applied through the argument fnscale = -1 in the control argument. But the principle is identical.↩︎\nThis means that, whereas the standard Normal returns a single output, the Multivariate Normal returns a vector of outputs, one for each parameter in \\(\\theta\\), which should also be the length of the diagonal (or alternatively either the number of rows or columns) of \\(\\Sigma\\).↩︎\nThe values will not be identical because the values for \\(\\eta\\), and so \\(\\sigma^2\\), have not been fixed at the true value in this example.↩︎\nI was expecting to cover it in the current post, but this is probably enough content for now!↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html",
    "href": "posts/glms/lms-are-glms-part-06/index.html",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "",
    "text": "In the previous post in this series, I presented a function for calculating the log likelihood of a standard linear regression with a Normal error term. I then built a very simple dataset, ten data points linking \\(x\\) to \\(y\\), and showed how the log likelihood varied as a combination of different candidate values for the model’s intercept and slope terms (\\(\\beta_0\\) and \\(\\beta_1\\) respectively).\nThe aim of this this post is to show how the best parameter combinations tend to be estimated from a model’s log likelihood in practice, using an optimisation algorithm that iteratively tries out new parameter values, and keeps trying and trying until some kind of condition is met. This is what the last figure in the first post is trying to illustrate."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-06/index.html#aim",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "",
    "text": "In the previous post in this series, I presented a function for calculating the log likelihood of a standard linear regression with a Normal error term. I then built a very simple dataset, ten data points linking \\(x\\) to \\(y\\), and showed how the log likelihood varied as a combination of different candidate values for the model’s intercept and slope terms (\\(\\beta_0\\) and \\(\\beta_1\\) respectively).\nThe aim of this this post is to show how the best parameter combinations tend to be estimated from a model’s log likelihood in practice, using an optimisation algorithm that iteratively tries out new parameter values, and keeps trying and trying until some kind of condition is met. This is what the last figure in the first post is trying to illustrate."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#optimisation-algorithms-getting-there-faster",
    "href": "posts/glms/lms-are-glms-part-06/index.html#optimisation-algorithms-getting-there-faster",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Optimisation algorithms: getting there faster",
    "text": "Optimisation algorithms: getting there faster\nIn the previous post, we ‘cheated’ a bit when using the log likelihood function, fixing the value for one of the parameters \\(\\sigma^2\\) to the value we used when we generated the data, so we could instead look at how the log likelihood surface varied as different combinations of \\(\\beta_0\\) and \\(\\beta_1\\) were plugged into the formula. \\(\\beta_0\\) and \\(\\beta_1\\) values ranging from -5 to 5, and at steps of 0.1, were considered: 101 values of \\(\\beta_0\\), 101 values of \\(\\beta_1\\), and so over 10,0001 unique \\(\\{\\beta_0, \\beta_1\\}\\) combinations were stepped through. This approach is known as grid search, and seldom used in practice (except for illustration purposes) because the number of calculations involved can very easily get out of hand. For example, if we were to use it to explore as many distinct values of \\(\\sigma^2\\) as we considered for \\(\\beta_0\\) and \\(\\beta_1\\), the total number of \\(\\{\\beta_0, \\beta_1, \\sigma^2 \\}\\) combinations we would crawl through would be over 100,000 2 rather than over 10,000.\nOne feature we noticed with the likelihood surface over \\(\\beta_0\\) and \\(\\beta_1\\) in the previous post is that it appears to look like a hill, with a clearly defined highest point (the region of maximum likelihood) and descent in all directions from this highest point. Where likelihood surfaces have this feature of being single-peaked in this way (known as ‘unimodal’), then a class of algorithms known as ‘hill climbing algorithms’ can be applied to find the top of such peaks in a way that tends to be both quicker (fewer steps) and more precise than the grid search approach used for illustration in the previous post."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#code-recap",
    "href": "posts/glms/lms-are-glms-part-06/index.html#code-recap",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Code recap",
    "text": "Code recap\nLet’s copy over the code we used in the previous post for:\n\n\nCalculating log likelihood\n\n\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\nAnd\n\n\nGenerating our tame toy dataset of 10 data points\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nTo recap, the toy dataset looks as follows:\n\n\nCode\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#optim-our-robo-chauffeur",
    "href": "posts/glms/lms-are-glms-part-06/index.html#optim-our-robo-chauffeur",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "optim: our Robo-Chauffeur",
    "text": "optim: our Robo-Chauffeur\nNote how the llNormal function takes a single argument, pars, which packages up all the specific candidate parameter values we want to try out. In our previous post, we also had a ‘feeder function’, feed_to_ll, which takes the various \\(\\beta\\) candidate values from the grid and packages them into pars. In our previous post, we had to specify the candidate values to try to feed to llNormal packages inside pars.\nBut we don’t have to do this. We can instead use an algorithm to take candidate parameters, try them out, then make new candidate parameters and try them out, for us. Much as a taxi driver needs to know where to meet a passenger, but doesn’t want the passenger to tell them exactly which route to take, we just need to specify a starting set of values for the parameters to optimise. R’s standard way of doing this is with the optim function. Here’s it in action:\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe optim function returns a fairly complex output structure, with the following components:\n\npar: the values for the parameters (in our case \\(\\{\\beta_0, \\beta_1, \\eta \\}\\)) which the optimisation algorithm ended up with.\nvalue: the value returned by the function fn when the optim routine was stopped.\ncounts: the number of times the function fn was repeatedly called by optim before optim decided it had had enough\nconvergence: whether the algorithm used by optim completed successfully (i.e. reached what it considers a good set of parameter estimates in par), or not.\n\nIn this case, convergence is 0, which (perhaps counterintuitively) indicates a successful completion. counts indicates that optim called the log likelihood function 216 times before stopping, and par indicates values of \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\eta = -1.34\\}\\) were arrived at. As \\(\\sigma^2 = e^\\eta\\), this means \\(\\theta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\sigma^2 = 0.26 \\}\\). As a reminder, the ‘true’ values are \\(\\{\\beta_0 = 2.50, \\beta_1 = 1.40, \\sigma^2 = 0.25\\}\\).\nSo, the optim algorithm has arrived at pretty much the correct answers for all three parameters, in 216 calls to the log likelihood function, whereas for the grid search approach in the last post we made over 10,000 calls to the log likelihood function for just two of the three parameters.\nLet’s see if we can get more information on exactly what kind of path optim took to get to this set of parameter estimates. We should be able to do this by specifying a value in the trace component in the control argument slot…"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#comparisons",
    "href": "posts/glms/lms-are-glms-part-06/index.html#comparisons",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Comparisons",
    "text": "Comparisons\nFor comparison let’s see what lm and glm produce.\nFirst lm:\n\n\nCode\ntoy_df &lt;- tibble(\n    x = x, \n    y = y\n)\n\n\nmod_lm &lt;- lm(y ~ x, data = toy_df)\nsummary(mod_lm)\n\n\n\nCall:\nlm(formula = y ~ x, data = toy_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6082 -0.3852 -0.1668  0.2385  1.1092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5813 on 7 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 \nF-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07\n\n\n\\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\), i.e. the same to 2 decimal places.\nAnd now with glm:\n\n\nCode\nmod_glm &lt;- glm(y ~ x, data = toy_df, family = gaussian(link = \"identity\"))\n\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nOnce again, \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\)"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#discussion",
    "href": "posts/glms/lms-are-glms-part-06/index.html#discussion",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Discussion",
    "text": "Discussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#coming-up",
    "href": "posts/glms/lms-are-glms-part-06/index.html#coming-up",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Coming up",
    "text": "Coming up\nIn the next part of this series, we’ll see how other outputs available from optim can be used to estimate uncertainty in the parameters of interest, how this information can be used to produce the kinds of estimates of standard errors around coefficients which are summarised in glm and lm summary() functions, and which many (ab)users of statistical models obsess about when star-gazing, and how information about uncertainty in parameter estimates allows for more honest model-based predictions."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-06/index.html#footnotes",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(101^2 = 10201\\)↩︎\n\\(101^3 = 1030301\\)↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html",
    "href": "posts/glms/lms-are-glms-part-09/index.html",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "",
    "text": "The last eight posts in this series have taken us into some fairly arcane territory, including concepts like the use of link functions and statistical families within GLM, the likelihood theory of inference and its relation to Bayes’ Rule, and how models are fit in practice using optimisation algorithms. In the last couple of posts we showed how optim(), R’s standard optimisation function, can be used to recover not just the maximum likelihood (point) estimates of a series of parameters to be estimated, but also estimates of how much uncertainty there is about these estimates: both singularly, which gives rise to measures like standard errors, Z scores and P-values - the place where sadly all too many statistical analyses stop at; and jointly, through the calculation of the Hessian and corresponding variance-covariance matrix of uncertainty about the parameter vector.\nIn the last post, we showed how known uncertainty about the parameter values in the statistical model can be represented by using the point estimates \\(\\dot{\\theta}\\) and variance-covariance measure of uncertainty \\(\\Sigma\\) can be used to produce a long series of plausible joint estimates of \\(\\tilde{\\theta}\\) (the parameter estimates with uncertainty) by passing the above as parameters to the multivariate normal distribution and taking repeated draws."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#recap",
    "href": "posts/glms/lms-are-glms-part-09/index.html#recap",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "",
    "text": "The last eight posts in this series have taken us into some fairly arcane territory, including concepts like the use of link functions and statistical families within GLM, the likelihood theory of inference and its relation to Bayes’ Rule, and how models are fit in practice using optimisation algorithms. In the last couple of posts we showed how optim(), R’s standard optimisation function, can be used to recover not just the maximum likelihood (point) estimates of a series of parameters to be estimated, but also estimates of how much uncertainty there is about these estimates: both singularly, which gives rise to measures like standard errors, Z scores and P-values - the place where sadly all too many statistical analyses stop at; and jointly, through the calculation of the Hessian and corresponding variance-covariance matrix of uncertainty about the parameter vector.\nIn the last post, we showed how known uncertainty about the parameter values in the statistical model can be represented by using the point estimates \\(\\dot{\\theta}\\) and variance-covariance measure of uncertainty \\(\\Sigma\\) can be used to produce a long series of plausible joint estimates of \\(\\tilde{\\theta}\\) (the parameter estimates with uncertainty) by passing the above as parameters to the multivariate normal distribution and taking repeated draws."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-09/index.html#aim",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Aim",
    "text": "Aim\nIn this post, we’ll now, finally, show how this knowledge can be applied to do something with statistical models that ought to be done far more often: report on what King, Tomz, and Wittenberg (2000) calls quantities of interest, including predicted values, expected values, and first differences. Quantities of interest are not the direction and statistical significance (P-values) that many users of statistical models convince themselves matter, leading to the kind of mindless stargazing summaries of model outputs described in post four. Instead, they’re the kind of questions that someone, not trained to think that stargazing is satisfactory, might reasonably want answers to. These might include:\n\nWhat is the expected income of someone who completes course X in the five years after graduation? (Expected values)\nWhat is the expected range of incomes of someone who completes course X in the five years after graduation? (Predicted values)\nWhat is the expected difference in incomes between someone who completes course X, compared to course Y, in the five years after graduation? (First Differences)\n\nIn post four, we showed how to answer some of the questions of this form, for both standard linear regression and logistic regression. We showed that for linear regression such answers tend to come directly from the summary of coefficients, but that for logistic regression such answers tend to be both more ambiguous and dependent on other factors (such as gender of graduate, degree, ethnicity, age and so on), and require more processing in order to produce estimates for.\nHowever, we previously produced only point estimates for these questions, and so in a sense misled the questioner with the apparent certainty of our estimates. We now know, from post eight, that we can use information about parameter uncertainty to produce parameter estimates \\(\\tilde{\\theta}\\) that do convey parameter uncertainty, and so we can do better than the point estimates alone to answer such questions in way that takes into account such uncertainty, with a range of values rather than a single value."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#method",
    "href": "posts/glms/lms-are-glms-part-09/index.html#method",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Method",
    "text": "Method\nLet’s make use of our toy dataset one last time, and go through the motions to produce the \\(\\tilde{\\theta}\\) draws we ended with on the last post:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\n\n\nCode\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\n\nLet’s now look at our toy data again, and decide on some specific questions to answer:\n\n\nCode\nlibrary(tidyverse)\ntoy_df &lt;- tibble(x = x, y = y)\n\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() \n\n\n\n\n\nWithin the data itself, we have only supplied x and y values for whole numbers of x between -3 and 5. But we can use the model to produce estimates for non-integer values of x. Let’s try 2.5. For this single value of x, we can produce both predicted values and expected values, by passing the same value of x to each of the plausible estimates of \\(\\theta\\) returned by the multivariate normal function above.\n\n\nCode\ncandidate_x &lt;- 2.5"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#expected-values",
    "href": "posts/glms/lms-are-glms-part-09/index.html#expected-values",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Expected values",
    "text": "Expected values\nHere’s an example of estimating the expected value of y for x = 2.5 using loops and standard algebra:\n\n\nCode\n# Using standard algebra and loops\nN &lt;- nrow(param_draws)\nexpected_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    expected_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"]\n}\n\nhead(expected_y_simpler)\n\n\n[1] 6.004068 5.859547 6.121791 5.987509 5.767047 6.395820\n\n\nWe can see just from the first few values that each estimate is slightly different. Let’s order the values from lowest to highest, and find the range where 95% of values sit:\n\n\nCode\nev_range &lt;- quantile(expected_y_simpler,  probs = c(0.025, 0.500, 0.975)) \n\nev_range\n\n\n    2.5%      50%    97.5% \n5.505104 5.898148 6.291150 \n\n\nThe 95% interval is therefore between 5.51 and 6.29, with the median (similar but not quite the point estimate) being 5.90. Let’s plot this against the data:\n\n\nCode\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  median(expected_y_simpler), size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = ev_range[1], yend = ev_range[3], colour = \"blue\")\n\n\n\n\n\nThe vertical blue line therefore shows the range of estimates for \\(Y|x=2.5\\) that contain 95% of the expected values given the draws of \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which we produced from the Multivariate Normal given the point estimates and Hessian from optim(). This is our estimated range for the expected value, not predicted value. What’s the difference?"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#predicted-values",
    "href": "posts/glms/lms-are-glms-part-09/index.html#predicted-values",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Predicted values",
    "text": "Predicted values\nOne clue about the difference between expected value lies in the parameters from optim() we did and did not use: Whereas we have both point estimates and uncertainty estimates for the parameters \\(\\{\\beta_0, \\beta_1, \\sigma^2\\}\\),1 we only made use of the the two \\(\\beta\\) parameters when producing this estimate.\nNow let’s recall the general model formula, from the start of King, Tomz, and Wittenberg (2000), which we repeated for the first few posts in the series:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nThe manual for Zelig, the (now defunct) R package that used to support analysis using this approach, states that for Normal Linear Regression these two components are resolved as follows:\nStochastic Component\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\mu_i = x_i \\beta\n\\]\nThe page then goes onto state that the expected value, \\(E(Y)\\), is :\n\\[\nE(Y) = \\mu_i = x_i \\beta\n\\]\nSo, in this case, the expected value is the systematic component only, and does not involve the dispersion parameter in the stochastic component, which for normal linear regression is the \\(\\sigma^2\\) term. That’s why we didn’t use estimates of \\(\\sigma^2\\) when simulating the expected values.\nBut why is this? Well, it comes from the expectation operator, \\(E(.)\\). This operator means something like, return to me the value that would be expected if this experiment were performed an infinite number of times.\nThere are two types of uncertainty which give rise to variation in the predicted estimate: sampling uncertainty, and stochastic variation. In the expected value condition, this second source of variation falls to zero,2 leaving only the influence of sampling uncertainty, as in uncertainty about the true value of the \\(\\beta\\) parameters, remaining on uncertainty on the predicted outputs.\nFor predicted values, we therefore need to reintroduce stochastic variation as a source of variation in the range of estimates produced. Each \\(\\eta\\) value we have implies a different \\(\\sigma^2\\) value in the stochastic part of the equation, which we can then add onto the variation caused by parameter uncertainty alone:\n\n\nCode\nN &lt;- nrow(param_draws)\npredicted_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    predicted_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"] + \n        rnorm(\n            1, mean = 0, \n            sd = sqrt(exp(param_draws[i, \"eta\"]))\n        )\n}\n\nhead(predicted_y_simpler)\n\n\n[1] 4.802092 6.706397 7.073450 6.118750 6.757717 7.461254\n\n\nLet’s now get the 95% prediction interval for the predicted values, and compare them with the expected values predicted interval earlier\n\n\nCode\npv_range &lt;- \n    quantile(\n        predicted_y_simpler, \n        probs = c(0.025, 0.500, 0.975)\n    )\n\npv_range\n\n\n    2.5%      50%    97.5% \n4.766300 5.895763 7.055408 \n\n\nSo, whereas the median is similar to before, 5.90, the 95% interval is now from 4.77 to 7.063. This compares with the 5.51 to 6.29 range for the expected values. Let’s now plot this predicted value range just as we did with the expected values:\n\n\nCode\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  pv_range[2], size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = pv_range[1], yend = pv_range[3], colour = \"red\")\n\n\n\n\n\nClearly considerably wider."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#summary",
    "href": "posts/glms/lms-are-glms-part-09/index.html#summary",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Summary",
    "text": "Summary\nThis post is hopefully where our toy dataset, which we’ve been hauling with us since post five, can finally retire, happy in the knowledge that it’s taken us through some of the toughest parts of this blog series. The ideas developed over the last few posts can now finally be applied to answering some questions that are actually (or arguably) interesting!"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#coming-up",
    "href": "posts/glms/lms-are-glms-part-09/index.html#coming-up",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Coming up",
    "text": "Coming up\nThe next post covers the same kind of exercise we’ve performed for standard linear regression - specifying the likelihood function, and fitting it using optim() - but for logistic regression instead. This same kind of exercise could be repeated for all kinds of other model types. But hopefully this one additional example is sufficient."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-09/index.html#footnotes",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhere \\(\\sigma^2\\) is from \\(\\eta\\) and we defined \\(e^{\\eta} = \\sigma^2\\), a transformation which allowed optim() to search over an unbounded rather than bounded real number line↩︎\nIt can be easier to see this by using the more conventional way of expressing Normal linear regression: \\(Y_i = x_i \\beta + \\epsilon\\), where \\(\\epsilon \\sim Normal(0, \\sigma^2)\\). The expectation is therefore \\(E(Y_i) = E( x_i \\beta + \\epsilon ) = E(x_i \\beta) + E(\\epsilon)\\). For the first part of this equation, \\(E(x_i \\beta) = x_i \\beta\\), because the systematic component is always the same value, no matter how many times a draw is taken from the model. And for the second part, \\(E(\\epsilon) = 0\\), because Normal distributions are symmetrical around their central value over the long term: on average, every large positive value drawn from this distribution will become cancelled out by an equally large negative value, meaning the expected value returned by the distribution is zero. Hence, \\(E(Y) = x_i \\beta\\).↩︎\nBecause these estimates depend on random variation, these intervals may be slightly different to two decimal places than the values I’m quoting here.↩︎"
  },
  {
    "objectID": "posts/interactive-sliders/index.html",
    "href": "posts/interactive-sliders/index.html",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "",
    "text": "Below is an example of creating a plotly chart with an interactive slider using crosstalk.\nBy default, the plot shows the proportion of datazones in a local authority that are in the 15% most deprived datazones in Scotland. (Using the 2020 SIMD).\nThe slider allows different thresholds than the 15% default to be selected."
  },
  {
    "objectID": "posts/interactive-sliders/index.html#introduction",
    "href": "posts/interactive-sliders/index.html#introduction",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "",
    "text": "Below is an example of creating a plotly chart with an interactive slider using crosstalk.\nBy default, the plot shows the proportion of datazones in a local authority that are in the 15% most deprived datazones in Scotland. (Using the 2020 SIMD).\nThe slider allows different thresholds than the 15% default to be selected."
  },
  {
    "objectID": "posts/interactive-sliders/index.html#data-downloading-and-preparation",
    "href": "posts/interactive-sliders/index.html#data-downloading-and-preparation",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "Data Downloading and Preparation",
    "text": "Data Downloading and Preparation\nTo see the code itself, just click on the word ‘code’ to open up the block’.\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(crosstalk)\nlibrary(here)\n\n\n\n\nCode\nif(!file.exists(here(\"simd_data.xlsx\"))){\n  download.file(\n    url = \"https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2020/01/scottish-index-of-multiple-deprivation-2020-data-zone-look-up-file/documents/scottish-index-of-multiple-deprivation-data-zone-look-up/scottish-index-of-multiple-deprivation-data-zone-look-up/govscot%3Adocument/SIMD%2B2020v2%2B-%2Bdatazone%2Blookup.xlsx\",\n    destfile = here(\"simd_data.xlsx\"),\n    mode = \"wb\"\n  )\n}\n\ndta &lt;- openxlsx::readWorkbook(here(\"simd_data.xlsx\"), sheet = \"SIMD 2020v2 DZ lookup data\")\n\n\nThe code for the figure itself is below. It’s quite a convoluted process. There’s almost certaintly neater ways of doing this. The main thing to keep in mind is all the figures exist; just only one is visible at a time.\n\n\nCode\n# So let's construct a new aval containing the different x-y tuples given the threshold selected\n\ncalc_prop_deprived &lt;- function(q, dta){\n    dta %&gt;% \n      group_by(HBname) %&gt;% \n      summarise(prop_deprived = mean(pct_rank &lt; q)) %&gt;% \n      ungroup()\n}\n\ndf_rank &lt;- \n  dta %&gt;% \n    select(HBname, SIMD2020v2_Rank) %&gt;% \n    mutate(pct_rank = SIMD2020v2_Rank / max(SIMD2020v2_Rank))\n\n\nshared_df &lt;- tibble(\n  dep_quants = seq(0.05, 0.95, by = 0.05)\n) %&gt;% \n  mutate(derived_props = map(dep_quants, calc_prop_deprived, dta = df_rank)) %&gt;% \n  unnest(derived_props) %&gt;% \n  mutate(undep_quants = 1 - dep_quants) \n\n\n# Now to put it in the structure, and set active for `dep_quants = 0.15`\n\n\nunique_dep_quants &lt;- unique(shared_df$dep_quants)\nn_steps &lt;- length(unique_dep_quants)\n\ndep_vals &lt;- list()\nfor (step in 1:n_steps){\n  tmp &lt;- \n    shared_df %&gt;% \n      filter(dep_quants == unique_dep_quants[step]) %&gt;% \n      select(HBname, prop_deprived) %&gt;% \n      mutate(HBname = reorder(HBname, prop_deprived))\n  \n  dep_vals[[step]] &lt;- list(\n    visible = FALSE,\n    name = paste0('Quantile: ', unique_dep_quants[step]),\n    x=tmp$prop_deprived,\n    y=tmp$HBname\n    \n  ) \n}\n\n# 15% is the third list object \n\ndep_vals[3][[1]]$visible = TRUE\n\n# Now visualise \n\n# create steps and plot all traces\ndep_steps &lt;- list()\nfig &lt;- plot_ly() \nfor (i in c(3, 1, 2, 4:n_steps)) { # Start with 3 as this is 15% and this should determine the default HB order \n fig &lt;- add_bars(fig,x=dep_vals[i][[1]]$x,  y=dep_vals[i][[1]]$y, visible = dep_vals[i][[1]]$visible, \n                 name = dep_vals[i][[1]]$name, orientation = 'h', hoverinfo = 'x+y', color = I(\"gray\"),\n                 showlegend = FALSE) %&gt;% \n   layout(\n      title = list(\n        text = glue::glue(\"Proportion of datazones in Health Boards at least this deprived\")\n      ),\n      xaxis = list(\n        title = \"Proportion this deprived in Health Board\",\n        range = list(0, 1)\n      ),\n      yaxis = list(\n        title = \"Health Board\"\n      )\n   )\n\n  step &lt;- list(args = list('visible', rep(FALSE, length(dep_vals))),\n               method = 'restyle')\n  step$args[[2]][i] = TRUE  \n  step$label = unique_dep_quants[i]\n  dep_steps[[i]] = step \n}  \n#names(dep_steps) &lt;- unique_dep_quants\n\nfig &lt;- fig %&gt;%\n  layout(sliders = list(list(active = 2,\n                             currentvalue = list(prefix = \"Deprivation: \"),\n                             steps = dep_steps)))\n\nfig\n\n\n\n\n\n\nAs you can see, there’s still some work to do regarding formatting. But it works!"
  },
  {
    "objectID": "posts/interactive-sliders/index.html#static",
    "href": "posts/interactive-sliders/index.html#static",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "Static",
    "text": "Static\nFor comparison, here’s the same data used to produce a static plot\n\n\nCode\n# Now to put it in the structure, and set active for `dep_quants = 0.15`\n\ndf_15pc &lt;- shared_df |&gt; \n  filter(between(dep_quants, 0.149, 0.151)) |&gt; \n  select(-dep_quants, -undep_quants)\n\ndf_15pc |&gt;\n  mutate(pct_deprived = 100 * prop_deprived) |&gt; \n  ggplot(aes(y= pct_deprived, x = fct_reorder(HBname, pct_deprived))) + \n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(\n      label = ifelse(df_15pc$prop_deprived &gt; 0, sprintf(\"%.1f\", pct_deprived), \"\")\n    ), \n    color = \"white\",\n    hjust = 1, \n    nudge_y = -0.5\n  ) + \n  coord_flip() + \n  labs(\n    x = \"Health Board\",\n    y = \"Percent of datazones in 15% most deprived proportion of Scotland\",\n    title = \"Percent of datazones in Health Board in 15% most deprived areas of Scotland\",\n    subtitle = \"SIMD 2020\"\n  ) + \n  geom_hline(yintercept = 0)"
  },
  {
    "objectID": "posts/optimised-for-twitter/index.html",
    "href": "posts/optimised-for-twitter/index.html",
    "title": "Optimised for Twitter?",
    "section": "",
    "text": "I’ve finally got around to ‘optimising the Quarto blog for Twitter’, by following the guidance in this section of Quarto’s website and adding the following declaration to the _quarto.yml file in the project root.\nwebsite:\n  twitter-card:\n   site: \"@jonminton\"\nHowever, is Twitter optimised for Twitter, these days? Is Twitter even Twitter!?"
  },
  {
    "objectID": "posts/scientific-illustration-unit-circle/index.html",
    "href": "posts/scientific-illustration-unit-circle/index.html",
    "title": "Scientific Illustrations: Annotating the unit circle",
    "section": "",
    "text": "Here’s an example of a scientific illustration I’ve just produced to illustrate some scenarios I’m modelling for my work on economic inactivity determinants.\nI have two continuous variables (derived from the GHQ-12), mental health and physical health. Both are standardised so they have the same scale.\nBut I’m interested in the effects of improving/changing ‘health’ in general, which is obviously composed of both mental health and physical health, but not measured directly.\nAs the two variables are standardised, however, I can model an improvement in health in general as a change in both mental health and physical health concurrently.\nHowever, I want to compare like-with-like: scenarios in which the total ‘amount’ of intervention effect is kept constant, but the relative contribution of the two health components is varied.\nThis is where a little trigonometry comes in. 1 All interventions on the grey unit circle in Figure 1 represent possible scenarios in which the total amount of health change is constant, but where the relative contribution of mental and physical health is varied.\nThe aim of the scientific illustration is to make this intuition a bit clearer to understand!\nCode\nlibrary(tidyverse)\nlibrary(geomtextpath)\n\npos_y &lt;- function(x) {sqrt(1 - x^2)}\nx = seq(0, 1, by = 0.001)\ndta &lt;- tibble(\n  x = x\n) |&gt; \n  mutate(\n    y = pos_y(x)\n  )\n\ndta |&gt; \n  ggplot(aes(x = x, y = y)) + \n    geom_line(color = \"grey\") + \n    coord_equal() + \n    labs(x = \"Physical Health (Standardised)\",\n         y = \"Mental Health (Standardised)\",\n         title = \"Health improvement scenarios modelled\") + \n  theme_minimal() + \n  annotate(\"point\", x = 1, y = 0) + \n  annotate(\"point\", x = 0, y = 1) + \n  annotate(\"point\", x = 1/ sqrt(2), y = 1/ sqrt(2)) + \n  annotate(\"point\", x =  2 / sqrt(5), y = 1 / sqrt(5)) + \n  annotate(\"point\", x = 1 / sqrt(5), y = 2 / sqrt(5)) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, xend = 0, yend = 1), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S1: MH Only\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  ) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, xend = 1, yend = 0), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S2: PH Only\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  ) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, xend = 1/sqrt(2), yend = 1/sqrt(2)), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S3: Equal Gain\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  ) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, yend = 2/sqrt(5), xend = 1/sqrt(5)), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S4: MH Bias\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  )  +\n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, yend = 1/sqrt(5), xend = 2/sqrt(5)), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S5: PH Bias\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  )  \n\n\n\n\n\nFigure 1: Modelling various intervention scenarios"
  },
  {
    "objectID": "posts/scientific-illustration-unit-circle/index.html#footnotes",
    "href": "posts/scientific-illustration-unit-circle/index.html#footnotes",
    "title": "Scientific Illustrations: Annotating the unit circle",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPeople who claim you’ll never need trigonometry once you leave school are wrong. It’s valuable about once a decade!↩︎"
  },
  {
    "objectID": "posts/the-other-left-right-divide/index.html",
    "href": "posts/the-other-left-right-divide/index.html",
    "title": "The Other Left-Right Divide: Iain McGilchrist and the Battle of the Hemispheres",
    "section": "",
    "text": "Iain McGilchrist on the mythos and the machine\n\n\nIn Watching The English, Fox (2005) writes that:\n\nAt the most basic level, an underlying rule in all English conversation is the proscription of ‘earnestness’. … [The] English are probably more acutely sensitive than any other nation to the distinction between ‘serious’ and ‘solemn’, between ‘sincerity’ and ‘earnestness’.\n… [The] Importance of Not Being Earnest rule is really quite simple. Seriousness is acceptable, solemnity is prohibited. Sincerity is allowed, earnestness is strictly forbidden. Pomposity and self-importance are outlawed. Serious matters can be spoken of seriously, but one must never take oneself too seriously. [p. 62]\n\nA serial violator of the Importance of Not Being Earnest Rule is Damien Walter, producer and host of the Science Fiction podcast, which states its mission as being to explore “the best in SF storytelling [and to ask] what happens when logos meets mythos, reason meets imagination and science … meets fiction”. English former Guardian journalist Damien Walter (alternately Damien G Walter) is very earnest. Which might explain why he doesn’t live in England anymore.\nIn the latest podcast, the very earnest Englishman Damien G Walter interviews the very earnest Scotsman Iain McGilchrist, talking broadly around McGilchrist’s somewhat mythic framing of the left-right divide. McGilchrist’s left-right divide isn’t a divide between the political Left and Right, but a divide between the two hemispheres of the brain.\nMcGilchrist professes that his left-right hemispheric divide isn’t mere pop science, attributing certain temperaments or qualities, like reason and creativity, to one or the other hemisphere. Instead his argument seems marginally more subtle that that, something like:\n\nThe left hemisphere’s domain is the centre. It’s the part of the brain that takes charge when you choose to focus on an object, grasp it, manipulate it, name it, take it apart and put it back together, use or abuse it as a tool.\nThe right hemisphere’s domain is the periphery. It’s what notices and contextualizes all around you, and so provides the context through which one can relate to and negotiate with the totality of the world.\n\nMcGilchrist’s broader thesis appears to be that broadly left-hemispheric thinking has become somewhat over-dominant in modern culture, leading to an overly atomistic and instrumentalist way of thinking. Everything is thought about, to some extent, in terms of how it can be used, grasped, broken down and thought about as machines and systems. Walter makes the intriguing observation that this may help explain a tendency towards literal-mindedness in much commentary and critique of modern sci-fi and fantasy, which appears blind or indifferent to underlying mythos and symbolism that stories are drawing from. I think there’s much compelling about this literal-mindedness observation, even if I’m somewhat more ambivalent about the left-right hemispheric distinction drawn by McGilchrist more generally, especially in terms of the trends or tendencies he’s proposing.\nSince starting this blog in late November, I’ve discovered most of my posts tend to focus either on statistics or stories. These dual preoccupations don’t completely map onto the left-right hemispheric distinction - for example there’s a lot of contextualisation (right-thinking) involved in finding meaning in statistical outputs; and there is value in thinking about stories in a somewhat mechanical, graspable-component-like way - but it’s not a bad first approximation. I find stories valuable to think about, especially where they bring an intense quality of emotional engagement and I want to know why. Sometimes I even risk treating the exploration and interpretation of stories with the earnestness they deserve (even when writing about Robocop).\n\n\n\n\nReferences\n\nFox, K. 2005. Watching the English: The Hidden Rules of English Behaviour. Hodder & Stoughton. https://books.google.co.uk/books?id=tNZfLeHSFvQC."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html",
    "title": "Tidytuesday 2024-01-23",
    "section": "",
    "text": "This week’s TidyTuesday used data from the UK ONS which was explored in the 2023 article ’Why do children and young people in smaller towns do better academically than those in larger towns?’."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#background",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#background",
    "title": "Tidytuesday 2024-01-23",
    "section": "",
    "text": "This week’s TidyTuesday used data from the UK ONS which was explored in the 2023 article ’Why do children and young people in smaller towns do better academically than those in larger towns?’."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#aims",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#aims",
    "title": "Tidytuesday 2024-01-23",
    "section": "Aims",
    "text": "Aims\nOur first aim was to try to replicate the headline finding from the article above: that children in smaller towns have better average educational outcomes than in larger towns. We also sought to replicate and improve on the ‘beeswarm’ plot used in the original article, and to look at other factors which may explain differences in educational qualifications."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#package-loading",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#package-loading",
    "title": "Tidytuesday 2024-01-23",
    "section": "Package loading",
    "text": "Package loading\n\nlibrary(tidyverse)\nlibrary(ggbeeswarm) # for the beeswarm plot"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#data",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#data",
    "title": "Tidytuesday 2024-01-23",
    "section": "Data",
    "text": "Data\n\n# ee &lt;- tidytuesdayR::tt_load('2024-01-23') |&gt;\n#   purrr::pluck(1)\n\n# Direct link to get past API rate limit issue using tt_load()\nee &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-01-23/english_education.csv')\n\npurrr::pluck(1) was used because the data contained only a single dataset, but by default the tt_load function returns a list. So, the pluck(1) function takes the first element of the list, which in this case is in effect turning the data into a dataframe."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#counting-towns-in-data",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#counting-towns-in-data",
    "title": "Tidytuesday 2024-01-23",
    "section": "Counting towns in data",
    "text": "Counting towns in data\n\nee |&gt;\n  count(size_flag, sort=T) |&gt;\n  knitr::kable(caption = \"Counts of small/med/city class\")\n\n\nCounts of small/med/city class\n\n\nsize_flag\nn\n\n\n\n\nSmall Towns\n662\n\n\nMedium Towns\n331\n\n\nLarge Towns\n89\n\n\nCity\n18\n\n\nInner London BUA\n1\n\n\nNot BUA\n1\n\n\nOther Small BUAs\n1\n\n\nOuter london BUA\n1\n\n\n\n\n\nThere are 662 small towns, 331 medium towns, and 89 large towns"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#removing-oddball-locations-and-londons-and-factoring",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#removing-oddball-locations-and-londons-and-factoring",
    "title": "Tidytuesday 2024-01-23",
    "section": "Removing oddball locations and Londons and factoring",
    "text": "Removing oddball locations and Londons and factoring\n\nee |&gt;\n  mutate(town_size = factor(size_flag, levels = c(\"Small Towns\", \"Medium Towns\", \"Large Towns\"), ordered=T)) |&gt;\n  filter(!is.na(town_size)) -&gt; ee_fact"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#summary-by-group",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#summary-by-group",
    "title": "Tidytuesday 2024-01-23",
    "section": "Summary by group",
    "text": "Summary by group\n\nee_fact |&gt;\n  group_by(town_size) |&gt;\n  summarise(count = n(),\n            `mean ed score` = mean(education_score),\n            `sd ed score` = sd(education_score),\n            se = `sd ed score`/count^0.5,\n            `total population` = sum(population_2011)) |&gt;\n  mutate(across(where(is.numeric), round, 3)) |&gt;\n  knitr::kable()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\ntown_size\ncount\nmean ed score\nsd ed score\nse\ntotal population\n\n\n\n\nSmall Towns\n662\n0.297\n3.887\n0.151\n6880216\n\n\nMedium Towns\n331\n-0.253\n3.324\n0.183\n12213733\n\n\nLarge Towns\n89\n-0.811\n2.298\n0.244\n10466343"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#anova-for-smallmedlarge-towns",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#anova-for-smallmedlarge-towns",
    "title": "Tidytuesday 2024-01-23",
    "section": "ANOVA for small/med/large towns",
    "text": "ANOVA for small/med/large towns\nWe built a series of linear regression models, and used ANOVA to compare between them. A low p-value from ANOVA, when comparing two or more models that are ‘nested’, can be taken as a signal that the more complex/unrestricted of the models should be used.\n\nmod_base &lt;- lm(education_score ~ town_size, data = ee_fact)\nsummary(mod_base)\n\n\nCall:\nlm(formula = education_score ~ town_size, data = ee_fact)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.3246  -2.5270  -0.1996   2.3052  11.5749 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -0.255991   0.151293  -1.692  0.09093 . \ntown_size.L -0.783553   0.288560  -2.715  0.00673 **\ntown_size.Q -0.003547   0.232530  -0.015  0.98783   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.615 on 1079 degrees of freedom\nMultiple R-squared:  0.009593,  Adjusted R-squared:  0.007758 \nF-statistic: 5.226 on 2 and 1079 DF,  p-value: 0.005513\n\nmod_dep &lt;- lm(education_score ~ town_size + income_flag, data = ee_fact)\nsummary(mod_dep)\n\n\nCall:\nlm(formula = education_score ~ town_size + income_flag, data = ee_fact)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.4402 -1.8983 -0.0131  1.8447  9.2254 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        -2.29448    0.14533 -15.788  &lt; 2e-16 ***\ntown_size.L                         0.51810    0.22501   2.303   0.0215 *  \ntown_size.Q                        -0.01572    0.17726  -0.089   0.9293    \nincome_flagLower deprivation towns  5.31339    0.19207  27.664  &lt; 2e-16 ***\nincome_flagMid deprivation towns    1.82601    0.23489   7.774 1.77e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.754 on 1077 degrees of freedom\nMultiple R-squared:  0.4261,    Adjusted R-squared:  0.424 \nF-statistic: 199.9 on 4 and 1077 DF,  p-value: &lt; 2.2e-16\n\nmod_dep2 &lt;- lm(education_score ~ town_size * income_flag, data = ee_fact)\nsummary(mod_dep2)\n\n\nCall:\nlm(formula = education_score ~ town_size * income_flag, data = ee_fact)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.576 -1.868 -0.049  1.788  9.090 \n\nCoefficients:\n                                               Estimate Std. Error t value\n(Intercept)                                    -2.19244    0.15155 -14.466\ntown_size.L                                     0.94843    0.28240   3.359\ntown_size.Q                                    -0.10774    0.24096  -0.447\nincome_flagLower deprivation towns              4.77144    0.30602  15.592\nincome_flagMid deprivation towns                1.59836    0.31953   5.002\ntown_size.L:income_flagLower deprivation towns -1.36508    0.60094  -2.272\ntown_size.Q:income_flagLower deprivation towns -0.11774    0.44806  -0.263\ntown_size.L:income_flagMid deprivation towns   -0.76520    0.61668  -1.241\ntown_size.Q:income_flagMid deprivation towns   -0.04965    0.48199  -0.103\n                                               Pr(&gt;|t|)    \n(Intercept)                                     &lt; 2e-16 ***\ntown_size.L                                    0.000811 ***\ntown_size.Q                                    0.654869    \nincome_flagLower deprivation towns              &lt; 2e-16 ***\nincome_flagMid deprivation towns               6.62e-07 ***\ntown_size.L:income_flagLower deprivation towns 0.023309 *  \ntown_size.Q:income_flagLower deprivation towns 0.792769    \ntown_size.L:income_flagMid deprivation towns   0.214935    \ntown_size.Q:income_flagMid deprivation towns   0.917974    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.749 on 1073 degrees of freedom\nMultiple R-squared:  0.4305,    Adjusted R-squared:  0.4262 \nF-statistic: 101.4 on 8 and 1073 DF,  p-value: &lt; 2.2e-16\n\nanova(mod_base, mod_dep, mod_dep2)\n\nAnalysis of Variance Table\n\nModel 1: education_score ~ town_size\nModel 2: education_score ~ town_size + income_flag\nModel 3: education_score ~ town_size * income_flag\n  Res.Df     RSS Df Sum of Sq        F  Pr(&gt;F)    \n1   1079 14097.2                                  \n2   1077  8168.3  2    5928.9 392.3729 &lt; 2e-16 ***\n3   1073  8106.8  4      61.5   2.0343 0.08749 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod_dep, mod_dep2)\n\nAnalysis of Variance Table\n\nModel 1: education_score ~ town_size + income_flag\nModel 2: education_score ~ town_size * income_flag\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1   1077 8168.3                              \n2   1073 8106.8  4    61.479 2.0343 0.08749 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe summary from mod_dep indicates that deprivation tertile, using the IMD income domain, may have more of an effect than town size, and in the opposite direction."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#beeswarm-plot",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#beeswarm-plot",
    "title": "Tidytuesday 2024-01-23",
    "section": "Beeswarm plot",
    "text": "Beeswarm plot\nWe reproduce the beeswarm plot from the original article, but colouring areas by income tertile:\n\nee_fact |&gt;\n  mutate(income_flag = factor(income_flag, levels = c(\"Lower deprivation towns\", \"Mid deprivation towns\", \"Higher deprivation towns\"))) |&gt;\n  ggplot(aes(x = town_size, y = education_score, color = income_flag)) +\n  geom_beeswarm() +\n  coord_flip() +\n  theme(legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"#73b8fd\", \"#0068c6\", \"#003b7c\"))"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#conclusion",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#conclusion",
    "title": "Tidytuesday 2024-01-23",
    "section": "Conclusion",
    "text": "Conclusion\nWe were able to replicate the headline finding from the article, and the type of visualisation used. But we also identified area deprivation as an important (and likely a more important) determinant of education scores."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-more-life-expectancy/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-more-life-expectancy/index.html",
    "title": "Tidy Tuesday on Life Expectancy - Part Two",
    "section": "",
    "text": "In the previous week’s TidyTuesday session, we looked at Life Expectancy.\nFor this week, the Tidy Tuesday dataset of the week was of Christmas films. However, as public health folks we felt more interested in continuing to look at life expectancy, so continued with the previous week’s dataset.\nThis session was led by Andrew Saul."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-more-life-expectancy/index.html#introduction",
    "href": "posts/tardy-tuesday/tidy-tuesday-more-life-expectancy/index.html#introduction",
    "title": "Tidy Tuesday on Life Expectancy - Part Two",
    "section": "",
    "text": "In the previous week’s TidyTuesday session, we looked at Life Expectancy.\nFor this week, the Tidy Tuesday dataset of the week was of Christmas films. However, as public health folks we felt more interested in continuing to look at life expectancy, so continued with the previous week’s dataset.\nThis session was led by Andrew Saul."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-more-life-expectancy/index.html#script-and-outputs-from-session",
    "href": "posts/tardy-tuesday/tidy-tuesday-more-life-expectancy/index.html#script-and-outputs-from-session",
    "title": "Tidy Tuesday on Life Expectancy - Part Two",
    "section": "Script and outputs from session",
    "text": "Script and outputs from session\nLoading some packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\n\nUse the tidytuesdayR package to load the data (rather than a direct link):\n\n\nCode\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-05')\n\n\n\n    Downloading file 1 of 3: `life_expectancy.csv`\n    Downloading file 2 of 3: `life_expectancy_different_ages.csv`\n    Downloading file 3 of 3: `life_expectancy_female_male.csv`\n\n\nPopulate the content of the list above into three separate datasets:\n\n\nCode\nle &lt;- tuesdata[[1]]\nle_diff &lt;- tuesdata[[2]]\nle_gender &lt;- tuesdata[[3]]\n\n\nHave a quick look at the data\n\n\nCode\nglimpse(le)\n\n\nRows: 20,755\nColumns: 4\n$ Entity         &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanist…\n$ Code           &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\",…\n$ Year           &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1…\n$ LifeExpectancy &lt;dbl&gt; 27.7275, 27.9634, 28.4456, 28.9304, 29.2258, 29.9206, 3…\n\n\nCode\nglimpse(le_diff)\n\n\nRows: 20,755\nColumns: 9\n$ Entity           &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghani…\n$ Code             &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG…\n$ Year             &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958,…\n$ LifeExpectancy0  &lt;dbl&gt; 27.7275, 27.9634, 28.4456, 28.9304, 29.2258, 29.9206,…\n$ LifeExpectancy10 &lt;dbl&gt; 49.1459, 49.2941, 49.5822, 49.8634, 49.9306, 50.4315,…\n$ LifeExpectancy25 &lt;dbl&gt; 54.4422, 54.5644, 54.7998, 55.0286, 55.1165, 55.4902,…\n$ LifeExpectancy45 &lt;dbl&gt; 63.4225, 63.5006, 63.6476, 63.7889, 63.8481, 64.0732,…\n$ LifeExpectancy65 &lt;dbl&gt; 73.4901, 73.5289, 73.6018, 73.6706, 73.7041, 73.8087,…\n$ LifeExpectancy80 &lt;dbl&gt; 83.7259, 83.7448, 83.7796, 83.8118, 83.8334, 83.8760,…\n\n\nCode\nglimpse(le_gender)\n\n\nRows: 19,922\nColumns: 4\n$ Entity               &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afg…\n$ Code                 &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", …\n$ Year                 &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1…\n$ LifeExpectancyDiffFM &lt;dbl&gt; 1.261900, 1.270601, 1.288300, 1.306601, 1.276501,…\n\n\nThere are fields code and entity, where entity tends to be more verbose/descriptive. Entities include geographic regions, countries, economic groupings etc. (So fairly messy, definitely not mutally exclusive and exhaustive)\n\n\nCode\nle_diff %&gt;% \n  count(Entity) %&gt;% \n  pull(Entity)\n\n\nWe decided to look at a series of countries from across the world.\n\n\nCode\ncountries &lt;- c(\"Germany\", \"United Kingdom\", \"Saudi Arabia\", \"South Africa\",\n               \"South Korea\", \"Japan\", \"Vietnam\", \"Argentina\", \"Venezuela\", \"France\")\n\n\nToday we looked at life expectency in a selection of countries from 1900\n\n\nCode\nle1900 &lt;- le %&gt;% \n  filter(Entity %in% countries,\n         Year&gt;=1900) \n\nle1900 %&gt;% \n  ggplot(aes(x=Year, y=LifeExpectancy))+\n  geom_line()+\n  facet_wrap(vars(Entity))\n\n\n\n\n\nWe then looked at the change in life expectency per year\n\n\nCode\nle1900 %&gt;% \n  group_by(Entity) %&gt;% \n  mutate(lag_diff = LifeExpectancy - lag(LifeExpectancy, order_by = Year),\n         sign = lag_diff&gt;0) %&gt;% \n  ggplot(aes(x=Year, y=lag_diff))+\n  geom_point(aes(colour = sign))+\n  geom_hline(yintercept = 0)+\n  facet_wrap(vars(Entity))\n\n\n\n\n\nWe changed the axis magnification of each country, so that the changes were more readily observable\n\n\nCode\nle1900lag &lt;- le1900 %&gt;% \n  group_by(Entity) %&gt;% \n  mutate(lag_diff = LifeExpectancy - lag(LifeExpectancy, order_by = Year),\n         sign = lag_diff&gt;0)\n\n le1900lag %&gt;% \n  ggplot(aes(x=Year, y=lag_diff))+\n  geom_point(aes(colour = sign))+\n  geom_hline(yintercept = 0)+\n  facet_wrap(vars(Entity), scales = \"free_y\")\n\n\n\n\n\nFinally, we examined variability in the change of life expectency altered for UK, France and Germany. Here is can be seen that variability in life expectancy dramatically increased around the First and Second World Wars. Data for Germany was incomplete for this period.\nTo do this we made use of the slider package, and within this the slide_index function, to produce a rolling standard deviation of annual changes.\n\n\nCode\nlibrary(slider)\nle1900lag %&gt;% \n  arrange(Year) %&gt;% \n  filter(Entity %in% c(\"United Kingdom\", \"France\", \"Germany\")) %&gt;% \n  mutate(roll_sd = slide_index_dbl(lag_diff, Year, .before = 4, .after = 4, .f = sd, .complete = T)) %&gt;% \n  ggplot(aes(x=Year, y=roll_sd, color = Entity))+\n  geom_line()"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html",
    "title": "Tidy Tuesday 30 Jan 2024: Groundhogs",
    "section": "",
    "text": "The latest TidyTuesday dataset is on Groundhog Days, a North American tradition in which the behaviours of specific groundhogs are used to make predictions about the weather over the next six weeks, as immortalised in the eponymous sci-fi rom-com featuring Bill Murray.\nOddly, the data provided does not include meteorological information on whether the groundhogs’ predictions are accurate. (Who knows? Maybe they are!) But the data do include latitude, longitude, and other geographic information. So, we decided to see if we could plot these Groundhog Day events on an interactive map.\nFirst we load the data, using the bespoke tidytuesdayR package:\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\n# The number of queries made via tidyTuesdayR is limited. The commented code below shows how to extract the TidyTuesday data using the tidytuesdayR package. Instead I'll link directly:\n# dat &lt;- tidytuesdayR::tt_load('2024-01-30')\nCode\n# predictions &lt;- dat |&gt;\n#   pluck(1) # that's the predictions\nCode\n# dat |&gt;\n#   pluck(2) # that's the groundhogs\nCode\n# groundhogs &lt;- dat |&gt;\n#   pluck(2) \n\n# Direct approach \n\ngroundhogs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-01-30/groundhogs.csv')\n\n\nRows: 75 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): slug, shortname, name, city, region, country, source, current_pred...\ndbl  (4): id, latitude, longitude, predictions_count\nlgl  (2): is_groundhog, active\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\npredictions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-01-30/predictions.csv')\n\n\nRows: 1462 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): details\ndbl (2): id, year\nlgl (1): shadow\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nWe now have the two datasets in separate objects. Let’s look at some of the information in the description field\nCode\ngroundhogs |&gt;\n  filter(!is_groundhog) |&gt;\n  select(description)\n\n\n# A tibble: 43 × 1\n   description                                                                  \n   &lt;chr&gt;                                                                        \n 1 Octoraro Orphie, from Quarryville Pennsylvania, is a taxidermied world-renow…\n 2 Concord Charlie is a presumed groundhog from Athens, West Virginia. In a tra…\n 3 Lander Lil, a bronze statue of a prairie dog, has been predicting the future…\n 4 Groundhog puppet Manitoba Merv has been predicting the arrival of spring at …\n 5 Schnogadahl Sammi is a literally unpronounceable taxidermied groundhog mount…\n 6 Poor Richard is a taxidermied groundhog who prognosticates for the Slumberin…\n 7 Balzac Billy is the “Prairie Prognosticator”, a man-sized gopher mascot who …\n 8 Every year on February 2nd, Myerstown’s favorite groundhog “Uni” is placed o…\n 9 Grover the Groundhog and Sweet Arrow Sue are a taxidermied groundhog couple …\n10 Stormy Marmot is a plush yellow-bellied marmot from Aurora, Colorado. He is …\n# ℹ 33 more rows\nCode\npredictions |&gt;\n  count(shadow, sort=T)\n\n\n# A tibble: 3 × 2\n  shadow     n\n  &lt;lgl&gt;  &lt;int&gt;\n1 TRUE     665\n2 FALSE    652\n3 NA       145\nCode\n# dat |&gt;\n#   pluck(1) |&gt;\n#   left_join(groundhogs) \npredictions |&gt; \n    left_join(groundhogs)\n\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 1,462 × 20\n      id  year shadow details         slug  shortname name  city  region country\n   &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;  &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n 1     1  1886 NA     Groundhog Day … punx… Phil      Punx… Punx… Penns… USA    \n 2     1  1887 TRUE   First Official… punx… Phil      Punx… Punx… Penns… USA    \n 3     1  1888 TRUE   Saw Shadow.     punx… Phil      Punx… Punx… Penns… USA    \n 4     1  1889 NA     No Record.      punx… Phil      Punx… Punx… Penns… USA    \n 5     1  1890 FALSE  No Shadow.      punx… Phil      Punx… Punx… Penns… USA    \n 6     1  1891 NA     No Record.      punx… Phil      Punx… Punx… Penns… USA    \n 7     1  1892 NA     No Record.      punx… Phil      Punx… Punx… Penns… USA    \n 8     1  1893 NA     No Record.      punx… Phil      Punx… Punx… Penns… USA    \n 9     1  1894 NA     No Record.      punx… Phil      Punx… Punx… Penns… USA    \n10     1  1895 NA     No Record.      punx… Phil      Punx… Punx… Penns… USA    \n# ℹ 1,452 more rows\n# ℹ 10 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, source &lt;chr&gt;,\n#   current_prediction &lt;chr&gt;, is_groundhog &lt;lgl&gt;, type &lt;chr&gt;, active &lt;lgl&gt;,\n#   description &lt;chr&gt;, image &lt;chr&gt;, predictions_count &lt;dbl&gt;"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html#mapping",
    "href": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html#mapping",
    "title": "Tidy Tuesday 30 Jan 2024: Groundhogs",
    "section": "Mapping",
    "text": "Mapping\nWe decided to try using Leaflet to plot where the Groundhog day events occurred.\n\n\nCode\n#install.packages('leaflet')\nlibrary(leaflet)\n\n\nWe start by following one of the first examples in the Leaflet intro page above, adding markers to geolocate the sightings/events, and a popup with the name (assigned to) the Groundhog:\n\n\nCode\ngroundhogs |&gt;\n  select(lat = latitude, lng = longitude, popup = name) |&gt;\n  leaflet() |&gt;\n  addTiles()|&gt;\n  addMarkers(~lng, ~lat, popup = ~popup)\n\n\n\n\n\n\nWe next wanted to colour these markers based on whether the predictions are classed as ‘active’ or not. This was slightly more tricky, but the example given in the Awesome Icons section of the markers part of the Leaflet documentation gave the following code pattern we could work with:\n```{r}\n# first 20 quakes\ndf.20 &lt;- quakes[1:20,]\n\ngetColor &lt;- function(quakes) {\n  sapply(quakes$mag, function(mag) {\n  if(mag &lt;= 4) {\n    \"green\"\n  } else if(mag &lt;= 5) {\n    \"orange\"\n  } else {\n    \"red\"\n  } })\n}\n\nicons &lt;- awesomeIcons(\n  icon = 'ios-close',\n  iconColor = 'black',\n  library = 'ion',\n  markerColor = getColor(df.20)\n)\n\nleaflet(df.20) %&gt;% addTiles() %&gt;%\n  addAwesomeMarkers(~long, ~lat, icon=icons, label=~as.character(mag))\n\n```\nSo, in the above addAwesomeMarkers() is used in place of addMarkers(), and takes an argument icon. A vector of icons is created of the same length as the number of rows of the dataframe, whose colour is determined through the getColor function.\nIn our case we are interested in the active column, which has just two mutually exclusive categories: TRUE and FALSE. So we just need two colours to be specified\n\n\nCode\n# We saw if we could implement the code pattern above using list columns, but were not successful\n\n# colouring markers\n# groundhogs_icons &lt;- groundhogs |&gt;\n#   mutate(markerColor = ifelse(active, \"green\", \"red\")) |&gt;\n#   rowwise() |&gt;\n#   mutate(icon = list(awesomeIcons(\n#     icon = 'ios-close',\n#     iconColor = 'black',\n#     library = 'ion',\n#     markerColor = markerColor\n#   )))\n\ngetColor &lt;- function(groundhogs) {\n  sapply(groundhogs$active, function(active) {\n    if(active) {\n      \"green\"  } else {\n          \"red\"  } })\n}\n\n# create vector of matching vectors\nicons &lt;- awesomeIcons(\n  icon = 'ios-close',\n  iconColor = 'black',\n  library = 'ion',\n  markerColor = getColor(groundhogs)\n)\n\n# now with active/inactive icons\n\ngroundhogs |&gt;\n  select(lat = latitude, lng = longitude, popup = name) |&gt;\n  leaflet() |&gt;\n  addTiles()|&gt;\n  addAwesomeMarkers(~lng, ~lat, popup = ~popup, icon = icons)\n\n\n\n\n\n\nChallenge complete! As we would expect, most predictions are not currently active."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html#other-possibilities",
    "href": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html#other-possibilities",
    "title": "Tidy Tuesday 30 Jan 2024: Groundhogs",
    "section": "Other possibilities",
    "text": "Other possibilities\nSome other things we could have explored include:\n\nAttempting to link to appropriate meteorological data to see if the predictions came true at more than chance rates (likely a challenge)\nNatural Language Programming to identify patterns and key terms in the free text fields like description\nAdditional customisation of the leaflet maps, such as including additional popup fields, further customising the icons based on multiple variables, and adding date sliders to give a third dimension (latitude, longitude, and date) to the user display"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html#additional",
    "href": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html#additional",
    "title": "Tidy Tuesday 30 Jan 2024: Groundhogs",
    "section": "Additional",
    "text": "Additional\nAndrew presented the following code solution for how to use plotly to produce multiple traces based on summary stats:\n\n\nCode\nlibrary(plotly)\n\ndf &lt;- \n  mpg %&gt;% \n  summarise(avg_city = mean(cty), .by = c(manufacturer, year))\n\nmanfs &lt;- df %&gt;% distinct(manufacturer) %&gt;% pull()\n\np &lt;- plot_ly()\n\nfor(manf in manfs){\n  df_manf &lt;- df %&gt;% \n    filter(manufacturer == manf)\n  p &lt;- add_trace(p,\n                 mode = \"lines+markers\",\n                 x = ~year,\n                 y = ~avg_city,\n                 data = df_manf) # must include new df as data for plolty layer\n}\n\np"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "",
    "text": "This week’s Tidy Tuesday compares life expectancy across the globe and is available here:"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#loading-the-data",
    "href": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#loading-the-data",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Loading the data",
    "text": "Loading the data\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\ndata_url &lt;- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-12-05/life_expectancy_different_ages.csv\"\n \ndta &lt;- read_csv(data_url)\n\n# Alternatively\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-05')\n\n\n\n    Downloading file 1 of 3: `life_expectancy.csv`\n    Downloading file 2 of 3: `life_expectancy_different_ages.csv`\n    Downloading file 3 of 3: `life_expectancy_female_male.csv`\n\n\n\n\nCode\nnames(tuesdata)\n\n\n[1] \"life_expectancy\"                \"life_expectancy_different_ages\"\n[3] \"life_expectancy_female_male\"   \n\n\nCode\nlife_expectancy &lt;- tuesdata$life_expectancy\n\nn_distinct(life_expectancy$Entity)\n\n\n[1] 261"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#setting-a-global-plot-theme",
    "href": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#setting-a-global-plot-theme",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Setting a global plot theme",
    "text": "Setting a global plot theme\n\n\nCode\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#comparing-life-expectancy-across-regions",
    "href": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#comparing-life-expectancy-across-regions",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Comparing life expectancy across regions",
    "text": "Comparing life expectancy across regions\n\n\nCode\nregions &lt;- life_expectancy %&gt;%\n  filter(str_detect(Entity, \"region\"))\n\nregions %&gt;%\n  ggplot(aes(x = Year, y = LifeExpectancy)) +\n  geom_line(aes(col = Entity)) +\n  theme(legend.position = \"top\") +\n  annotate(geom = \"text\",\n           x = 1960, y = 50,\n           label = \"What happened here?\") +\n  geom_vline(xintercept = 2019, linetype = 2) +\n  annotate(geom = \"text\", x = 2019, y = 75,\n           label = \"Start of COVID pandemic\",\n           hjust = 1)"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#difference-in-life-expectancy-between-more-and-less-developed-regions",
    "href": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#difference-in-life-expectancy-between-more-and-less-developed-regions",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Difference in life expectancy between more and less developed regions",
    "text": "Difference in life expectancy between more and less developed regions\n\n\nCode\nregions %&gt;%\n  filter(Entity %in% c(\"More developed regions\",\n                       \"Less developed regions\"))  %&gt;%\n  arrange(Year, Entity) %&gt;%\n  group_by(Year) %&gt;%\n  mutate(difference = LifeExpectancy - lag(LifeExpectancy)) %&gt;%\n  filter(!is.na(difference)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = Year, y = difference)) +\n  geom_area(alpha = 0.5) +\n  expand_limits(y = 0) +\n  labs(title = \"Difference in life expectancy between more developed and less developed regions\",\n       y = \"Difference in life expectancy (years)\")\n\n\n\n\n\nWe look at life expectancy at different ages in three specific countries.\n\n\nCode\ndata_tidy &lt;-\n  dta |&gt;\n    pivot_longer(\n      cols = LifeExpectancy0:LifeExpectancy80\n    ) |&gt;\n    mutate(\n      starting_age = str_remove(name, \"LifeExpectancy\") %&gt;%\n        as.numeric()\n    ) |&gt;\n    select(-name) |&gt;\n    rename(e_x = value)\n\n\n\n\nCode\ndata_tidy |&gt;\n  filter(\n    Entity %in% c(\n      \"Nigeria\", \"Iran\",\n      \"South Africa\"\n    )\n  ) |&gt;\n  arrange(Year)  |&gt;\n  ggplot(aes(Year, e_x, group = factor(starting_age), colour = factor(starting_age))) +\n  geom_line() +\n  facet_wrap(~Entity)\n\n\n\n\n\nSandra Nwobi, who suggested the three countries above, provides the following summary:\n\nOf the three developing countries—Iran, South Africa, and Nigeria—Nigeria has a significantly higher zero-age death rate in the late 50s and early 60s. This can be attributed to a number of factors, including socioeconomic instability, political unrest, malnutrition, and limited access to healthcare. Comparing this result to South Africa and Iran, it is comparatively higher. However, there have been noticeable improvements in Nigeria during the 1980s, with a steady increase. Nevertheless, much work needs to be done to combat this in Nigeria, as it performs significantly worse than the other two countries.\n\nThere was a noticeable decline in data in the early 2000s, particularly in South Africa. Health crises like HIV/AIDS, which may have affected people between the ages of 0 and 25, as well as a number of social and economic problems may have contributed to this decline.\n\nIran’s data indicates consistent growth across all age groups over the years, with the exception of a general decline in 2020 that was likely caused by the COVID-19 virus. Out of the three countries, South Africa is the most affected, maybe as a result of a much older demography compared to Nigeria."
  },
  {
    "objectID": "posts/new-frasiers-eerie-familiarity/index.html",
    "href": "posts/new-frasiers-eerie-familiarity/index.html",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "",
    "text": "New Frasier.. like the Old Frasier\nAs with new Beavis and Butt-Head, which I’ve written about previously, Paramount+ includes access to new Frasier, a return of the vainglorious pratfalling public psychiatrist to the small screen after the original series ended in 2004.\nThough Frasier himself has returned, none of the supporting characters have. Instead he’s surrounded by an entirely (apparently) new cast of supporting characters, and the series is set in a brand new (but also very old) location. Given this, we might expect Frasier (2023) to feel very different to Frasier (1993)…\nBut it doesn’t. It feels eerily familiar. Despite almost everything, apart from the title character, being different, Frasier (2023) somehow feels largely the same as Frasier (1993).\nThe aim of this post is to try to think through why, with New Frasier, despite almost everything being different, almost everything is also the same."
  },
  {
    "objectID": "posts/new-frasiers-eerie-familiarity/index.html#characters-the-situation-in-situation-comedies",
    "href": "posts/new-frasiers-eerie-familiarity/index.html#characters-the-situation-in-situation-comedies",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "Characters: The Situation in Situation Comedies",
    "text": "Characters: The Situation in Situation Comedies\nCommercially successful TV series, back in the 1990s, weren’t really meant to go anywhere in terms of character and characterisation. Whereas Joseph Campbell’s Monomyth, which countless films use as their narrative template, focuses on how a sequence of events and experiences lead to irreconcilable change in the central character, the formula for a successful TV series depends on the central character not changing. Things happen to the central character, but like a boulder in a stream, the central character ultimately remains largely unchanged and unmoved as a result. The reason for this was largely due to the value to an audience of familiarity, which brings a sense of warmth to characters, and also not to burn through an ultimately finite supply of Heroic Fuel: There’s only so many times a character can face adversity, the call to adventure, the descent into the Underworld, look one’s Adversary in the eyes, almost die (literally or figuratively), ultimately triumph, and return to the light wounded, wiser but ultimately stronger. For an episodic series, if a character is shown to be broken and remoulded every week, before too long the audience will start to feel they’re made more of clay than flesh and bone.\nSo if the Monomyth can’t be used as the main narrative engine of a TV series, what can? For sitcoms, the clue’s partially in the name: the situation. And for most successful sitcoms, including Frasier, much of the situation comes from the interplay between characters.\nHow did this work in Frasier (1993)? Well, in the original Frasier the following supporting characters were introduced:\n\nMarty. Frasier’s father, a retired police officer. Whereas Frasier is booksmart, Marty is streetsmart. Frasier and Marty are both smart, but orientated towards fundamentally different forms of knowledge and competence. They might be related, but they’ve always swum in different waters.\nNiles. Frasier’s brother. Whereas Marty is too dissimilar to Frasier, Niles is too similar. Frasier’s knowledge and interests are esoteric, high culture not mainstream, and so there are few people in the world who will understand him. Niles does. But their world of high culture is so exclusive it’s also small. And it’s competitive, their academic and professional overachievement fueled by unquensionable egotism and self doubt. So, as well as Niles being Frasier’s closest friend, he’s also his closest rival.\nRos. Frasier’s Radio producer. Like Marty she’s streetsmart (albeit in the ‘streets’ of media production). And like Frasier she’s competitive. Because she’s more worldly wise than Frasier, and his boss, she exploits and manipulates Frasier to achieve her own ambitions, which often don’t align exactly with his own. She is, in a platonic sense, Frasier’s pimp.\nDaphne. The Help. Marty’s live-in carer, launderer and folder of clothes, cooker of foods, provider of basic needs. Daphne is economically dependent on Frasier’s largesse, and appears to be somewhat naive. However this appearance of naivity is sometimes shown to be an illusion.\n\nWhereas in Frasier (2023) there are the following supporting characters:\n\nFreddy. Frasier’s son, a working firefighter. Whereas Frasier is only booksmart, Freddy is also streetsmart. Freddy actively rejected the path to high culture that Frasier set him on. They might be related, but they swim in different waters.\nAlan. Frasier’s university buddy. Whereas Freddy is too dissimilar to Frasier, Alan is too similar. Frasier’s knowledge and interests are estoteric, high culture not mainstream, and so there are few people in the world who will understand him. Alan does. But their world of high culture is so exclusive it’s also small. And it’s competitive. Alan, however, has tenure, something Frasier covets, so in this sense, as well as being Fraiser’s closest friend Alan is, if not exactly a rival, someone Frasier finds himself measuring himself against, and finding wanting.1\nOlivia. Frasier’s boss. As well as being an academic, she’s also a manager of academics, and so a practitioner of the Dark Arts of academic self promotion. Like Frasier she’s competitive, especially with her sister, who’s also a senior academic. She is, in a platonic sense, Frasier’s pimp, and calls him her ‘dancing bear’.\nEve. Not The Help, but a single mother Freddy helped, and so Frasier must support too. Eve is somewhat economically dependent on Frasier’s largesse, living rent-free in one of his apartments. She has good social instincts and works at a bar, but considers herself an actor, though has a naively delusional sense of her own abilities in this field.\n\nIf the second series of descriptions seems similar to the first, this may help explain how and why Frasier (2023) is eerily familiar to viewers of Frasier (1993). Despite some differences, there is almost a one-to-one mapping between the main supporting characters in both Frasiers.2 Though it might not declare itself as such, Frasier (2023) is not just a sitcom, but a sci-fi sitcom, as it appears, like an inversion of Dr Who, that everyone except the lead character has regenerated into a new body."
  },
  {
    "objectID": "posts/new-frasiers-eerie-familiarity/index.html#character-based-situational-combinations",
    "href": "posts/new-frasiers-eerie-familiarity/index.html#character-based-situational-combinations",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "Character-based situational combinations",
    "text": "Character-based situational combinations\nSo, Frasier (2023) takes the same supporting character archetypes as Frasier (1993) and regenerates them. Why? I think this is because of the way even a small number of supporting characters can generate a large number of character-based situational combinations, and so a great deal of fuel for episodic stories, each based on how the main character interacts with one, two, or possibly three of the other characters. The Wikipedia article on Combination goes into a painful amount of detail on this.\nSay there are four primary characters supporting characters. 3 There are then four ways (F, A, O and E) that a single supporting character can interact with the main character. This comes intuitively, but also from the Binomial Coefficient \\(C(n, k) = \\frac{n!}{k!(n-k)!}\\), where \\(!\\) indicates factorial. We can work out the total number of combinations of four characters as follows:\n\n\nCode\nlibrary(tidyverse)\nmy_binomial &lt;- function(n,k) {factorial(n) / (factorial(k) * factorial(n-k))}\n\nn_characters &lt;- 4\n\ndf &lt;- tibble(\n    n_characters_interacting = 0:4\n    ) |&gt;\n    mutate(\n        n_comb_with_this_many_chars = map_int(n_characters_interacting, function(x) my_binomial(n_characters, x))\n    ) |&gt;\n    mutate(\n        cumulative_combinations = cumsum(n_comb_with_this_many_chars)\n    )\n\ndf \n\n\n# A tibble: 5 × 3\n  n_characters_interacting n_comb_with_this_many_chars cumulative_combinations\n                     &lt;int&gt;                       &lt;int&gt;                   &lt;int&gt;\n1                        0                           1                       1\n2                        1                           4                       5\n3                        2                           6                      11\n4                        3                           4                      15\n5                        4                           1                      16\n\n\nSo, with 4 supporting characters, there are 16 combinations of interactions with Frasier (where 0 characters interacting would be Frasier soliloquizing, say if he gets stuck in a lift). This isn’t a huge number of situations, but more than a modern season. But this is just combinations, not permutations: a situation in which Alan verbs Olivia, for example, would be different to one in which Olivia verbs Alan, but in combinatorials counted as the same.4 It also excludes any B plots not involving Frasier. For this we simply need to change the n in the above from 4 to 5, and exclude k=0 from the option, as a story involving no characters probably wouldn’t work…\n\n\nCode\nn_characters &lt;- 5\n\ndf &lt;- tibble(\n    n_characters_interacting = 1:5\n    ) |&gt;\n    mutate(\n        n_comb_with_this_many_chars = map_int(n_characters_interacting, function(x) my_binomial(n_characters, x))\n    ) |&gt;\n    mutate(\n        cumulative_combinations = cumsum(n_comb_with_this_many_chars)\n    )\n\ndf \n\n\n# A tibble: 5 × 3\n  n_characters_interacting n_comb_with_this_many_chars cumulative_combinations\n                     &lt;int&gt;                       &lt;int&gt;                   &lt;int&gt;\n1                        1                           5                       5\n2                        2                          10                      15\n3                        3                          10                      25\n4                        4                           5                      30\n5                        5                           1                      31\n\n\nAllowing Frasier not to be in every story, the number of character-based combinations increases to 31, which seems plenty of basic story types from which between 5 (one scene) and 30 (one show) minutes of content could be derived. And as mentioned, this is just combinations, not permutations, where order matters. If looking at permutations, then the number of sequences with five characters is \\(5!\\),5 or 120, but we also need to include four, three, and two character sequences too, ie. \\(5! + 4! + 3! + 2!\\), which brings up the number of permutations to 152.6"
  },
  {
    "objectID": "posts/new-frasiers-eerie-familiarity/index.html#so-what",
    "href": "posts/new-frasiers-eerie-familiarity/index.html#so-what",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "So what?",
    "text": "So what?\nOkay, I’ve gone into the technical details a bit more than I was expecting to. The point is that even with a fairly small number of characters, the number of possible situations and circumstances that derive entirely from placing characters in a room together, and thinking how they might interact with each other, quickly becomes large enough to avoid being repetitive despite being familiar. Of course, additional supporting characters, guest stars, and scenarios all help increase the number of stories even further, but just having a small number of well defined characters, and imagining the narrative molecules and compounds these character elements may form when forced to mix, appears to do the bulk of the storytelling. With a sitcom, with interesting and well defined characters, in a sense it seems the stories write themselves.\nAnd why almost the same characters, rather than just the same number of characters? I think this was because over a decade of Frasier provides plenty of experience about what these character combinations produce. So, why start from scratch?7"
  },
  {
    "objectID": "posts/new-frasiers-eerie-familiarity/index.html#conclusion",
    "href": "posts/new-frasiers-eerie-familiarity/index.html#conclusion",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "Conclusion",
    "text": "Conclusion\nI think a second season is likely."
  },
  {
    "objectID": "posts/new-frasiers-eerie-familiarity/index.html#footnotes",
    "href": "posts/new-frasiers-eerie-familiarity/index.html#footnotes",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBeing an English character played by an English actor (Unlike Freddy and Olivia, who are both American characters played by English actors), Alan’s character is also a poorly dressed high functioning alcoholic.↩︎\nThe one exception to this is David Crane, son of Niles and Daphne. Initially it appeared the ‘Niles’ archetype from Frasier-1 had undergone some kind of Narrative Fission Event and been split into both Alan and David. On further viewing, however, it becomes apparent David is instead an intruder from another show, being effectively a toned down version of Sheldon Cooper from the Big Bang Theory, and being more a caricuture than a character.↩︎\nI’m excluding David as a primary supporting character as he appears to be a caricature, but he may become more fleshed out over time.↩︎\nSituations best described using intransitive verbs are probably closer to combinatorials not permutations. A pizza with ham and mushroom would be much like a pizza with mushroom and ham. Similarly a plot in which Alan and Olivia eat dinner together would be much like a plot in which Oliva and Alan eat dinner together. However a situation in which Alan invites Olivia to dinner would be different to a situation in which Olivia invites Alan to dinner!↩︎\nfactorial(5) in R↩︎\nfactorial(5) + factorial(4) + factorial(3) + factorial(2) in R↩︎\nAs with the phrase “History doesn’t repeat, but it does rhyme”, we probably shouldn’t assume exactly the same plots will occur in Frasier 2023 as with Frasier 1993. If it were to, we would expect Alan, who’s in his sixties, to become emphatuated with Eve, who’s in her twenties. I suspect this won’t happen, unless there’s a reboot of the reboot in 20 more years featuring Leonardo Dicaprio↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Jon. I’m interested in epidemiology, data science, cities, population structure, software development, R, javascript, python, the two cultures, cats, pop culture, storytelling and ideology, irrational rationality, taking dumb things seriously (and vice versa), cooking, and cats. Welcome to my Quarto blog, which I started in late 2023."
  }
]