[
  {
    "objectID": "glms.html",
    "href": "glms.html",
    "title": "Statistical Modelling: Theory and Practice",
    "section": "",
    "text": "Here are posts so far on statistical modelling, now arranged by section/grouping for easier navigation."
  },
  {
    "objectID": "glms.html#core-material-principles-of-statistical-inference-and-simulation",
    "href": "glms.html#core-material-principles-of-statistical-inference-and-simulation",
    "title": "Statistical Modelling: Theory and Practice",
    "section": "Core material: Principles of Statistical Inference and Simulation",
    "text": "Core material: Principles of Statistical Inference and Simulation\n\nSection 1: Introduction\nA reintroduction to statistics from the perspective of generalised linear modelling.\n\n\n\n\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nPart One: Model fitting as parameter calibration\n\n\n3 min\n\n\n\n\nPart Two: Systematic components and link functions\n\n\n3 min\n\n\n\n\nPart Three: glm is just fancy lm\n\n\n4 min\n\n\n\n\nPart Four: why only betas just look at betas\n\n\n18 min\n\n\n\n\n\n\nNo matching items\n\n\n\n\nSection 2: Likelihood and simulation theory\nA deep dive into the core principles and concepts underlying statistical modelling using likelihood, and how to use models for honest prediction and simulation\n\n\n\n\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nPart Five: Traversing the Likelihood Landscape\n\n\n9 min\n\n\n\n\nPart Six: The Robo-Chauffeur\n\n\n7 min\n\n\n\n\nPart Seven: Feeling Uncertain\n\n\n10 min\n\n\n\n\nPart Eight: Guessing what a landscape looks like by feeling the curves beneath our feet\n\n\n10 min\n\n\n\n\nPart Nine: Answering questions with honest uncertainty: Expected values and Predicted values\n\n\n10 min\n\n\n\n\nPart Ten: Log Likelihood estimation for Logistic Regression\n\n\n8 min\n\n\n\n\n\n\nNo matching items\n\n\n\n\nSection 3: A complete example\nThe application of the above material to a specific dataset, starting with model fitting and simple prediction functions, and ending with Bayesian modelling\n\n\n\n\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nPart Eleven: Honest Predictions the easier way\n\n\n14 min\n\n\n\n\nPart Twelve: Honest Predictions the slightly-less easier way\n\n\n13 min\n\n\n\n\nPart Thirteen: On Marbles and Jumping Beans\n\n\n21 min\n\n\n\n\n\n\nNo matching items\n\n\n\n\nNotes on core section\nI consider the production of this material something of a public service. More on the background to the series, which includes my own background, is available in this post here."
  },
  {
    "objectID": "glms.html#additional-materials",
    "href": "glms.html#additional-materials",
    "title": "Statistical Modelling: Theory and Practice",
    "section": "Additional Materials",
    "text": "Additional Materials\nDeep dives into specific topic areas. These sections can be approached in any order, so long as the core section has been read and understood first.\n\nCausal Inference\nAn opinionated discussion of the topic and challenges of causal inference.\n\n\n\n\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nPart Fourteen: A non-technical but challenging introduction to causal inference…\n\n\n5 min\n\n\n\n\nPart Fifteen: Causal Inference: The platinum and gold standards\n\n\n10 min\n\n\n\n\nPart Sixteen: Causal Inference: How to try to do the impossible\n\n\n16 min\n\n\n\n\nPart Seventeen: Causal Inference: Controlling and Matching Approaches\n\n\n14 min\n\n\n\n\nPart Eighteen: Causal Inference: Some closing thoughts\n\n\n16 min\n\n\n\n\n\n\nNo matching items\n\n\n\n\nTime Series\nAn introduction to time series, focused around the ARIMA modelling framework.\n\n\n\n\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nPart Nineteen: Time Series: Introduction and Autoregression\n\n\n10 min\n\n\n\n\nPart Twenty: Time Series: Integration\n\n\n12 min\n\n\n\n\nPart Twenty One: Time Series: The Moving Average Model\n\n\n9 min\n\n\n\n\nPart Twenty Two: Time Series - ARIMA in practice\n\n\n9 min\n\n\n\n\nPart Twenty Three: Time series and seasonality\n\n\n13 min\n\n\n\n\nPart Twenty Four: Time series - Vector Autoregression and multivariate models\n\n\n13 min\n\n\n\n\nTime series: Some closing remarks\n\n\n11 min\n\n\n\n\n\n\nNo matching items\n\n\n\n\nResampling Methods/Hacker Stats\nAn introduction to resampling methods (AKA ‘Hacker Stats’), including bootstrapping and permutation testing.\n\n\n\n\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nHacker Stats: Intro and overview\n\n\n9 min\n\n\n\n\nA brief introduction to bootstrapping\n\n\n8 min\n\n\n\n\nPermutation Testing, and the intuition of the Null hypothesis, with Base R\n\n\n11 min\n\n\n\n\nGetting started with the infer package\n\n\n10 min\n\n\n\n\nResampling for post-stratification\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/edinburgh-js.html#background-and-motivation",
    "href": "slides/edinburgh-js.html#background-and-motivation",
    "title": "Quarto for Technical blogging",
    "section": "Background and motivation",
    "text": "Background and motivation\n\nData Scientist Curious About Software Development\n\n~ 20 years using R\nInscrutible Shiny\n\nSoftware development at a distance\n\nCodeclan Software Development (Cohort E63) 2023\n\nPython\nJavascript"
  },
  {
    "objectID": "slides/edinburgh-js.html#separating-the-challenges",
    "href": "slides/edinburgh-js.html#separating-the-challenges",
    "title": "Quarto for Technical blogging",
    "section": "Separating the Challenges",
    "text": "Separating the Challenges\n\n\n\nWebsite\n\nWritten in React\nSemantic HTML\nHand coded CSS\nLittle touched\n\n\n\n\nBlog\n\nWritten in Quarto\nNot perfectly customised but ‘good enough’\nCan focus on what I’m writing about, not how I’m writing it\nUpdated most weeks (Currently around 80 posts)"
  },
  {
    "objectID": "slides/edinburgh-js.html#whats-quarto",
    "href": "slides/edinburgh-js.html#whats-quarto",
    "title": "Quarto for Technical blogging",
    "section": "What’s Quarto?",
    "text": "What’s Quarto?\n\n\n\nSuccessor to RMarkdown\nAdjacent to Jupyter Notebooks\n\n\n\nLanguage invariant\n\nR\nPython\nJulia\nObservable.js\n\nIDE invariant\n\nRStudio\nVisual Studio Code\n\n\n\n\nOutput invariant\n\nfile formats\n\nhtml\npdf\nword\n\nproduct types\n\nScientific notebooks\nManuscripts\nSlide decks (including this, using reveal.js)\nGeneric websites and online books\nBlogs"
  },
  {
    "objectID": "slides/edinburgh-js.html#chunks-and-fences",
    "href": "slides/edinburgh-js.html#chunks-and-fences",
    "title": "Quarto for Technical blogging",
    "section": "Chunks and Fences",
    "text": "Chunks and Fences\n\n\n\nChunks for computing\n\nMermaid\nlatex\nPython\nR\nobservable etc\n\n\n\n\nFences for styling\n\nColumns\nSidebars\nFormatting and colouring"
  },
  {
    "objectID": "slides/edinburgh-js.html#getting-started",
    "href": "slides/edinburgh-js.html#getting-started",
    "title": "Quarto for Technical blogging",
    "section": "Getting started",
    "text": "Getting started\n\nExcellent how-tos on website\n\nBlog set-up\nIncluding how to deploy"
  },
  {
    "objectID": "slides/edinburgh-js.html#customising",
    "href": "slides/edinburgh-js.html#customising",
    "title": "Quarto for Technical blogging",
    "section": "Customising",
    "text": "Customising\n\nAll about the YAML/Frontmatter\nCode chunk options\nStyles/themes through bootstrap/bootswatch\nSearch\n\nDefault\nAlgolia integration\nListing/Collection pages"
  },
  {
    "objectID": "slides/edinburgh-js.html#in-practice",
    "href": "slides/edinburgh-js.html#in-practice",
    "title": "Quarto for Technical blogging",
    "section": "In practice",
    "text": "In practice\n\nThe site\nThe github repo"
  },
  {
    "objectID": "slides/edinburgh-js.html#concluding-thoughts",
    "href": "slides/edinburgh-js.html#concluding-thoughts",
    "title": "Quarto for Technical blogging",
    "section": "Concluding thoughts",
    "text": "Concluding thoughts\n\nGood enough blogging platform\nLow barrier to reentry\nAbstracts away enough software dev complexity, allowing more focus on data science complexity\n\nBack to software dev at a distance (only now more through choice)."
  },
  {
    "objectID": "unpop.html",
    "href": "unpop.html",
    "title": "Unpop",
    "section": "",
    "text": "I tend to take ‘unserious’ things seriously (as well as ‘serious’ things unseriously). Often if I’m watching, listening to or reading something, something nominally ‘unserious’ or ‘lowbrow’ or ‘poppy’, it will trigger a cascade of broader ideas, thoughts and concepts. 1\nAs well as an extensive series of posts on statistical theory, I’ve also written some posts inspired by various specific pop cultural artefacts, such as specific books, films, TV shows and games. In this section I’ve aimed to collate these disparate reflections into a single place. I’ve called it ‘unpop’ because - though many of the things I’m writing about are part of pop culture, I’m probably not picking them up when they’re most popular, and neither am I treating them in a way that’s particular conventional or ‘poppy’.\nOften, the posts in this emergent series are going to be less technical than in the technical series. However, that doesn’t mean I can promise these posts will be free of technical content, concepts or ideas. If you’re scared of algebra, graphs, big words or code, this whole blogsite probably isn’t for you!"
  },
  {
    "objectID": "unpop.html#introduction",
    "href": "unpop.html#introduction",
    "title": "Unpop",
    "section": "",
    "text": "I tend to take ‘unserious’ things seriously (as well as ‘serious’ things unseriously). Often if I’m watching, listening to or reading something, something nominally ‘unserious’ or ‘lowbrow’ or ‘poppy’, it will trigger a cascade of broader ideas, thoughts and concepts. 1\nAs well as an extensive series of posts on statistical theory, I’ve also written some posts inspired by various specific pop cultural artefacts, such as specific books, films, TV shows and games. In this section I’ve aimed to collate these disparate reflections into a single place. I’ve called it ‘unpop’ because - though many of the things I’m writing about are part of pop culture, I’m probably not picking them up when they’re most popular, and neither am I treating them in a way that’s particular conventional or ‘poppy’.\nOften, the posts in this emergent series are going to be less technical than in the technical series. However, that doesn’t mean I can promise these posts will be free of technical content, concepts or ideas. If you’re scared of algebra, graphs, big words or code, this whole blogsite probably isn’t for you!"
  },
  {
    "objectID": "unpop.html#footnotes",
    "href": "unpop.html#footnotes",
    "title": "Unpop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s an occupational hazard of being me.↩︎"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-data-wrangling/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-data-wrangling/index.html",
    "title": "Tidy Tuesday: Self-generated data challenge",
    "section": "",
    "text": "For this session we chose to focus on a data wrangling challenge that Andrew brought to the table, rather than the most recent dataset. The challenge involved converting slightly messy data structure from the first format seen below, to the second (tidy) structure:\n\nlibrary(tidyverse)\n\ndf &lt;-   tribble(~`...1`, ~`...2`, ~`...3`, ~`...4`, ~`...5`, ~`...6`, ~`...7`, ~`...8`, ~`...9`,\n          NA,NA, NA, \"House Stats in Nov\", NA, \"Flat Stats in Nov\", NA, \"All Stats in Nov\", NA,\n          \"Region\", \"LA\", \"LACode\", \"Count\", \"Sold\", \"Count\", \"Sold\", \"Count\", \"Sold\",\n          \"Scotland\",  \"Minyip\", \"M394932\", \"1000\", \"900\", \"600\", \"300\", \"1600\", \"1200\")\n\ndf\n\n# A tibble: 3 × 9\n  ...1     ...2   ...3    ...4               ...5  ...6        ...7  ...8  ...9 \n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 &lt;NA&gt;     &lt;NA&gt;   &lt;NA&gt;    House Stats in Nov &lt;NA&gt;  Flat Stats… &lt;NA&gt;  All … &lt;NA&gt; \n2 Region   LA     LACode  Count              Sold  Count       Sold  Count Sold \n3 Scotland Minyip M394932 1000               900   600         300   1600  1200 \n\n# desired output\ntribble(~\"region\",  ~\"la\", ~\"la_code\", ~\"house_count\", ~\"house_sold\", ~\"flat_count\", ~\"flat_sold\", ~\"all_count\", ~\"all_sold\",\n        \"Scotland\",  \"Minyip\", \"M394932\", \"1000\", \"900\", \"600\", \"300\", \"1600\", \"1200\") \n\n# A tibble: 1 × 9\n  region   la     la_code house_count house_sold flat_count flat_sold all_count\n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    \n1 Scotland Minyip M394932 1000        900        600        300       1600     \n# ℹ 1 more variable: all_sold &lt;chr&gt;\n\n\nAndrew had a prepared solution. But Kate led the session by developing another solution from scratch"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-data-wrangling/index.html#introduction",
    "href": "posts/tardy-tuesday/tidy-tuesday-data-wrangling/index.html#introduction",
    "title": "Tidy Tuesday: Self-generated data challenge",
    "section": "",
    "text": "For this session we chose to focus on a data wrangling challenge that Andrew brought to the table, rather than the most recent dataset. The challenge involved converting slightly messy data structure from the first format seen below, to the second (tidy) structure:\n\nlibrary(tidyverse)\n\ndf &lt;-   tribble(~`...1`, ~`...2`, ~`...3`, ~`...4`, ~`...5`, ~`...6`, ~`...7`, ~`...8`, ~`...9`,\n          NA,NA, NA, \"House Stats in Nov\", NA, \"Flat Stats in Nov\", NA, \"All Stats in Nov\", NA,\n          \"Region\", \"LA\", \"LACode\", \"Count\", \"Sold\", \"Count\", \"Sold\", \"Count\", \"Sold\",\n          \"Scotland\",  \"Minyip\", \"M394932\", \"1000\", \"900\", \"600\", \"300\", \"1600\", \"1200\")\n\ndf\n\n# A tibble: 3 × 9\n  ...1     ...2   ...3    ...4               ...5  ...6        ...7  ...8  ...9 \n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;              &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 &lt;NA&gt;     &lt;NA&gt;   &lt;NA&gt;    House Stats in Nov &lt;NA&gt;  Flat Stats… &lt;NA&gt;  All … &lt;NA&gt; \n2 Region   LA     LACode  Count              Sold  Count       Sold  Count Sold \n3 Scotland Minyip M394932 1000               900   600         300   1600  1200 \n\n# desired output\ntribble(~\"region\",  ~\"la\", ~\"la_code\", ~\"house_count\", ~\"house_sold\", ~\"flat_count\", ~\"flat_sold\", ~\"all_count\", ~\"all_sold\",\n        \"Scotland\",  \"Minyip\", \"M394932\", \"1000\", \"900\", \"600\", \"300\", \"1600\", \"1200\") \n\n# A tibble: 1 × 9\n  region   la     la_code house_count house_sold flat_count flat_sold all_count\n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    \n1 Scotland Minyip M394932 1000        900        600        300       1600     \n# ℹ 1 more variable: all_sold &lt;chr&gt;\n\n\nAndrew had a prepared solution. But Kate led the session by developing another solution from scratch"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-data-wrangling/index.html#katies-solution",
    "href": "posts/tardy-tuesday/tidy-tuesday-data-wrangling/index.html#katies-solution",
    "title": "Tidy Tuesday: Self-generated data challenge",
    "section": "Katie’s solution",
    "text": "Katie’s solution\n\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(fillr)\n \ndf &lt;- \n  tribble(~`...1`, ~`...2`, ~`...3`, ~`...4`, ~`...5`, ~`...6`, ~`...7`, ~`...8`, ~`...9`,\n          NA,NA, NA, \"House Stats in Nov\", NA, \"Flat Stats in Nov\", NA, \"All Stats in Nov\", NA,\n          \"Region\", \"LA\", \"LACode\", \"Count\", \"Sold\", \"Count\", \"Sold\", \"Count\", \"Sold\",\n          \"Scotland\",  \"Minyip\", \"M394932\", \"1000\", \"900\", \"600\", \"300\", \"1600\", \"1200\")\n \nnm1 &lt;- unlist(df[1,])\nnm2 &lt;- unlist(df[2,])\nnm1\n\n                ...1                 ...2                 ...3 \n                  NA                   NA                   NA \n                ...4                 ...5                 ...6 \n\"House Stats in Nov\"                   NA  \"Flat Stats in Nov\" \n                ...7                 ...8                 ...9 \n                  NA   \"All Stats in Nov\"                   NA \n\nnm2\n\n    ...1     ...2     ...3     ...4     ...5     ...6     ...7     ...8 \n\"Region\"     \"LA\" \"LACode\"  \"Count\"   \"Sold\"  \"Count\"   \"Sold\"  \"Count\" \n    ...9 \n  \"Sold\" \n\nnm1 &lt;- str_extract(nm1, \"\\\\w*(?=\\\\s)\")\nnm1 &lt;- fill_missing_previous(nm1)\nnm1[is.na(nm1)] &lt;- \"\"\nnm1\n\n[1] \"\"      \"\"      \"\"      \"House\" \"House\" \"Flat\"  \"Flat\"  \"All\"   \"All\"  \n\nnms &lt;- paste(nm1, nm2)\n \nnames(df) &lt;- nms\n \ndf &lt;- clean_names(df[-(1:2),])\ndf\n\n# A tibble: 1 × 9\n  region   la     la_code house_count house_sold flat_count flat_sold all_count\n  &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    \n1 Scotland Minyip M394932 1000        900        600        300       1600     \n# ℹ 1 more variable: all_sold &lt;chr&gt;\n\n\nThis solution involved a mixture of tidyverse and base R expressions. Brendan made the point that tidyverse is great at doing 80% of the work needed quickly, but sometimes base R is needed to complete the remaining 20%."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-data-wrangling/index.html#other-solutions",
    "href": "posts/tardy-tuesday/tidy-tuesday-data-wrangling/index.html#other-solutions",
    "title": "Tidy Tuesday: Self-generated data challenge",
    "section": "Other solutions",
    "text": "Other solutions\nAndrew proposed the following function for doing the data cleaning.\n\nmerge_rows &lt;- function(df){ # eg. lst[[1]]\n\n  # convert rows1&2 into columns\n  row1 &lt;- df[1, ] |&gt; t() \n  row2 &lt;- df[2, ] |&gt; t() \n  # remove selected text then fill down NAs\n  row1 &lt;- str_remove(row1, \" Stats in Nov\") |&gt; \n    as_tibble() |&gt; \n    fill(value) |&gt; \n    pull() |&gt; replace_na(\"\")\n\n  row3 &lt;- str_c(row1, \" \", row2) |&gt; \n    str_trim()\n  # create same header vector as original df\n  header_vec &lt;- character()\n  for (i in seq_along(df)){\n    header_vec[i] &lt;- str_c(\"...\", i)\n  }\n  # create tibble with header and 1st row of df\n  tib &lt;-\n    t(row3) |&gt; \n    as_tibble() |&gt; \n    set_names(header_vec) \n  return(tib)\n}\n \ncreate_header_df &lt;- function(df) {\n  merge_rows(df) |&gt; \n    bind_rows(df) |&gt; \n    janitor::row_to_names(1) |&gt; \n    clean_names()\n}\n \ncreate_header_df(df) |&gt; \n  slice(-c(1:2))\n\n# A tibble: 0 × 18\n# ℹ 18 variables: na &lt;chr&gt;, na_2 &lt;chr&gt;, na_3 &lt;chr&gt;, na_4 &lt;chr&gt;, na_5 &lt;chr&gt;,\n#   na_6 &lt;chr&gt;, na_7 &lt;chr&gt;, na_8 &lt;chr&gt;, na_9 &lt;chr&gt;, na_10 &lt;chr&gt;, na_11 &lt;chr&gt;,\n#   na_12 &lt;chr&gt;, na_13 &lt;chr&gt;, na_14 &lt;chr&gt;, na_15 &lt;chr&gt;, na_16 &lt;chr&gt;,\n#   na_17 &lt;chr&gt;, na_18 &lt;chr&gt;\n\n\nAnd Brendan suggested the following tidyverse solution:\n\nnice_names &lt;- tibble(one = unlist(df[1,]), \n       two = unlist(df[2,])) |&gt;\n  mutate(one = str_replace_all(one,  \"Stats in Nov\", \"\")) |&gt;\n  fill(one) |&gt;\n  mutate(three = str_replace_all(paste(one, two), \"NA \", \"\")) |&gt;\n  pull(three)\n\nnames(df) &lt;- nice_names\ndf |&gt;\n  janitor::clean_names()\n\n# A tibble: 1 × 9\n  scotland_na minyip_na m394932_na x1000_na x900_na x600_na x300_na x1600_na\n  &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   \n1 Scotland    Minyip    M394932    1000     900     600     300     1600    \n# ℹ 1 more variable: x1200_na &lt;chr&gt;"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-data-wrangling/index.html#discussion",
    "href": "posts/tardy-tuesday/tidy-tuesday-data-wrangling/index.html#discussion",
    "title": "Tidy Tuesday: Self-generated data challenge",
    "section": "Discussion",
    "text": "Discussion\nAfter Kate solved Andrew’s problem with lots of time to spare, she then ran through a real-world example that makes use of some similar approaches. The use of unlist() to extract vectors to work on was a big part of both solutions.\nWe all agreed Kate was excellent with these kinds of data wrangling challenges, despite coding with no prior warning of the challenge and wet hair."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-christmas-films/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-christmas-films/index.html",
    "title": "Tidy Tuesday: Christmas films",
    "section": "",
    "text": "A shorter and even tardier Tidy Tuesday this week, given we gave ourselves only half an hour rather than the usual hour to look over the most recent dataset.\nThe dataset was about Christmas films.\nOur first question: is Die Hard a Christmas film?\nNot according to the methods used to produce the dataset. If a film doesn’t have Christmas or equivalent in its title, it’s not coming in!\n\nloading\n\n\ntt &lt;- tidytuesdayR::tt_load('2023-12-12')\n\nOnly 7 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\nOnly 7 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\nOnly 7 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\nOnly 7 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\nOnly 7 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\nOnly 6 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\n--- Compiling #TidyTuesday Information for 2023-12-12 ----\n\n\nOnly 5 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\n--- There are 2 files available ---\n\n\nOnly 4 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\n--- Starting Download ---\n\n\nOnly 4 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\n    Downloading file 1 of 2: `holiday_movies.csv`\n\n\nOnly 3 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\n    Downloading file 2 of 2: `holiday_movie_genres.csv`\n\n\nOnly 2 Github queries remaining until 2024-02-22 11:44:38 AM GMT.\n\n\n--- Download complete ---\n\ndf1 &lt;- tt[[1]]\ndf2 &lt;- tt[[2]]\n\n\n\ncount of films by year\n\n\ndf1 %&gt;%\n  count(year, sort = TRUE)\n\n# A tibble: 91 × 2\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  2021   183\n 2  2022   173\n 3  2020   172\n 4  2019   143\n 5  2018   129\n 6  2023   107\n 7  2017   102\n 8  2015    76\n 9  2016    75\n10  2012    68\n# ℹ 81 more rows\n\n\n\nhow many films by year -plot with log on y axis\n\n\ndf1 %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n))+\n  geom_point()+\n  #stat_smooth()+\n  scale_y_log10()\n\n\n\n\n\nhow many films by year -plot with log on y axis\nfilter by 1960 onwards\n\n\ndf1 %&gt;%\n  filter(year &gt;= 1960) %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n))+\n  geom_point()+\n  #stat_smooth()+\n  scale_y_log10()\n\n\n\n\n\nhow many films by year -plot with log on y axis\nfilter by 1960 onwards\n\n\ndf1 %&gt;%\n  filter(year &gt;= 1960) %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n))+\n  geom_point()+\n  stat_smooth(method = \"lm\")+\n  scale_y_log10()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nquestions\n\nhow are they published? [cinema / streaming?]\nis it on imdb?\nfull inclusion of 2023?\nare more recent films rubbish?\n\n\ndf1 %&gt;%\n  \n  group_by(year) %&gt;%\n  summarise(avg_rating = mean(average_rating)) %&gt;%\n  ggplot(aes(x = year, y = avg_rating))+\n  geom_point()\n\n\n\n\n\nnumber of films vs avg rating\nfewer films may drive extreme values\nnumber of films vs avg rating\n\n\ndf1 %&gt;%\n  \n  group_by(year) %&gt;%\n  summarise(\n    avg_rating = mean(average_rating), \n    n_films = n() ) %&gt;%\n  ggplot(aes(x = n_films, y = avg_rating))+\n  geom_point()"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-american-idol/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-american-idol/index.html",
    "title": "Tardy Tuesday: American Idol",
    "section": "",
    "text": "This session looked at data on American Idol. Abram had already made a head-start with the analysis so (with some encouragement) led the session:"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-american-idol/index.html#preparation",
    "href": "posts/tardy-tuesday/tidy-tuesday-american-idol/index.html#preparation",
    "title": "Tardy Tuesday: American Idol",
    "section": "Preparation",
    "text": "Preparation\nLoading the package\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(magrittr)\n\n\nAttaching package: 'magrittr'\n\nThe following object is masked from 'package:purrr':\n\n    set_names\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\nThe following object is masked from 'package:stats':\n\n    filter\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(lubridate)\n# install.packages(\"styler\")\n# library(styler)\n\nLoading the data\n\nauditions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-07-23/auditions.csv')\n\nRows: 142 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): audition_city, audition_venue, episodes, episode_air_date, callbac...\ndbl  (2): season, tickets_to_hollywood\ndate (4): audition_date_start, audition_date_end, callback_date_start, callb...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\neliminations &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-07-23/eliminations.csv')\n\nRows: 456 Columns: 46\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (44): place, gender, contestant, top_36, top_36_2, top_36_3, top_36_4, t...\ndbl  (1): season\nlgl  (1): comeback\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfinalists &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-07-23/finalists.csv')\n\nRows: 190 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): Contestant, Birthday, Birthplace, Hometown, Description\ndbl (1): Season\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nratings &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-07-23/ratings.csv')\n\nRows: 593 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): episode, airdate, 18_49_rating_share, timeslot_et, dvr_18_49, dvr_...\ndbl  (4): season, show_number, viewers_in_millions, nightlyrank\nlgl  (1): ref\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nseasons &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-07-23/seasons.csv')\n\nRows: 18 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (8): winner, runner_up, original_release, original_network, hosted_by, j...\ndbl (2): season, no_of_episodes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nsongs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-07-23/songs.csv')\n\nRows: 2429 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): season, week, contestant, song, artist, song_theme, result\ndbl (1): order\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAbram made use of the slightly exotic %&lt;&gt;% pipe, which passes its output back to its first argument.\nSome data tidying and basic exploration:\n\nsongs %&lt;&gt;% mutate(artist = if_else(artist == \"*NSYNC\", \"NSYNC\", artist))\nsongs_n &lt;- songs %&gt;% group_by(artist, song) %&gt;% summarise(n = n()) %&gt;% arrange(-n)\n\n`summarise()` has grouped output by 'artist'. You can override using the\n`.groups` argument.\n\nartists_n &lt;- songs %&gt;% group_by(artist) %&gt;% summarise(n = n()) %&gt;% arrange(-n)\nwinning_songs &lt;- songs %&gt;% group_by(artist, song, result) %&gt;% summarise(n = n())\n\n`summarise()` has grouped output by 'artist', 'song'. You can override using\nthe `.groups` argument."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-american-idol/index.html#exploration",
    "href": "posts/tardy-tuesday/tidy-tuesday-american-idol/index.html#exploration",
    "title": "Tardy Tuesday: American Idol",
    "section": "Exploration",
    "text": "Exploration\nFirst we looked at viewing figures by show number and by season\n\nratings %&gt;% filter(!is.na(viewers_in_millions)) %&gt;%\n  ggplot(aes(x = show_number, y = viewers_in_millions)) + geom_point() + geom_line() + facet_wrap(vars(season), scales = \"free_y\")\n\n\n\n\nThen some preparation of the date column to get it in date format:\n\nratings %&lt;&gt;% mutate(airdate = if_else(season == 13, paste0(airdate, \", 2014\"), airdate),\n                    proper_airdate = mdy(airdate))\n\nThen a visualisation over time\n\nratings %&gt;% ggplot(aes(x = proper_airdate, y = viewers_in_millions)) + geom_point() +\n  expand_limits(y = 0) + stat_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 3 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nTabulation:\n\ntabyl(ratings$season)\n\n ratings$season  n    percent\n              1 25 0.04215852\n              2 41 0.06913997\n              3 44 0.07419899\n              4 43 0.07251265\n              5 41 0.06913997\n              6 41 0.06913997\n              7 42 0.07082631\n              8 40 0.06745363\n              9 43 0.07251265\n             10 39 0.06576728\n             11 40 0.06745363\n             12 37 0.06239460\n             13 39 0.06576728\n             15 24 0.04047218\n             16 19 0.03204047\n             17 19 0.03204047\n             18 16 0.02698145\n\n\nNow average views\n\naverage_views &lt;- ratings %&gt;% group_by(season) %&gt;% summarise(avg_views = mean(viewers_in_millions, na.rm = TRUE))\n\nWe saw a jump in most seasons at the very end, so decided to look at how big a proportional jump this was:\n\nrel_views &lt;- ratings %&gt;% group_by(season) %&gt;% slice_tail(n=2) %&gt;%\n  summarise(relative_views = viewers_in_millions[2]/viewers_in_millions[1])\n\n\nrel_views %&gt;% ggplot(aes(x = season, y = relative_views)) + geom_point()\n\n\n\n\nThen average views\n\naverage_views %&gt;% ggplot(aes(season, avg_views)) + geom_line() + expand_limits(y = 0)\n\n\n\n\nNow to join average views (over whole season) to with jump at the end (rel_views) to see if any obvious relationship:\n\nfull_join(average_views, rel_views) %&gt;% ggplot(aes(x = avg_views, y = relative_views)) + geom_point()\n\nJoining with `by = join_by(season)`\n\n\n\n\n\nNope..\nAnother pattern we saw was that the first episode in a season seemed to be about the most popular, then there was a drop-off over time\n\nratings %&lt;&gt;% group_by(season) %&gt;% arrange(show_number) %&gt;%\n  mutate(share_of_first = viewers_in_millions / viewers_in_millions[1])\n\nratings %&gt;%\n  ggplot(aes(show_number, share_of_first, group = season, color = as.factor(season))) +\n  geom_line()\n\n\n\n\nFinally, we looked at an interactive visualising using the ggplotly() convenience function using the plotly package:\n\ngg&lt;- ratings %&gt;% filter(season &gt;= 3) %&gt;%\n  ggplot(aes(show_number, share_of_first, group = season, color = as.factor(season))) +\n  geom_point() + scale_y_log10()\n\nggplotly(gg)"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html",
    "title": "Tidy Tuesday: 20 Feb 2024 - R Grants",
    "section": "",
    "text": "This TidyTuesday session investigated the funding of intrastructure steering committee grants from the R consortium over time, and was led by Kennedy Owuso-Afriyie."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#introduction",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#introduction",
    "title": "Tidy Tuesday: 20 Feb 2024 - R Grants",
    "section": "",
    "text": "This TidyTuesday session investigated the funding of intrastructure steering committee grants from the R consortium over time, and was led by Kennedy Owuso-Afriyie."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#data-loading",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#data-loading",
    "title": "Tidy Tuesday: 20 Feb 2024 - R Grants",
    "section": "Data loading",
    "text": "Data loading\nWe looked at two options for loading the dataset: one using the tidytuesdayR package; the other linking to the url directly.\n\n\nCode\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n \nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n \n \n# tuesdata &lt;- tidytuesdayR::tt_load('2024-02-20')\n# ## OR\n# tuesdata &lt;- tidytuesdayR::tt_load(2024, week = 8)\n \n# isc_grants &lt;- tuesdata$isc_grants\n \n# Option 2: Read directly from GitHub\n \nisc_grants &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-20/isc_grants.csv')\n \nisc_grants\n\n\n# A tibble: 85 × 7\n    year group title                          funded proposed_by summary website\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt;  \n 1  2023     1 The future of DBI (extension …  10000 \"Kirill Mü… \"This … &lt;NA&gt;   \n 2  2023     1 Secure TLS Communications for…  10000 \"Charlie G… \"The p… &lt;NA&gt;   \n 3  2023     1 volcalc: Calculate predicted …  12265 \"Kristina … \"This … &lt;NA&gt;   \n 4  2023     1 autotest: Automated testing o…   3000 \"Mark Padg… \"The p… &lt;NA&gt;   \n 5  2023     1 api2r: An R Package for Auto-…  15750 \"Jon Harmo… \"This … &lt;NA&gt;   \n 6  2022     2 D3po: R Package for Easy Inte…   8000 \"Mauricio … \"The D… &lt;NA&gt;   \n 7  2022     2 Tooling and Guidance for Tran…   8000 \"Maëlle Sa… \"Tooli… &lt;NA&gt;   \n 8  2022     2 Online Submission and Review …  22000 \"Simon Urb… \"The O… &lt;NA&gt;   \n 9  2022     2 Upgrading SatRdays Website Te…   6000 \"Ben Ubah\"  \"The U… &lt;NA&gt;   \n10  2022     2 Building the “Spatial Data Sc…  25000 \"Orhun Ayd… \"The B… &lt;NA&gt;   \n# ℹ 75 more rows\n\n\nSome questions we initially thought about asking:\n\nAre there any keywords that stand out in the titles or summaries of awarded grants?\nHave the funded amounts changed over time?\n\nAs a fairly new user to R, Kennedy focused on the second question, creating a bar plot of funding over time using ggplot. Meanwhile, Clarke and Clark investigated and proposed some approaches for addressing the first question."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#graph-of-funding-over-time",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#graph-of-funding-over-time",
    "title": "Tidy Tuesday: 20 Feb 2024 - R Grants",
    "section": "Graph of funding over time",
    "text": "Graph of funding over time\n\n\nCode\nfunding_by_year &lt;- isc_grants %&gt;% \n  group_by(year) %&gt;% \n  summarise(total_funded = sum(funded)) %&gt;% \n  ungroup()\n \nfunding_by_year %&gt;% \n  ggplot(aes(x=year, y=total_funded)) + \n  geom_col() + \n  labs(\n    x = \"Year\", \n    y = \"total funded in dollars\",\n    title = \"Total funding by year\",\n    caption = \"source: TidyTuesday\",\n    subtitle = \"2018 is a bit weird\" \n  )\n\n\n\n\n\nWe discussed piping with the %&gt;% operator, and the value this has for being able to develop code step-by-step in a way similar to human languages.\n\nWe said, when we see &lt;- or -&gt;, this should be read as ‘is assigned to’.\nAnd we said, when we see the %&gt;% (or |&gt;) operator in a script, this should be read as, and then.\nWe noted how R can tell when it encounters an incomplete expression, and so doesn’t evaluate, just as when someone hears a sentence that ends ‘and then’, they know it’s not really the end of the sentence.\n\nWe also discussed how when making a graph, we should consider how objective or how subjective we should be when presenting the image to the viewer. This will depend on the audience. In our example, the x axis, y axis, title and caption labels are all just objective information. However the subtitle is more subjective, and so more our opinion rather than something no one could reasonably disagree with."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#tidy-text-to-get-important-key-words",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#tidy-text-to-get-important-key-words",
    "title": "Tidy Tuesday: 20 Feb 2024 - R Grants",
    "section": "Tidy Text to get important key words",
    "text": "Tidy Text to get important key words\nBrendan offered the following code chunk to explore the content of the free text summary field in the dataset:\n\n\nCode\n#install.packages(\"tidytext\")\n#install.packages(\"SnowballC\")\nlibrary(tidytext)\nlibrary(SnowballC) # for wordStem\n \nisc_grants |&gt;\n  unnest_tokens(word, summary) |&gt;\n  anti_join(get_stopwords()) |&gt;\n  mutate(stem = wordStem(word))\n\n\n# A tibble: 6,242 × 8\n    year group title                      funded proposed_by website word  stem \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;\n 1  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    prop… prop…\n 2  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    most… most…\n 3  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    focu… focus\n 4  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    main… main…\n 5  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    supp… supp…\n 6  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    dbi   dbi  \n 7  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    dbit… dbit…\n 8  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    test  test \n 9  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    suite suit \n10  2023     1 The future of DBI (extens…  10000 Kirill Mül… &lt;NA&gt;    three three\n# ℹ 6,232 more rows\n\n\nThis pulled out words (other than stopwords1) from the summary field, and identified the stem of these words. This potentially means the number of unique stems can be compared, rather than the number of unique words.\nAntony suggested that, as the summaries are all about supporting a technical programing language, some additional words are also so common they should also be considered stopwords. He also produced a wordcloud visualisation showing the most common non-stopwords in the corpus of summary text”\n\n\nCode\n# tidytext with SnowballC ----\n\n# Tokenize the text\n\n\nmy_stop_words &lt;- \n  bind_rows(\n    get_stopwords(),\n    tibble(\n      word = c(\"r\",\"package\",\"data\",\"users\",\"project\",\"cran\",\"community\",\"use\",\n               \"development\",\"documentation\",\"can\",\"also\",\"system\",\"new\",\"code\",\n               \"available\",\"existing\",\"support\",\"make\",\"two\",\"build\"),\n      lexicon = \"tony's custom stop words\"\n    )\n  )\n\nmy_stop_words &lt;- \n  my_stop_words %&gt;% \n  mutate(stem = wordStem(word))\n\ntokens &lt;- \n  isc_grants %&gt;%\n  unnest_tokens(word, summary) %&gt;% \n  mutate(stem = wordStem(word)) %&gt;% \n  anti_join(my_stop_words, by = \"stem\")\n\n\ntoken_frequency &lt;- tokens %&gt;% count(word) %&gt;% arrange(-n)\n\n\n\n# View the processed stems\nwordcloud::wordcloud(words = token_frequency$word, \n          freq = token_frequency$n, min.freq = 1,\n          max.words = 20, random.order = FALSE, rot.per = 0.35, \n          colors = RColorBrewer::brewer.pal(8, \"Dark2\"))"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#footnotes",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-grants/index.html#footnotes",
    "title": "Tidy Tuesday: 20 Feb 2024 - R Grants",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStop words are terms that are so common within sentences they don’t really add much unique information. They’re words like ‘and’, ‘the’, ‘an’, and so on.↩︎"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-leapyears/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-leapyears/index.html",
    "title": "Tidy Tuesday 27 Feb 2024: Leap Years",
    "section": "",
    "text": "The latest TidyTuesday dataset was on births, deaths and other historical events that occurred in leap years, i.e. those years that include 29 February (such as 2024!). Further details are here.\nMyriam led the session, and Antony provided additional code for performing text field analysis after the session.\nAlso, Emu the cat had the following contribution to make:\n\n43e’/;£@@@@@@@@@@.1"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-leapyears/index.html#introduction",
    "href": "posts/tardy-tuesday/tidy-tuesday-leapyears/index.html#introduction",
    "title": "Tidy Tuesday 27 Feb 2024: Leap Years",
    "section": "",
    "text": "The latest TidyTuesday dataset was on births, deaths and other historical events that occurred in leap years, i.e. those years that include 29 February (such as 2024!). Further details are here.\nMyriam led the session, and Antony provided additional code for performing text field analysis after the session.\nAlso, Emu the cat had the following contribution to make:\n\n43e’/;£@@@@@@@@@@.1"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-leapyears/index.html#the-session",
    "href": "posts/tardy-tuesday/tidy-tuesday-leapyears/index.html#the-session",
    "title": "Tidy Tuesday 27 Feb 2024: Leap Years",
    "section": "The session",
    "text": "The session\nWe started by loading some packages\n\n# Option 1: tidytuesdayR package \n## install.packages(\"tidytuesdayR\")\n## install.packages(\"waldo\")\n## install.packages(\"tidytext\")\n## install.packages(\"textdata\")\n \nlibrary(tidytuesdayR)\nlibrary(tidyverse)\nlibrary(waldo)\nlibrary(tidytext)\nlibrary(textdata)\n\nWe then had two ways of loading the data, in this case three datasets. As usual I’m switching to the url-based approach for the blog post\n\n# tuesdata &lt;- tidytuesdayR::tt_load('2024-02-27')\n# ## OR\n# tuesdata &lt;- tidytuesdayR::tt_load(2024, week = 9)\n \n# events &lt;- tuesdata$events\n# births &lt;- tuesdata$births\n# deaths &lt;- tuesdata$deaths\n \n# Option 2: Read directly from GitHub\n \nevents &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-27/events.csv')\nbirths &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-27/births.csv')\ndeaths &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-27/deaths.csv')\n\nWe noticed the births data include mention of at least one Pope. We wanted to explore more and less robust ways of finding popes in the births and deaths dataset\nWe could start by just looking for whether the word Pope is in the person field of births\n\nstr_detect(births$person, \"Pope\")\n\n  [1]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE\n\n\nWe then used a little expression to make the query not case sensitive:\n\ndeaths %&gt;% filter(str_detect(person, \"(?i)Pope\"))\n\n# A tibble: 1 × 4\n  year_death person        description year_birth\n       &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            &lt;dbl&gt;\n1        468 Pope Hilarius &lt;NA&gt;                NA\n\n\nAnother approach is to use ignore_case in the regex() function:\n\ndeaths %&gt;% filter(str_detect(person, regex(\"pope\", ignore_case = TRUE)))\n\n# A tibble: 1 × 4\n  year_death person        description year_birth\n       &lt;dbl&gt; &lt;chr&gt;         &lt;chr&gt;            &lt;dbl&gt;\n1        468 Pope Hilarius &lt;NA&gt;                NA\n\n\nThe only persons with pope in their name appear to be actual popes, not people who just happen to have the letters ‘pope’ in their surname.\nNext we looked at number of events by year. We used two tidyverse approaches to producing this, one using group_by and summarise, the other using count.\n\nnumber_events &lt;- events %&gt;% \n  group_by(year) %&gt;% \n  summarise(n= n())\nnumber_events\n\n# A tibble: 29 × 2\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1   888     1\n 2  1504     1\n 3  1644     1\n 4  1704     1\n 5  1712     1\n 6  1720     1\n 7  1768     1\n 8  1796     1\n 9  1892     1\n10  1908     1\n# ℹ 19 more rows\n\n\n\nnumber_events_2 &lt;- events %&gt;% \n  count(year)\nnumber_events_2\n\n# A tibble: 29 × 2\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1   888     1\n 2  1504     1\n 3  1644     1\n 4  1704     1\n 5  1712     1\n 6  1720     1\n 7  1768     1\n 8  1796     1\n 9  1892     1\n10  1908     1\n# ℹ 19 more rows\n\n\nWe then tried different comparator functions to see if they all agreed the contents were identical, with some mixed and confusing results:\n\nwaldo::compare(number_events, number_events_2)\n\n✔ No differences\n\n\nwaldo says they are the same.\n\nidentical(number_events, number_events_2)\n\n[1] FALSE\n\n\nidentical says they are not identical\n\nsetequal(number_events, number_events_2)\n\n[1] TRUE\n\n\nBut setequal doesn’t find differences\n\nall.equal(number_events, number_events_2)\n\n[1] \"Attributes: &lt; Names: 1 string mismatch &gt;\"                                              \n[2] \"Attributes: &lt; Length mismatch: comparison on first 2 components &gt;\"                     \n[3] \"Attributes: &lt; Component \\\"class\\\": Lengths (3, 4) differ (string compare on first 3) &gt;\"\n[4] \"Attributes: &lt; Component \\\"class\\\": 3 string mismatches &gt;\"                              \n[5] \"Attributes: &lt; Component 2: Modes: numeric, externalptr &gt;\"                              \n[6] \"Attributes: &lt; Component 2: Lengths: 29, 1 &gt;\"                                           \n[7] \"Attributes: &lt; Component 2: target is numeric, current is externalptr &gt;\"                \n\n\nAll equal reports a number of differences, related to the attributes (metadata) between the two objects being compared.\nCuriouser and Curiouser…\nNow let’s plot the number of events over time\n\nnumber_events %&gt;% \n  ggplot(aes(x = year, y = n))+\n  geom_col()\n\n\n\n\nWe wanted to know if there was anyone who was both recorded as being born and dying in a leap year:\n\nperson_bd &lt;- births %&gt;% \n  inner_join(deaths, by = \"person\")\n\nperson_bd\n\n# A tibble: 1 × 7\n  year_birth.x person      description.x year_death.x year_death.y description.y\n         &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;                &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;        \n1         1812 James Miln… Scottish-Aus…         1880         1880 Scottish-Aus…\n# ℹ 1 more variable: year_birth.y &lt;dbl&gt;\n\n\nOne person (born in Scotland!)\nWe then looked text analysis, and in particular sentiment analysis of the content of the descriptio field:\n\nbirths %&gt;% \n  unnest_tokens(word, description) %&gt;% \n  anti_join(get_stopwords()) %&gt;% \n  left_join(get_sentiments(\"afinn\"))\n\n# A tibble: 432 × 5\n   year_birth person        year_death word       value\n        &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1       1468 Pope Paul III       1549 &lt;NA&gt;          NA\n 2       1528 Albert V            1579 duke          NA\n 3       1528 Albert V            1579 bavaria       NA\n 4       1528 Domingo Báñez       1604 spanish       NA\n 5       1528 Domingo Báñez       1604 theologian    NA\n 6       1572 Edward Cecil        1638 1st           NA\n 7       1572 Edward Cecil        1638 viscount      NA\n 8       1572 Edward Cecil        1638 wimbledon     NA\n 9       1576 Antonio Neri        1614 florentine    NA\n10       1576 Antonio Neri        1614 priest        NA\n# ℹ 422 more rows\n\n\n\nevents %&gt;% \n  unnest_tokens(word, event) %&gt;% \n  anti_join(get_stopwords()) %&gt;% \n  left_join(get_sentiments(\"afinn\"))\n\n# A tibble: 418 × 3\n    year word       value\n   &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;\n 1   888 odo           NA\n 2   888 count         NA\n 3   888 paris         NA\n 4   888 crowned       NA\n 5   888 king          NA\n 6   888 west          NA\n 7   888 francia       NA\n 8   888 france        NA\n 9   888 archbishop    NA\n10   888 walter        NA\n# ℹ 408 more rows\n\n\nHere’s the words in the afinn object with the highest (most positive) sentiment\n\nget_sentiments(\"afinn\") %&gt;% \n  arrange(desc(value)) \n\n# A tibble: 2,477 × 2\n   word         value\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 breathtaking     5\n 2 hurrah           5\n 3 outstanding      5\n 4 superb           5\n 5 thrilled         5\n 6 amazing          4\n 7 awesome          4\n 8 brilliant        4\n 9 ecstatic         4\n10 euphoric         4\n# ℹ 2,467 more rows\n\n\nAnd here’s an exploration of average sentiment by (leap)year based on the events description field:\n\nevents |&gt;\n  unnest_tokens(word, event) |&gt;\n  anti_join(get_stopwords()) |&gt;\n  right_join(get_sentiments(\"afinn\")) |&gt;\n  group_by(year) |&gt;\n  summarise(mean_sentiment = mean(value)) |&gt;\n  ggplot(aes(x = year, y = mean_sentiment)) +\n  geom_point() +\n  geom_smooth()"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-leapyears/index.html#antonys-script",
    "href": "posts/tardy-tuesday/tidy-tuesday-leapyears/index.html#antonys-script",
    "title": "Tidy Tuesday 27 Feb 2024: Leap Years",
    "section": "Antony’s script",
    "text": "Antony’s script\nLoad libraries\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(countrycode)\n\nsome extra data sets re nationalities\n\ndemonym &lt;- readr::read_csv(\"https://raw.githubusercontent.com/knowitall/chunkedextractor/master/src/main/resources/edu/knowitall/chunkedextractor/demonyms.csv\",\n                           col_names = c(\"demonym\",\"geography\"))\n\ndemonym$demonym &lt;- tolower(demonym$demonym)\ndemonym$geography &lt;- tolower(demonym$geography)\ncountry &lt;- tibble(country=countrycode::codelist$country.name.en)\n\nLoad data\n\n# tuesdata &lt;- tidytuesdayR::tt_load('2024-02-27')\n\n# list2env(tuesdata,.GlobalEnv)\n\nglimpse(events)\n\nRows: 37\nColumns: 2\n$ year  &lt;dbl&gt; 888, 1504, 1644, 1704, 1712, 1720, 1768, 1796, 1892, 1908, 1912,…\n$ event &lt;chr&gt; \"Odo, count of Paris, is crowned king of West Francia (France) b…\n\nglimpse(births)\n\nRows: 121\nColumns: 4\n$ year_birth  &lt;dbl&gt; 1468, 1528, 1528, 1572, 1576, 1640, 1692, 1724, 1736, 1792…\n$ person      &lt;chr&gt; \"Pope Paul III\", \"Albert V\", \"Domingo Báñez\", \"Edward Ceci…\n$ description &lt;chr&gt; NA, \"Duke of Bavaria\", \"Spanish theologian\", \"1st Viscount…\n$ year_death  &lt;dbl&gt; 1549, 1579, 1604, 1638, 1614, 1704, 1763, 1822, 1784, 1868…\n\nglimpse(deaths)\n\nRows: 62\nColumns: 4\n$ year_death  &lt;dbl&gt; 468, 992, 1460, 1528, 1592, 1600, 1604, 1712, 1744, 1792, …\n$ person      &lt;chr&gt; \"Pope Hilarius\", \"Oswald of Worcester\", \"Albert III\", \"Pat…\n$ description &lt;chr&gt; NA, \"Anglo-Saxon archbishop and saint\", \"Duke of Bavaria-M…\n$ year_birth  &lt;dbl&gt; NA, 925, 1401, 1504, 1536, 1529, 1530, 1653, 1683, 1728, 1…\n\n\nWhich cohort of leap day births is most represented in Wikipedia’s data?\nAre any years surprisingly underrepresented compared to nearby years?\nWhat other patterns can you find in the data?\nhow many popes?\n\nbirths %&gt;% \n  mutate(is_pope = grepl(\"pope\",tolower(paste(person,description)))) %&gt;% \n  count(is_pope)\n\n# A tibble: 2 × 2\n  is_pope     n\n  &lt;lgl&gt;   &lt;int&gt;\n1 FALSE     120\n2 TRUE        1\n\n\ncount births by century —-\n\ngetCenturyCorrected &lt;- function(year) {\n  if (year %% 100 == 0) {\n    century &lt;- year / 100\n  } else {\n    century &lt;- ceiling(year / 100)\n  }\n  return(century)\n}\n\ngetCenturyCorrected(1900)\n\n[1] 19\n\ngetCenturyCorrected(1901)\n\n[1] 20\n\n\n\nbirths %&gt;% \n  mutate(century=sapply(year_birth,getCenturyCorrected)) %&gt;% \n  count(century)\n\n# A tibble: 7 × 2\n  century     n\n    &lt;dbl&gt; &lt;int&gt;\n1      15     1\n2      16     4\n3      17     2\n4      18     3\n5      19    11\n6      20    99\n7      21     1\n\n\ndo count() and summarise(n=n()) give identical dataframes? Not always —-\n\nx &lt;- births %&gt;% count(year_birth)\ny &lt;- births %&gt;% group_by(year_birth) %&gt;% summarise(n=n())\n\nidentical(attributes(x), attributes(y))\n\n[1] FALSE\n\nnames(x)==names(y)\n\n[1] TRUE TRUE\n\nidentical(\n  x,\n  y\n)\n\n[1] FALSE\n\n\na rough stab (clearly flawed) at parsing nationality —-\n\nbirths_nationality &lt;-\n  bind_rows(\n    births %&gt;%\n      tidytext::unnest_tokens(word, description) %&gt;%\n      anti_join(tidytext::get_stopwords(), \"word\") %&gt;%\n      left_join(\n        demonym,\n        by = c(word = \"geography\"),\n        relationship = \"many-to-many\"\n      ) %&gt;%\n      left_join(demonym, by = \"demonym\"),\n    \n    births %&gt;%\n      tidytext::unnest_tokens(word, description) %&gt;%\n      anti_join(tidytext::get_stopwords(), \"word\") %&gt;%\n      left_join(demonym, c(word = \"demonym\")) %&gt;%\n      left_join(demonym, \"geography\", relationship = \"many-to-many\")\n  )\n\nbirths_nationality %&gt;% count(geography) %&gt;% arrange(-n)\n\n# A tibble: 35 × 2\n   geography         n\n   &lt;chr&gt;         &lt;int&gt;\n 1 &lt;NA&gt;            732\n 2 united states   216\n 3 australia        40\n 4 england          39\n 5 canada           24\n 6 zealand          12\n 7 spain            10\n 8 wales             8\n 9 france            6\n10 turkey            6\n# ℹ 25 more rows\n\n\nNow a pretty wordcloud\n\nevents %&gt;% \n  unnest_tokens(word, event) %&gt;% \n  anti_join(get_stopwords(),\"word\") %&gt;% \n  count(word) %&gt;% \n  {wordcloud::wordcloud(words = .$word, \n            freq = .$n, min.freq = 1,\n            max.words = 20, random.order = FALSE, rot.per = 0.35, \n            colors = RColorBrewer::brewer.pal(8, \"Dark2\"))}\n\n\n\n\nA neutral word should have a sentiment score of 0, not NA. Let’s make that change…\n\nafinn_sentiments &lt;- get_sentiments('afinn')\n\nevents %&gt;% \n  unnest_tokens(word, event) %&gt;% \n  anti_join(get_stopwords(),\"word\") %&gt;% \n  left_join(afinn_sentiments,\"word\") %&gt;% \n  # filter(!is.na(value)) %&gt;% \n  replace_na(list(value=0)) %&gt;% \n  mutate(century = sapply(year,getCenturyCorrected)) %&gt;% \n  group_by(century) %&gt;% \n  summarise(mean_sentiment = mean(value)) %&gt;% \n  ggplot(aes(x=century,y=mean_sentiment))+geom_line()"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-leapyears/index.html#footnotes",
    "href": "posts/tardy-tuesday/tidy-tuesday-leapyears/index.html#footnotes",
    "title": "Tidy Tuesday 27 Feb 2024: Leap Years",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI don’t think even regex can help us with this one.↩︎"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-roaming-us-holidays/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-roaming-us-holidays/index.html",
    "title": "Tidy Tuesday: Roaming US Holidays",
    "section": "",
    "text": "For this Tardy Tuesday session we looked at the following Tidy Tuesday data challenge, which involved identifying the date when various public holidays in the USA (a rare thing) are expected to occur each year.\nBrendan led/‘scribed’ the session"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-roaming-us-holidays/index.html#analysis",
    "href": "posts/tardy-tuesday/tidy-tuesday-roaming-us-holidays/index.html#analysis",
    "title": "Tidy Tuesday: Roaming US Holidays",
    "section": "Analysis",
    "text": "Analysis\nWe used the tidytuesdayR package to load the data, then pushed these to the global environment using list2env.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ntidytuesdayR::tt_load('2024-06-18') |&gt;\n  list2env(envir = .GlobalEnv)\n\n--- Compiling #TidyTuesday Information for 2024-06-18 ----\n--- There are 2 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `federal_holidays.csv`\n    Downloading file 2 of 2: `proposed_federal_holidays.csv`\n\n\n--- Download complete ---\n\n\n&lt;environment: R_GlobalEnv&gt;\n\n\nOur main dataset looked as follows:\n\nfederal_holidays\n\n# A tibble: 11 × 6\n   date  date_definition official_name year_established date_established details\n   &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;                    &lt;dbl&gt; &lt;date&gt;           &lt;chr&gt;  \n 1 Janu… fixed date      New Year's D…             1870 1870-06-28       \"Celeb…\n 2 Janu… 3rd monday      Birthday of …             1983 1983-11-02       \"Honor…\n 3 Febr… 3rd monday      Washington's…             1879 NA               \"Honor…\n 4 May … last monday     Memorial Day              1868 NA               \"Honor…\n 5 June… fixed date      Juneteenth N…             2021 2021-06-17       \"Comme…\n 6 July… fixed date      Independence…             1870 NA               \"Celeb…\n 7 Sept… 1st monday      Labor Day                 1894 NA               \"Honor…\n 8 Octo… 2nd monday      Columbus Day              1968 NA               \"Honor…\n 9 Nove… fixed date      Veterans Day              1938 NA               \"Honor…\n10 Nove… 4th thursday    Thanksgiving…             1941 NA               \"Tradi…\n11 Dece… fixed date      Christmas Day             1870 NA               \"The m…\n\n\nWe were interested in those ‘roaming holidays’ where the date column contains a range of dates, and the date definition contains information on the criterion used to determine the specific date for a given year.\nWe decided to try to solve the problem manually for MLK day, which should be the third monday in January.\n\n# find 3rd monday of january 202x\nwday(\"2024-06-17\")\n\n[1] 2\n\ndate_range &lt;- \"January 15–21\"\nyear &lt;- 2024\n\n# find monday (2) in date range\n\nstart_date &lt;- \"January 15 2024\"\nend_date &lt;- \"January 21 2024\"\nmdy(start_date)\n\n[1] \"2024-01-15\"\n\nstr_split(date_range, \"–\")\n\n[[1]]\n[1] \"January 15\" \"21\"        \n\nmonth &lt;- str_extract(date_range, \"[a-zA-Z]+\") \n\ndates &lt;- str_extract_all(date_range, \"[0-9]+\") |&gt;\n  unlist()\n\nstart_end &lt;- ymd(paste(year, month, dates))\n\n# ymd(\"2024-01-16\") %within% interval(start_end[1], start_end[2])\n\ndates_between &lt;- seq(from = start_end[1], to = start_end[2], by = 1)\n\ndates_between[wday(dates_between) == 2]\n\n[1] \"2024-01-15\"\n\n\nThen, we generalised this slightly by producing a function that finds the date of MLK day for different given years:\n\nmlk_day &lt;- function(year){\n  date_range &lt;- \"January 15–21\"\n  \n  month &lt;- stringr::str_extract(date_range, \"[a-zA-Z]+\") \n  \n  dates &lt;- stringr::str_extract_all(date_range, \"[0-9]+\") |&gt;\n    unlist()\n  \n  start_end &lt;- lubridate::ymd(paste(year, month, dates))\n  \n  dates_between &lt;- seq(from = start_end[1], to = start_end[2], by = 1)\n  \n  dates_between[wday(dates_between) == 2] # update day for general\n}\n\nmlk_day(2025)\n\n[1] \"2025-01-20\"\n\nmap_vec(1983:2025, mlk_day)\n\n [1] \"1983-01-17\" \"1984-01-16\" \"1985-01-21\" \"1986-01-20\" \"1987-01-19\"\n [6] \"1988-01-18\" \"1989-01-16\" \"1990-01-15\" \"1991-01-21\" \"1992-01-20\"\n[11] \"1993-01-18\" \"1994-01-17\" \"1995-01-16\" \"1996-01-15\" \"1997-01-20\"\n[16] \"1998-01-19\" \"1999-01-18\" \"2000-01-17\" \"2001-01-15\" \"2002-01-21\"\n[21] \"2003-01-20\" \"2004-01-19\" \"2005-01-17\" \"2006-01-16\" \"2007-01-15\"\n[26] \"2008-01-21\" \"2009-01-19\" \"2010-01-18\" \"2011-01-17\" \"2012-01-16\"\n[31] \"2013-01-21\" \"2014-01-20\" \"2015-01-19\" \"2016-01-18\" \"2017-01-16\"\n[36] \"2018-01-15\" \"2019-01-21\" \"2020-01-20\" \"2021-01-18\" \"2022-01-17\"\n[41] \"2023-01-16\" \"2024-01-15\" \"2025-01-20\"\n\n\nFinally, we generalised this further with a function to find the date of roaming holidays for many different types of holiday and years:\n\nany_day &lt;- function(year, date, date_definition){\n  \n  day &lt;- stringr::str_extract(date_definition, \" .+$\") |&gt;\n    str_trim() \n  \n  weekdays &lt;- paste0(c(\"sun\", \"mon\", \"tues\", \"wednes\", \"thurs\", \"fri\", \"satur\"), \"day\")\n  \n  day_no &lt;- which(weekdays == day)\n  \n  month &lt;- stringr::str_extract(date, \"[a-zA-Z]+\") \n  \n  dates &lt;- stringr::str_extract_all(date, \"[0-9]+\") |&gt;\n    unlist()\n  \n  start_end &lt;- lubridate::ymd(paste(year, month, dates))\n  \n  dates_between &lt;- seq(from = start_end[1], to = start_end[2], by = 1)\n  \n  dates_between[wday(dates_between) == day_no] # update day for general\n}\n\nany_day(2024, \"January 15–21\", \"3rd monday\")\n\n[1] \"2024-01-15\"\n\nmovers &lt;- federal_holidays |&gt;\n  filter(date_definition != \"fixed date\") |&gt;\n  expand_grid(year = 1983:2024) \n\nmovers &lt;- movers |&gt;\n  bind_cols(actual_date = pmap_vec(list(year = movers$year, date = movers$date, date_definition = movers$date_definition), any_day))\n\nmovers\n\n# A tibble: 252 × 8\n   date  date_definition official_name year_established date_established details\n   &lt;chr&gt; &lt;chr&gt;           &lt;chr&gt;                    &lt;dbl&gt; &lt;date&gt;           &lt;chr&gt;  \n 1 Janu… 3rd monday      Birthday of …             1983 1983-11-02       Honors…\n 2 Janu… 3rd monday      Birthday of …             1983 1983-11-02       Honors…\n 3 Janu… 3rd monday      Birthday of …             1983 1983-11-02       Honors…\n 4 Janu… 3rd monday      Birthday of …             1983 1983-11-02       Honors…\n 5 Janu… 3rd monday      Birthday of …             1983 1983-11-02       Honors…\n 6 Janu… 3rd monday      Birthday of …             1983 1983-11-02       Honors…\n 7 Janu… 3rd monday      Birthday of …             1983 1983-11-02       Honors…\n 8 Janu… 3rd monday      Birthday of …             1983 1983-11-02       Honors…\n 9 Janu… 3rd monday      Birthday of …             1983 1983-11-02       Honors…\n10 Janu… 3rd monday      Birthday of …             1983 1983-11-02       Honors…\n# ℹ 242 more rows\n# ℹ 2 more variables: year &lt;int&gt;, actual_date &lt;date&gt;"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-eclipse/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-eclipse/index.html",
    "title": "Tidy Tuesday: Solar Eclipses",
    "section": "",
    "text": "The most recent TidyTuesday session looked at data about solar eclipses in the USA, and was led by Myriam. The repo readme is here"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-eclipse/index.html#loading-the-data",
    "href": "posts/tardy-tuesday/tidy-tuesday-eclipse/index.html#loading-the-data",
    "title": "Tidy Tuesday: Solar Eclipses",
    "section": "Loading the data",
    "text": "Loading the data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Let's use the tidytuesdayR load package\n\nall_data &lt;- tidytuesdayR::tt_load('2024-04-09')\n\n--- Compiling #TidyTuesday Information for 2024-04-09 ----\n--- There are 4 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 4: `eclipse_annular_2023.csv`\n    Downloading file 2 of 4: `eclipse_total_2024.csv`\n    Downloading file 3 of 4: `eclipse_partial_2023.csv`\n    Downloading file 4 of 4: `eclipse_partial_2024.csv`\n\n\n--- Download complete ---"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-eclipse/index.html#tidying-the-data",
    "href": "posts/tardy-tuesday/tidy-tuesday-eclipse/index.html#tidying-the-data",
    "title": "Tidy Tuesday: Solar Eclipses",
    "section": "Tidying the data",
    "text": "Tidying the data\nThe data are a list of dataframes. Each dataframe has a similar data structure. We decided to spend some time tidying these dataframes, then combining them again into a single dataframe with additional attributes\n\neclipse_annular_2023 &lt;- all_data$eclipse_annular_2023 |&gt;\n    mutate(year = 2023, type = \"annular\") |&gt;\n    pivot_longer(contains(\"eclipse\"), names_to = \"event_number\", values_to = \"event_datetime\")\neclipse_total_2024 &lt;- all_data$eclipse_total_2024 |&gt;\n    mutate(year = 2024, type = \"total\") |&gt;\n    pivot_longer(contains(\"eclipse\"), names_to = \"event_number\", values_to = \"event_datetime\")\neclipse_partial_2023 &lt;- all_data$eclipse_partial_2023 |&gt;\n    mutate(year = 2023, type = \"partial\") |&gt;\n    pivot_longer(contains(\"eclipse\"), names_to = \"event_number\", values_to = \"event_datetime\")\neclipse_partial_2024 &lt;- all_data$eclipse_partial_2024 |&gt;\n    mutate(year = 2024, type = \"partial\") |&gt;\n    pivot_longer(contains(\"eclipse\"), names_to = \"event_number\", values_to = \"event_datetime\")\n\ndata_tidied &lt;- bind_rows(\n    list(eclipse_annular_2023, eclipse_partial_2023, eclipse_total_2024, eclipse_partial_2024)\n) |&gt;\n    mutate(event_number = str_remove(event_number, \"eclipse_\") %&gt;% as.numeric())\n\ndata_tidied\n\n# A tibble: 325,881 × 8\n   state name           lat   lon  year type    event_number event_datetime\n   &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;time&gt;        \n 1 AZ    Chilchinbito  36.5 -110.  2023 annular            1 15:10:50      \n 2 AZ    Chilchinbito  36.5 -110.  2023 annular            2 15:56:20      \n 3 AZ    Chilchinbito  36.5 -110.  2023 annular            3 16:30:29      \n 4 AZ    Chilchinbito  36.5 -110.  2023 annular            4 16:33:31      \n 5 AZ    Chilchinbito  36.5 -110.  2023 annular            5 17:09:40      \n 6 AZ    Chilchinbito  36.5 -110.  2023 annular            6 18:02:10      \n 7 AZ    Chinle        36.2 -110.  2023 annular            1 15:11:10      \n 8 AZ    Chinle        36.2 -110.  2023 annular            2 15:56:50      \n 9 AZ    Chinle        36.2 -110.  2023 annular            3 16:31:21      \n10 AZ    Chinle        36.2 -110.  2023 annular            4 16:34:06      \n# ℹ 325,871 more rows"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-eclipse/index.html#graphing-the-data",
    "href": "posts/tardy-tuesday/tidy-tuesday-eclipse/index.html#graphing-the-data",
    "title": "Tidy Tuesday: Solar Eclipses",
    "section": "Graphing the data",
    "text": "Graphing the data\nAs we do not expect cities/towns to move between years, we thought if we plotted the lon and lat as points we will get an impression of the USA\n\ndata_tidied |&gt; \n    ggplot(aes(lon, lat)) + \n    geom_point()\n\n\n\n\nIndeed we do! Though we thought it might be more straightforward to focus on the main US territory\n\ndata_tidied |&gt;\n    filter(\n        between(lon, -150, -50),\n        between(lat, 22, 50)\n    ) |&gt;\n    ggplot(aes(lon, lat)) + \n    geom_point()\n\n\n\n\nWe now have an indirect map/signal of population density in the USA!"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-eclipse/index.html#eclipse-type-in-2024",
    "href": "posts/tardy-tuesday/tidy-tuesday-eclipse/index.html#eclipse-type-in-2024",
    "title": "Tidy Tuesday: Solar Eclipses",
    "section": "Eclipse type in 2024",
    "text": "Eclipse type in 2024\nWe explored the four different datasets using filtering. For 2024 the types were total and partial. They look as follows:\n\ndata_tidied |&gt;\n    filter(\n        between(lon, -150, -50),\n        between(lat, 22, 50)\n    ) |&gt;\n    filter(year == 2024) |&gt;\n    filter(event_number == 1) |&gt;\n    ggplot(aes(lon, lat)) + \n    geom_point() + \n    facet_wrap(~type)\n\n\n\n\nWe realised total is a swathe of locations cut through the rest of the USA. We therefore thought it might be good to show the points coloured by whether they are flagged as total or partial in eclipse type\n\ndata_tidied |&gt;\n    filter(\n        between(lon, -150, -50),\n        between(lat, 22, 50)\n    ) |&gt;\n    filter(year == 2024) |&gt;\n    filter(event_number == 1) |&gt;\n    mutate(is_total = type == \"total\") |&gt;\n    ggplot(aes(lon, lat)) + \n    geom_point(aes(colour = is_total))\n\n\n\n\nAnd that’s where we got to. We recombined two datasets to show which parts of the USA were in the path of the total eclipse. (Nick mentioned that he’d seen data suggesting AirBnB prices were especially high for properties in this swathe!)"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-eclipse/index.html#going-further",
    "href": "posts/tardy-tuesday/tidy-tuesday-eclipse/index.html#going-further",
    "title": "Tidy Tuesday: Solar Eclipses",
    "section": "Going further",
    "text": "Going further\nWe could have looked at doing something similar with the annular and partial data for 2023:\n\ndata_tidied |&gt;\n    filter(\n        between(lon, -150, -50),\n        between(lat, 22, 50)\n    ) |&gt;\n    filter(year == 2023) |&gt;\n    filter(event_number == 1) |&gt;\n    mutate(is_annular = type == \"annular\") |&gt;\n    ggplot(aes(lon, lat)) + \n    geom_point(aes(colour = is_annular))\n\n\n\n\nThis shows how the swathe the 2023 eclipse epicentre cut through the USA was different to the 2024 eclipse path.\nWe could also have made use of the datetime column to show how the eclipse happened at different times in different parts of the USA:\n\ndata_tidied |&gt;\n    filter(\n        between(lon, -150, -50),\n        between(lat, 22, 50)\n    ) |&gt;\n    filter(year == 2024) |&gt;\n    filter(event_number == 1) |&gt;\n    mutate(is_total = type == \"total\") |&gt;\n    mutate(start_time = min(event_datetime)) |&gt;\n    mutate(time_since_start = event_datetime - start_time) |&gt;\n    ggplot(aes(lon, lat)) + \n    geom_point(aes(colour = time_since_start, alpha = is_total)) + \n    scale_alpha_manual(values = c(`FALSE` = 0.01, `TRUE` = 1))\n\nDon't know how to automatically pick scale for object of type &lt;difftime&gt;.\nDefaulting to continuous.\n\n\n\n\n\nWe can see from this that the event seemed to start on the west coast and move east.\nFinally, we could have looked at adding a basemap.\nI tried following this tutorial to get a basemap using ggmap. Unfortunately, ggmap now requires registering API keys (and credit card details) with Google. So this exercise is as yet incomplete!"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-double/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-double/index.html",
    "title": "Double Tardy Tuesday",
    "section": "",
    "text": "I’ve been exceptionally tardy updating the Tardy Tuesday blog the last couple of weeks. So there are a couple of weeks worth of code to update with at once. Unlike previous times I’m going to list only myself as the author of this blog, as none of the contributors deserve any blame for my tardiness!\nAdditionally, the scripts will be presented more ‘as-is’ than on previous occasions, without as much additional discussion or amendments."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-double/index.html#introduction",
    "href": "posts/tardy-tuesday/tidy-tuesday-double/index.html#introduction",
    "title": "Double Tardy Tuesday",
    "section": "",
    "text": "I’ve been exceptionally tardy updating the Tardy Tuesday blog the last couple of weeks. So there are a couple of weeks worth of code to update with at once. Unlike previous times I’m going to list only myself as the author of this blog, as none of the contributors deserve any blame for my tardiness!\nAdditionally, the scripts will be presented more ‘as-is’ than on previous occasions, without as much additional discussion or amendments."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-double/index.html#common-package-dependencies",
    "href": "posts/tardy-tuesday/tidy-tuesday-double/index.html#common-package-dependencies",
    "title": "Double Tardy Tuesday",
    "section": "Common package dependencies",
    "text": "Common package dependencies\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-double/index.html#mutant-moneyball",
    "href": "posts/tardy-tuesday/tidy-tuesday-double/index.html#mutant-moneyball",
    "title": "Double Tardy Tuesday",
    "section": "Mutant Moneyball",
    "text": "Mutant Moneyball\nThis TidyTuesday dataset involved understanding the relationship between the value of old Mavel comics and the appearance of particular characters in those comics. I’m not sure it was a good or bad thing that we didn’t know the names of most of the characters…\nThe session was led by Brendan, who wrote (with our support) the script below:\n\ntidytuesdayR::tt_load('2024-03-19') \n\n--- Compiling #TidyTuesday Information for 2024-03-19 ----\n\n\n--- There is 1 file available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 1: `mutant_moneyball.csv`\n\n\n--- Download complete ---\n\n\nAvailable datasets:\n    mutant_moneyball \n    \n\nmutant_moneyball &lt;- tidytuesdayR::tt_load('2024-03-19') |&gt;\n  pluck(1)\n\n--- Compiling #TidyTuesday Information for 2024-03-19 ----\n\n\n--- There is 1 file available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 1: `mutant_moneyball.csv`\n\n\n--- Download complete ---\n\nstr(mutant_moneyball)\n\nspc_tbl_ [26 × 45] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Member                : chr [1:26] \"warrenWorthington\" \"hankMcCoy\" \"scottSummers\" \"bobbyDrake\" ...\n $ TotalIssues           : num [1:26] 139 119 197 123 164 68 48 190 120 167 ...\n $ TotalIssues60s        : num [1:26] 61 62 63 62 63 8 9 0 0 0 ...\n $ TotalIssues70s        : num [1:26] 35 38 69 35 58 13 13 36 36 36 ...\n $ TotalIssues80s        : num [1:26] 20 9 56 6 14 43 19 121 84 115 ...\n $ TotalIssues90s        : num [1:26] 23 10 9 20 29 4 7 33 0 16 ...\n $ totalIssueCheck       : num [1:26] 139 119 197 123 164 68 48 190 120 167 ...\n $ TotalValue_heritage   : num [1:26] 1108558 957993 1152230 1086749 1132091 ...\n $ TotalValue60s_heritage: num [1:26] 929056 929776 933616 929776 933616 ...\n $ TotalValue70s_heritage: num [1:26] 154585 20705 188635 154585 179899 ...\n $ TotalValue80s_heritage: num [1:26] 23957 6631 29240 1514 16868 ...\n $ TotalValue90s_heritage: num [1:26] 960 881 739 874 1708 ...\n $ TotalValue_ebay       : num [1:26] 27377 24972 29964 26902 29158 ...\n $ TotalValue60s_ebay    : num [1:26] 23335 23377 23420 23377 23420 ...\n $ TotalValue70s_ebay    : num [1:26] 3362 1224 5431 3362 4903 ...\n $ TotalValue80s_ebay    : num [1:26] 583 289 1031 70 665 ...\n $ TotalValue90s_ebay    : num [1:26] 97 82 82 93 170 21 41 334 0 100 ...\n $ 60s_Appearance_Percent: chr [1:26] \"96.83%\" \"98.41%\" \"100.00%\" \"98.41%\" ...\n $ 70s_Appearance_Percent: chr [1:26] \"50.72%\" \"55.07%\" \"100.00%\" \"50.72%\" ...\n $ 80s_Appearance_Percent: chr [1:26] \"14.60%\" \"6.57%\" \"40.88%\" \"4.38%\" ...\n $ 90s_Appearance_Percent: chr [1:26] \"51.11%\" \"22.22%\" \"20.00%\" \"44.44%\" ...\n $ PPI60s_heritage       : chr [1:26] \"$15,230.43 \" \"$14,996.39 \" \"$14,819.30 \" \"$14,996.39 \" ...\n $ PPI70s_heritage       : chr [1:26] \"$4,416.71 \" \"$544.87 \" \"$2,733.84 \" \"$4,416.71 \" ...\n $ PPI80s_heritage       : chr [1:26] \"$1,197.85 \" \"$736.78 \" \"$522.14 \" \"$252.33 \" ...\n $ PPI90s_heritage       : chr [1:26] \"$41.74 \" \"$88.10 \" \"$82.11 \" \"$43.70 \" ...\n $ PPI60s_ebay           : chr [1:26] \"$382.54 \" \"$377.05 \" \"$371.75 \" \"$377.05 \" ...\n $ PPI70s_ebay           : chr [1:26] \"$96.06 \" \"$32.21 \" \"$78.71 \" \"$96.06 \" ...\n $ PPI80s_ebay           : chr [1:26] \"$29.15 \" \"$32.11 \" \"$18.41 \" \"$11.67 \" ...\n $ PPI90s_ebay           : chr [1:26] \"$4.22 \" \"$8.20 \" \"$9.11 \" \"$4.65 \" ...\n $ TotalValue60s_wiz     : chr [1:26] \"$7,913.00 \" \"$7,953.00 \" \"$7,993.00 \" \"$7,953.00 \" ...\n $ TotalValue70s_wiz     : chr [1:26] \"$1,105.00 \" \"$851.00 \" \"$1,979.00 \" \"$1,105.00 \" ...\n $ TotalValue80s_wiz     : chr [1:26] \"$226.00 \" \"$89.00 \" \"$438.00 \" \"$48.00 \" ...\n $ TotalValue90s_wiz     : chr [1:26] \"$65.75 \" \"$38.50 \" \"$39.25 \" \"$62.00 \" ...\n $ TotalValue60s_oStreet : chr [1:26] \"$68,160.00 \" \"$68,390.00 \" \"$68,590.00 \" \"$68,390.00 \" ...\n $ TotalValue70s_oStreet : chr [1:26] \"$7,360.00 \" \"$5,260.00 \" \"$11,675.00 \" \"$7,360.00 \" ...\n $ TotalValue80s_oStreet : chr [1:26] \"$975.00 \" \"$431.00 \" \"$1,427.00 \" \"$137.00 \" ...\n $ TotalValue90s_oStreet : chr [1:26] \"$123.00 \" \"$81.00 \" \"$74.00 \" \"$108.00 \" ...\n $ PPI60s_wiz            : chr [1:26] \"$129.72 \" \"$128.27 \" \"$126.87 \" \"$128.27 \" ...\n $ PPI70s_wiz            : chr [1:26] \"$31.57 \" \"$22.39 \" \"$28.68 \" \"$31.57 \" ...\n $ PPI80s_wiz            : chr [1:26] \"$11.30 \" \"$9.89 \" \"$7.82 \" \"$8.00 \" ...\n $ PPI90s_wiz            : chr [1:26] \"$2.86 \" \"$3.85 \" \"$4.36 \" \"$3.10 \" ...\n $ PPI60s_oStreet        : chr [1:26] \"$1,117.38 \" \"$1,103.06 \" \"$1,088.73 \" \"$1,103.06 \" ...\n $ PPI70s_oStreet        : chr [1:26] \"$210.29 \" \"$138.42 \" \"$169.20 \" \"$210.29 \" ...\n $ PPI80s_oStreet        : chr [1:26] \"$48.75 \" \"$47.89 \" \"$25.48 \" \"$22.83 \" ...\n $ PPI90s_oStreet        : chr [1:26] \"$5.35 \" \"$8.10 \" \"$8.22 \" \"$5.40 \" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Member = col_character(),\n  ..   TotalIssues = col_double(),\n  ..   TotalIssues60s = col_double(),\n  ..   TotalIssues70s = col_double(),\n  ..   TotalIssues80s = col_double(),\n  ..   TotalIssues90s = col_double(),\n  ..   totalIssueCheck = col_double(),\n  ..   TotalValue_heritage = col_double(),\n  ..   TotalValue60s_heritage = col_double(),\n  ..   TotalValue70s_heritage = col_double(),\n  ..   TotalValue80s_heritage = col_double(),\n  ..   TotalValue90s_heritage = col_double(),\n  ..   TotalValue_ebay = col_double(),\n  ..   TotalValue60s_ebay = col_double(),\n  ..   TotalValue70s_ebay = col_double(),\n  ..   TotalValue80s_ebay = col_double(),\n  ..   TotalValue90s_ebay = col_double(),\n  ..   `60s_Appearance_Percent` = col_character(),\n  ..   `70s_Appearance_Percent` = col_character(),\n  ..   `80s_Appearance_Percent` = col_character(),\n  ..   `90s_Appearance_Percent` = col_character(),\n  ..   PPI60s_heritage = col_character(),\n  ..   PPI70s_heritage = col_character(),\n  ..   PPI80s_heritage = col_character(),\n  ..   PPI90s_heritage = col_character(),\n  ..   PPI60s_ebay = col_character(),\n  ..   PPI70s_ebay = col_character(),\n  ..   PPI80s_ebay = col_character(),\n  ..   PPI90s_ebay = col_character(),\n  ..   TotalValue60s_wiz = col_character(),\n  ..   TotalValue70s_wiz = col_character(),\n  ..   TotalValue80s_wiz = col_character(),\n  ..   TotalValue90s_wiz = col_character(),\n  ..   TotalValue60s_oStreet = col_character(),\n  ..   TotalValue70s_oStreet = col_character(),\n  ..   TotalValue80s_oStreet = col_character(),\n  ..   TotalValue90s_oStreet = col_character(),\n  ..   PPI60s_wiz = col_character(),\n  ..   PPI70s_wiz = col_character(),\n  ..   PPI80s_wiz = col_character(),\n  ..   PPI90s_wiz = col_character(),\n  ..   PPI60s_oStreet = col_character(),\n  ..   PPI70s_oStreet = col_character(),\n  ..   PPI80s_oStreet = col_character(),\n  ..   PPI90s_oStreet = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# filter by totalIssueCheck == TotalIssues for missing issues, then remove both\n\nmutant_moneyball |&gt;\n  filter(totalIssueCheck != TotalIssues) |&gt;\n  nrow()\n\n[1] 0\n\n# split TotalIssues and Member into issues data\n\nappearances &lt;- mutant_moneyball |&gt;\n  select(Member, contains(\"ssues\")) |&gt;\n  pivot_longer(!Member) |&gt;\n  mutate(first = str_to_title(str_extract(Member, \"^[[:lower:]]*\"))) |&gt;\n  mutate(last = str_remove(Member, \"^[[:lower:]]*\" )) |&gt;\n  mutate(Member = paste(first, last), .keep=\"unused\") \n\nappearances |&gt;\n  filter(name == \"TotalIssues\") |&gt;\n  arrange(desc(value))\n\n# A tibble: 26 × 3\n   Member             name        value\n   &lt;chr&gt;              &lt;chr&gt;       &lt;dbl&gt;\n 1 Scott Summers      TotalIssues   197\n 2 Ororo Munroe       TotalIssues   190\n 3 Peter Rasputin     TotalIssues   169\n 4 Charles Xavier     TotalIssues   169\n 5 Logan Howlett      TotalIssues   167\n 6 Jean Grey          TotalIssues   164\n 7 Warren Worthington TotalIssues   139\n 8 Bobby Drake        TotalIssues   123\n 9 Kurt Wagner        TotalIssues   120\n10 Hank McCoy         TotalIssues   119\n# ℹ 16 more rows\n\nbest_xm &lt;- appearances |&gt;\n  slice_max(value, n=10) |&gt;\n  pull(Member)\n\nappearances |&gt;\n  mutate(first = str_to_title(str_extract(Member, \"^[[:lower:]]*\"))) |&gt;\n  mutate(last = str_remove(Member, \"^[[:lower:]]*\" )) |&gt;\n  mutate(Member = paste(first, last), .keep=\"unused\") \n\n# A tibble: 130 × 3\n   Member                name           value\n   &lt;chr&gt;                 &lt;chr&gt;          &lt;dbl&gt;\n 1 \" Warren Worthington\" TotalIssues      139\n 2 \" Warren Worthington\" TotalIssues60s    61\n 3 \" Warren Worthington\" TotalIssues70s    35\n 4 \" Warren Worthington\" TotalIssues80s    20\n 5 \" Warren Worthington\" TotalIssues90s    23\n 6 \" Hank McCoy\"         TotalIssues      119\n 7 \" Hank McCoy\"         TotalIssues60s    62\n 8 \" Hank McCoy\"         TotalIssues70s    38\n 9 \" Hank McCoy\"         TotalIssues80s     9\n10 \" Hank McCoy\"         TotalIssues90s    10\n# ℹ 120 more rows\n\ndates &lt;- tribble(\n  ~start, ~end, ~decade,\n  1963, 1969, 60,\n  1970, 1979, 70,\n  1980, 1989, 80,\n  1990, 1992, 90\n)\n\nplot &lt;- appearances |&gt;\n  mutate(decade = as.numeric(str_extract(name, \"\\\\d{2}\"))) |&gt;\n  filter(!is.na(decade)) |&gt;\n  left_join(dates) |&gt;\n  mutate(year_range = 1 + end - start) |&gt;\n  mutate(rate = value / year_range) |&gt;\n  relocate(last_col()) |&gt;\n  filter(Member %in% best_xm) |&gt;\n  ggplot(aes(x = start, y = rate, colour = Member)) +\n  geom_line() +\n  geom_point() +\n  theme(legend.position = \"bottom\")\n\nJoining with `by = join_by(decade)`\n\nplotly::ggplotly(plot)\n\n\n\n\n\n\nRecollections\nFor this session we focused more on regex than perhaps we thought we would, with a particular focus on how to produce nicely readable names for different characters. We also focused on producing metrics like appearances per year, given that some of the time periods were full decades, but others were just of a few years within the decade."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-double/index.html#ncaa-mens-march-madness",
    "href": "posts/tardy-tuesday/tidy-tuesday-double/index.html#ncaa-mens-march-madness",
    "title": "Double Tardy Tuesday",
    "section": "NCAA Men’s March Madness",
    "text": "NCAA Men’s March Madness\nThis session was based around data from a basketball competition. It had information both on how well different teams performed against each other, and also how well various members of the public thought they would perform, meaning it can be used to assess how well expectations match with reality.\nThis particular session was led by Nic, who supplied the code below.\n\nif (is.na(utils::packageVersion(\"pacman\"))){\n  install.packages(\"pacman\")\n}\nlibrary(pacman)\np_load(tidytuesdayR)\n \n### Download last tuesday's data\n \ntt_data &lt;- tt_load('2024-03-26')\n\n--- Compiling #TidyTuesday Information for 2024-03-26 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `team-results.csv`\n    Downloading file 2 of 2: `public-picks.csv`\n\n\n--- Download complete ---\n\ntt_data\n\nAvailable datasets:\n    team-results \n    public-picks \n    \n\n### Assign the datasets to our global environment\n \nlist2env(tt_data, globalenv())\n\n&lt;environment: R_GlobalEnv&gt;\n\n# Load packages -----------------------------------------------------------\n \np_load(tidyverse, # The tidyverse\n       cheapr, # Cheap (fast & efficient) functions\n       cppdoubles, # Floating-point comparisons\n       timeplyr, # Date-Time manipulation\n       tidytext, # Text manipulation\n       phsmethods, phsstyles) # PHS functions\n \n \n# Exploratory -------------------------------------------------------------\n \n \noverview(`team-results`)\n\nobs: 236 \ncols: 20 \n\n----- Numeric -----\n          col   class n_missing p_complete n_unique   mean   p0   p25   p50\n1      TEAMID numeric         0          1      236 124.33    1 63.75 123.5\n2        PAKE numeric         0          1       75  -0.01 -6.7  -0.8  -0.2\n3    PAKERANK numeric         0          1       75 114.53    1    56   118\n4        PASE numeric         0          1       79      0 -8.5  -0.9 -0.15\n5    PASERANK numeric         0          1       79 114.78    1    59   111\n6       GAMES numeric         0          1       38   8.01    1     2     4\n7           W numeric         0          1       31      4    0     0     1\n8           L numeric         0          1       15      4    1     1     3\n9  WINPERCENT numeric         0          1       55   0.26    0     0  0.25\n10        R64 numeric         0          1       15   4.07    1     1     3\n11        R32 numeric         0          1       15   2.03    0     0     1\n12        S16 numeric         0          1       10   1.02    0     0     0\n13         E8 numeric         0          1        8   0.51    0     0     0\n14         F4 numeric         0          1        6   0.25    0     0     0\n15         F2 numeric         0          1        5   0.13    0     0     0\n16      CHAMP numeric         0          1        4   0.06    0     0     0\n17       TOP2 numeric         0          1       11   0.51    0     0     0\n      p75 p100   iqr    sd\n1  185.25  245 121.5 70.66\n2     0.5   12   1.3  2.21\n3     175  236   119 68.46\n4     0.5 10.6   1.4  2.35\n5     175  236   116 68.58\n6       9   53     7 10.26\n7       4   40     4  7.38\n8       6   15     5  3.31\n9     0.5  0.8   0.5  0.26\n10      6   15     5  3.44\n11      3   15     3  3.01\n12      1    9     1  2.06\n13      0    7     0  1.28\n14      0    5     0  0.78\n15      0    4     0  0.51\n16      0    3     0  0.35\n17      0   12     0  1.56\n\n----- Categorical -----\n           col     class n_missing p_complete n_unique n_levels\n1         TEAM character         0          1      236       NA\n2    F4PERCENT character         0          1      119       NA\n3 CHAMPPERCENT character         0          1       72       NA\n                min    max\n1 Abilene Christian   Yale\n2             0.00% 98.80%\n3             0.00%  9.10%\n\noverview(`public-picks`)\n\nobs: 64 \ncols: 9 \n\n----- Numeric -----\n     col   class n_missing p_complete n_unique    mean   p0     p25    p50\n1   YEAR numeric         0          1        1    2024 2024    2024   2024\n2 TEAMNO numeric         0          1       64 1045.39 1012 1028.75 1045.5\n      p75 p100  iqr    sd\n1    2024 2024    0     0\n2 1062.25 1079 33.5 19.71\n\n----- Categorical -----\n     col     class n_missing p_complete n_unique n_levels   min    max\n1   TEAM character         0          1       64       NA Akron   Yale\n2    R64 character         0          1       64       NA 1.50% 98.41%\n3    R32 character         0          1       62       NA 0.67% 93.59%\n4    S16 character         0          1       60       NA 0.33% 80.22%\n5     E8 character         0          1       54       NA 0.17%  8.37%\n6     F4 character         0          1       42       NA 0.07%  9.99%\n7 FINALS character         0          1       37       NA 0.03%  9.27%\n\n`public-picks` |&gt;\n  pivot_longer(R64:FINALS) |&gt; \n  group_by(YEAR, TEAMNO) |&gt; \n  mutate(stage = row_number()) %&gt;%\n  mutate(perc = as.numeric(str_remove(value, \"%\")),\n         perc = perc / 100) %&gt;%\n  ggplot(aes(x = stage, y = perc)) + \n  geom_line(aes(group = paste(YEAR, TEAM)))\n\n\n\n  # geom_col()\n\n\ntop_predicted_teams &lt;- `public-picks` |&gt;\n  pivot_longer(R64:FINALS) |&gt; \n  group_by(YEAR, TEAMNO) |&gt; \n  mutate(stage = row_number()) %&gt;%\n  mutate(perc = as.numeric(str_remove(value, \"%\")),\n         perc = perc / 100) %&gt;%\n  filter(stage == 6) %&gt;%\n  arrange(desc(perc)) %&gt;%\n  ungroup() %&gt;%\n  slice(1:5)\n\n`team-results` %&gt;%\n  count(CHAMP)\n\n# A tibble: 4 × 2\n  CHAMP     n\n  &lt;dbl&gt; &lt;int&gt;\n1     0   227\n2     1     4\n3     2     4\n4     3     1\n\n`team-results` %&gt;%\n  count(CHAMP)\n\n# A tibble: 4 × 2\n  CHAMP     n\n  &lt;dbl&gt; &lt;int&gt;\n1     0   227\n2     1     4\n3     2     4\n4     3     1\n\n`team-results` %&gt;%\n  filter(F2 &gt;= 2)\n\n# A tibble: 9 × 20\n  TEAMID TEAM    PAKE PAKERANK  PASE PASERANK GAMES     W     L WINPERCENT   R64\n   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;\n1     24 Butler   7          4   8.7        4    26    17     9      0.654     9\n2     40 Conne…   8.6        2  10.6        1    29    23     6      0.793     9\n3     50 Duke     2         24   0         78    46    34    12      0.739    14\n4     68 Gonza…   3.1       18   3.8       14    47    32    15      0.681    15\n5     86 Kansas   4.2       13  -1.8      205    53    40    13      0.755    15\n6     90 Kentu…   6.9        6   8.5        5    43    32    11      0.744    12\n7    114 Michi…   7          4   7.6        6    35    24    11      0.686    11\n8    135 North…  12          1   9.8        2    50    39    11      0.78     13\n9    228 Villa…   4.8        9   4         11    40    29    11      0.725    13\n# ℹ 9 more variables: R32 &lt;dbl&gt;, S16 &lt;dbl&gt;, E8 &lt;dbl&gt;, F4 &lt;dbl&gt;, F2 &lt;dbl&gt;,\n#   CHAMP &lt;dbl&gt;, TOP2 &lt;dbl&gt;, F4PERCENT &lt;chr&gt;, CHAMPPERCENT &lt;chr&gt;\n\ntop_predicted_teams %&gt;%\n  inner_join(`team-results`, by = \"TEAM\")\n\n# A tibble: 5 × 26\n   YEAR TEAMNO TEAM         name  value stage   perc TEAMID  PAKE PAKERANK  PASE\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt; &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1  2024   1067 Connecticut  FINA… 34.9…     6 0.349      40   8.6        2  10.6\n2  2024   1038 North Carol… FINA… 12.1…     6 0.121     135  12          1   9.8\n3  2024   1033 Purdue       FINA… 10.2…     6 0.102     167  -4.4      232  -4.4\n4  2024   1056 Houston      FINA… 9.27%     6 0.0927     76   0         76   2.1\n5  2024   1053 Iowa St.     FINA… 4.78%     6 0.0478     83  -2        209  -1.5\n# ℹ 15 more variables: PASERANK &lt;dbl&gt;, GAMES &lt;dbl&gt;, W &lt;dbl&gt;, L &lt;dbl&gt;,\n#   WINPERCENT &lt;dbl&gt;, R64 &lt;dbl&gt;, R32 &lt;dbl&gt;, S16 &lt;dbl&gt;, E8 &lt;dbl&gt;, F4 &lt;dbl&gt;,\n#   F2 &lt;dbl&gt;, CHAMP &lt;dbl&gt;, TOP2 &lt;dbl&gt;, F4PERCENT &lt;chr&gt;, CHAMPPERCENT &lt;chr&gt;\n\n\n\nShoutouts\nPlease check out Nic’s timeplyr package, which is now on CRAN and was masterfully presented at a previous EdinbR R users’ group meeting."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-english-monarchs/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-english-monarchs/index.html",
    "title": "Tidy Tuesday: English Monarchs",
    "section": "",
    "text": "The most recent Tidy Tuesday dataset was about English kings (including female kings - there should be a word for that) and their consorts.\nFor a change, Jon led the session.\nLoad the data:\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nenglish_monarchs_marriages_df &lt;- readr::read_csv(\n  'https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-08-20/english_monarchs_marriages_df.csv'\n  )\n\n\nRows: 83 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): king_name, king_age, consort_name, consort_age, year_of_marriage\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nData tidying - a lot of missing age information for older records, especially for the consorts:\n\n\nCode\nmar_tidy &lt;-\n  english_monarchs_marriages_df |&gt; \n    mutate(\n      king_age = str_remove_all(king_age, \"\\\\?|\\\\–|\\\\(|\\\\)\") %&gt;% as.numeric(),\n      consort_age = str_remove_all(consort_age, \"\\\\?|\\\\–|\\\\(|\\\\)\") %&gt;% as.numeric(),\n      year_of_marriage= str_remove_all(year_of_marriage, \"\\\\?|\\\\–|\\\\(|\\\\)\") %&gt;% as.numeric()\n    )\n\n\nFirst plot: age of ‘king’ against age of consort:\n\n\nCode\nmar_tidy |&gt; \n  ggplot(aes(x = king_age, y = consort_age)) +\n  geom_point() +\n  labs(title = \"King Age vs Consort Age\",\n       x = \"King Age\",\n       y = \"Consort Age\",\n       subtitle = \"Most kings were older than their consorts. QE2 Highlighted\"\n  ) +\n  coord_equal(xlim = c(0, 65), ylim = c(0, 65)) +\n  geom_abline(intercept =0, slope = 1, linetype = \"dashed\") +\n  geom_vline(xintercept = 16, linetype = \"dashed\") + \n  geom_point(aes(x = king_age, y = consort_age), shape = 17, size = 3, colour = \"purple\", data = \n               mar_tidy |&gt; filter(king_name == \"Elizabeth II\")\n             )\n\n\nWarning: Removed 25 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nWe added a special royal purple triangle for QE2.\nWe wanted to know\n\nWere the ‘kings’ whose consorts were older than them actually Queens?\n\n\n\nCode\nmar_tidy |&gt; \n  filter(king_age &lt; consort_age) |&gt; \n  select(king_name, consort_name, king_age, consort_age, year_of_marriage) |&gt; \n  distinct() |&gt; \n  arrange(king_age, consort_age)\n\n\n# A tibble: 8 × 5\n  king_name    consort_name                king_age consort_age year_of_marriage\n  &lt;chr&gt;        &lt;chr&gt;                          &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;\n1 Richard II   Anne of Bohemia                   15          16             1382\n2 Mary II      William III                       15          27             1677\n3 Henry VIII   Catherine of Aragon               18          24             1509\n4 Anne         George of Denmark                 18          30             1683\n5 Henry II     Eleanor of Aquitaine              19          30             1152\n6 Elizabeth II Philip of Greece and Denma…       21          26             1947\n7 Edward IV    Elizabeth Woodville               22          27             1464\n8 George IV    Maria Anne Fitzherbert            23          29             1785\n\n\nFemale ‘kings’ are over-represented but not the majority\nNow difference between king and consort age by year of marriage\n\n\nCode\nmar_tidy |&gt; \n  mutate(age_diff = king_age - consort_age) |&gt;\n  ggplot(aes(x = year_of_marriage, y = age_diff)) +\n  geom_point() +\n  labs(title = \"Difference between King and Consort Age by Year of Marriage\",\n       x = \"Year of Marriage\",\n       y = \"Difference between King and Consort Age\"\n  ) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_smooth()\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\nWarning: Removed 25 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 25 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nThe age gap between kings and consorts appears to have been falling from around 1500. (Note: neither of King Charles’ consorts are in the dataset as he was not a king at the time of either marriage)\nWas there any obvious tendency for English Kings to have multiple wives, and - like Leo DeCaprio - keep partnering with young women as they age?\n\n\nCode\nmar_tidy |&gt; \n  group_by(king_name) |&gt; \n  mutate(king_count = n()) |&gt; \n  filter(king_count &gt; 1) |&gt; \n  filter(year_of_marriage &gt; 1000) \n\n\n# A tibble: 25 × 6\n# Groups:   king_name [11]\n   king_name       king_age consort_name consort_age year_of_marriage king_count\n   &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;            &lt;dbl&gt;      &lt;int&gt;\n 1 Æthelred the U…       34 Emma of Nor…          17             1002          2\n 2 Cnut                  18 Aelfgifu of…          NA             1013          2\n 3 Cnut                  22 Emma of Nor…          NA             1017          2\n 4 Harold Godwins…       24 Edith Swann…          19             1044          2\n 5 Harold Godwins…       42 Ealdgyth              NA             1064          2\n 6 Henry I               32 Matilda of …          20             1100          2\n 7 Henry I               53 Adeliza of …          18             1121          2\n 8 John                  23 Isabel of G…          16             1189          2\n 9 John                  34 Isabella of…          12             1200          2\n10 Edward I              15 Eleanor of …          13             1254          2\n# ℹ 15 more rows\n\n\nFor many marriages the ages of the consorts weren’t known (the records are from hundreds of years ago, so maybe that’s to be expected).\n\n\nCode\nmar_tidy |&gt; \n  group_by(king_name) |&gt; \n  mutate(king_count = n()) |&gt; \n  filter(king_count &gt; 1) |&gt; \n  filter(!is.na(consort_age)) |&gt;\n  ggplot(aes(x = king_age, y = consort_age)) +\n  geom_line() +\n  geom_point() +\n  coord_equal() + \n  coord_equal(xlim = c(0, 65), ylim = c(0, 65)) +\n  geom_abline(intercept =0, slope = 1, linetype = \"dashed\") +\n  facet_wrap(~king_name) + \n  labs(\n    x = \"Age of king\",\n    y = \"Age of consort\",\n    title = \"Ages of kings and consorts for multiple-marrying kings\"\n  )\n\n\nCoordinate system already present. Adding new coordinate system, which will\nreplace the existing one.\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\nHenry VIII is clearly an outlier in terms of the number of marriages he had. But there’s no clear tendency for English kings to marry with Hollywood tendencies.\nEven where some kings have been married multiple times, the age of the consorts might only have been recorded one or zero times.\nFor many of the older records honourifics (and in some cases dishonourifics) were applied to the name. These were identified by the king name having the word ‘the’ in them:\n\n\nCode\nmar_tidy |&gt;\n  filter(str_detect(king_name, \" the \")) |&gt; \n  pull(king_name) |&gt;\n  unique()\n\n\n[1] \"Alfred the Great\"     \"Edward the Elder\"     \"Edgar the Peaceful\"  \n[4] \"Edward the Martyr\"    \"Æthelred the Unready\" \"Edward the Confessor\"\n[7] \"Henry the Young King\"\n\n\nBetter to be ‘unready’ or a ‘martyr’? (probably best to be unready to be a martyr!)"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html",
    "title": "Tidytuesday 2024-01-23",
    "section": "",
    "text": "This week’s TidyTuesday used data from the UK ONS which was explored in the 2023 article ’Why do children and young people in smaller towns do better academically than those in larger towns?’."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#background",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#background",
    "title": "Tidytuesday 2024-01-23",
    "section": "",
    "text": "This week’s TidyTuesday used data from the UK ONS which was explored in the 2023 article ’Why do children and young people in smaller towns do better academically than those in larger towns?’."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#aims",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#aims",
    "title": "Tidytuesday 2024-01-23",
    "section": "Aims",
    "text": "Aims\nOur first aim was to try to replicate the headline finding from the article above: that children in smaller towns have better average educational outcomes than in larger towns. We also sought to replicate and improve on the ‘beeswarm’ plot used in the original article, and to look at other factors which may explain differences in educational qualifications."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#package-loading",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#package-loading",
    "title": "Tidytuesday 2024-01-23",
    "section": "Package loading",
    "text": "Package loading\n\nlibrary(tidyverse)\nlibrary(ggbeeswarm) # for the beeswarm plot"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#data",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#data",
    "title": "Tidytuesday 2024-01-23",
    "section": "Data",
    "text": "Data\n\n# ee &lt;- tidytuesdayR::tt_load('2024-01-23') |&gt;\n#   purrr::pluck(1)\n\n# Direct link to get past API rate limit issue using tt_load()\nee &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-01-23/english_education.csv')\n\npurrr::pluck(1) was used because the data contained only a single dataset, but by default the tt_load function returns a list. So, the pluck(1) function takes the first element of the list, which in this case is in effect turning the data into a dataframe."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#counting-towns-in-data",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#counting-towns-in-data",
    "title": "Tidytuesday 2024-01-23",
    "section": "Counting towns in data",
    "text": "Counting towns in data\n\nee |&gt;\n  count(size_flag, sort=T) |&gt;\n  knitr::kable(caption = \"Counts of small/med/city class\")\n\n\nCounts of small/med/city class\n\n\nsize_flag\nn\n\n\n\n\nSmall Towns\n662\n\n\nMedium Towns\n331\n\n\nLarge Towns\n89\n\n\nCity\n18\n\n\nInner London BUA\n1\n\n\nNot BUA\n1\n\n\nOther Small BUAs\n1\n\n\nOuter london BUA\n1\n\n\n\n\n\nThere are 662 small towns, 331 medium towns, and 89 large towns"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#removing-oddball-locations-and-londons-and-factoring",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#removing-oddball-locations-and-londons-and-factoring",
    "title": "Tidytuesday 2024-01-23",
    "section": "Removing oddball locations and Londons and factoring",
    "text": "Removing oddball locations and Londons and factoring\n\nee |&gt;\n  mutate(town_size = factor(size_flag, levels = c(\"Small Towns\", \"Medium Towns\", \"Large Towns\"), ordered=T)) |&gt;\n  filter(!is.na(town_size)) -&gt; ee_fact"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#summary-by-group",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#summary-by-group",
    "title": "Tidytuesday 2024-01-23",
    "section": "Summary by group",
    "text": "Summary by group\n\nee_fact |&gt;\n  group_by(town_size) |&gt;\n  summarise(count = n(),\n            `mean ed score` = mean(education_score),\n            `sd ed score` = sd(education_score),\n            se = `sd ed score`/count^0.5,\n            `total population` = sum(population_2011)) |&gt;\n  mutate(across(where(is.numeric), round, 3)) |&gt;\n  knitr::kable()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\ntown_size\ncount\nmean ed score\nsd ed score\nse\ntotal population\n\n\n\n\nSmall Towns\n662\n0.297\n3.887\n0.151\n6880216\n\n\nMedium Towns\n331\n-0.253\n3.324\n0.183\n12213733\n\n\nLarge Towns\n89\n-0.811\n2.298\n0.244\n10466343"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#anova-for-smallmedlarge-towns",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#anova-for-smallmedlarge-towns",
    "title": "Tidytuesday 2024-01-23",
    "section": "ANOVA for small/med/large towns",
    "text": "ANOVA for small/med/large towns\nWe built a series of linear regression models, and used ANOVA to compare between them. A low p-value from ANOVA, when comparing two or more models that are ‘nested’, can be taken as a signal that the more complex/unrestricted of the models should be used.\n\nmod_base &lt;- lm(education_score ~ town_size, data = ee_fact)\nsummary(mod_base)\n\n\nCall:\nlm(formula = education_score ~ town_size, data = ee_fact)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.3246  -2.5270  -0.1996   2.3052  11.5749 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -0.255991   0.151293  -1.692  0.09093 . \ntown_size.L -0.783553   0.288560  -2.715  0.00673 **\ntown_size.Q -0.003547   0.232530  -0.015  0.98783   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.615 on 1079 degrees of freedom\nMultiple R-squared:  0.009593,  Adjusted R-squared:  0.007758 \nF-statistic: 5.226 on 2 and 1079 DF,  p-value: 0.005513\n\nmod_dep &lt;- lm(education_score ~ town_size + income_flag, data = ee_fact)\nsummary(mod_dep)\n\n\nCall:\nlm(formula = education_score ~ town_size + income_flag, data = ee_fact)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.4402 -1.8983 -0.0131  1.8447  9.2254 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        -2.29448    0.14533 -15.788  &lt; 2e-16 ***\ntown_size.L                         0.51810    0.22501   2.303   0.0215 *  \ntown_size.Q                        -0.01572    0.17726  -0.089   0.9293    \nincome_flagLower deprivation towns  5.31339    0.19207  27.664  &lt; 2e-16 ***\nincome_flagMid deprivation towns    1.82601    0.23489   7.774 1.77e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.754 on 1077 degrees of freedom\nMultiple R-squared:  0.4261,    Adjusted R-squared:  0.424 \nF-statistic: 199.9 on 4 and 1077 DF,  p-value: &lt; 2.2e-16\n\nmod_dep2 &lt;- lm(education_score ~ town_size * income_flag, data = ee_fact)\nsummary(mod_dep2)\n\n\nCall:\nlm(formula = education_score ~ town_size * income_flag, data = ee_fact)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.576 -1.868 -0.049  1.788  9.090 \n\nCoefficients:\n                                               Estimate Std. Error t value\n(Intercept)                                    -2.19244    0.15155 -14.466\ntown_size.L                                     0.94843    0.28240   3.359\ntown_size.Q                                    -0.10774    0.24096  -0.447\nincome_flagLower deprivation towns              4.77144    0.30602  15.592\nincome_flagMid deprivation towns                1.59836    0.31953   5.002\ntown_size.L:income_flagLower deprivation towns -1.36508    0.60094  -2.272\ntown_size.Q:income_flagLower deprivation towns -0.11774    0.44806  -0.263\ntown_size.L:income_flagMid deprivation towns   -0.76520    0.61668  -1.241\ntown_size.Q:income_flagMid deprivation towns   -0.04965    0.48199  -0.103\n                                               Pr(&gt;|t|)    \n(Intercept)                                     &lt; 2e-16 ***\ntown_size.L                                    0.000811 ***\ntown_size.Q                                    0.654869    \nincome_flagLower deprivation towns              &lt; 2e-16 ***\nincome_flagMid deprivation towns               6.62e-07 ***\ntown_size.L:income_flagLower deprivation towns 0.023309 *  \ntown_size.Q:income_flagLower deprivation towns 0.792769    \ntown_size.L:income_flagMid deprivation towns   0.214935    \ntown_size.Q:income_flagMid deprivation towns   0.917974    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.749 on 1073 degrees of freedom\nMultiple R-squared:  0.4305,    Adjusted R-squared:  0.4262 \nF-statistic: 101.4 on 8 and 1073 DF,  p-value: &lt; 2.2e-16\n\nanova(mod_base, mod_dep, mod_dep2)\n\nAnalysis of Variance Table\n\nModel 1: education_score ~ town_size\nModel 2: education_score ~ town_size + income_flag\nModel 3: education_score ~ town_size * income_flag\n  Res.Df     RSS Df Sum of Sq        F  Pr(&gt;F)    \n1   1079 14097.2                                  \n2   1077  8168.3  2    5928.9 392.3729 &lt; 2e-16 ***\n3   1073  8106.8  4      61.5   2.0343 0.08749 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod_dep, mod_dep2)\n\nAnalysis of Variance Table\n\nModel 1: education_score ~ town_size + income_flag\nModel 2: education_score ~ town_size * income_flag\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1   1077 8168.3                              \n2   1073 8106.8  4    61.479 2.0343 0.08749 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe summary from mod_dep indicates that deprivation tertile, using the IMD income domain, may have more of an effect than town size, and in the opposite direction."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#beeswarm-plot",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#beeswarm-plot",
    "title": "Tidytuesday 2024-01-23",
    "section": "Beeswarm plot",
    "text": "Beeswarm plot\nWe reproduce the beeswarm plot from the original article, but colouring areas by income tertile:\n\nee_fact |&gt;\n  mutate(income_flag = factor(income_flag, levels = c(\"Lower deprivation towns\", \"Mid deprivation towns\", \"Higher deprivation towns\"))) |&gt;\n  ggplot(aes(x = town_size, y = education_score, color = income_flag)) +\n  geom_beeswarm() +\n  coord_flip() +\n  theme(legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"#73b8fd\", \"#0068c6\", \"#003b7c\"))"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#conclusion",
    "href": "posts/tardy-tuesday/tidy-tuesday-education-townsize/index.html#conclusion",
    "title": "Tidytuesday 2024-01-23",
    "section": "Conclusion",
    "text": "Conclusion\nWe were able to replicate the headline finding from the article, and the type of visualisation used. But we also identified area deprivation as an important (and likely a more important) determinant of education scores."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-womens-football/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-womens-football/index.html",
    "title": "Tidy Tuesday: Women’s Football",
    "section": "",
    "text": "After a bit of a (in Jon’s view) dearth of interesting datasets within Tidy Tuesday, this week brought something worth looking at to the table: datasets on women’s football over the last few years, including changes in its popularity, as measured by attendance.\nKate led this session."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-womens-football/index.html#setting-up",
    "href": "posts/tardy-tuesday/tidy-tuesday-womens-football/index.html#setting-up",
    "title": "Tidy Tuesday: Women’s Football",
    "section": "Setting up",
    "text": "Setting up\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(broom)\n\ntuesdata &lt;- tidytuesdayR::tt_load(2024, week = 29)\n\n\n--- Compiling #TidyTuesday Information for 2024-07-16 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `ewf_appearances.csv`\n    Downloading file 2 of 3: `ewf_matches.csv`\n    Downloading file 3 of 3: `ewf_standings.csv`\n\n\n--- Download complete ---\n\n\nCode\newf_appearances &lt;- tuesdata$ewf_appearances\newf_matches &lt;- tuesdata$ewf_matches\newf_standings &lt;- tuesdata$ewf_standings"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-womens-football/index.html#questions-questions",
    "href": "posts/tardy-tuesday/tidy-tuesday-womens-football/index.html#questions-questions",
    "title": "Tidy Tuesday: Women’s Football",
    "section": "Questions, questions…",
    "text": "Questions, questions…\nFirstly, we decided to look at growth attendance over time:\n\n\nCode\newf_matches %&gt;% \n  group_by(season) %&gt;% \n  summarise(attendance = median(attendance, na.rm = TRUE)) %&gt;% \n  ggplot() +\n  geom_col(aes(x = season, y = attendance)) +\n  theme(axis.text.x = element_text(angle = 90))\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\nFor some reason, there was no attendance in the 2020-2021 season. Definitely something unexpected that we should investigate further, as 2020 was a completely normal year in every way\nWhat about trends in season, e.g. are the first matches more popular than the rest, much like the first episodes of TV series tend to be watched more than the rest of the series?\n\n\nCode\newf_matches %&gt;% \n  group_by(season) %&gt;% \n  mutate(match_no = order(date)) %&gt;% \n  ggplot(aes(x = match_no, y = attendance, colour = season, group = season)) +\n  geom_point() +\n  facet_wrap(~ tier, scales = \"free\") +\n  scale_y_log10()\n\n\nWarning: Removed 1188 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nNote we used a log y scale as attendance seems to be very variable between matches. However we couldn’t see any obvious trend within season.\nWe also decided just to focus on tier 1\n\n\nCode\newf_matches %&gt;% \n  filter(tier == 1) %&gt;% \n  group_by(season) %&gt;% \n  mutate(match_no = order(date)) %&gt;% \n  ggplot(aes(x = match_no, y = attendance, colour = season, group = season)) +\n  geom_point() +\n  scale_y_log10()\n\n\nWarning: Removed 178 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-womens-football/index.html#modelling",
    "href": "posts/tardy-tuesday/tidy-tuesday-womens-football/index.html#modelling",
    "title": "Tidy Tuesday: Women’s Football",
    "section": "Modelling",
    "text": "Modelling\nWe then decided to try to model factors that could predict (log) attendance. First a model predicting log-attendance on home team id, match number, and season, but without interaction terms. And then a model with interaction terms:\n\n\nCode\newf_matches %&gt;% \n  filter(tier == 1) %&gt;% \n  group_by(season) %&gt;% \n  mutate(match_no = order(date)) %&gt;% \n  lm(log10(attendance) ~ home_team_id + match_no + season, data = .) -&gt;\n  mod1\n\newf_matches %&gt;% \n  filter(tier == 1) %&gt;% \n  group_by(season) %&gt;% \n  mutate(match_no = order(date)) %&gt;% \n  lm(log10(attendance) ~ home_team_id + match_no + season + season*match_no, data = .) -&gt;\n  mod2\n\n\nHow to compare? Well, as mod1 can be considered as a restricted version of mod2 (the interaction term coefficents set to 0) we can use ANOVA to see if the additional complexity of the unrestricted model, mod2, is ‘worth it’:\n\n\nCode\nanova(mod2)\n\n\nAnalysis of Variance Table\n\nResponse: log10(attendance)\n                 Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nhome_team_id     17 63.340  3.7259 42.4441 &lt; 2.2e-16 ***\nmatch_no          1  4.241  4.2414 48.3166 6.604e-12 ***\nseason           12 56.470  4.7058 53.6073 &lt; 2.2e-16 ***\nmatch_no:season  12  1.508  0.1257  1.4314    0.1453    \nResiduals       982 86.203  0.0878                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nanova(mod1, mod2) # same as above but pulls out specific test that we are interested in\n\n\nAnalysis of Variance Table\n\nModel 1: log10(attendance) ~ home_team_id + match_no + season\nModel 2: log10(attendance) ~ home_team_id + match_no + season + season * \n    match_no\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    994 87.711                           \n2    982 86.203 12    1.5079 1.4314 0.1453\n\n\nWe concluded:\nInteraction term not significant - proceed with mod1\nNow to look at the coefficients on mod1:\n\n\nCode\nsummary(mod1)\n\n\n\nCall:\nlm(formula = log10(attendance) ~ home_team_id + match_no + season, \n    data = .)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.90496 -0.19277 -0.03661  0.14062  1.51043 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          2.9890968  0.0541578  55.192  &lt; 2e-16 ***\nhome_team_idT-002-T -0.2686225  0.0619191  -4.338 1.58e-05 ***\nhome_team_idT-003-T -0.3359697  0.0446896  -7.518 1.24e-13 ***\nhome_team_idT-005-T -0.2652441  0.0524152  -5.060 4.98e-07 ***\nhome_team_idT-006-T -0.2461753  0.0464112  -5.304 1.39e-07 ***\nhome_team_idT-008-T -0.0507938  0.0425091  -1.195 0.232413    \nhome_team_idT-011-T -0.4073355  0.0693986  -5.870 5.95e-09 ***\nhome_team_idT-013-T -0.5394900  0.0448460 -12.030  &lt; 2e-16 ***\nhome_team_idT-014-T -0.2734024  0.0619209  -4.415 1.12e-05 ***\nhome_team_idT-016-T -0.2675245  0.0598158  -4.472 8.62e-06 ***\nhome_team_idT-017-T -0.3209426  0.0440625  -7.284 6.60e-13 ***\nhome_team_idT-020-T -0.0394211  0.0447805  -0.880 0.378900    \nhome_team_idT-021-T  0.0452529  0.0575592   0.786 0.431939    \nhome_team_idT-023-T -0.4128162  0.0497442  -8.299 3.40e-16 ***\nhome_team_idT-027-T -0.4112510  0.0693942  -5.926 4.27e-09 ***\nhome_team_idT-028-T -0.3241747  0.0575895  -5.629 2.36e-08 ***\nhome_team_idT-030-T -0.3459956  0.0530576  -6.521 1.11e-10 ***\nhome_team_idT-031-T -0.3978321  0.0708242  -5.617 2.52e-08 ***\nmatch_no            -0.0002474  0.0003161  -0.783 0.434030    \nseason2012-2012     -0.0706150  0.0641552  -1.101 0.271298    \nseason2013-2013     -0.0381315  0.0604345  -0.631 0.528215    \nseason2014-2014      0.0752879  0.0611843   1.231 0.218797    \nseason2015-2015      0.2112739  0.0626453   3.373 0.000774 ***\nseason2016-2016      0.2591016  0.0582373   4.449 9.60e-06 ***\nseason2017-2017      0.1782820  0.0691801   2.577 0.010107 *  \nseason2017-2018      0.1231090  0.0579269   2.125 0.033812 *  \nseason2018-2019      0.1606422  0.0562308   2.857 0.004368 ** \nseason2019-2020      0.4160694  0.0578761   7.189 1.28e-12 ***\nseason2021-2022      0.3912164  0.0568712   6.879 1.06e-11 ***\nseason2022-2023      0.7350856  0.0570183  12.892  &lt; 2e-16 ***\nseason2023-2024      0.8347182  0.0565907  14.750  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2971 on 994 degrees of freedom\n  (178 observations deleted due to missingness)\nMultiple R-squared:  0.5858,    Adjusted R-squared:  0.5733 \nF-statistic: 46.86 on 30 and 994 DF,  p-value: &lt; 2.2e-16\n\n\nWe had some fun trying to find the most and least popular teams. The reference team, team 1, happens to be about the most popular team.\n\n\nCode\newf_appearances %&gt;% \n  filter(season == \"2023-2024\") %&gt;% \n  select(team_id, team_name) %&gt;% \n  distinct() %&gt;%\n  arrange(team_id) |&gt;\n  view()\n\n\nWe thought we could do more to visualise the difference in apparent popularity between teams, producing some ‘tie fighter’ graphs (also known as blobograms, apparently)\n\n\nCode\ntidy(mod1) %&gt;% \n  filter(str_detect(term, \"home_team\")) %&gt;% \n  mutate(term = str_remove(term, \"home_team_id\")) %&gt;% \n  ggplot() +\n  geom_point(aes(x = estimate, y = term)) +\n  geom_errorbarh(aes(xmin = estimate - 2*std.error, xmax = estimate + 2*std.error, y = term)) +\n  geom_vline(aes(xintercept = 0), linetype = \"dashed\")\n\n\n\n\n\nPoor old team 13! (Everton Women)\nCompared with the reference team (Arsenal Women) only 3 teams looked similarly popular: team 8 (Chelsea Women), team 20 (Manchester City), and team 21 (Manchester United).\nThen we decided to do the same kind of thing for season, which should showing growing popularity over time:\n\n\nCode\ntidy(mod1) %&gt;% \n  filter(str_detect(term, \"season\")) %&gt;% \n  mutate(term = str_remove(term, \"season\")) %&gt;% \n  ggplot() +\n  geom_point(aes(x = estimate, y = term)) +\n  geom_errorbarh(aes(xmin = estimate - 2*std.error, xmax = estimate + 2*std.error, y = term)) +\n  geom_vline(aes(xintercept = 0), linetype = \"dashed\")\n\n\n\n\n\nUp, up and away! (mostly…)"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-womens-football/index.html#conclusion",
    "href": "posts/tardy-tuesday/tidy-tuesday-womens-football/index.html#conclusion",
    "title": "Tidy Tuesday: Women’s Football",
    "section": "Conclusion",
    "text": "Conclusion\nSome places to go:\n\nWhy the decline in 2016, 2017, 2018?\nWhat caused the jumps in 2015, 2019, 2022?"
  },
  {
    "objectID": "posts/tardy-tuesday-series/index.html",
    "href": "posts/tardy-tuesday-series/index.html",
    "title": "Tardy Tuesdays: My Second Series",
    "section": "",
    "text": "There’s now enough posts related to the Tidy Tuesdays sessions I run weekly to, I think, justify putting it in a separate collection. Just check out the ‘Tardy Tuesday’ tab near the top left of the page!"
  },
  {
    "objectID": "posts/unpop/new-frasiers-eerie-familiarity/index.html",
    "href": "posts/unpop/new-frasiers-eerie-familiarity/index.html",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "",
    "text": "New Frasier.. like the Old Frasier\nAs with new Beavis and Butt-Head, which I’ve written about previously, Paramount+ includes access to new Frasier, a return of the vainglorious pratfalling public psychiatrist to the small screen after the original series ended in 2004.\nThough Frasier himself has returned, none of the supporting characters have. Instead he’s surrounded by an entirely (apparently) new cast of supporting characters, and the series is set in a brand new (but also very old) location. Given this, we might expect Frasier (2023) to feel very different to Frasier (1993)…\nBut it doesn’t. It feels eerily familiar. Despite almost everything, apart from the title character, being different, Frasier (2023) somehow feels largely the same as Frasier (1993).\nThe aim of this post is to try to think through why, with New Frasier, despite almost everything being different, almost everything is also the same."
  },
  {
    "objectID": "posts/unpop/new-frasiers-eerie-familiarity/index.html#characters-the-situation-in-situation-comedies",
    "href": "posts/unpop/new-frasiers-eerie-familiarity/index.html#characters-the-situation-in-situation-comedies",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "Characters: The Situation in Situation Comedies",
    "text": "Characters: The Situation in Situation Comedies\nCommercially successful TV series, back in the 1990s, weren’t really meant to go anywhere in terms of character and characterisation. Whereas Joseph Campbell’s Monomyth, which countless films use as their narrative template, focuses on how a sequence of events and experiences lead to irreconcilable change in the central character, the formula for a successful TV series depends on the central character not changing. Things happen to the central character, but like a boulder in a stream, the central character ultimately remains largely unchanged and unmoved as a result. The reason for this was largely due to the value to an audience of familiarity, which brings a sense of warmth to characters, and also not to burn through an ultimately finite supply of Heroic Fuel: There’s only so many times a character can face adversity, the call to adventure, the descent into the Underworld, look one’s Adversary in the eyes, almost die (literally or figuratively), ultimately triumph, and return to the light wounded, wiser but ultimately stronger. For an episodic series, if a character is shown to be broken and remoulded every week, before too long the audience will start to feel they’re made more of clay than flesh and bone.\nSo if the Monomyth can’t be used as the main narrative engine of a TV series, what can? For sitcoms, the clue’s partially in the name: the situation. And for most successful sitcoms, including Frasier, much of the situation comes from the interplay between characters.\nHow did this work in Frasier (1993)? Well, in the original Frasier the following supporting characters were introduced:\n\nMarty. Frasier’s father, a retired police officer. Whereas Frasier is booksmart, Marty is streetsmart. Frasier and Marty are both smart, but orientated towards fundamentally different forms of knowledge and competence. They might be related, but they’ve always swum in different waters.\nNiles. Frasier’s brother. Whereas Marty is too dissimilar to Frasier, Niles is too similar. Frasier’s knowledge and interests are esoteric, high culture not mainstream, and so there are few people in the world who will understand him. Niles does. But their world of high culture is so exclusive it’s also small. And it’s competitive, their academic and professional overachievement fueled by unquensionable egotism and self doubt. So, as well as Niles being Frasier’s closest friend, he’s also his closest rival.\nRos. Frasier’s Radio producer. Like Marty she’s streetsmart (albeit in the ‘streets’ of media production). And like Frasier she’s competitive. Because she’s more worldly wise than Frasier, and his boss, she exploits and manipulates Frasier to achieve her own ambitions, which often don’t align exactly with his own. She is, in a platonic sense, Frasier’s pimp.\nDaphne. The Help. Marty’s live-in carer, launderer and folder of clothes, cooker of foods, provider of basic needs. Daphne is economically dependent on Frasier’s largesse, and appears to be somewhat naive. However this appearance of naivity is sometimes shown to be an illusion.\n\nWhereas in Frasier (2023) there are the following supporting characters:\n\nFreddy. Frasier’s son, a working firefighter. Whereas Frasier is only booksmart, Freddy is also streetsmart. Freddy actively rejected the path to high culture that Frasier set him on. They might be related, but they swim in different waters.\nAlan. Frasier’s university buddy. Whereas Freddy is too dissimilar to Frasier, Alan is too similar. Frasier’s knowledge and interests are estoteric, high culture not mainstream, and so there are few people in the world who will understand him. Alan does. But their world of high culture is so exclusive it’s also small. And it’s competitive. Alan, however, has tenure, something Frasier covets, so in this sense, as well as being Fraiser’s closest friend Alan is, if not exactly a rival, someone Frasier finds himself measuring himself against, and finding wanting.1\nOlivia. Frasier’s boss. As well as being an academic, she’s also a manager of academics, and so a practitioner of the Dark Arts of academic self promotion. Like Frasier she’s competitive, especially with her sister, who’s also a senior academic. She is, in a platonic sense, Frasier’s pimp, and calls him her ‘dancing bear’.\nEve. Not The Help, but a single mother Freddy helped, and so Frasier must support too. Eve is somewhat economically dependent on Frasier’s largesse, living rent-free in one of his apartments. She has good social instincts and works at a bar, but considers herself an actor, though has a naively delusional sense of her own abilities in this field.\n\nIf the second series of descriptions seems similar to the first, this may help explain how and why Frasier (2023) is eerily familiar to viewers of Frasier (1993). Despite some differences, there is almost a one-to-one mapping between the main supporting characters in both Frasiers.2 Though it might not declare itself as such, Frasier (2023) is not just a sitcom, but a sci-fi sitcom, as it appears, like an inversion of Dr Who, that everyone except the lead character has regenerated into a new body."
  },
  {
    "objectID": "posts/unpop/new-frasiers-eerie-familiarity/index.html#character-based-situational-combinations",
    "href": "posts/unpop/new-frasiers-eerie-familiarity/index.html#character-based-situational-combinations",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "Character-based situational combinations",
    "text": "Character-based situational combinations\nSo, Frasier (2023) takes the same supporting character archetypes as Frasier (1993) and regenerates them. Why? I think this is because of the way even a small number of supporting characters can generate a large number of character-based situational combinations, and so a great deal of fuel for episodic stories, each based on how the main character interacts with one, two, or possibly three of the other characters. The Wikipedia article on Combination goes into a painful amount of detail on this.\nSay there are four primary characters supporting characters. 3 There are then four ways (F, A, O and E) that a single supporting character can interact with the main character. This comes intuitively, but also from the Binomial Coefficient \\(C(n, k) = \\frac{n!}{k!(n-k)!}\\), where \\(!\\) indicates factorial. We can work out the total number of combinations of four characters as follows:\n\n\nCode\nlibrary(tidyverse)\nmy_binomial &lt;- function(n,k) {factorial(n) / (factorial(k) * factorial(n-k))}\n\nn_characters &lt;- 4\n\ndf &lt;- tibble(\n    n_characters_interacting = 0:4\n    ) |&gt;\n    mutate(\n        n_comb_with_this_many_chars = map_int(n_characters_interacting, function(x) my_binomial(n_characters, x))\n    ) |&gt;\n    mutate(\n        cumulative_combinations = cumsum(n_comb_with_this_many_chars)\n    )\n\ndf \n\n\n# A tibble: 5 × 3\n  n_characters_interacting n_comb_with_this_many_chars cumulative_combinations\n                     &lt;int&gt;                       &lt;int&gt;                   &lt;int&gt;\n1                        0                           1                       1\n2                        1                           4                       5\n3                        2                           6                      11\n4                        3                           4                      15\n5                        4                           1                      16\n\n\nSo, with 4 supporting characters, there are 16 combinations of interactions with Frasier (where 0 characters interacting would be Frasier soliloquizing, say if he gets stuck in a lift). This isn’t a huge number of situations, but more than a modern season. But this is just combinations, not permutations: a situation in which Alan verbs Olivia, for example, would be different to one in which Olivia verbs Alan, but in combinatorials counted as the same.4 It also excludes any B plots not involving Frasier. For this we simply need to change the n in the above from 4 to 5, and exclude k=0 from the option, as a story involving no characters probably wouldn’t work…\n\n\nCode\nn_characters &lt;- 5\n\ndf &lt;- tibble(\n    n_characters_interacting = 1:5\n    ) |&gt;\n    mutate(\n        n_comb_with_this_many_chars = map_int(n_characters_interacting, function(x) my_binomial(n_characters, x))\n    ) |&gt;\n    mutate(\n        cumulative_combinations = cumsum(n_comb_with_this_many_chars)\n    )\n\ndf \n\n\n# A tibble: 5 × 3\n  n_characters_interacting n_comb_with_this_many_chars cumulative_combinations\n                     &lt;int&gt;                       &lt;int&gt;                   &lt;int&gt;\n1                        1                           5                       5\n2                        2                          10                      15\n3                        3                          10                      25\n4                        4                           5                      30\n5                        5                           1                      31\n\n\nAllowing Frasier not to be in every story, the number of character-based combinations increases to 31, which seems plenty of basic story types from which between 5 (one scene) and 30 (one show) minutes of content could be derived. And as mentioned, this is just combinations, not permutations, where order matters. If looking at permutations, then the number of sequences with five characters is \\(5!\\),5 or 120, but we also need to include four, three, and two character sequences too, ie. \\(5! + 4! + 3! + 2!\\), which brings up the number of permutations to 152.6"
  },
  {
    "objectID": "posts/unpop/new-frasiers-eerie-familiarity/index.html#so-what",
    "href": "posts/unpop/new-frasiers-eerie-familiarity/index.html#so-what",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "So what?",
    "text": "So what?\nOkay, I’ve gone into the technical details a bit more than I was expecting to. The point is that even with a fairly small number of characters, the number of possible situations and circumstances that derive entirely from placing characters in a room together, and thinking how they might interact with each other, quickly becomes large enough to avoid being repetitive despite being familiar. Of course, additional supporting characters, guest stars, and scenarios all help increase the number of stories even further, but just having a small number of well defined characters, and imagining the narrative molecules and compounds these character elements may form when forced to mix, appears to do the bulk of the storytelling. With a sitcom, with interesting and well defined characters, in a sense it seems the stories write themselves.\nAnd why almost the same characters, rather than just the same number of characters? I think this was because over a decade of Frasier provides plenty of experience about what these character combinations produce. So, why start from scratch?7"
  },
  {
    "objectID": "posts/unpop/new-frasiers-eerie-familiarity/index.html#conclusion",
    "href": "posts/unpop/new-frasiers-eerie-familiarity/index.html#conclusion",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "Conclusion",
    "text": "Conclusion\nI think a second season is likely."
  },
  {
    "objectID": "posts/unpop/new-frasiers-eerie-familiarity/index.html#footnotes",
    "href": "posts/unpop/new-frasiers-eerie-familiarity/index.html#footnotes",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBeing an English character played by an English actor (Unlike Freddy and Olivia, who are both American characters played by English actors), Alan’s character is also a poorly dressed high functioning alcoholic.↩︎\nThe one exception to this is David Crane, son of Niles and Daphne. Initially it appeared the ‘Niles’ archetype from Frasier-1 had undergone some kind of Narrative Fission Event and been split into both Alan and David. On further viewing, however, it becomes apparent David is instead an intruder from another show, being effectively a toned down version of Sheldon Cooper from the Big Bang Theory, and being more a caricuture than a character.↩︎\nI’m excluding David as a primary supporting character as he appears to be a caricature, but he may become more fleshed out over time.↩︎\nSituations best described using intransitive verbs are probably closer to combinatorials not permutations. A pizza with ham and mushroom would be much like a pizza with mushroom and ham. Similarly a plot in which Alan and Olivia eat dinner together would be much like a plot in which Oliva and Alan eat dinner together. However a situation in which Alan invites Olivia to dinner would be different to a situation in which Olivia invites Alan to dinner!↩︎\nfactorial(5) in R↩︎\nfactorial(5) + factorial(4) + factorial(3) + factorial(2) in R↩︎\nAs with the phrase “History doesn’t repeat, but it does rhyme”, we probably shouldn’t assume exactly the same plots will occur in Frasier 2023 as with Frasier 1993. If it were to, we would expect Alan, who’s in his sixties, to become emphatuated with Eve, who’s in her twenties. I suspect this won’t happen, unless there’s a reboot of the reboot in 20 more years featuring Leonardo Dicaprio↩︎"
  },
  {
    "objectID": "posts/unpop/beavis-and-butthead-is-dumbface/index.html",
    "href": "posts/unpop/beavis-and-butthead-is-dumbface/index.html",
    "title": "Beavis and Butt-Head: When a physics graduate dons Dumbface",
    "section": "",
    "text": "Mike Judge’s Beavis and Butt-Head\n\n\nA second season of “Mike Judge’s Beavis & Butt-Head” is now available on Paramount+, continuing a series that began on MTV in the early 1990s. I’ve been watching it, generally enjoying it, but feeling a gnawing sense of discomfort while doing so. Here’s why.\nMike Judge is a physics graduate, whose other credits include: Silicon Valley, a sitcom about tech startups; King of the Hill, a surprisingly gentle and sympathetic animated sitcom about a lower middle class social conservative family; and Idiocracy, a feature length science fiction comedy whose premise is that, “People are getting dumber; society’s getting dumber; at this rate someone who’s average now will be considered a genius a few generations from now.”\nIn Idiocracy, the proposed mechanism for the world’s dumbing down is a kind of dysgenic selective breeding. Whereas smarter people, with their careful planning and fantastic career opportunities, equivocate and defer the decision to have children, dumber and more feckless people, who don’t tend to do much thinking or planning, and wouldn’t be giving up on any great opportunities, continue to breed like rabbits, or even viruses. Dumber people have a higher R number, so will outbreed smarter people until almost everyone in society’s dumb. From an evolutionary perspective, dumb is the winning strategy.\nIf this sounds like the kind of plotline a eugenicist might come up with, I think you might be right. The alternative is that Judge is a black pilled cynic, a wannabe eugenicist, who just wishes, like Marxism, it would only work in practice. Watching a Judge film or TV show is being invited to judge, to find others inferior and wanting, and so feel superior. But that short-term feeling of superiority is fleeting; what lingers is the sense of loneliness, of being ‘the only adult in the room’, the hell of other people, when the other people are idiots.\nThe intellectual elitism, and sneering at the dumb, that finds most full expression in Idiocracy, has always been present in Beavis & Butt-Head. To an extent that’s the entire plot. Beavis & Butt-Head’s lack of intellect is extrapolated to such an extent it becomes grotesquely surreal. And they combine this lack of intelligence with a lack of almost any detectable virtues or redeeming qualities, with the possible exception of Beavis’ sense of loyalty to Butt-Head, a loyalty that is often presented as misplaced, enabling the pair’s co-dependence and Butt-Head’s constant physical and emotional abuse of Beavis, his only friend in the world.\nOther targets of Beavis & Butt-Head’s humour are those characters who overestimate the two title characters, treat them with kindness, and try to help them. This includes their hippy teacher, Mr Van Driessen, who is frequently seen to permit delinquency and disruption from the titular pair, apparently to the detriment both of the pair themselves, and the rest of the class. Other recent episodes feature a kindly middle-aged couple, who happily provide the pair with provisions with which their own home will be attacked; someone who heroically rescues them from a sewer (which the pair mistakenly believe is Hell, as in their illiteracy they misread ‘Department of Sanitation’ as ‘De Apartment of Satan’); and their ever forgiving and kindly neighbour (and Hank Hill prototype?) Mr Anderson, whose property the pair damage and steal without apparent repercussion (except of the karmic variety).\nPerhaps the most depressing segments in the recent Beavis and Butt-Head are those titled Old Beavis and Butt-Head. Breaking a forth wall in long-run cartoon series, the premise of these segments is that we might expect that someone who was a teenager in the early 1990s might be middle aged (or ‘old’, from their former teenage self’s perspective) in the 2020s. And so these age-appropriate versions of the characters are presented. By now, the segments suggest, Butt-Head is jobless, obese, and living off disability payments. Beavis is wrinkly and crag-toothed, but has at least managed, after decades of (not) trying, to get a job, working as Butt-Head’s full time (taxdollar funded) carer.\nMike Judge, as well as creating the series, also voices both characters. In doing so, and in the context of his own academic achievements, just how wretchedly they are drawn, and his other outputs, I think he does the vocal equivalent of ‘donning Dumbface’. Beavis and Butt-Head aren’t just incapable and inferior along narrowly intellectual or academic lines, but in every conceivable way. Every thing they say (with the exception of some of their commentary segments), every thing they do, every scrape and escapade they put themselves in, is yet more evidence of their incorrigible worthlessness, and every attempt to help them as coming from a well-intentioned but misplaced belief that they could ever be better than they are. If we can’t get rid of people like them, the show seems to be suggesting, the best we can do is laugh at them mercilessly. (Maybe behind their backs, just to be safe.)\nAs mentioned, I’ve been watching the new series, and against my better nature enjoying it. It’s a guilty pleasure. Hopefully the above goes to illustrate just how guilty."
  },
  {
    "objectID": "posts/unpop/hari-superior-inferior-superior-story-teller/index.html",
    "href": "posts/unpop/hari-superior-inferior-superior-story-teller/index.html",
    "title": "Johan Hari: Superior Inferior Superior Storyteller",
    "section": "",
    "text": "Magic Pill"
  },
  {
    "objectID": "posts/unpop/hari-superior-inferior-superior-story-teller/index.html#introduction",
    "href": "posts/unpop/hari-superior-inferior-superior-story-teller/index.html#introduction",
    "title": "Johan Hari: Superior Inferior Superior Storyteller",
    "section": "Introduction",
    "text": "Introduction\nA number of factors aligned in recent weeks to encourage me to buy, and even read, Johan Hari’s latest book, Magic Pill: The Extraordinary Benefits and Disturbing Risks of the New Weight Loss Drugs. These included:\n\nOn Hari:\n\nA highly effusive and friendly interview with Hari on the Honestly podcast\nA highly critical and mocking ‘take-down’ of Hari and his credentials and aptitude as a popular science writer on The Studies Show podcast\n\nOn Weight Loss Drugs:\n\nThe recent South Park special on the new weight loss drugs\nA series of interesting interviews and conversations with a digital pharmacy which in recent months has been pivoting some of their operations and business model towards the supply of some of these drugs\n\n\nSo, this post will be about two things: Hari’s effectiveness and role as a popular science writer; and the potential effects and implications of the new weight loss drugs.\n\n\n\nSouth Park: The End of Obesity"
  },
  {
    "objectID": "posts/unpop/hari-superior-inferior-superior-story-teller/index.html#johan-hari-superior-inferior-superior-storyteller",
    "href": "posts/unpop/hari-superior-inferior-superior-story-teller/index.html#johan-hari-superior-inferior-superior-storyteller",
    "title": "Johan Hari: Superior Inferior Superior Storyteller",
    "section": "Johan Hari: Superior Inferior Superior Storyteller",
    "text": "Johan Hari: Superior Inferior Superior Storyteller\nIn one adjectivally loaded term, that’s what I make of Johan Hari. Like George Orwell’s self designation as ‘lower upper middle class’, and Donald Rumsfeld’s infamous unknown unknown/known unknown/known known typology, I expect (and to an extent hope) this term is something that may initially appear as word salad, but on closer inspection and unpacking turns out to be insightful. So let’s unpack:\n\nsuperior (inferior (superior storyteller)): a superior storyteller, who’s somewhat inferior, but as an inferior superior story-teller is somewhat superior.\n\nThe term superior storyteller comes from the sociologist and historian Charles Tilly’s book Why? what happens when people give reasons… and why. Tilly offers a two-by-two typology of types of response to why questions, as shown below:\n\nCharles Tilly’s Why Typology\n\n\n\n\n\n\n\n\nPopular\nSpecialised\n\n\n\n\nFormulas\nConventions\nCodes\n\n\nCause-Effect Accounts\nStories\nTechnical Accounts\n\n\n\nIn brief, formulas offer if-this-then-that explanations, whereas cause-effect accounts offer that-because-this explanations. Formulas offer prescriptions for action, whereas cause-effect accounts how one thing leads to another. Science is largely concerned with the production, falsification and validation of precise and useful technical accounts. But both the level of detail required to describe causal influence in complex and complicated systems of relationship, and the need to do so with precision and without ambiguity, leads to a level of detail in terms of elements and their influence on each other that can be cognitively overwhelming, and a way of expressing such accounts - such as through algebra and graphs - that is rareified and so publicly inaccessible. Stories, by contrast, are often simpler in terms of the number of elements and types of relationship being proposed as causally related, and looser and more ambiguous in terms of the precision of language used, but have lower cognitive demands and require less training to parse and interpret.\nTilly’s term Superior Story is something like a reasonably efficient and accessible storyfication of a technical account, i.e. a reasonably accessible story based around a reasonably accurate, though often highly simplified and stylised, retelling of a technical account. In Tilly’s words:\n\nSuperior Stories? Like everyday stories, superior stories simplify their causes and effects. They maintain unity of time and place, deal with a limited number of actors and actions, as they concentrate on how those actions cause other actions. They omit or minimize errors, unanticipated consequences, indirect effects, incremental effects, simultaneous effects, feedback effects, and environmental effects. But within their limited frames they get the actors, actions, causes and effects right. By the standards of a relevant and credible technical account, they simplify radically, but everything they say is true. Superior stories make at least a portion of the truth accessible to nonspecialists.\n\n(pp. 171-172)\nSo, aside from those popular science writers focused on taxonomy (raw material for the pub bore, the compulsive quizzing factshitter), the aim of a science writer should be to be a superior storyteller. And I think in Magic Pill Hari does largely manage to place himself in this camp.\nBut then why is Hari an inferior superior storyteller? Quite simply, because he doesn’t seem to be especially good at doing science, or employing scientific reasoning himself. This is most obviously the case when it comes to discussing risks of adverse events, such as this passage (quoted at length) on elevated thyroid cancer risk:\n\nThe third [risk] is far more serious. A few months after Daniel told me there was no safety signal attached to these drugs, one was raised for the first time. The European Medicines Agency – the regulatory body for the European Union – announced ‘a thyroid cancer safety signal’ for all GLP-1 agonists. This means that they were beginning to monitor the drugs for potentially causing thyroid cancer. They did this because of a worrying piece of research that was published in France by Jean-Luc Faillie, who is a professor of medical pharmacology at the University Hospital of Montpellier and also in charge of the National Pharmacovigilance Survey of these drugs for the French Medicine Agency. He told me that for several years it’s been known that when GLP-1 agonists are given to rats and mice ‘they have shown an increased risk of thyroid cancer’. It is also known that human beings ‘have GLP-1 receptors in their thyroid tissue’, so it’s conceivable that messing with GLP-1 might mess with your thyroid. So Jean-Luc decided he and his team needed to dig into this. France has one of the largest medical databases in the world, so they went back and analysed the data for all the patients with type 2 diabetes who had taken these drugs for one to three years, in the period between 2006 and 2018. They then compared those patients to a sample of diabetics who had not taken these drugs. Their findings were startling. He said bluntly: ‘We show there is an increased risk of about 50 to 75 per cent more’ of you developing thyroid cancer. He told me it’s important not to misread this. This doesn’t mean that if you take the drug, you have a 50 to 75 per cent chance of developing thyroid cancer. It means that if you take the drug, your chances will be 50 to 75 per cent higher than they would have been had you not taken the drug. Nonetheless, this seemed to me to be a disturbing increase. In most of the commentary on this study, it was repeatedly argued that it was a low risk. I said to Jean-Luc that maybe I was being dumb, but those figures don’t seem low to me. ‘Yeah. It’s not low,’ he said. ‘In epidemiology in general, when you have a 50 per cent increase, it’s quite a thing.’ But then he explained why many scientists would still reasonably continue to describe this as a low risk. ‘The incidence of thyroid cancer is very low. It’s not a very frequent cancer.’ (Currently, around 1.2 per cent of people will get thyroid cancer in their lifetimes, and 84 per cent of them survive it.) ‘So if you increase [levels by] 50 per cent, there is an increased incidence, but it remains low.’ But he added: ‘Given the exposure [of these drugs] to millions of patients, there will be some cases of thyroid cancer that maybe we could avoid.’\n\n\nHari, Johann. Magic Pill (pp. 106-107). Bloomsbury Publishing. Kindle Edition.\n\nSo, Hari appears somewhat baffled by the distinction between relative and absolute risk, the kind of distinction that an hour or two reading and reasoning through the examples and methods proposed by Gerd Gigerenzer decades ago in Reckoning with Risk and similar publications would have helped to clarify. Instead, he appears throughout the book to adopt something like an Oracular Perspective with regards to scientific experts. i.e. he approaches scientists as something like Delphic Oracles, who themselves have access to some kind of arcane knowledge called science that he, a mere mortal, cannot hope to access himself. Instead, the best he can do is find these oracles, ask them questions, and try as best he can to understand the wise but gnostic utterances they deliver in response. In short: Hari doesn’t do science; he speaks to scientists.\nSo can a science writer who doesn’t do science be a good science writer? I guess the answer’s yes and no. (Very gnostic, I know!) Despite largely relying on scientists to ‘do the work’ in terms of making sense of data and findings, Hari’s book manages to be generally well structured and well reasoned, tackling questions about and raised by these drugs and their blockbuster popularity from multiple perspectives: the benefits and risks of the drugs themselves, their apparent popular adoption through channels of downwards cultural diffusion (starting with rich celebrities, then cascading to the middle classes, then hopefully to people who actually most need it), their broader obesogenic environment in Anglophone nations, the conflict between body positivity and acceptance and mitigating obesity-related health harms, and the curious case of Japan and its cultural resistance to Western trends in obesity. Hari manages to cover the right bases, in roughly the right order, and (I expect) draw broadly the right conclusions, while making broadly the right hedges and caveats. All the while still not really doing science.\nAnd now the final qualifier: superior. Why is Hari a superior inferior superior storyteller?\nBecause as a storyteller, he’s bloody good! His book reads very well; it’s a joy to read. He brings personal anecdote (perhaps too much), drama, mystery, complex characterisation, and intrigue into a book ostensibly about stabbing oneself in the belly once a week with some drugs. By presenting scientists as oracles, he presents himself as a journeyman going on a sacred quest full of intrigue and surprise. By centring himself as the journeyman (perhaps too much), he flips back and forwards in time to invest the reader in his origin story as a ‘bad eater’, and the fallen soldiers to obesity he’s loved and lost along the way. Hari knows how to tell a story, the Campbell-style mythic architecture and elements that need to be in place to tell stories that are engaging and compelling, and he manages almost flawlessly to use this ability throughout Magic Pill.\nAnd this ability and commitment to storytelling in non-fiction was, of course, part of Hari’s ‘downfall’ as a British journalist: sometimes Hari had a tendency to prefer the ‘truthy’ to the true. Infamously he was found, on multiple occasions, to have ‘quoted’ interviewees saying things that they did not, technically, say. Instead, having read and researched the authors, he sometimes confected ‘truthy’ quotes from the interviewees that expressed, to Hari’s mind, views he thought the interviewees held, but in more articulate and compelling ways than they happened to express in person on the specific occasion he interviewed them. Hari appears to have engaged in the linguistic equivalent of ‘airbrushing’, attempting to make the words used by interviewees prettier, pithier and punchier than those that were, technically, said and heard. On other occasions he appears to have confected quotations for dramatic effect, to punch up the stakes involved in the narrative. The fact such words were not, ‘technically’, uttered as stated appears to have been secondary to the compulsion to spin a good yarn.\nAnd in the UK, Hari’s wrongdoing was ‘found out’, leading to some frustration and incredulity that, in the USA, Hari’s reputation appears almost entirely untainted. Like Jeremy Clarkson, Hari appears to have ‘failed upwards’, being heavily rewarded rather than punished for his misdeeds. Hence, perhaps, in part (alongside Hari’s unwillingness or inability to ‘do science’), the degree of vitriol and opprobrium directed towards Hari in the Studies Show podcast?"
  },
  {
    "objectID": "posts/unpop/hari-superior-inferior-superior-story-teller/index.html#and-the-magic-pills-themselves",
    "href": "posts/unpop/hari-superior-inferior-superior-story-teller/index.html#and-the-magic-pills-themselves",
    "title": "Johan Hari: Superior Inferior Superior Storyteller",
    "section": "And the ‘Magic Pills’ themselves?",
    "text": "And the ‘Magic Pills’ themselves?\nSemaglutides appear ascendant in the Hype Cycle, and may still not have peaked. So, the medium and long-term effects of this class of drugs being used for weight loss and health improvement is likely to fall significantly short of the most hyperbolic and enthusiastic pronouncements being made about them. However, even a moderate fraction of the hyped peak of expectations will still represent a seismic change to obesity, to obesity-related health harm, and to public health, which could well represent the start of a turning point in adverse trends in the obesogenic environment. These drugs may well turn out to be to obesity as vaping is tobacco: a type of technology that could genuinely break or substantially attenuate the health harms caused by one of the primary ‘avoidable’ causes of morbidity and longevity loss.\nAnd like vaping, there are plenty of reasons to find elements of this ‘solution’ unpalatable. Both are commercially generated solutions to commercially generated problems, and as a result their widespread adoption appears to risk ‘rewarding’ the private sector twice: both with the sale of addictive ‘poisons’ (cigarettes; ultra-processed foods) and with the sale of life sustaining ‘antidotes’ to these very same poisons (vapes; semaglutides). Although in the case of semaglutides the same exact private sector companies aren’t both providers of poisons and antidotes (with the possible exception of manufacturers of some antidepressants, whose side effects can include substantial weight gain and reduction in satiety), the idea that the private sector stands to win twice over - from harming and unharming public health - is clearly from first principles distasteful.\nThen there are wicked issues of efficiency and equity to consider. In the US, manufacturers appear to have successfully price-gouged their way to excess profitability, with prices many times higher than in other high income nations. So, while the health need tends to be higher than in Europe, given higher rates of obesity, access appears, if anything, inversely correlated with need and benefit, not least because in high income nations obesity tends to be a disease of poverty, but this same poverty (together with the absence of an integrated and effective healthcare system) means those with greatest potential to benefit tend to have least means of access.\nAnd in the UK, the situation also appears messy and complex. Like in the USA, the initial adopters may not have been those with greatest potential to benefit, with demand driven more by those seeking slimness than those needing to avoid the worst consequences of fatness. And though the margins appear lower than the US, the market appears largely driven by private spending and consumption rather than benefit maximisation from an epidemiological or public health perspective. And here’s the dilemma for both regulation and public provision. If there’s too much, or the wrong kind, of regulation then a grey or black market in these drugs risks emerging very quickly, leading to even greater variability in quality and consistency in what people are injecting and by which people. If there’s too little regulation, and too little active engagement from bodies like the NHS, then the risks of both abuse and suboptimal use are much increased: abuse in the sense of those with unrealistic body image (such as those at risk of anorexia) using the drugs to cosh their appetite to excess and starve themselves; and suboptimal use in the sense of the main consumers becoming those who do not have the greatest obesity- and (pre)diabetes-related health risks.\nIf, once the soufle of hype collapses, and it’s clear from a realistic evaluation of their effectiveness there’s still enough underlying substance, then this class of drugs has the potential to be to obesity, pre-diabetes and diabetes, as statins are to cardiovascular risk: something taken and prescribed widely, prophalactically, and continually, but to (albeit broad) subpopulations known to have elevated risk. Statins have, within a generation, managed to shift the curves on cardiovascular risk, and are almost certainly part of the reason cardiovascular mortality has given way to cancer mortality as the leading cause of death. But this class of drugs is also different to statins in a number of ways: they are both much more expensive, and have much greater misuse potential. No one takes statins for reasons of vanity, because it leads to changes in their appearance in ways that are generally considered socially desirable; whereas for many users of semaglutides this may be the primary reason to take these drugs."
  },
  {
    "objectID": "posts/unpop/hari-superior-inferior-superior-story-teller/index.html#some-fermi-estimation-would-semaglutides-as-statins-be-affordable",
    "href": "posts/unpop/hari-superior-inferior-superior-story-teller/index.html#some-fermi-estimation-would-semaglutides-as-statins-be-affordable",
    "title": "Johan Hari: Superior Inferior Superior Storyteller",
    "section": "Some Fermi Estimation: Would semaglutides-as-statins be affordable?",
    "text": "Some Fermi Estimation: Would semaglutides-as-statins be affordable?\nLet’s do a little bit of Fermi estimation to get a sense of the scale of the cost of broad spectrum NHS prescription of semaglutides in a similar way to statin adoption: In the UK, around one in four adults is obese. If we take the prices here as broadly indicative of cost, then at £200 a month the prophalctic use of such drugs for an obese person will be around £2400 a year. Let’s say instead the mass purchase of such drugs can lead to a halving of the costs of procurement, and to simplify say each individual’s treatment ‘only’ costs £1000 a year. If the adult population is around 56 million, and one quarter are obese, then there are around 14 million obese people. If the NHS were to use semaglutides similarly to statins, then the upper limit of the annual cost will be around £14 billion. According to this source the NHS England budget is around £155 billion. Let’s therefore assume the UK NHS budget is around £200 billion.\nVery approximately, the adoption of a semaglutides-as-statins policy by the NHS could therefore increase the annual running costs to the NHS, at least in the short term, by around 5-10%. This is actually somewhat less expensive than I was expecting, with price of course being the critical factor. If the prices charged in the US were charged here, then this could balloon an order of magnitude: 50-100% of NHS budget! I guess mass public purchase of these drugs could both reduce the unit cost, through mass discounting, but could also (in the shorter term) increase the cost, by increasing demand against finite supply. And of course not everyone with obesity may take up an offer of semaglutides even if free at the point of use for them, which would reduce both the longer term population effectiveness but also the shorter term cost to the public purse.\nSo, something like an NHS adoption of these drugs as obesity and diabetes analogues to statins would be expensive, but wouldn’t necessarily be unaffordable. However it would likely be expensive enough that it would have to be a political decision more so than a decision made by the NHS alone. Its adoption could be a quintessential example of spend-to-save, with reductions in incidence and treatment of downstream consequences of obesity and diabetes potentially leading to it becoming cost saving to the NHS within a few years. Unfortunately, both the current Conservative government, and the overwhelmingly likely next government, appear similarly committed to further austerity, and to a somewhat naive and simplistic way of thinking about public spending that fails to distinguish between spending on investment - which are likely to either save money in the longer term, or reduce the ratio of public debt to GDP by encouraging growth - and other sources of spending, even though even Andy Haldane, the former Chief Economist at the Bank of England, believes the government needs to make this distinction, and spend much more on investment.\nA political commitment to increase the NHS budget by a tenth in the short term, with a reasonable expectation that this increased short term cost could become cost neutral for the NHS within a decade, and have substantial and positive growth spillover effects in the broader economy through improved working age health and productivity, could be a very wise commitment to make. However, this commitment seems far from what currently on the cards, and instead a continuation of further de facto privatisation of UK healthcare, due to ever poorer service quality leading to those with the means to go private to avoid waitings lists that are now in the millions, sadly appears more likely.\nAll the above speculation depends on other unknowns and unknowables too, however. Perhaps after the hype has collapsed it’ll turn out these drugs are much less effective, and much more harmful, than currently believed. More wickedly, perhaps the drugs do turn out to be effective, greatly increasing satiety and so reducing demand for ultra processed food, but as a consequence the economy shrinks rather than grows, because more of it (e.g. Domino’s and Deliveroo) depends on feeding people the junk food that makes them sick, than benefits from the population being well?!"
  },
  {
    "objectID": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html",
    "href": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html",
    "title": "Jason Statham vs the Nepobaby Cyberscammers",
    "section": "",
    "text": "“Expose the Corruption. Protect the Hive”\nBack in January, I saw The Beekeeper at the local cinema.\nThe whole point of a film like The Beekeeper is not to think too much. But as usual, given I’m still thinking about aspects of it over two months later, on that front I’ve failed. Let’s explore why."
  },
  {
    "objectID": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#what-is-a-jason-statham",
    "href": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#what-is-a-jason-statham",
    "title": "Jason Statham vs the Nepobaby Cyberscammers",
    "section": "What is a Jason Statham?",
    "text": "What is a Jason Statham?\nJason Statham is not a good actor. Or at least, not an actor with great versatility or dramatic range. Recently I found another Statham-fronted film attacking my eyeballs at home: Guy Ritchie’s Operation Fortune: Ruse de Guerre. Everything about Statham’s acting ability can be understood through the way he delivers the following line in the above film:\n\nYou’re a spineless jellyfish!\n\nStatham emphasises the adjective, spineless. But surely, as an invertebrate, a jellyfish is by definition spineless. The adjective adds no information, and stressing it only highlights the emptiness of the qualifier. An actor with even an iota of interest or understanding of dialogue would have realised that, even without license of ad-lib - “You’re spineless. A jellyfish.” - either a flat reading, or a stress on the noun, would have made the made the stupidity of the line slightly less obvious.\nBut Statham isn’t employed as a good actor in the traditional sense. Instead, he’s employed as a physical presense, a looming, fulminating broad-shouldered block of hardened meat and bone; small, pillbox eyes defensively shielded behind a thick bony brow-ridge and a slightly flattened nose, inset inside a head with bald pate at the top, and square jaw at the bottom that’s permanently sprouting thick five o’clock shadow. All of which sits atop a vehicle of wide shoulders, broad chest and heavy arms which culminate in two permanently clenched fists.\nSo, Statham is employed as an exemplar of a violence delivery system. And the appeal for the audience is in seeing this system in action, being used to deliver violence against adversaries whom ‘we know’ should be the recipients of relentless brunt force trauma, often in terminal doses."
  },
  {
    "objectID": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#the-adversaries",
    "href": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#the-adversaries",
    "title": "Jason Statham vs the Nepobaby Cyberscammers",
    "section": "The Adversaries",
    "text": "The Adversaries\nWhich brings me back to The Beekeeper. What defines a target of violence as ‘deserving’, and so the violence itself as being ‘righteous’ rather than simply unjust or criminal? And given the social and political fissures that exist in modern society, for which potential targets will there be sufficient implicit unanimity amongst the audience - of all political stripes - that the vicarious sadistic pleasure of seeing harm done to people will never be tinged or sullied by a sense of sympathy towards those on the receiving end?\nFor The Beekeeper, and as the title of this post suggests, the answer is nepobaby cyberscammers.\n\nNepobaby: Someone with unearned privilege, without obvious ability or talent, who is in a position of power simply because they were fortuitous in having rich and talented parents who give them opportunities they do not deserve, and which in a more just world more deserving people would have had instead.\nCyberscammers: Someone who cons and manipulates people into giving personal details which allow their money or assets to be stolen, often using social engineering and psychological manipulation. This frequently involves pretending to the victim to be the exact opposite of what one is, such as a bank or agency charged with protecting the victim against cyberscamming.\n\nSo, unlike with the line ‘spineless jellyfish’, both components of the descriptor of the adversary do work to tell the viewer why the characters being served up to get hurt deserve to get hurt. The nepobaby cyberscammers of The Beekeeper not just start off with undeserved wealth, but actively seek to gain yet more. And they do so in a way that creates clear and innocent victims, whom they scare and manipulate through bold-faced lies, acting by turns with sociopathic indifference and sadistic pleasure at the misfortune of their victims. They are an unmixed evil, parasites, harming others in the pursuit of sheer unquenshable greed."
  },
  {
    "objectID": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#action-films-as-revenge-dramas",
    "href": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#action-films-as-revenge-dramas",
    "title": "Jason Statham vs the Nepobaby Cyberscammers",
    "section": "Action Films as Revenge Dramas",
    "text": "Action Films as Revenge Dramas\nThe Beekeeper, like a great many low brow but popular films often described simply as ‘Action’, is really another example of a concealed genre. In this case, the Revenge Tragedy or Revenge Drama. This genre has existed in English literature for hundreds of years, and includes such Shakespearean masterpieces as Hamlet and Titus Andronicus.1 As Hollywood Action flicks, many of the subtleties and overt tragedy of the old lineage have been streamlined away, leaving a simple formula that, once the following event and character archetype placeholders have been filled, can be endlessly permuted and performed.\n\nAn Innocent Victim has a Great Injustice done to them by a Machiavellian Villain, leading to Irreperable Harm (usually death) being caused to the victim. A Revenger becomes aware of the Great Injustice done to the Innocent Victim, and agrees to take on a sacred duty to undertake a vengeful mission to balance the Great Injustice by doing Irreperable Harm to the Machiavellian Villain.\n\nIn the case of The Beekeeper, the Innocent Victim is the titular character’s landlady, a kindly retired woman who acts as treasurer for a charity. The Great Injustice done by the Machiavellian Villain, the Nepobaby Cyberscammer-in-chief, is that she is conned out of not just her life savings, but those of the charity too, leading to the Irreperable Harm that she takes her own life.\nIn the Hollywoodised retelling, the Revenger is a strange kind of Hero, and so aspects of Campbell’s Monomyth may be involved. The sacred duty is much like the Call to Adventure, and as with the Call to Adventure the Revenger may initially reject or resist this call (“I don’t do that anymore”). But ultimately they do perform their sacred duty, bringing justice to the world by balancing harm with harm.\nThe Revenger differs from the Hero in at least two ways, however. Firstly they are pre-transformed, already equipped with both the scars and the abilities gifted them by previous adventures. The Revenger may even have been a Hero in the past, an ex-Hero, someone past their prime, now looking into desolate oblivion and mediocrity. And that brings us to the second main point of difference: The Revenger acts not for their own betterment, not to improve themself, but with ultimately altruistic purpose, even if in so doing they sacrifice themself. In The Beekeeper this idea is captured by the phrase “protect the hive”, delivered with Statham’s usual gruff hamminess.\nSo, if this is the basic structure of the modern Revenge Drama, let’s consider some variations in firstly the Innocent Victim placeholder, and then in the Machiavellian Villain placeholder."
  },
  {
    "objectID": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#gender-and-the-innocent-victim",
    "href": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#gender-and-the-innocent-victim",
    "title": "Jason Statham vs the Nepobaby Cyberscammers",
    "section": "Gender and The Innocent Victim",
    "text": "Gender and The Innocent Victim\nBoth historically and contemporaneously, both the roles of Innocent Victim and Revenger have been highly gendered: the Innocent Victim is usually female, and the Revenger usually male. This has given rise to a tired literary trope, coined by Gail Simone in 1999 as the Women in Refrigerators trope. In stories that use this trope, the violent death of a woman occurs early on in the first act, and this forms the motivation for a male character to become a Revenger for the rest of the story.\nWhen the Innocent Victim and Revenger placeholders are filled in along these gendered lines, it generates some genuinely moot debates about the extent to which such stories are sexist, and if so to which sex. By moot, I mean this in the legal sense: a debate in which the facts on both sides appear similarly strong, and so which side wins is likely to depend primarily on the presentational and rhetorical qualities brought by both the counsel for the prosecution or counsel for the defence.\nThe argument for why the trope is sexist against women is that it sidelines women into mere plot devices used to tell stories about men going on adventures. The women are expendable in these stories; the men are indispensible. As long as the hero survives their adventure, they could always get a new girlfriend or wife, but the woman killed can never get her life back (though may occasionally appear as an apparition reminding the Hero of his need to ‘finish the job’ at times of despair and exhaustion). Additionally, the telling of these stories may relish, or appear to relish, in describing exactly how the victim was tormented and tortured to death in sadistic detail. Finally, the reader or viewer may get the impression that the innocent female victim is not always selected by the storyteller at random, but perhaps because she has exhibited certain qualities, such as trying to act with independence and agency, which the storyteller believes should be reserved for the male lead.\nBy contrast, the argument for why the trope is sexist against men seems to be that both men and women alike consider, all else being equal, men to be worth less than women. So, the death of a male friend or relative of a male protagonist would be understood to be less of an incentive to become a Revenger than the death of a female lover, friend or relative. This argument builds on what’s known as the Male Expendability Hypothesis which, according to Wikipedia, is “the idea that the lives of human males are of less concern to a population than those of human females because they are less necessary for population replacement.” This argument helps not just explain why the Innocent Victim in these stories are female, but also why the Revenger in these stories are male: it is the male Revenger who is willing to sacrifice himself for the good of the populace because he, and society at large, implicitly understands that he is worth less to society than the woman who has already died.\nAs mentioned, I consider this argument genuinely moot, by which I don’t mean such discussions aren’t useful to have, but do mean it can’t be resolved to the satisfaction of both parties. It’s also likely to be the case that, for various specific examples, both arguments are correct. The Revenge Drama can be both sexist against woman, because it considers women’s stories to be less narratively interesting and important, and also sexist against men, because it implicitly condones and promotes the myth of Male Expendability in society at large.\nWhat does seem to have come about from this debate, however, are at least some attempts to apply the Innocent Victim placeholder in slightly less cliched ways. Famously, for example, the Innocent Victim that lights John’s Wick is his dead pet dog, not his wife or girlfriend.\nIn The Beekeeper, there is a slight adaptation of the trope. As The Revenger’s landlord, the Innocent Victim is in no way financially dependent on him, and their relationship is strictly platonic (and in Statham’s case thankfully laconic too). And it’s the Innocent Victim’s daughter, a police officer, who reminds the Revenger of his sacred duty. In general, however, The Beekeeper’s filling in of the Innocent Victim placeholder is not particularly original, and largely follows the gendered conventions as described above."
  },
  {
    "objectID": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#the-villain-of-a-thousand-faces",
    "href": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#the-villain-of-a-thousand-faces",
    "title": "Jason Statham vs the Nepobaby Cyberscammers",
    "section": "The Villain of a Thousand Faces",
    "text": "The Villain of a Thousand Faces\nNo. What I found interesting about The Beekeeper is its choice of Machiavellian Villain. Why Nepobaby Cyberscammers? From a narrative perspective, this choice seems to complicate things unnecessarily. As Nepobaby, the main villains are painted as so lacking in cunning and competence they should make for an effortlessly easy adversary. Instead, it’s the Nepobaby’s (symbolic) Father and (literal) Mother 2 figures who provide the necessary strategising and challenge required for the revenge fantasy to be feature length.\nSo again, why Nepobaby Cyberscammers? This is where, for me, it gets interesting. My guess is that, if The Beekeeper came out early 2024, it was probably first conceived and pitched in early to mid 2021.3 This means the core story, including the choice of Machiavellian Villain, must have absorbed some of the dominant societal concerns at the time. And this was a time when both trust in politicians and the government was low, and when - due to ever more people working remotely - the ease with which cyberscammers could pretend to be people’s bosses, banks, or other institutions, was much increased. With so much commerce and business taking place remotely, and so many people using their own computers and set-ups, rather than working in offices whose access strictly controlled by security and IT departments, there was probably never a better time for cyberscammers to ply their trade.\nAnd that got me thinking about something broader still about the genre, and why it might be worth paying attention to who the Machiavellian Villains they include, and how they change over time. My modest proposal is broadly as follows:\n\nIn Revenger’s Dramas, the variation over time in Machiavellian Villain placeholder tends to be much greater than the variation in Revenger. Additionally, the choice of Machiavellian Villain will tend to be reflective of concerns shared implicitly by either majority, or sizeable minority, of the general population.\n\nSo, that’s my modest proposal. Revenger’s Dramas are, potentially, societal litmus tests, showing changing trends in the dominant concerns that people in a society have at different points in time. And if The Beekeeper, and its commercial success, is anything to go by, it seems street gangs, yuppies, drug dealers and Russians are now out of the Revenger’s crosshair; and corrupt politicians, deep state bureaucrats, nepobabies and cyberscammers are in."
  },
  {
    "objectID": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#coda-yes-revenge-dramas-are-problematic",
    "href": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#coda-yes-revenge-dramas-are-problematic",
    "title": "Jason Statham vs the Nepobaby Cyberscammers",
    "section": "Coda: Yes, Revenge Dramas are ‘problematic’",
    "text": "Coda: Yes, Revenge Dramas are ‘problematic’\nIn the description above I’ve tried to discuss the modern Hollywood Action film as a streamlined descendent of a long lineage of Revenge Tragedies, which includes such literary classics as Shakespeare’s Hamlet. However, in this description I’ve aimed not to offer any judgement about whether I consider the popularity of the genre to be a ‘good’ or ‘bad’ thing from any moral or ethical perspective. Instead, I’ve aimed to discuss the genre more in terms of the mechanics of storytelling.\nA general standpoint I hold is that popular, mainstream stories - low-brow, ‘trashy’ popcorn flicks, and so on - should generally be considered more important to engage with and understand than stories that tend to appeal only to limited and niche audiences. This is because popular stories can only be popular if they are somehow able to speak to and resonate with widely held beliefs and ways of thinking about the world. It’s Hollywood, not Arthouse, that tends to provide the window into more people’s inner worlds. And if a Hollywood genre really does seem to be a continuation of the telling of stories that have been told continuously for hundreds or thousands of years, and that somehow keeps getting independently reinvented across disparate communities across the world, then perhaps they help us glimpse something not just about ourselves now, but at that elusive and contestable quality known as human nature.\nAnd for the Action flick, the origins don’t just start with Shakespeare. Of course, he didn’t get his ideas from nowhere. Before there was Hamlet, there was Amleth, the story of a Viking prince avenging his father’s murder, which preexisted Hamlet as a written text for at least two centuries, and as part of the oral tradition for perhaps hundreds more.\nThe story of Amleth was adapted into the 2022 film The Northman, a bold, bloody, self-serious film with a Hollywood budget but Arthouse sensibilities and appeal. In The Northman, revenge is taken not just against the direct party that wronged him, but against his family and friends too. Amleth has no qualms about collective punishment, and if I remember correctly kills women and children affiliated with the nominal villain simply to psychologically harm him. No target is off limit, and no act considered too cruel for this primordial Revenger.\nTo conscious modern sensibilities, “an eye for an eye” is considered a barbaric call to violence. But stories like Amleth suggest that, at one time, it may instead have been a call for restraint, a counsel against blinding not just the individual who wronged you, but their family too, and maybe just anyone who looks, speaks or dresses in any way similar to them. When violence against an individual is considered justified, just a couple of steps beyond is the justification for violence against ever expanding circles of groups that contain the individual at their centre: their direct family, their close friends, their tribe, their clan, their creed and their race. Step too deeply into the psychological appeal of the modern Action flick and you find untethered barbarism and genocide."
  },
  {
    "objectID": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#footnotes",
    "href": "posts/unpop/jason-statham-vs-the-nepobaby-cyberscammers/index.html#footnotes",
    "title": "Jason Statham vs the Nepobaby Cyberscammers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo clarify: The Beekeeper is not a masterpiece.↩︎\nAnother intriguing choice that follows from selecting Nepobaby Cyberscammers as the main antagonist is the choice of parental occupation. For The Beekeeper the parents are in politics and government. And the mother figure is imbued with political attributes of both Donald Trump and Hilary Clinton, leading to a Rorcharch-like political composite. Republicans will look at this character and think, “Yup, that’s Hilary”, whereas Democrats will look at the same character and think, “That’s The Donald in a Dress”, and both types of viewer will assume it’s the other guy being weakly satirised.↩︎\nI just checked. According to the Wikipedia article it was first announced in August 2021.↩︎"
  },
  {
    "objectID": "posts/unpop/obama-dialectic-identity/index.html",
    "href": "posts/unpop/obama-dialectic-identity/index.html",
    "title": "Thoughts on Dreams from My Father",
    "section": "",
    "text": "Dreams from my father"
  },
  {
    "objectID": "posts/unpop/obama-dialectic-identity/index.html#on-edinburghs-brilliant-book-boxes",
    "href": "posts/unpop/obama-dialectic-identity/index.html#on-edinburghs-brilliant-book-boxes",
    "title": "Thoughts on Dreams from My Father",
    "section": "On Edinburgh’s Brilliant Book boxes",
    "text": "On Edinburgh’s Brilliant Book boxes\nOne of the first differences I noticed about living in Edinburgh, as compared with Glasgow, is the presence and use of free book boxes. Little boxes where people are invited both to donate books they’ve finished with, and pick up books they might be interested in reading. Such boxes appear to be both well used and - aside from the occasional spray can tag - well cared for in Edinburgh, whereas (and I really want to be shown to be wrong on this point) I suspect their life expectancy in Glasgow - the time before their doors get ripped from their hinges and contents thrown into the river - would be measured in the days or weeks.\nWhat the free book boxes provide, in addition to free books (and often free DVDs), are windows into past trends. Books that, five or ten or twenty years ago, everyone bought, but now whose entertainment or informational value, and definitely value as cultural signifier, has long been spent for the original owners. So of course, this provides an opportunity for people who didn’t latch onto the reading trends of yesteryear to try, very belatedly, to try to work out what the fuss was about first (or second) time around.\nAll of which is a contextual preamble for why I’ve now, not ten or twenty years ago, read Barack Obama’s Dreams from My Father, the book he was first invited to write in the early 1990s, after becoming the first Black president of the Harvard Law Review, originally published in 1995, then reissued - as attention renewed and intensified with his broader political ambitions and achievements - in 2004. So, what do I think about it?"
  },
  {
    "objectID": "posts/unpop/obama-dialectic-identity/index.html#on-the-books-three-sections",
    "href": "posts/unpop/obama-dialectic-identity/index.html#on-the-books-three-sections",
    "title": "Thoughts on Dreams from My Father",
    "section": "On the book’s Three Sections",
    "text": "On the book’s Three Sections\nAs the subtitle of this post suggests, Dreams From My Father seems to me largely a dialectical treatise on questions of ethnic and cultural identity, from someone whose personal and family history offer legitimate claims to multiple identities. The book has three broad sections - ‘Origins’, ‘Chicago’, and ‘Kenya’ - and in each section a different aspect of Obama’s identity is explored.\nThe first section, Origins, follows Obama’s childhood, raised primarily by his White American mother in Indonesia, then later also by his mother’s extended family in Hawaii. Capital poor but culturally and cognitively rich, Obama’s earlier experiences read, paradoxically, of those of an upwardly mobile and aspirant migrant family - with grand ambitions and hopes for their future and their children’s future - drawn to the promise of the US as a fundamentally meritocratic society where hard work and talent is rewarded. And indeed, for Obama, it is. However, Obama is a visible minority in Hawaii, and as Hawaii, though off the mainland, is still part of the USA, the apparently totalising binary thinking about race that predominates elsewhere - that one is either Black or White, and if one’s at all black one’s all Black (i.e. the ‘one-drop rule’) - is something that Obama experiences, and so questions of racial identity become ever more salient. Obama leans into this binary, for example chastising another mixed heritage student who describes herself as ‘mixed’ rather than ‘Black’, and becomes drawn ever more into the history and politics of the African American experience.\nThe second section, Chicago, describes the apotheosis of this quest, and Obama’s work as a community organiser in predominantly Black and socioeconomically deprived neighbourhoods of Chicago, for which he quit a white collar job in which he was rapidly advancing. He’s given a way into this form of applied activism by Marty Kaufman - of New York, Jewish, and in the American Race Binary ‘White’ - but due to his own ethnic appearance Obama is more easily accepted into the communities he aims to serve than Kaufman; he also repeatedly claims or implies his own ethnicity means he has a more genuine connection and commitment to such communities. Here Obama encounters what appears to be the two inseperable sides of the same coin of working class African American experience: on the one hand, the high bonding capital that gives rise of racial solidarity; on the other hand, the low bridging capital that arises from suspicion of outsiders (especially those who are coded as ethnically other), as well as a highly localised and seemingly parochial focus and set of concerns. The call of new white collar career opportunities is heard by Obama, and those Chicago denizens he helped through community organising tend to be both grateful for his support, but unsurprised by his impending departure.\nThe third section, Kenya, follows Obama as he visits Kenya to meet members of his extended family on his father’s side. He encounters the realities of privation and service quality in a nation substantially poorer than the USA, an apparent (and likely at times genuine) enthusiasm for his visit from persons who can claim some degree of common geneology, but also a sense - perhaps - of being even marked out as Other in Kenya, as a result of his accent, clothing and relative affluence, than he was in either Hawaii or Chicago. He also discovers just how fictitious aspects of the Pan-African Mythos that many African Americans developed and clung onto tends to be, the dream of visiting an ancestral land where almost everyone is Black, and so by extension Black people are more in charge of their own destiny than in the USA. From this comes an explanation for why many of those African Americans who visit Africa leave feeling dissilusioned and disappointed. The issue seems to be that, for the most part, in Africa ‘Black’ is not a meaningful category of belonging. Instead the most salient aspects of an individual’s identity are likely to be tribal, or more broadly extra-familial. In Kenya Obama is not Black as in the USA, but alternately Luo and Western (and so by implication rich)."
  },
  {
    "objectID": "posts/unpop/obama-dialectic-identity/index.html#neurodiversity",
    "href": "posts/unpop/obama-dialectic-identity/index.html#neurodiversity",
    "title": "Thoughts on Dreams from My Father",
    "section": "Neurodiversity",
    "text": "Neurodiversity\nThe beliefs that identities should be both more expansive and less determinative appear to have served Obama’s titular father poorly. Despite being both driven and intellectually gifted - a Harvard trained economist - the fortunes of the Old Man, as his extended family referred to him, depended much less on his gifts and experience, and much more on tribal politicking, and whether his tribe’s political power was in the ascent or descent. Once he shifted employment from an American oil company to the government, then his identity as Luo stood him in good stead when the Luo were ascendent, but poorly when Kikuyu were ascendent. And his fortunes worsened further when he alienated both Luo and Kikuyu alike by arguing against tribalism, and for a broader Kenyan identity and solidarity. The Old Man - who didn’t live long enough to really deserve the moniker - died fairly soon after the tides had started to turn back in his favour, leaving a small estate but - as we now know - a world-changing genetic legacy.\nWhen it comes to the search for the links between Obama’s (the Second’s) heritage and identity, one of the most obvious answers seems almost to be hiding in plain sight, in an extended oral geneological account near the end of the book. Obama (the son) was extraordinarily capable and driven because Obama (the father) was extraordinary capable and driven, and Obama The Father was this way because his own father, Onyango, was perhaps even more extaordinary:\n\nEven from the time that he was a boy, your grandfather Onyango was strange. It is said of him that he had ants up his anus, because he could not sit still. He would wander off on his own for many days, and when he returned he would not say where he had been. He was very serious always - he never laughed or played games with the other children, and never made jokes. He was always curious about other people’s business, which is how he learned to be a herbalist. You should known that a herbalist is different from a shaman - what the white man calls a witch doctor. A shaman casts spells and speaks to the spirit world. The herbalist knows various plants that will cure certain illnesses or wounds, how to pack a special mud to that a cut will heal. As a boy, your grandfather sat in the hut of the herbalist in his village, watching and listening carefully while the other boys played, and in this way he gained knowledge.\n\n\nWhen your grandfather was still a boy, we began to hear that the white man had come to Kisumu town. It was said that these white men had skin as soft as a child’s but that they rode on a ship that roared like thunder and had sticks that burst with fire. Before this time, no one in our village had seen white men - only Arab traders who sometimes came to sell us sugar and cloth. But even that was rare, for our people did not use much sugar, and we did not wear cloth, only a goatskin that covered our genitals. When the elders heard these stories, they discussed it among themselves and advised the men to stay away from Kisumu until this white man was better understood.\n\n\nDespite this warning, Onyango become curious and decided that he must see these white men for himself. One day he disappeared, and on one knew where he had gone. Then, many months later, while Obama’s other sons were working the land, Onyango returned to the village. He was wearing the trousers of a white man, and a shirt like a white man, and shoes that covered his feet.\n\nIn modern terminology, therefore, Obama’s grandfather might be described as neurodiverse, drawn to understanding how technologies work and how he could make these technologies work for him. The same orientation towards the world that led to an interest in the tribal technology of herbalism also led to understanding the advanced technologies - literacy, numeracy, administrative systems, transport, weaponry - of these strange and sinister colonialists; advanced technologies that he quickly came to understand were all that fundamentally differentiated these people from his own, and which were the source of the power inbalance between the two peoples. Like a one-man Japan, Onyango appeared to devote much of his life to understanding, mastering and applying these colonial technologies, but ultimately with a view to narrowing the power imbalance between Luo and colonialist, and so reducing the ease with which the latter could exploit and subjugate the former.\nAnd so, this strange herbalist begot a Harvard educated economist, and the Harvard educated economist begot a Harvard educated lawyer, who became the 44th president of the USA."
  },
  {
    "objectID": "posts/unpop/obama-dialectic-identity/index.html#dialectics-and-the-law",
    "href": "posts/unpop/obama-dialectic-identity/index.html#dialectics-and-the-law",
    "title": "Thoughts on Dreams from My Father",
    "section": "Dialectics and the Law",
    "text": "Dialectics and the Law\nIn the subtitle of this post I’ve described Dreams from my Father as dialectical. It’s perhaps worth clarifying what I mean by this, as well as why Obama’s dialectical treatment of issues like identity appears to fit with a bias towards system thinking that appears, in part, to have a geneological component.\nA dialectical argument involves a Thesis, and Antithesis, and then a Synthesis. A Thesis is a clearly expressed and pure position on something, an Antithesis is an equally clearly expressed and pure position on something that appears to be incompatible with the Thesis; and a Synthesis is the consequence of, nevertheless, finding a means of reconciling both Thesis and Antithesis.\nFor a trained lawyer, dialectical argumentation is something that likely quickly becomes second nature. What is the Thesis but the case put forward by the prosecution? And what is Antithesis but the case put forward by the defence? And then, to the extent not all charges have to be unanimously proven or not proven, what is a judge’s ruling but a Synthesis of the facts and arguments as put together by both sides?\nLaw, at least in the Common Law tradition as practiced both in the UK and USA, is accretive and incremental. The laws as passed by primary lawmakers, i.e. politicians, are broad sketches. The precise blueprints which emerge from trying to operationalise these sketches come from case law. What does the broad principle actually mean in this circumstance? And is there an existing piece of case law that’s similar enough to what we’re looking at that, in this case, we have the blueprint and not the sketch? As the number of test cases increases, so does the behavioural resolution of society. Eventually, there’s a memory and a map for how society should behaviour, fairly and responsibly, for every social eventuality.\nFor someone practiced in this tradition, this mode of reasoning, the concept that “the arc of history bends towards justice” just about makes sense. By allowing claims and counterclaims to be expressed as clearly as they can, and finding and supporting impartial judges who can weigh and synthesise both sides, something like progress just keeps happening. This is the cautious hope of the progressive centrist, and differs markedly in its incrementalism from the radical conception of history and dialectics associated with Marx through Hegel, in which the arc of history doesn’t so much bend towards justice, as break towards justice, the thesis of each epoch crashing down violently under the accumulated weight of its antithetical contradictions. Bending is gentle change; breaking is violent change."
  },
  {
    "objectID": "posts/unpop/obama-dialectic-identity/index.html#dreams-and-nightmares-of-a-radical",
    "href": "posts/unpop/obama-dialectic-identity/index.html#dreams-and-nightmares-of-a-radical",
    "title": "Thoughts on Dreams from My Father",
    "section": "Dreams (and Nightmares) of a Radical",
    "text": "Dreams (and Nightmares) of a Radical\nDreams from my Father appears frequently to try to flesh out, understand, sympathise with and inhabit a range of distinct - and apparently contradictory and irreconcilible - notions of self identity: White, middle-class, internationalist; Black, urban, working class; African, Kenyan, Lou; a father’s son, a grandfather’s grandson, a mother’s son. Obama’s preferred first name changed over the course of the three sections: From ‘Barry’, to ‘Barack’, then back to ‘Barry’ again. This quality of bringing intense, analytical curiosity and equinimity towards each facet of a complex question, then offering a position of synthesis without tarnishing or distorting any facet, was perhaps one of Obama’s greatest gifts as President.\nWhen it comes to race in the USA, however, the cognitive cultural dominance of the One Drop Rule - that if one is at all black one is All Black - perhaps meant that the part was sometimes conflated with the whole. Rather than genuinely accepting Obama as an extraordinary man of many facets and identities, he was ‘just’ the USA’s First Black President. Many people, it seemed, couldn’t see beyond this concept, with both supporters and critics alike thinking the simple fact of being a Black President connoted a much greater degree of radical departure from the past - a breaking not a bending of the arc of history - than this studious, cautious lawyer with a baritone voice would ever have been willing to deliver."
  },
  {
    "objectID": "posts/unpop/dune-pt-1-after-dune-pt-2/index.html",
    "href": "posts/unpop/dune-pt-1-after-dune-pt-2/index.html",
    "title": "My thoughts on Dune (Part 1)",
    "section": "",
    "text": "I’ve just finished watching Dune (Part 2). Rather than trying to write something right now, while my imagination and eyeballs and earholes are recovering from getting sandblasted for three hours, below’s something I wrote after watching Dune (Part 1).\nSince watching the first film, I read the book, then the next book, then (I think) the book after that. The momentum ran out, however, somewhere in God Emperor of Dune, even though the basic conceit of the fourth book had the chimera-like appeal I discussed briefly in this post on Robocop.\nAnyway, without further ado, here’s what I wrote after watching the first Dune film, back in 2021:"
  },
  {
    "objectID": "posts/unpop/dune-pt-1-after-dune-pt-2/index.html#background",
    "href": "posts/unpop/dune-pt-1-after-dune-pt-2/index.html#background",
    "title": "My thoughts on Dune (Part 1)",
    "section": "",
    "text": "I’ve just finished watching Dune (Part 2). Rather than trying to write something right now, while my imagination and eyeballs and earholes are recovering from getting sandblasted for three hours, below’s something I wrote after watching Dune (Part 1).\nSince watching the first film, I read the book, then the next book, then (I think) the book after that. The momentum ran out, however, somewhere in God Emperor of Dune, even though the basic conceit of the fourth book had the chimera-like appeal I discussed briefly in this post on Robocop.\nAnyway, without further ado, here’s what I wrote after watching the first Dune film, back in 2021:"
  },
  {
    "objectID": "posts/unpop/dune-pt-1-after-dune-pt-2/index.html#thoughts-on-dune-written-311021",
    "href": "posts/unpop/dune-pt-1-after-dune-pt-2/index.html#thoughts-on-dune-written-311021",
    "title": "My thoughts on Dune (Part 1)",
    "section": "Thoughts on Dune (Written 31/10/21)",
    "text": "Thoughts on Dune (Written 31/10/21)\n\n\n\n\n\n\nDune Part 1: Taking Silly Seriously\n\n\n\n\n\n\n\nDune (not) by David Lynch: Seriously Surreal\n\n\n\n\n\nRecently I saw Dune. The new version, Denis’ (without an s) version. You know, the director who takes really prestigious 80s sci-fi films and tries to make them more prestigious, more grand, more ‘authentic’ to the directors’ and the writers’ and the fans’ vision of what the story really is than it ever was before? You know, the director of Bladerunner 20-40-something, featuring the replicant killer with the boyish face and the big sad eyes, and his platonic sex hologram virtual assistant whom he can’t have sex with, because she’s virtual, and so she pimps out a human prostitute for him as a welcome home gift, the version featuring bright rural bits and hazy rubbish bits of the Bladerunner universe as well as the dirty dark city bits from the first film? You know, the film where that allegedly predatory rock star chewed carpet for at least twenty excruciating minutes, even though the carpet was computer rendered, because of course that’s how a self-aggrandising rockstar-engineer god would behave? You know, the film with an ageing Harrison Ford reprising his role from the first film, looking and moving oddly stiff and fragile, not so much because his character’s way past his expiry date, but because of the arthritis that comes with human senescence? You know, that Denis-without-an-s?\nAnyway, this time he’s remade another 80s sci-fi film, based on a 60s sci-fi book. The 80s film was made by David Lynch. Except it wasn’t. Lynch took his name off the film, and the version we saw then was made by a shadowy cabal of Money Men, who wanted another Star Wars, and didn’t take kindly to an arthouse director known for visually grotesque surrealism picking out the visually grotesque images, ideas and senses from the source material and spending millions of dollars building sets and props that showed, in bright Technicolor glory, just how visually grotesque and surreal they are. When I saw the 80s film, rewatching in full over one of many timeless lockdowns, I saw a beautiful hot mess. Lynch’s scenes stick out, claw to the mind, try to claw out, from inside the Money Men’s rushed film. A giant space-folding worm-baby in a tube, with a rubbery vagina-face, flanked by brightly-colored avant-garde priests in full regalia - that’s Lynch. An obese man who hovers silently to tower over a serve with a conveniently installed heart valve, which he pulls like a bathroom plug, causing blood to gurgle out like a low pressure tap - that’s Lynch. A skinny and mute Sting dancing with a knife in a loincloth - that bit’s Lynch. The fifteen minute montage of Kyle Mclochlan dressed in a space suit, flanked by others similarly attired, pointing a sound gun at stumbling masked legions and shouting ‘kaplow!’ (or words to that effect) to make the bad guys fall over - that’s not Lynch.1 That’s the Money Men, looking to turn the rushes back into the Star Wars-like swashbuckler they wanted, and hoped,desperately, would bring a positive return to their investment. The Money Men rushed with the rushes, demanding that a very big book get turned into a not-too-long film. That’s why the 80s film isn’t just David Lynch weird, it’s weird weird.\nDenis-without-an-s’ Dune doesn’t rush. It’s much better paced, because it stops midway. It only covers about half the story. It’s a promise of a sequel, and a promise I hope gets kept, if just for curiosity’s sake. Denis-without-an-s’ Dune is a swashbuckler, however, and I think that’s because Dune is a swashbuckler, as well as quite a few other things too.\nHow do space swashbucklers work? I mean, if it’s space, it’s probably the future, with advanced technology, and so advanced weaponry. We know in our world that no army was won a battle or war with blades and clubs when the other side has bullets and rockets, so why in a future setting (or in Star Wars’ case an as-if-future setting, preamble notwithstanding) would melee weapons, rather than projectile weapons, tend to predominate? In Dune’s case, it’s because of a magic shield, a personal force field2 whose mechanism of action is never properly explained but whose properties are that, the greater the force, the greater the resistance. Or maybe it’s the greater the speed, the greater the deflection. Again, it’s not well explained. It’s advanced technology, advanced so far as to effectively be magic, and therefore beyond explanation. To the extent it works like anything we have, it seems to work like a Non-Newtonian fluid, like oobleck (cornstarch suspended in water) or custard. Fire a cannonball into a tub of oobleck, and this real magic stuff will stiffen up and resist this great spherical mass, potentially becoming like an impenetrable sense-defying wall. Take the same cannonball and simply place it on top of this oobleck vat, however, and the stuff will yield, not bother resisting or deflecting at all, and the ball will just glide on through, reaching the bottom of the vat a few seconds later. So, in Dune, the people and the buildings and vehicles have a kind of invisible oobleck forcefield around them, so fast moving bullets won’t get through to them, but comparatively slow moving blades will. In Dune, warriors cloud themselves in magical invisible cheap custard force fields; that’s why they can travel the universe, colonise planets, bend time and place, but still fight with swords and knives. It’s obvious really, just a consequence that everyone (or at least every empire and army) has in the world of Dune.\n\n\n\nOobleck. Dune has Space Oobleck, because the Space prefix makes everything more plausible. Source: https://science-u.org/experiments/oobleck.html\n\n\nSame question, different answer: Why do the warriors of Dune fight with knives and swords? Because Dune’s a swashbuckler, and Frank Herbert wanted it to be. The magic custard’s just a narrative device, a conceit, to allow things to happen that way. Frank Herber wanted his heroes and villains to fight like they did in mediaeval times, or at least like Victorian popular fiction authors portrayed mediaeval characters as fighting. Frank Herbert, maybe, like stories about knights and pirates, alternating trading blows and witty ripostes, in which the winners and losers depended mainly on skill, and skill was largely a function of virtue, along with where an encounter occurs within a narrative arc. So Dune’s a swashbuckler, because Frank Herbert wanted it so, and so, sixty years later, Denis-with-no-s’ Dune is also a swashbuckler, just as the Money Men wanted Lynch’s/not-Lynch’s Dune to be many decades earlier. Don’t think too much about the space custard, just enjoy the swashbuckling.\nBut of course Herbert didn’t just want Dune to be a swashbuckler in space, but much more besides. Dune, as far as I can tell, is Space Lawrence in Space Arabia, where Space Lawrence meets Space Pocahontas and forges a romantic as well as military union the Space Arabs (who, by implication, must also be Space Indians, if the Fremen Chani is Space Pocahontas). But that’s not all, Space Lawrence is also a Space Prince, avenging his father’s death through an act of Spakespearean regicide. And he’s not just a Space Prince, but also a Space Witch, with burgeoning mind control abilities being bestowed and developed by his Space Witch mother. Because you see, Space Lawrence the Space Prince’s father defied royal edict and imperial custom to marry into magic, to beguile a member of a sacred Space Coven, who decades later still don’t know what to make of this unusual union, bringing together the archetypically patriarchal power structure of a King who can control armies and ships with the archetypically matriarchal power structure of the Space Coven, whose domains appear to be of mind and Mother Nature. So, that’s who Paul is: just just Space Lawrence, freedom fighter and rebel, but also a Space Prince with a divine right to rule, and a Space Witch with magic powers.\nAnd what are they fighting for? Not just honour, but Spice. And what’s Spice? Spice is Space Nootropic, able to turn people into human calculators known as mentats. But it’s also Space Oil, able to power and generate technologies of conquest that expand imperial territory and powers by orders of magnitude. Space also seems to be a Space Psychedelic, opening up new windows of perception into the wonders of the world. And there’s more, Spice also seems to be a Space Mutagen, unleashing the potential of the Human Race and bringing in new phases of forced evolution like Arthur C Clark’s monolith or any number of conceits, reflecting any number of contemporaneous interests and concerns - radiation, nanotechnology, alien visitation etc - from any number of American Superhero franchises.\nIn short, Spice is good, apparently good at pretty much everything, and so good it’s worth fighting for, and risking almost anything to get hold of and keep hold of.\nAnd where does Spice come from? Only Dune, an arid, near uninhabitable sand planet. And where, even more specifically, does Spice come from? Well, from Space Words, giant mindless remorseless manifestations of the unconquerable forces of Nature, primitive forces so powerful no Space Tech can cow or conquer them. Spice is manna, essential biblical nourishment for a chosen people, but unlike the manna of the Old Testament, which comes from above, this manna comes from below.\nHave I missed anything? The grotesque evil of the Harkonen, I guess, and the shadowy underhanded realpolitik of the emperor too. But of course the Hero needs his Foil, and his Journey can’t go exactly to plan. These plot elements just follow from the need to centre on a Hero’s Journey, and so seem to be necessary elements given the key story being told. Regardless, there’s clearly a lot going on in Dune. As a story, it seems to be almost as rapacious and voracious as its worms, hoovering in plot points, ideas, historical frames of reference and concepts from seemingly anywhere and everywhere, without any apparent initial regard for how these plundered elements might ever be made to fit together into something that looks like a coherent totality.\nBut at the same time, it’s clear Dune wants to be more than a grab bag of ideas and story fragments. It also wants to be a world, and worlds are places where people (not just heroes and villains) live and act, and act under the expectation that the world will respond to their actions in consistent and predictable ways. To put it another way, worlds need rules, and rules impose limits. A story unmoored from a world, a system of consistent rules defining the consequences of actions, is a story where characters’ actions and motivations lack consequences, because the characters don’t live in a place, but in a fever dream, which is to stay that the characters live or matter in no meaningful sense whatsoever.\nThere’s a golden rule of improv, I’ve read, which can be boiled down to, ‘Yes, and’. Someone takes something, adds a premise to it, and passes it on. Reject a new premise, or try to delete an old one, and the world falls apart, the other performers glower and fall silent, and the audience’s coughs and rustling papers become deafeningly loud. The problem of Dune is the problem of an improv scene that’s run on in an author’s head for months or years on end: yes and yes and yes and yes and yes and… It’s the problem of an over accumulative imagination, a mental magpie whose nest neighbours a tip, a hoarder whose house is now a warren and whose floors creak under the weight of decades of accumulation and block out the sunlight. Having some stuff, some ideas, is great. Having too many, however, is a burden. The problem with Dune, which seems to be a problem with Dune the novel not Dune by Denis or David, is the problem of sci-fi, and also the opportunity of sci fi. The opportunity of sci-fi is that of filling an empty room with whatever you’ve ever wanted, in any order and configuration you see fit. Conversely, the problem of sci-fi is in realising when the room is already too full, and when its contents are about to topple onto its occupant. That’s the problem of Dune, and the problem of sci-fi, and why I’m looking forward to seeing Denis-without-an-s’ realisation of Frank Herbert’s hot mess of a story in Dune, part two. As well as the intended narrative peril the story brings through the trials it inflicts on its characters, there’s the unintended narrative peril affecting the entire world: like a giant game of conceptual Jenga, is this new idea, or that new idea, the idea that’s going to cause the entire grab-bag scrapheap world of Dune to tumble into itself, to collapse like a pile of sand?"
  },
  {
    "objectID": "posts/unpop/dune-pt-1-after-dune-pt-2/index.html#footnotes",
    "href": "posts/unpop/dune-pt-1-after-dune-pt-2/index.html#footnotes",
    "title": "My thoughts on Dune (Part 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough, I now realise, the sound guns were Lynch’s addition, a substitution from the knives and swords in Herbert’s novel, because apparently Lynch didn’t want to make a ‘kung fu’ film. Regardless, I’m referring mainly to the uneven pace of the film, the rushing, papering over and shoehorning in of story elements through use of montage and voiceover.↩︎\nOne addition made to the shields in Denis’ Dune is colour coding: blue when the shield ‘works’, red when it ‘fails’. This way the colour red can be associated with a character’s demise in a sword fight, without the red being that of the defeated party’s blood. Given Denis’ Dune is a 12A certificate, and more inclusive certifications were likely a condition of a large budget, the red-glowing shield shorthand appears a fairly clever way of showing-without-showing. The need to avoid being too graphically explicit may also explain why much of the film, especially those scenes involving the dastardly Harkonen, is extremely dark.↩︎"
  },
  {
    "objectID": "posts/unpop/on-sweary-soaps/index.html",
    "href": "posts/unpop/on-sweary-soaps/index.html",
    "title": "On Sweary Soap Operas: A Concealed Television Genre",
    "section": "",
    "text": "This is not a doll\nBack in the 1980s I used to play with He-Man dolls. My favourite came pre-decomposed: Modulok and Multi-bot. Both were collections of interchangeable heads, torsos and limbs - alien parts for Modulok; robot parts for Multi-bot - that could joined up in more ways that a young boy could count. The two characters could be combined, creating oddly cute orgosynthetic monstrosities: HR Giger by Mattel circling Lego’s IP, as it were.\nNevertheless, if my father asked me if I was enjoying playing with my He-Man dolls, I’d be quick to correct him. “They’re not dolls!”, I’d tell him. “They’re action figures!” If my father then asked, “What’s the difference?”, I think I’d be ready to confabulate a distinction, usually related to the functionality - “press this button and it can talk or punch, or talk to who it’s punching” - or its durability. Given MultiModulokbot starts off broken up, and is intended to be torn limb from torso from head from limb, I may have had a point on the latter, but more through luck than judgement.\nI was thinking about the doll/action figure distinction recently after realising that, over the last few weeks, I’d been tricked into watching a couple of soap operas, and worse still occasionally even enjoying doing so. The first of these was Billions, which I initially watched hoping it would be to Hedge Funds what The Wire was to Street Gangs. With a top investigative journalist as one of its executive producers, and an assault course of legal and financial jargon to contend with from the first minute, I thought maybe it might at least reach the navel of the Wire’s mantel. But by around the fourth series I realised fictionalised socioethnographic investigation was never what Billions wanted to be, and if it had ever played with the idea of saying something meaningful about the ultrawealthy,1 it had no interest in this any more. No. Billions, I’d realised, was quite happy being something like Dynasty with F-bombs, and C-bombs, and BDSM, strung together mainly around a kind of baroque storyline involving a preposterous love triangle, or maybe a love chevron, or maybe a love human centipede. In Billions, various ludicrous hypermasculine archetypes, sometimes played by women or non-binary actors, act as if every decision they make is life-or-death, that they’re one step away from destroying each other, vanquishing their foes, and achieving ultimate victory. Yet there they are, two years later, three years later, all in one piece, none-the-worse, still acting as if they’re still dancing on an existential precipice, and that maybe this scheme is the one that will finally seal the deal. (It doesn’t.)\nMore recently, I’ve found myself watching Loudermilk, about a former music journalist and recovering alcoholic who’s also a straight-talking foul-mouthed misanthropist with a heart of gold. Secondary characters call Loudermilk Loundermilk repeatedly, as if to remind viewers what show they’re watching, and attractive young women seem to find him appealing for no obvious reason. Much of the show involves men in recovery talking to each other in a room, hiding their love and concern for each other inside superficially cruel and callous insults. Plot twists abound - an affair here, a visit from a long-lost relative there - but ultimately it’s still the same set of characters, sitting in a room, loving to hate each other, and hating that they love each other.\nSeries like Billions and Loudermilk would never admit to being soap operas because, just as the young boy who played with action figures would never have played with dolls, so many viewers of sweary soap operas would never watch soap operas."
  },
  {
    "objectID": "posts/unpop/on-sweary-soaps/index.html#footnotes",
    "href": "posts/unpop/on-sweary-soaps/index.html#footnotes",
    "title": "On Sweary Soap Operas: A Concealed Television Genre",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI was thinking of shoehorning a reference to the characters of Billions thinking that they’re the “masters of the universe”, but are also ultimately no more realistic than those portrayed in He-Man, but decided against it!↩︎"
  },
  {
    "objectID": "posts/unpop/andy-weir-utopian-engfi/index.html",
    "href": "posts/unpop/andy-weir-utopian-engfi/index.html",
    "title": "Why can’t we just get on with making and fixing stuff?",
    "section": "",
    "text": "The Martian, 2015 film\n\n\nI tend to read non-fiction rather than fiction, but some of the novels I have consumed over the last few years are those by Andy Weir, whose 2011 debut novel The Martian was adapted into the 2015 film of the same name directed by Ridley Scott and starring Matt Damon. So far he’s published three print novels: The Martian (2011, 2014); Artemis (2017), and Project Hail Mary (2021)\nWeir’s stories are often categorised as science fiction, and as a first approximation that’s not inaccurate. However, I think science fiction doesn’t quite do justice to what’s distinct about Weir’s novels, or at least substantial parts of them. Instead, I think the genre of Weir’s novels are better understood as engineering fiction, or Eng-Fi.\nWhat are the key features of Eng-Fi? I think they include the follows:\n\nA strong focus on a protagonist operating in a universe governed by physical laws that closely approximate our own. 1\nA protagonist with a deep, broad and applied knowledge of how technologies and tools work, grounded in an understanding of the physical laws and scientific processes such technologies are known to utilise in order to work.\nThe narrative confabulation of situations in which the protagonist is required to solve a series of engineering challenges to avoid serious and adverse outcomes, with suboptimal time constraints and resources.2\nBodging, failure, and iteration: The interim solution to one engineering challenge has some flaws, which then leads to a problem, requiring another engineering solution.\nWorkings out: Very detailed and extensive discussions about how the protagonist went about trying to solve engineering challenges, including the many times they failed. So, the narrative equivalent of focusing at least as much on the methods and supplementary appendix sections of a scientific paper, as well as any scientific lab books and notes, as the final published results.\n\nThough grounded largely in physical reality, Weir’s novels often appear strangely utopian when it comes to social and psychological realities. This includes:\n\nValourisation of scientists and engineers: People who can ‘do stuff’ and ‘make stuff’, and understand the methods and techniques required to solve complex problems, are valued and valourised by society as a whole.\nPolitically effective technocracies: There are enough technically and scientifically capable people in positions of political power to be able to understand that something is a problem in need of a solution, and to devote sufficient resource and expertise to it being solved.\nProblem-solving and discovery as a universal human aspiration: The idea that people should try to understand the physical world well enough to solve complex problems, rather than just (say) write poetry about the human condition, is taken as a given.\nSpace-faring: Willingness to devote substantial public and private resource to space flight and colonisation, without (say) political pushback arguing that such resources should be better spent on either welfare or warfare, is also just taken as a given.\nCommon humanity: In situations of mutual peril and opportunity, the human race will come together to meet their collective challenges.\n\nSo, Weir’s Eng-Fi seems to combine physical realism with psycho-sociological utopianism. This might sound like a criticism, but in terms of compelling story-telling, it’s not! Not all of Weir’s novels are pure Eng-Fi. I’d categorise them as follows:\n\nThe Martian:3 90% Eng-Fi; 10% Sci-Fi\nArtemis: 35% Eng-Fi; 15% Sci-Fi; 25% Noir4; 25% Heist5\nProject Hail Mary: 55% Eng-Fi; 45% Sci-Fi\n\nAnd of the three, it’s the two with more Eng-Fi I’ve found the more compelling, both because of their attempted commitment to physical realism, but also because of their arguably less realistic portrayal of politics, psychology and sociology. In Weir’s stories, when push comes to shove, it’s the engineers and scientists, not the politicians and generals, who are called on to save the day. As compared with the dystopian sci-fi that predominates, in which humanity is both the catalyst - usually through its influence on the climate - and also accelerant - usually through its pettyminded tribal politicking - of its own demise, there’s something beautifully, poignantly hopeful about the collective sensemaking expressed in Weir’s stories."
  },
  {
    "objectID": "posts/unpop/andy-weir-utopian-engfi/index.html#andy-weir-and-eng-fi",
    "href": "posts/unpop/andy-weir-utopian-engfi/index.html#andy-weir-and-eng-fi",
    "title": "Why can’t we just get on with making and fixing stuff?",
    "section": "",
    "text": "The Martian, 2015 film\n\n\nI tend to read non-fiction rather than fiction, but some of the novels I have consumed over the last few years are those by Andy Weir, whose 2011 debut novel The Martian was adapted into the 2015 film of the same name directed by Ridley Scott and starring Matt Damon. So far he’s published three print novels: The Martian (2011, 2014); Artemis (2017), and Project Hail Mary (2021)\nWeir’s stories are often categorised as science fiction, and as a first approximation that’s not inaccurate. However, I think science fiction doesn’t quite do justice to what’s distinct about Weir’s novels, or at least substantial parts of them. Instead, I think the genre of Weir’s novels are better understood as engineering fiction, or Eng-Fi.\nWhat are the key features of Eng-Fi? I think they include the follows:\n\nA strong focus on a protagonist operating in a universe governed by physical laws that closely approximate our own. 1\nA protagonist with a deep, broad and applied knowledge of how technologies and tools work, grounded in an understanding of the physical laws and scientific processes such technologies are known to utilise in order to work.\nThe narrative confabulation of situations in which the protagonist is required to solve a series of engineering challenges to avoid serious and adverse outcomes, with suboptimal time constraints and resources.2\nBodging, failure, and iteration: The interim solution to one engineering challenge has some flaws, which then leads to a problem, requiring another engineering solution.\nWorkings out: Very detailed and extensive discussions about how the protagonist went about trying to solve engineering challenges, including the many times they failed. So, the narrative equivalent of focusing at least as much on the methods and supplementary appendix sections of a scientific paper, as well as any scientific lab books and notes, as the final published results.\n\nThough grounded largely in physical reality, Weir’s novels often appear strangely utopian when it comes to social and psychological realities. This includes:\n\nValourisation of scientists and engineers: People who can ‘do stuff’ and ‘make stuff’, and understand the methods and techniques required to solve complex problems, are valued and valourised by society as a whole.\nPolitically effective technocracies: There are enough technically and scientifically capable people in positions of political power to be able to understand that something is a problem in need of a solution, and to devote sufficient resource and expertise to it being solved.\nProblem-solving and discovery as a universal human aspiration: The idea that people should try to understand the physical world well enough to solve complex problems, rather than just (say) write poetry about the human condition, is taken as a given.\nSpace-faring: Willingness to devote substantial public and private resource to space flight and colonisation, without (say) political pushback arguing that such resources should be better spent on either welfare or warfare, is also just taken as a given.\nCommon humanity: In situations of mutual peril and opportunity, the human race will come together to meet their collective challenges.\n\nSo, Weir’s Eng-Fi seems to combine physical realism with psycho-sociological utopianism. This might sound like a criticism, but in terms of compelling story-telling, it’s not! Not all of Weir’s novels are pure Eng-Fi. I’d categorise them as follows:\n\nThe Martian:3 90% Eng-Fi; 10% Sci-Fi\nArtemis: 35% Eng-Fi; 15% Sci-Fi; 25% Noir4; 25% Heist5\nProject Hail Mary: 55% Eng-Fi; 45% Sci-Fi\n\nAnd of the three, it’s the two with more Eng-Fi I’ve found the more compelling, both because of their attempted commitment to physical realism, but also because of their arguably less realistic portrayal of politics, psychology and sociology. In Weir’s stories, when push comes to shove, it’s the engineers and scientists, not the politicians and generals, who are called on to save the day. As compared with the dystopian sci-fi that predominates, in which humanity is both the catalyst - usually through its influence on the climate - and also accelerant - usually through its pettyminded tribal politicking - of its own demise, there’s something beautifully, poignantly hopeful about the collective sensemaking expressed in Weir’s stories."
  },
  {
    "objectID": "posts/unpop/andy-weir-utopian-engfi/index.html#in-praise-of-neurodiversity",
    "href": "posts/unpop/andy-weir-utopian-engfi/index.html#in-praise-of-neurodiversity",
    "title": "Why can’t we just get on with making and fixing stuff?",
    "section": "In praise of neurodiversity",
    "text": "In praise of neurodiversity\nOn Weir’s wikipedia page, it’s stated that Weir’s parents worked as a physicist and an electrical engineer, and that Weir himself worked as a computer programmer. To the extent none of this is surprising, it’s because we implicitly understand that there’s something in common between these kinds of occupation, that they’re all about tractable rule-bound problem solving, that persons with similar psychological and neurological profiles are drawn disproportionately to work in such fields, and perhaps also that there may be a genetic component to such a predisposition.\nPerhaps it’s also not a stretch to assume that those factors which predispose people towards engineering, physics and programming also predispose them towards science fiction? Steve Silberman’s book Neurotribes, subtitled The legacy of Autism and how to think smarter about people who think differently, includes 47 references to ‘science fiction’, with an interest in science fiction being noted as relatively common amongst those more likely to be diagnosed with some form of autism or exhibit characteristics associated with this ‘disorder’.\nAs both title and subtitle of Silberman’s book makes clear, there are reasons both for considering the temperament and way of thinking, long associated and pathologised (and more recently disassociated but arguably still pathologised) by the likes of Hans Asperger and his followers, to have some kind of genetic component; and also to be thought of as something other than a neurological or developmental deficit or disorder, not so much as something ‘missing’ from those ‘with autism’ (or more recently ‘on the spectrum’) that’s present in everone else, but more as a distinct and valuable way of thinking that fits some environments and situations better than others."
  },
  {
    "objectID": "posts/unpop/andy-weir-utopian-engfi/index.html#evolutionary-origin-story",
    "href": "posts/unpop/andy-weir-utopian-engfi/index.html#evolutionary-origin-story",
    "title": "Why can’t we just get on with making and fixing stuff?",
    "section": "Evolutionary Origin Story",
    "text": "Evolutionary Origin Story\n\n\n\nPrefrontal Cortex (Source: Wikipedia)\n\n\nThe evolutionary origin story that makes sense to me - though it’s almost certainly too simple to count as ‘true’ - is the follows: Soon after the development of advanced communication in the human animal came the development of advanced mis-communication, and soon after the development of advanced mis-communication came the development of even more advanced counter-mis-communication. More simply: first we learned to tell the truth, then we learned to lie, then we learned to detect whether someone is being truthful, then we learned to make lies sound truthful, then we learned to better detect lies even when they are made to sound truthful… and so on, and so on.\nAnd by ‘and so on’, I really mean ‘and so on’ to the power of a lot. For millions of years humans, as social creatures, have been stuck in a cognitive arms race with ourselves. Our brains grew: larger, more energy hungry, and more physically unweildy. As infants, we are more vulnerable and underdeveloped than other mammals, with heads so large they can barely fit through birth canals (leading to much increased risk of complication during childbirth), and so heavy we can barely lift them for the first few weeks of life. But we also grew ever better able to communicate with others, and to miscommunicate with others, and to tell truth from lies.\nBut the end result, other than our massive brains - and in particular our massive pre-frontal cortexes - has been a stalemate. Because both lying and lie-detection co-evolved, buoyed by and attempting to outmaneauver each other, in the end we’re about as good at both, with the end result being that people - normally developed, neurotypical, well socialised people - tend to be about as good at lying as lie-detection, and so overall not especially good at using social cues to tell when someone’s telling the truth. 6\nSo, the legacy of complex human social co-evolution: massive brains, and an obsessive pre-occupation with other people, and trying to figure out if they mean what they say, or are saying what they mean."
  },
  {
    "objectID": "posts/unpop/andy-weir-utopian-engfi/index.html#what-does-it-really-mean-to-think-differently",
    "href": "posts/unpop/andy-weir-utopian-engfi/index.html#what-does-it-really-mean-to-think-differently",
    "title": "Why can’t we just get on with making and fixing stuff?",
    "section": "What does it really mean to think differently?",
    "text": "What does it really mean to think differently?\nThe origin story of neurodiversity then, or at least those aspects that lead to a love of Sci-Fi and Eng-Fi, comes from then asking: What would happen if, for just small proportion of people, just some of the evolved human capacity for advanced, complex reasoning were diverted away from the stalemate of social reasoning, and towards the pursuit of understanding more about the broader natural and physical world?\nThe short answer may well be, in its more extreme form, autism. But the longer and more interesting answer may be something like a neurological profile and disposition that both, at the individual level, leads to acute challenges and difficulties negotiating with complex social relationships and realities; but also, over the longer term, and at a societal level, is instrumental for causing those advancements that fundamentally change the material circumstances in which everyone lives.\nBut for this kind of neurodiversity, I suspect we’d still sitting around a fire, dressed in loincloths, lying to each other, in ever more sophisticated ways, about where the berries and wilderbeast are. But for this kind of neurodiversity, if someone in their twenties died, we’d still be looking for which sky or earth gods to appease, rather than to discover tetanus and antibiotics. Those with this kind of neurotype might struggle more than most to live in the world, but they’re also instrumental in making the world in which they struggle to live.\nIn a world without this kind of neurodiversity, I also suspect there’d be a lot less Eng-Fi, with their plucky problem-solvers and reasonable statesmen and politicans, as well as fewer means (other than orally, across a campfire) of reading about and listening to such stories. It would be a much poorer world, both materially and cognitively, that I’m grateful I don’t live in!"
  },
  {
    "objectID": "posts/unpop/andy-weir-utopian-engfi/index.html#footnotes",
    "href": "posts/unpop/andy-weir-utopian-engfi/index.html#footnotes",
    "title": "Why can’t we just get on with making and fixing stuff?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is one obvious point of departure from much of sci-fi, which can focus instead on asking “what if” one or more rules of the universe were different.↩︎\nIn The Martian the set-up is essentially “solve engineering challenges or die”; whereas in Project Hail Mary it’s “solve engineering challenges or humanity dies”.↩︎\nBasically Robinson Crusoe in space… so was Robinson Crusoe the first Eng-Fi novel? I guess it depends on how much detail it goes into about how exactly Crusoe makes fire and makes shelter!↩︎\nThink Chinatown… but on The Moon!↩︎\nThink Ocean’s Eleven… but on The Moon!↩︎\nTwo ways of improving lie-detection are: i) to focus on logical inconsistency and external verification for the statements others make, for which text transcripts of statements can be more effective than verbal or video records of those statements; ii) to look for ‘tells’ in someone’s behaviour in parts of their body they are less likely to seek to ‘lie with’. For FBI interrogator Joe Navarro, this leads to the suggestion to look at people’s feet more than their faces.↩︎"
  },
  {
    "objectID": "posts/background-to-my-stats-series/index.html",
    "href": "posts/background-to-my-stats-series/index.html",
    "title": "On the background to my statistical inference series",
    "section": "",
    "text": "My blog series on statistical inference and modelling has, at the time of writing, 13 parts, and a feature-length reading time. 1 I’ve been strongly motivated to write this because it covers what I consider the essential theory and practice necessary to be a competent user of statistical methods. In this post I’ll go a bit more into my own background, and how I came to pick up this knowledge.\nMy first degree was in the applied physical sciences: electronic engineering. From this I learned two things: firstly, not to be afraid of algebra and coding; secondly, that I didn’t want to do electronic engineering as a career. So I moved into the social sciences, and this move took me to the health sciences, demography and epidemiology.\nThe move from the applied physical to the social and health sciences made me realise I’d learned something else from the engineering course: a pair of expectations about methods training. The first expectation was that the methods taught should allow the substantive questions of interest in the field to be addressed. The second expectation was that methods should be taught with sufficient rigour and formalism to ensure students attending the same course, and being sufficiently attentive in that course, leave with a common understanding of what’s been taught and exactly how they are applied.\nI wish I could honestly say otherwise, but in my experience the methods taught in much of the social sciences in the UK fell short of the standards of rigour and application that are just taken as given in an engineering course. Qualitative methods courses tend to trade in abstract nouns and unfalsifiable declarations - how does one really know whether one’s employing a feminist methodology, or a critical realist epistemology, or a post-structuralist framing, when asking people why they’re so sad, or angry, or poor? And most of the quantitative methods training, at least when I first encountered them, took the form of telling people what buttons to click, in which order, after opening up a copy of SPSS. Press this button, then this button, then this button, then look at this number here, and check it’s under 0.05, and look for the number of stars in this row, and so on.\nWhen I started a PhD in the quantitative social sciences I was highly unskilled. I sat in on some general social science methods courses, some econometrics, some first year probability and statistics courses run by the maths departments, but still didn’t feel I knew how to use the methods of quantitative research with the same level of rigour and understanding that I’d been used to in the engineering course. So I kept searching.\nThe training course that finally changed this was Gov 2001, a course that’s been run annually by Harvard university for decades, and seems to have become something of an institution. The course teaches statistical inference from the ground up, from the first principles of likelihood and probability, but also doesn’t scrimp on the practicalities of application. It’s also highly applied, with students evaluated on whether they can, at the end of the course, replicate and improve upon an article that’s already been published. It also emphasises the family resemblances between statistical models, the way almost all specific models are just different versions of an underlying ‘mother model’ (my term) which comprises two linked equations.\nI took the course as a distance student over a decade ago, and still find its contents immensely valuable. The blog post series listed below is largely based on that course, though with my own idiosyncratic spin and emphasis. It’s quite technical in places, but the juice is worth the squeeze. If you follow along you will know and understand more about statistical models and their application than almost any UK graduate in a field other than statistics. 2"
  },
  {
    "objectID": "posts/background-to-my-stats-series/index.html#footnotes",
    "href": "posts/background-to-my-stats-series/index.html#footnotes",
    "title": "On the background to my statistical inference series",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMuch of this reading may be messages generated by R functions, however.↩︎\nA statement based on a great deal of personal experience, sadly. Statistical inference is still generally quite poorly explained, poorly understood, and poorly applied in much of the UK, especially when it comes to model building, comparision, interpretation and use for prediction.↩︎"
  },
  {
    "objectID": "posts/still-the-economy/index.html",
    "href": "posts/still-the-economy/index.html",
    "title": "It’s still the economy",
    "section": "",
    "text": "I’m back (writing 23 April) from a public health seminar on the relationship between the economy, poverty and health. Lots of food for thought. But the main thing I’m thinking about was an argument put forward by one of the speakers, which seemed to go along the following lines:\n\nCompared with the late 1990s, GDP per capita in the UK is now around 25% higher. Meanwhile, life expectancy is falling and relative poverty rates are getting worse. So these adverse health and poverty changes don’t mainly seem to be due to how big the economy is, or how fast it’s growing. Instead it seems to be more about how we make sure the economy works for us, rather than us for it, and so there should be more focus on making sure more people have more equal shares of the pie, rather than in simply making the pie bigger.\n\nI agree with the sentiment that greater equality of resourcing - reductions in both income and asset inequalities - is important. 1,2 But at the same time I think this interpretation greatly undervalues the importance of general economic growth as a determinant of subsequent stalling in longevity progress. Instead, I believe something like:\n\nSlowdowns in the fundamentals of economic growth rates in the UK may be the fundamental cause of subsequent slowdowns in growth in longevity, albeit in large part through the indirect (and avoidable) path of economic, social and political mismanagement known broadly as ‘austerity’."
  },
  {
    "objectID": "posts/still-the-economy/index.html#background",
    "href": "posts/still-the-economy/index.html#background",
    "title": "It’s still the economy",
    "section": "",
    "text": "I’m back (writing 23 April) from a public health seminar on the relationship between the economy, poverty and health. Lots of food for thought. But the main thing I’m thinking about was an argument put forward by one of the speakers, which seemed to go along the following lines:\n\nCompared with the late 1990s, GDP per capita in the UK is now around 25% higher. Meanwhile, life expectancy is falling and relative poverty rates are getting worse. So these adverse health and poverty changes don’t mainly seem to be due to how big the economy is, or how fast it’s growing. Instead it seems to be more about how we make sure the economy works for us, rather than us for it, and so there should be more focus on making sure more people have more equal shares of the pie, rather than in simply making the pie bigger.\n\nI agree with the sentiment that greater equality of resourcing - reductions in both income and asset inequalities - is important. 1,2 But at the same time I think this interpretation greatly undervalues the importance of general economic growth as a determinant of subsequent stalling in longevity progress. Instead, I believe something like:\n\nSlowdowns in the fundamentals of economic growth rates in the UK may be the fundamental cause of subsequent slowdowns in growth in longevity, albeit in large part through the indirect (and avoidable) path of economic, social and political mismanagement known broadly as ‘austerity’."
  },
  {
    "objectID": "posts/still-the-economy/index.html#slowing-economic-growth-fundamentals",
    "href": "posts/still-the-economy/index.html#slowing-economic-growth-fundamentals",
    "title": "It’s still the economy",
    "section": "Slowing economic growth fundamentals",
    "text": "Slowing economic growth fundamentals\nMany years ago, through the blog of Simon Wren-Lewis, I became aware of the way that, following the 2008 Global Financial Crisis (GFC), the long-term fundamentals of UK economic growth rates appears to have, for want of a better term, broken. Wren-Lewis has updated a graph of actual UK GDP per capita, against levels of GDP per capita that would have been expected if long-term trends had continued, a number of times. The most recent version of this I can find is from this post, with the figure itself linked to below:\n\n\n\nUK GDP per capita and trend (pre-GFC)\n\n\nI found an earlier version of this graph so extaordinary that I decided to look for the source data and try to replicate it:\n\n\n\nMy version of UK GDP per capita and trend (pre-GFC)\n\n\nThis version of the graph was up to 2016. Even then, the discrepancy between the amount per capita achieved in 2016, and the amount expected if more than half a century’s standard rate of economic growth had continued, was extraordinary: around £8,000 per person.\nAs past-me was kind enough to provide the source of the data and the code used to produce the graph, I should be able to update it:\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(scales)\n\ngdp_data_ons &lt;- read_csv(\"https://www.ons.gov.uk/generator?format=csv&uri=/economy/grossdomesticproductgdp/timeseries/abmi/qna\", skip = 9, col_names = c(\"year\", \"gdp\"))\n\n  \n# Now UK capita, using HMD as data go back further\nhmd_pop &lt;- read_table(\n    here::here(\"posts\", \"still-the-economy\", \"hmd_uk_pop.tsv\"), \n    skip = 2\n  ) |&gt;\n    select(year = Year, total = Total) |&gt;\n    group_by(year) |&gt;\n    summarise(population = sum(total)) |&gt;\n    ungroup() \n\ngdp_per_cap &lt;- \n    hmd_pop |&gt;\n    inner_join(\n        gdp_data_ons |&gt;\n            mutate(year = as.numeric(year))\n        ) |&gt;\n    mutate(gdp_per_cap = 10 ^ 6 * (gdp / population))\n\n# gdp_per_cap\n# # gdp_per_cap %&gt;% # gdp in millions\n# #   qplot(x = year, y = gdp_per_cap, data = .)\n\ngdp_per_cap %&gt;% \n  mutate(log_pcgdp = log(gdp_per_cap)) %&gt;% \n  filter(year &lt; 2008) %&gt;% \n  lm(log_pcgdp ~ year, data = .) %&gt;% \n  tidy() -&gt; coeffs\n\n# coeffs\n\ngdp_per_cap %&gt;% \n  mutate(predicted_gdp_per_cap = exp(coeffs[1,2] |&gt;pull(estimate) + (coeffs[2, 2] |&gt; pull(estimate) )* year) )  %&gt;% \n   ggplot(aes(x = year)) + \n   geom_point(aes(y = gdp_per_cap))  +\n  geom_line(aes(y = predicted_gdp_per_cap), linetype = \"dashed\") + \n  geom_ribbon(\n    aes(\n      ymin = ifelse(gdp_per_cap &lt; predicted_gdp_per_cap, gdp_per_cap, predicted_gdp_per_cap),\n      ymax = ifelse(gdp_per_cap &gt; predicted_gdp_per_cap, gdp_per_cap, predicted_gdp_per_cap)\n      ),\n    fill = \"lightgrey\", alpha = 0.4, colour = NA\n    ) + \n  scale_x_continuous(limits = c(1950, 2025), breaks = seq(1950, 2030, by = 10)) + \n  scale_y_continuous(limits = c(0, 50000), breaks = seq(5000, 50000, by = 5000), labels = comma) + \n  labs(y = \"GDP per capita in £\", x = \"Year\") + \n  theme_minimal() + \n  geom_vline(xintercept = 2008, linetype = \"dashed\") -&gt; long_term_gdp_trend\n\nlong_term_gdp_trend\n\n\n\n\n\nWe can see here just how large the gap between observed GDP per capita, and that which would have been expected if the long-term trend which existed between 1950 to 2007 had continued. By 2022, the last year in the series, the observed GDP per capita was under £35,000, whereas the projected rate for the same year, if the growth fundamentals that had held for over half a century previous had continued, would have been over £45,000. A gap between expectation and reality of over £10,000, or around 30% of current values.\nAnother way of looking at this same data is in terms of percentage change in GDP per capita from the previous year.\n\n\nCode\ngdp_per_cap |&gt;\n  arrange(year) |&gt; \n  mutate(\n    pct_change = 100 * ((gdp_per_cap - lag(gdp_per_cap)) / lag(gdp_per_cap))\n  ) |&gt;\n  mutate(\n    time_group = ifelse(year &lt; 2008, \"pre-2008\", \"2008+\")\n  ) |&gt;\n  ggplot(aes(year, pct_change)) + \n  geom_point(alpha = 0.2) + \n  geom_line(alpha = 0.1) +\n  stat_smooth(aes(group = time_group), method = \"lm\", \n  formula = y ~ 1, se = FALSE) + \n  geom_hline(yintercept = 0) + \n  geom_vline(xintercept = 2008, linetype = \"dashed\") + \n  labs(\n    x = 'Year',\n    y = 'Percentage change in GDP per capita from previous year',\n    title = \"Growth rates in GDP per capita up until and from 2008\"\n  ) + \n  scale_y_continuous(\n    breaks = seq(-11, 9, by = 1)\n  )\n\n\n\n\n\nEven with the sharpest annual recovery ever observed from 2020 to 2021, the average growth rate from 2008 remains a fraction of the level observed over the previous period of more than half a century."
  },
  {
    "objectID": "posts/still-the-economy/index.html#slowing-longevity-growth-fundamentals",
    "href": "posts/still-the-economy/index.html#slowing-longevity-growth-fundamentals",
    "title": "It’s still the economy",
    "section": "Slowing longevity growth fundamentals",
    "text": "Slowing longevity growth fundamentals\nWe can look at the corresponding trends in life expectancy at birth in a similar way.\n\n\nCode\nhmd_e0 &lt;- read_table(\n    here::here(\"posts\", \"still-the-economy\", \"hmd_uk_e0per.txt\"), \n    skip = 2\n  ) |&gt;\n    select(year = Year, total = Total) \n    \nhmd_e0 |&gt;\n  filter(year &gt;= 1950) |&gt;\n  ggplot(aes(year, total)) + \n  geom_line() +\n  expand_limits(y = 0) + \n  labs(x = \"Year\", y = \"Life expectancy at birth\", \n  title = \"Life expectancy at birth over time, UK\",\n  caption = \"Source: Human Mortality Database\"\n  )\n\n\n\n\n\nAs with the economic growth rates, we can look at the ‘growth rate’ of longevity either in absolute annual changes:\n\n\nCode\nhmd_e0 |&gt;\n  filter(year &gt;= 1949) |&gt;\n  arrange(year) |&gt;\n  mutate(ch_e0 = total - lag(total)) |&gt;\n  ggplot(aes(year, ch_e0)) + \n  geom_point(alpha = 0.2) +\n  geom_line(alpha = 0.1) + \n  geom_hline(yintercept = 0) +\n  stat_smooth(se = FALSE) +\n  labs(x = \"Year\", y = \"Change in life expectancy from previous year\", title = \"Change in annual life expectancy from previous year, in years, UK\",\n  subtitle = \"Blue line: Loess smoother\",\n  caption = \"Source: Human Mortality Database\")\n\n\n\n\n\nOr as a percentage change from the previous year:\n\n\nCode\nhmd_e0 |&gt;\n  filter(year &gt;= 1949) |&gt;\n  arrange(year) |&gt;\n  mutate(ch_e0_pct = 100 * (total - lag(total)) / lag(total)) |&gt;\n  ggplot(aes(year, ch_e0_pct)) + \n  geom_point(alpha = 0.2) +\n  geom_line(alpha = 0.1) + \n  geom_hline(yintercept = 0) +\n  stat_smooth(se = FALSE) +\n  labs(x = \"Year\", y = \"% Change in life expectancy from previous year\", title = \"Change in annual life expectancy from previous year, percent, UK\",\n  subtitle = \"Blue line: Loess smoother\",\n  caption = \"Source: Human Mortality Database\")\n\n\n\n\n\nEither way, but perhaps even more clearly with the percentage graph, it is clear there has been a substantive decline in the fundamentals of longevity growth in the UK in recent years, much as there has been in the fundamentals of economic growth.\nWithin at a couple of papers, I investigated whether there is any clear evidence of a breakpoint in the longevity trends, much as there is a clear 2008 breakpoint in the economic growth trends. There does appear to be, but with 2012, rather than 2008, as the most likely breakpoint year. Splitting the percent longevity growth trends into the pre 2012 and 2012+ periods gives the following average longevity growth trends:\n\n\nCode\nhmd_e0 |&gt;\n  filter(year &gt;= 1949) |&gt;\n  arrange(year) |&gt;\n  mutate(ch_e0_pct = 100 * (total - lag(total)) / lag(total)) |&gt;\n  mutate(period = ifelse(year &lt; 2012, \"Pre 2012\", \"2012+\")) |&gt;\n  ggplot(aes(year, ch_e0_pct)) + \n  geom_point(alpha = 0.2) +\n  geom_line(alpha = 0.1) + \n  geom_hline(yintercept = 0) +\n  stat_smooth(aes(group = period), method = \"lm\", formula = y ~ 1, se = FALSE) +\n  geom_vline(xintercept = 2012, linetype = \"dashed\") + \n  labs(x = \"Year\", y = \"% Change in life expectancy from previous year\", title = \"Change in annual life expectancy from previous year, percent, UK\",\n  subtitle = \"Blue line: Average % change before and from 2012\",\n  caption = \"Source: Human Mortality Database\")\n\n\n\n\n\nThe average longevity growth rate from 2012 onwards is negative. It appears that COVID-19 cannot explain the majority of this divergence from earlier trends:\n\n\nCode\nhmd_e0 |&gt;\n  filter(year &gt;= 1949) |&gt;\n  arrange(year) |&gt;\n  mutate(ch_e0_pct = 100 * (total - lag(total)) / lag(total)) |&gt;\n  mutate(period = ifelse(year &lt; 2012, \"Pre 2012\", \"2012+\")) |&gt;\n  filter(year != 2020) |&gt;\n  ggplot(aes(year, ch_e0_pct)) + \n  geom_point(alpha = 0.2) +\n  geom_line(alpha = 0.1) + \n  geom_hline(yintercept = 0) +\n  stat_smooth(aes(group = period), method = \"lm\", formula = y ~ 1, se = FALSE) +\n  geom_vline(xintercept = 2012, linetype = \"dashed\") + \n  labs(x = \"Year\", y = \"% Change in life expectancy from previous year\", title = \"Change in annual life expectancy from previous year, percent, UK\",\n  subtitle = \"Blue line: Average % change before and from 2012\",\n  caption = \"Source: Human Mortality Database\")\n\n\n\n\n\nNo. Even after removing the acute effects of COVID-19 in 2020, the new longevity growth rate is less than half that of the previous long term growth trend, much as after 2008, the new economic growth rate is less than half that of the previous long term growth trend."
  },
  {
    "objectID": "posts/still-the-economy/index.html#combined-series",
    "href": "posts/still-the-economy/index.html#combined-series",
    "title": "It’s still the economy",
    "section": "Combined series",
    "text": "Combined series\nAs both trends are now in percentages, let’s plot the two together:\n\n\nCode\ne0_pct_growth &lt;- \n  hmd_e0 |&gt;\n  filter(year &gt;= 1949) |&gt;\n  arrange(year) |&gt;\n  mutate(pct_change = 100 * (total - lag(total)) / lag(total)) |&gt;\n  mutate(period = ifelse(year &lt; 2012, \"Old\", \"New\")) |&gt;\n  mutate(\n    series = \"2. Life Expectancy at Birth\"\n  )\n\npcgdp_pct_growth &lt;- \ngdp_per_cap |&gt;\n  arrange(year) |&gt; \n  mutate(\n    pct_change = 100 * ((gdp_per_cap - lag(gdp_per_cap)) / lag(gdp_per_cap))\n  ) |&gt;\n  mutate(\n    period = ifelse(year &lt; 2008, \"Old\", \"New\")\n  ) |&gt; \n  mutate(\n    series = \"1. Per Capita GDP\"\n  )\n\nboth_series &lt;- \n  pcgdp_pct_growth |&gt;\n  bind_rows(e0_pct_growth) |&gt;\n  select(year, series, pct_change, period)\n\nif (!exists(\"both_series.csv\")) {\n  write.csv(both_series, \"both_series.csv\")\n}\n\nboth_series |&gt;\n  ggplot(aes(year, pct_change)) + \n  geom_point(alpha = 0.2) + \n  geom_line(alpha = 0.1) + \n  facet_wrap(vars(series), ncol = 1, scales = \"free_y\") +\n  stat_smooth(aes(group = period), method = \"lm\", formula = y ~ 1, se = FALSE) + \n  geom_hline(yintercept = 0) + \n  geom_vline(xintercept = 2008, linetype = \"dashed\") +\n  geom_vline(xintercept = 2012, linetype = \"dashed\") + \n  annotate(geom = \"rect\", xmin = 2008, xmax = 2012, ymin = -Inf, ymax = Inf, fill = \"red\", alpha = 0.2) +\n  labs(\n    x = \"Year\", \n    y = \"Percentage change from last year\",\n    title = \"Economic and longevity growth rates. UK\"\n  )\n\n\n\n\n\nIf event X occurs before event Y, it’s definitely possible that event X caused event Y. In this case, event X is a substantial and apparently (but hopefully not) permanent downwards shift in the fundamentals of economic growth, and event Y is a substantial and possibly (but hopefully not) permanent downwards shift in the fundamentals of longevity growth."
  },
  {
    "objectID": "posts/still-the-economy/index.html#conceptual-model",
    "href": "posts/still-the-economy/index.html#conceptual-model",
    "title": "It’s still the economy",
    "section": "Conceptual model",
    "text": "Conceptual model\nDoes the apparent lag between the breaking of long term economic growth trends, and the breaking of long term longevity growth trends, strengthen or weaken any claim that the break in the former caused the latter? I think it potentially strengthens the claim, though possibly likely through some intermediate stages. In particular, I can imagine the four or so year lag being largely explained by something like the following.\n\n\n\n\nflowchart LR \n\neconPre([Pre-GFC Economic Growth])\neconPost([Post-GFC Economic Growth])\nhealthPre([Pre-2012 Longevity Growth])\nhealthPost([Recent Longevity Growth])\nGFC{{Global Financial Crisis}}\nausterity[Austerity]\nsocialCare[Social Care]\nhealthCare[Health Care]\n\neconPre --&gt;|shock| GFC\n\nGFC --&gt;|Political Choice| austerity\n\nGFC --&gt;|- short term | econPost\n\nausterity --&gt;|- sustained | econPost\neconPost --&gt;|+ sustained | austerity\n\nhealthPre --&gt; healthPost\n\nGFC --&gt;|- short term| healthPost\nausterity --&gt;|-- worsens| socialCare\nausterity --&gt;|- worsens| healthCare\n\nsocialCare --&gt;|- worsens| healthPost\nhealthCare --&gt;|- worsens| healthPost\n\neconPost --&gt; |- worsens| healthPost\nhealthPost --&gt; | - worsens | econPost\neconPre --&gt; healthPre \n\n\n\n\n\n\nTwo things can be claimed about the above graph. Firstly, it’s too complex. Secondly, it’s too simple. Both are likely to be true.\nWhat I’m trying to sketch out is the idea that the 2008 GFC may have had a small, short term direct effect on health, but to the extent it may have led to the sustained fall in longevity growth, it’s likely to be through other pathways whose effect have been more sustained.\nIn particular, the political choice to impose austerity, implemented in response to the GFC (and the public deficit caused by ‘bailing out the banks’), can plausibly have harmed the longevity growth fundamentals both through its effects on social care (less protected) and health care (more protected), and also through its effects on choking off the prospect of any return to previous levels of economic growth by weakening the economy even further.\nHere we encounter one of the wicked feedback loops in the graph: Poorer economic growth leads to lower tax returns, higher budget debts and deficits, and so the justification for… yet more austerity. It’s this kind of feedback look, and the the cumulative effects each iteration on the loop has on longevity growth fundamentals, which I think may explain both why the post-2008 economic growth slowdown has been so sustained, and why the now-sustained longevity growth slowdown manifested a few years later.\nAnd that’s just one of the feedback loops I’ve drawn in this graph. Due to the high levels of health related working age inactivity, I’ve also put a path between the post-2012 longevity growth trends, and the post-2008 economic growth trends. Without any further exogenous influence or political mismanagement, both fundamentals may now be mutually reinforcing and self-sustaining: a new normal."
  },
  {
    "objectID": "posts/still-the-economy/index.html#concluding-thoughts",
    "href": "posts/still-the-economy/index.html#concluding-thoughts",
    "title": "It’s still the economy",
    "section": "Concluding thoughts",
    "text": "Concluding thoughts\nAs mentioned at the start, the relationship between economic growth and longevity growth is something I’ve been thinking about a lot. Not just for the last few days, but for years. I don’t think the link between economic performance and health performance is focused on enough, often perhaps because people interested in public health and epidemiology may not be particularly interested in economics, and those interested in economics may not be particularly interested in public health and epidemiology. And with the exception of - say - infectious disease modelling, neither field tends to be particularly interested in reasoning through the dynamics of complex systems.\nBoth of these factors matter, I think, because the economic and longevity growth trends in the UK appear so similar, and because of the observation that we have seen an adverse shift in the fundamentals of two systems - economic growth and longevity growth - that for decades prior were remarkably stable over the longer term.\nImagine a marble on a thin conveyor belt, only slightly wider than it. The conveyor belt is on a tall plinth, gradually ascending like a reverse aqueduct, and either side of it are rail-like buffers. The effect of these buffers is to produce negative feedback, compensating for any lateral forces, at right angles to the direction of the belt, and so ensuring even moderate perturbations affecting the marble do little to affect its longer-term trajectory.\nImagine now an especially large lateral force, causing audible strain and visible cracks in the compensatory buffer. And now imagine the engineer’s response is to remove this buffer (the red section in the sketch below) rather than strenghen it, or perhaps to remove upstream sections of the buffer to provide material to reinforce the recently caused damage.\n\n\n\nMarble Pillars\n\n\nWithout the buffer, the effect of a lateral force on the marble can suddenly become much greater, causing the marble to leave the conveyor belt, leave the plinth, and roll off to the side. In the sketch I imagine a ramp adjacent to the taller plinth. Once the marble begins entry onto this ramp, its downward momentum becomes self-sustaining, until it reaches a new plateau: plinth B. Plinth B contains another conveyor belt, and is also ascending. At some point in the future the marble will reach the height on runner B that it used to be at on plinth A. But at each and every time the marble’s height while on plinth B will never be as high as it would have been if it were still on plinth A.\nWhen I’m thinking about what a change in the fundamentals of growth (whether whether economic or longevity), I’m imagining something like a transition from the plinth A to plinth B. The UK’s changing from pillar A to pillar B wasn’t inevitable: political choices were made to remove the guard rails, and move us off a path that we’d been following for most people’s working lives. But once the buffer was removed, the damage may well have been self-sustaining and permanent.\nIn short: I think it very plausible that decline in economic growth caused decline in longevity growth, as well as further decline in economic growth (which caused further decline in longevity growth, which then has started to cause further decline in economic growth).\nBut it didn’t have to cause this change in the fundamentals.\nThat was a political choice."
  },
  {
    "objectID": "posts/still-the-economy/index.html#footnotes",
    "href": "posts/still-the-economy/index.html#footnotes",
    "title": "It’s still the economy",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough on asset inequality it should be noted that one of the speakers was funded by a poverty research organisation that is itself funded by an endowment from a 19th century to early 20th century industrialist, and one of the future speakers is also from an organisation funded and named after another 19th to early 20th century industrialist. So in a sense such charitable research organisations, bringing attention and raising awareness about inequalities in income, assets, and opportunities, could not have existed but for… massive inequalities in income, assets and opportunities!↩︎\nIncidentally, I think the most politically robust argument in favour of this position isn’t about fairness - both the left and right have fundamentally different concepts of fairness, so the fairness argument for redistribution (‘How can those people be allowed to be so rich when these people are allowed to be so poor?’) only resonates with one political group - but instead about efficiency. Because of the marginal utility of income the amount of additional improvement in wellbeing, happiness, health and opportunity that £1 can bring to someone with £10 or £100 is much greater than for someone with £10,000 or £100,000. For someone from an economic left-wing perspective, there’s no trade-off in greater redistribution between fairness and efficiency: more equal outcomes is both more fair and more efficient. But for someone from a right-leaning perspective there is a trade-off. And so I think focusing on efficiency rather than fairness, and demonstrating that even if one considers progressive taxation ‘unfair’ the net benefit to society is positive, is a more universally persuasive political argument, even if it tugs at left-wing heartstrings less strongly than the fairness argument.↩︎"
  },
  {
    "objectID": "posts/interactive-sliders/index.html",
    "href": "posts/interactive-sliders/index.html",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "",
    "text": "Below is an example of creating a plotly chart with an interactive slider using crosstalk.\nBy default, the plot shows the proportion of datazones in a local authority that are in the 15% most deprived datazones in Scotland. (Using the 2020 SIMD).\nThe slider allows different thresholds than the 15% default to be selected."
  },
  {
    "objectID": "posts/interactive-sliders/index.html#introduction",
    "href": "posts/interactive-sliders/index.html#introduction",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "",
    "text": "Below is an example of creating a plotly chart with an interactive slider using crosstalk.\nBy default, the plot shows the proportion of datazones in a local authority that are in the 15% most deprived datazones in Scotland. (Using the 2020 SIMD).\nThe slider allows different thresholds than the 15% default to be selected."
  },
  {
    "objectID": "posts/interactive-sliders/index.html#data-downloading-and-preparation",
    "href": "posts/interactive-sliders/index.html#data-downloading-and-preparation",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "Data Downloading and Preparation",
    "text": "Data Downloading and Preparation\nTo see the code itself, just click on the word ‘code’ to open up the block’.\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(crosstalk)\nlibrary(here)\n\n\n\n\nCode\nif(!file.exists(here(\"simd_data.xlsx\"))){\n  download.file(\n    url = \"https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2020/01/scottish-index-of-multiple-deprivation-2020-data-zone-look-up-file/documents/scottish-index-of-multiple-deprivation-data-zone-look-up/scottish-index-of-multiple-deprivation-data-zone-look-up/govscot%3Adocument/SIMD%2B2020v2%2B-%2Bdatazone%2Blookup.xlsx\",\n    destfile = here(\"simd_data.xlsx\"),\n    mode = \"wb\"\n  )\n}\n\ndta &lt;- openxlsx::readWorkbook(here(\"simd_data.xlsx\"), sheet = \"SIMD 2020v2 DZ lookup data\")\n\n\nThe code for the figure itself is below. It’s quite a convoluted process. There’s almost certaintly neater ways of doing this. The main thing to keep in mind is all the figures exist; just only one is visible at a time.\n\n\nCode\n# So let's construct a new aval containing the different x-y tuples given the threshold selected\n\ncalc_prop_deprived &lt;- function(q, dta){\n    dta %&gt;% \n      group_by(HBname) %&gt;% \n      summarise(prop_deprived = mean(pct_rank &lt; q)) %&gt;% \n      ungroup()\n}\n\ndf_rank &lt;- \n  dta %&gt;% \n    select(HBname, SIMD2020v2_Rank) %&gt;% \n    mutate(pct_rank = SIMD2020v2_Rank / max(SIMD2020v2_Rank))\n\n\nshared_df &lt;- tibble(\n  dep_quants = seq(0.05, 0.95, by = 0.05)\n) %&gt;% \n  mutate(derived_props = map(dep_quants, calc_prop_deprived, dta = df_rank)) %&gt;% \n  unnest(derived_props) %&gt;% \n  mutate(undep_quants = 1 - dep_quants) \n\n\n# Now to put it in the structure, and set active for `dep_quants = 0.15`\n\n\nunique_dep_quants &lt;- unique(shared_df$dep_quants)\nn_steps &lt;- length(unique_dep_quants)\n\ndep_vals &lt;- list()\nfor (step in 1:n_steps){\n  tmp &lt;- \n    shared_df %&gt;% \n      filter(dep_quants == unique_dep_quants[step]) %&gt;% \n      select(HBname, prop_deprived) %&gt;% \n      mutate(HBname = reorder(HBname, prop_deprived))\n  \n  dep_vals[[step]] &lt;- list(\n    visible = FALSE,\n    name = paste0('Quantile: ', unique_dep_quants[step]),\n    x=tmp$prop_deprived,\n    y=tmp$HBname\n    \n  ) \n}\n\n# 15% is the third list object \n\ndep_vals[3][[1]]$visible = TRUE\n\n# Now visualise \n\n# create steps and plot all traces\ndep_steps &lt;- list()\nfig &lt;- plot_ly() \nfor (i in c(3, 1, 2, 4:n_steps)) { # Start with 3 as this is 15% and this should determine the default HB order \n fig &lt;- add_bars(fig,x=dep_vals[i][[1]]$x,  y=dep_vals[i][[1]]$y, visible = dep_vals[i][[1]]$visible, \n                 name = dep_vals[i][[1]]$name, orientation = 'h', hoverinfo = 'x+y', color = I(\"gray\"),\n                 showlegend = FALSE) %&gt;% \n   layout(\n      title = list(\n        text = glue::glue(\"Proportion of datazones in Health Boards at least this deprived\")\n      ),\n      xaxis = list(\n        title = \"Proportion this deprived in Health Board\",\n        range = list(0, 1)\n      ),\n      yaxis = list(\n        title = \"Health Board\"\n      )\n   )\n\n  step &lt;- list(args = list('visible', rep(FALSE, length(dep_vals))),\n               method = 'restyle')\n  step$args[[2]][i] = TRUE  \n  step$label = unique_dep_quants[i]\n  dep_steps[[i]] = step \n}  \n#names(dep_steps) &lt;- unique_dep_quants\n\nfig &lt;- fig %&gt;%\n  layout(sliders = list(list(active = 2,\n                             currentvalue = list(prefix = \"Deprivation: \"),\n                             steps = dep_steps)))\n\nfig\n\n\n\n\n\n\nAs you can see, there’s still some work to do regarding formatting. But it works!"
  },
  {
    "objectID": "posts/interactive-sliders/index.html#static",
    "href": "posts/interactive-sliders/index.html#static",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "Static",
    "text": "Static\nFor comparison, here’s the same data used to produce a static plot\n\n\nCode\n# Now to put it in the structure, and set active for `dep_quants = 0.15`\n\ndf_15pc &lt;- shared_df |&gt; \n  filter(between(dep_quants, 0.149, 0.151)) |&gt; \n  select(-dep_quants, -undep_quants)\n\ndf_15pc |&gt;\n  mutate(pct_deprived = 100 * prop_deprived) |&gt; \n  ggplot(aes(y= pct_deprived, x = fct_reorder(HBname, pct_deprived))) + \n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(\n      label = ifelse(df_15pc$prop_deprived &gt; 0, sprintf(\"%.1f\", pct_deprived), \"\")\n    ), \n    color = \"white\",\n    hjust = 1, \n    nudge_y = -0.5\n  ) + \n  coord_flip() + \n  labs(\n    x = \"Health Board\",\n    y = \"Percent of datazones in 15% most deprived proportion of Scotland\",\n    title = \"Percent of datazones in Health Board in 15% most deprived areas of Scotland\",\n    subtitle = \"SIMD 2020\"\n  ) + \n  geom_hline(yintercept = 0)"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-15/index.html",
    "href": "posts/glms/causal-inference/lms-are-glms-part-15/index.html",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "",
    "text": "This is the second post on a short mini-series on causal inference. The previous post provided a non-technical introduction to the core challenge of causal inference, namely that the counterfactual is always unobserved, meaning at least half of the data required to really know the causal effect of something is always missing. In the previous post different historians made different assumptions about what the counterfactual would have looked like - what would have happened if something that did happen, hadn’t happened - and based on this came to very different judgements about the effect that Henry Dundas, an 18th century Scottish politician, had on the transatlantic slave trade.\nThis post is more technical, aiming to show: how awkward phrases like “What would have happened if something that did happen, hadn’t happened” are expressed algebraically; how the core problem of causal inference is expressed in this framework; the technical impossibility of addressing the question of causal inference from the Platinum Standard of estimating causal effects on individuals; and describe the reason why randomised controlled trials (RCTs) provide the Gold Standard for trying to estimate these effects for populations."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#introduction",
    "href": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#introduction",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "",
    "text": "This is the second post on a short mini-series on causal inference. The previous post provided a non-technical introduction to the core challenge of causal inference, namely that the counterfactual is always unobserved, meaning at least half of the data required to really know the causal effect of something is always missing. In the previous post different historians made different assumptions about what the counterfactual would have looked like - what would have happened if something that did happen, hadn’t happened - and based on this came to very different judgements about the effect that Henry Dundas, an 18th century Scottish politician, had on the transatlantic slave trade.\nThis post is more technical, aiming to show: how awkward phrases like “What would have happened if something that did happen, hadn’t happened” are expressed algebraically; how the core problem of causal inference is expressed in this framework; the technical impossibility of addressing the question of causal inference from the Platinum Standard of estimating causal effects on individuals; and describe the reason why randomised controlled trials (RCTs) provide the Gold Standard for trying to estimate these effects for populations."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#models-dont-care-about-causality-but-we-do",
    "href": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#models-dont-care-about-causality-but-we-do",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Models don’t care about causality… but we do",
    "text": "Models don’t care about causality… but we do\nThe first stage when using a statistical model is to take a big rectangle of data, \\(D\\), and split the columns of the data into two types:\n\nPredictor variables, usually denoted \\(X\\)\nResponse variables, usually denoted \\(y\\)\n\nWith the predictor variables and the response variables defined, the challenge of model fitting is then to find some combination of model parameters \\(\\theta\\) that minimises in some way the gap between the observed response values \\(y\\), and the predicted response values from the model \\(Y\\).\nThe first point to note is that, from the perspective of the model, it does not matter which variable or variables from \\(D\\) we choose to put in the predictor side \\(X\\) or the response side \\(y\\). Even if we put a variable from the future in as a predictor of something in the past, the optimisation algorithms will still work in exactly the same way, working to minimise the gap between observed and predicted responses. The only problem is such a model would make no sense from a causal perspective.\nThe model also does not ‘care’ about how we think about and go about defining any of the variables that go into the predictor side of the equation, \\(X\\). But again, we do. In particular, when thinking about causality it can be immensely helpful to imagine splitting the predictor columns up into some conceptually different types. This will be helpful for thinking about causal inference using some algebra."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#the-impossible-platinum-standard",
    "href": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#the-impossible-platinum-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "The (Impossible) Platinum Standard",
    "text": "The (Impossible) Platinum Standard\nIn some previous expressions of the data, \\(D\\), we used the subscript \\(i\\) to indicate the rows of the data which go into the model. Each of these rows is, by convention, a different observation. So, instead of saying the purpose of the model is to predict \\(y\\) on \\(X\\), it’s more precisely to predict \\(y_i\\) on \\(X_i\\), for all \\(i\\) in the data (i.e. all rows in \\(D\\)).\nNow let’s do some predictor variable fission and say, for our purposes, that:\n\\[\nX_i = \\{X_i^*, Z_i\\}\n\\]\nHere \\(Z_i\\) is an assignment variable, and takes either a value of 1, meaning ‘is assigned’, or 0, meaning ‘is not assigned’. The variable \\(X_i^*\\), by contrast, means ‘all other predictor variables’.\nFor individual observations \\(D_i\\) where \\(Z_i = 1\\), the individual is exposed (or treated) to something. And for individual observations \\(D_i\\) where \\(Z_i = 0\\), the individual is not exposed (or not treated) to that thing.\nThe causal effect of assignment, or treatment, for any individual observation is:\n\\[\nTE_i = y_i|(X_i^*, Z = 1) - y_i| (X_i^*, Z = 0)\n\\]\nThe fundamental problem of causal inference, however, is that for any individual observation \\(i\\), one of the two parts of this expression is always missing. If an individual \\(i\\) had been assigned, then \\(y_i|(X_i^*, Z=1)\\) is observed, but \\(y_i|(X_i^*, Z=0)\\) is unobserved. By contrast, if an individual \\(i\\) had not been assigned, then \\(y_i|(X_i^*, Z=0)\\) is observed, but \\(y_i|(X_i^*, Z=1)\\) is unobserved.\nAnother way to think about this is as a table, where the treatment effect for an individual involves comparing the outcomes reported in two columns of the same row, but the cells in one of these two columns is always missing:\n\n\n\n\n\n\n\n\n\nindividual\noutcome if treated\noutcome if not treated\ntreatment effect\n\n\n\n\n1\n4.8\n??\n??\n\n\n2\n3.7\n??\n??\n\n\n3\n??\n2.3\n??\n\n\n4\n3.1\n??\n??\n\n\n5\n??\n3.4\n??\n\n\n6\n??\n2.9\n??\n\n\n\nThe Platinum Standard of causal effect estimation would therefore be if the missing cells in the outcome columns could be accurately filled in, allowing the treatment effect for each individual to be calculated.\nHowever, this isn’t possible. It’s social science fiction, as we can’t split the universe and compare parallel realities: one in which what happened didn’t happen, and the other in which what didn’t happen happened.\nSo, what can be done?"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#the-everyday-fools-gold-standard",
    "href": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#the-everyday-fools-gold-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "The Everyday Fool’s Gold Standard",
    "text": "The Everyday Fool’s Gold Standard\nThere’s one thing you might be tempted to do with the kind of data shown in the table above: compare the average outcome in the treated group with the average outcome in the untreated group, i.e.:\n\\[\nATE = E(y | Z = 1) - E(y | Z = 0)\n\\]\nLet’s do this with the example above:\n\n\nCode\ne_y_z1 &lt;- mean(c(4.8, 3.7, 3.1))\ne_y_z0 &lt;- mean(c(2.3, 3.4, 2.9))\n\n\n# And the difference?\ne_y_z1 - e_y_z0\n\n\n[1] 1\n\n\nIn this example, the difference in the averages between the two groups is 1.0.1 Based on this, we might imagine the first individual, who was treated, would have had a score of 3.8 rather than 4.8, and the third individual, who was not treated, would have received a score of 3.3 rather than 2.3 if they had been treated.\nSo, what’s the problem with just comparing the averages in this way? Potentially, nothing. But potentially, a lot. It depends on the data and the problem. More specifically, it depends on the relationship between the assignment variable, \\(Z\\), and the other characteristics of the individual, which includes but is not usually entirely captured by the known additional characteristics of the individual, \\(X_i^*\\).\nLet’s give a specific example: What if I were to tell you that the outcomes \\(y_i\\) were waiting times at public toilets/bathrooms, and the assignment variable, \\(Z\\), takes the value 1 if the individual has been assigned to a facility containing urinals, and 0 if the individual has been assigned to a facility containing no urinals? Would it be right to infer that the difference in the average is the average causal effect of urinals in public toilets/bathrooms?\nI’d suggest not, because there are characteristics of the individual which govern assignment to bathroom type. What this means is that \\(Z_i\\) and \\(X_i^*\\) are coupled or related to each other in some way. So, any difference in the average outcome between those assigned to (or ‘treated with’) urinals could be due to the urinals themselves; or could be due to other ways that ‘the treated’ and ‘the untreated’ differ from each other systematically. We may be able to observe a difference, and to report that it’s statistically significant. But we don’t know how much, if any, of that difference is due to the exposure or treatment of primary interest to us, and how much is due to other ways in the ‘treated’ and ‘untreated’ groups differ.\nSo, we need some way of breaking the link between \\(Z\\) and \\(X^*\\). How do we do this?"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#why-randomised-controlled-trials-are-the-real-gold-standard",
    "href": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#why-randomised-controlled-trials-are-the-real-gold-standard",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Why Randomised Controlled Trials are the real Gold Standard",
    "text": "Why Randomised Controlled Trials are the real Gold Standard\nThe clue’s in the subheading. Randomised Controlled Trials (RCTs) are known as the Gold Standard for scientific evaluation of effects for a reason, and the reason is this: they’re explicitly designed to break the link between \\(Z\\) and \\(X^*\\). And not just \\(X^*\\), but any unobserved or unincluded characteristics of the individuals, \\(W^*\\), which might also otherwise influence assignment or selection to \\(Z\\) but we either couldn’t measure or didn’t choose to include.\nThe key idea of an RCT is that assignment to either a treated or untreated group, or to any additional arms of the trial, has nothing to do with the characteristics of any individual in the trial. Instead, the allocation is random, determined by a figurature (or historically occasionally literal) coin toss. 2\nWhat this random assignment means is that assignment \\(Z\\) should be unrelated to the known characteristics \\(X^*\\), as well as unknown characteristics \\(W^*\\). The technical term for this (if I remember correctly) is that assignment is orthogonal to other characteristics, represented algebraically as \\(Z \\perp X^*\\) and \\(Z \\perp W^*\\).\nThis doesn’t mean that, for any particular trial, there will be zero correlation between \\(Z\\) and other characteristics. Nor does it mean that the characteristics of participants will be the same across trial arms. Because of random variation there are always going to be differences between arms in any specific RCT. However, we know that, because we are aware of the mechanism used to allocate participants to treated or non-treated groups (or more generally to trial arms), the expected difference in characteristics will be zero across many RCTs. Along with increased observations, this is the reason why, in principle, a meta-analysis of methodologically identical RCTs should offer even greater precision as to the causal effect of a treatment than just relying on a single RCT. 3"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#summing-up-and-coming-up",
    "href": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#summing-up-and-coming-up",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Summing up and coming up",
    "text": "Summing up and coming up\nA key point to note is that, when analysing a properly conducted RCT to estimate a treatment effect, the ATE formula shown above, which is naive and likely to be biased when working with observational data, is likely to produce an unbiased estimate of the treatment effect. Because the trial design is sophisticated in the way it breaks the link between \\(Z\\) and everything else, the statistical analysis does not have to be sophisticated.\nThe flip side of this, however, is that when the data are observational, and it would be naive (as with the urinals and waiting times example) to assume that \\(Z\\) is unlinked to everything else known (\\(X^*\\)) and unknown (\\(W^*\\)), then more careful and bespoke statistical modelling approaches are likely to be required to recover non-biased causal effects. Such modelling approaches need to be mindful of both the platinum and gold standards presented above, and rely on modelling and other assumptions to try to simulate what the treatment effects would be if these unobtainable (platinum) and unobtained (gold) standards had been obtained.\nThe next post will start to delve into some of these approaches."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#footnotes",
    "href": "posts/glms/causal-inference/lms-are-glms-part-15/index.html#footnotes",
    "title": "Part Fifteen: Causal Inference: The platinum and gold standards",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is pure fluke. I didn’t choose the values to get a difference of exactly 1, but there we go…↩︎\nIn the gold-plated gold standard of the double-blind RCT, not even the people running the trial and interacting with participants would be aware of which treatment a participant has been assigned. They would simply be given a participant ID, find a pack containing the participant’s treatment, and give this pack to the participant. Only a statistician, who has access to a random number cypher, would know which participants are assigned to which treatment, and they might not know until the trial has concluded. The idea of all of these layers of secrecy in assignment is to reduce the possibility that those running the experiment could intentionally or unintentially inform participants about which treatment they’re receiving, and so create expectations in participants about the effectiveness or otherwise of the treatments, which could have an additional effect on the outcomes.↩︎\nIn practice, issues like methodological variation, and publication bias, mean that meta-analyses of RCTs are unlikely to provide as accurate and unbiased an estimate of treatment effect as we would hope for.↩︎"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-16/index.html",
    "href": "posts/glms/causal-inference/lms-are-glms-part-16/index.html",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "",
    "text": "This is the third post in a short mini-series on causal inference, which extends a much longer series on statistical theory and practice. After introducing the fundamental issue of causal inference, namely that the counterfactual is unobserved, through description alone in part 14, part 15 provided a more technical treatment of the same issues. We described the Platinum 1 Standard of data required for causal inference as involving observing the same individuals in two different scenarios - treated2 and untreated3 - which is not possible; and the Gold Standard as being a randomised controlled trial (RCT), which is sometimes possible, but tends to be time and resource intensive. The RCT is a mechanism for breaking the association between assignment to treatment \\(Z_i\\) and both known/included covariates \\(X^*_i\\) and unknown/unincluded characteristics \\(W_i\\); this link-breaking is described as orthogonality and represented algebraically as \\(Z_i \\perp X_i^*\\) and \\(Z_i \\perp W^*_i\\).\nThe purpose of this post is to introduce some of the statistical approaches used when the only data available are observational, and so do not meet the special properties required for robust causal inference estimation of an RCT."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#introduction",
    "href": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#introduction",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "",
    "text": "This is the third post in a short mini-series on causal inference, which extends a much longer series on statistical theory and practice. After introducing the fundamental issue of causal inference, namely that the counterfactual is unobserved, through description alone in part 14, part 15 provided a more technical treatment of the same issues. We described the Platinum 1 Standard of data required for causal inference as involving observing the same individuals in two different scenarios - treated2 and untreated3 - which is not possible; and the Gold Standard as being a randomised controlled trial (RCT), which is sometimes possible, but tends to be time and resource intensive. The RCT is a mechanism for breaking the association between assignment to treatment \\(Z_i\\) and both known/included covariates \\(X^*_i\\) and unknown/unincluded characteristics \\(W_i\\); this link-breaking is described as orthogonality and represented algebraically as \\(Z_i \\perp X_i^*\\) and \\(Z_i \\perp W^*_i\\).\nThe purpose of this post is to introduce some of the statistical approaches used when the only data available are observational, and so do not meet the special properties required for robust causal inference estimation of an RCT."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#method-one-controlling-for-variables",
    "href": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#method-one-controlling-for-variables",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Method One: ‘Controlling for’ variables",
    "text": "Method One: ‘Controlling for’ variables\nThe most familiar approach for trying to estimate the causal effect of treatment \\(Z\\) on outcome \\(Y\\) is to construct a multivariate 4 regression model. Here we make sure to include those ‘nuisance parameters’ \\(X^*\\) on the predictor side of the model’s equation, along with our treatment parameter of interest \\(Z\\). For each individual \\(i\\) in the dataset \\(D\\) we can therefore use the model, calibrated on the data, to produce a prediction of the outcome \\(Y_i\\) under both the treated scenario \\(Z_i = 1\\) and the untreated scenario \\(Z_i = 0\\). As post four discussed, in the specific case of linear regression, but few other model specifications, this causal effect estimate of treatment \\(Y_i | Z=1 - Y_i | Z = 0\\) can be gleamed directly from the \\(\\beta\\) coefficient for \\(Z\\). As post four also makes clear, for other model specifications, the process for estimating causal effects can be more involved.\nIt is worth pointing out that, when using models in this way, we are really ‘just’ producing estimates of first differences, the quantity of interest which we focused on in posts 11, 12, and 13. The model prediction approach is not fundamentally any different to that discussed previously, except for two things: firstly, that we will usually be averaging across first differences for multiple observations rather a single scenario; and secondly, that we will be interpreting the first differences (or rather their aggregation) as being a causal effect estimate.\nThere are actually two types of causal effect estimate we can produce using this approach, the Average Treatment Effect (ATE), and the Average Treatment Effect on the Treated (ATT). 5 The difference between ATE and ATT is that, for ATE, the counterfactuals are simulated for all observations in the dataset \\(D\\), and that these counterfactuals will be both for individuals which were observed as treated \\(Z=1\\) and untreated \\(Z=0\\). By contrast, for ATT, only those observations in the data which were observed as treated \\(Z=1\\) are included in the causal effect estimation,6 meaning that the counterfactual being modelled will always be of the scenario \\(Z=0\\).\nSo, what are the potential problems with modelling in this way?\n\nUnobserved and unincluded covariates. Remember in the previous part we introduced the term \\(W_i^*\\)? This refers to those factors which could affect assignment \\(Z_i\\) but which are not included in our model. They could either be: i) covariates that exist in the dataset \\(D\\) but we chose not to include in the model \\(M\\); or ii) covariates that are simply not recorded in the dataset \\(D\\), so even if we wanted to, we couldn’t include them. In an RCT, the random allocation mechanism breaks both the \\(X^* \\rightarrow Z\\) and the \\(W \\rightarrow Z\\) causal paths; we don’t have to observe or even know what these factors \\(W\\) might be for an RCT to block their influence. But a regression model can only really operate to attempt to attenuate the \\(X^* \\rightarrow Z\\) pathway.\nInsufficient or improper controls. Returning to our hamster tooth growth example of post 11, recall we looked at a number of different model specifications. Our starter model specification included ‘controls for’ both dosage and supplement, and so did our final model specification. But does this mean either model is equally good at ‘controlling for’ these factors? I’d suggest they aren’t, as though our final model specification included the same covariates \\(X\\) as the initial model specification, it represented the relationship between the predictor and response variables in a qualitatively different way. For the final model specification, the dosage variable was transformed by logging it; additionally, an interaction term was included between (transformed) dosage and supplement. The reasons for this were justified by the observed relationships and by measures of penalised model fit, but we do not know if this represents the ‘best possible’ model specification. And the specification used, and the assumptions contained and represented by the model specification, will affect the predictions the model produces, including the first differences used to produce the ATE and ATT causal effect estimates.\n\nOverall, just remember that, when a researcher states in a paper that they have used a model to ‘control for’ various factors and characteristics, this can often be more a statement of what the researcher aspired to do with the model rather than managed to do. There are often a great many researcher degrees of freedom in terms of how a particular observational dataset can be used to produce modelled estimates of causal effects, and these can markedly affect the effect estimates produced.\nSo, what are some alternatives?"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#method-two-matching-methods",
    "href": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#method-two-matching-methods",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Method Two: Matching methods",
    "text": "Method Two: Matching methods\nRemember the Platinum Standard: For each individual, with their own personal characteristics (\\(X_i^*\\)), we known if they were treated \\(Z_i = 1\\) or untreated \\(Z_i = 0\\). In the sci-fi scenario of the genuine Platinum Standard, we are able to observe a clone of each of these individuals in the parallel universe of the unobserved counterfactual.\nObviously we can’t do that in reality. But maybe was can do something, with the data we have, which allows us to do something like the Platinum Standard, individual level pairwise comparison, \\(Y_i | Z_i = 1 - Y_i | Z_i = 0\\), even though we only precisely observe each individual \\(i\\) in one of the two scenarios \\(Z=1\\) or \\(Z=0\\).\nWe can do this by relaxing the requirement that the counterfactual be of a clone of the observed individual, and so identical in every way except for treatment status, and instead allow them to be compared to someone who’s merely similar to them.\nLet’s think through an example:\n\nBilly is 72 years old, male, overweight but not obese, works part time as a carpenter but is largely retired, married for five years but before that a widower for three, hypertensive; scores in the 85th percentile for conscentiousness, and 40th percentile for openness, in the Big Five Personality scale; owns his own home, worked in a factory in his twenties, likes baked beans with his biweekly fish suppers, enjoys war films but also musicals, liked holidaying in Spain back in the 1990s when his children were still children; owns a thirteen year old dog with advancing arthritis, who when younger used to take him on regular brisk walks, but now has to be cajoled to leave the house, especially when it’s cold and wet outside. He lives in the North East of England, and when that young woman - who seemed friendly but a bit nervous and had that weird piece of metal through the middle of her nose - from the survey company knocked on the door four months ago, and asked him to rate his level of agreement to the statement, “I am satisfied with my life” on a seven point scale, he answered with ‘6 - agree’, but pursed his lips and took five seconds to answer this question.\n\nObviously we have a lot of information about Billy. But that doesn’t mean the survey company, and thus our dataset \\(D\\), knows all that we now know. So, some of the information in the above is contained in \\(X^*_i\\), but others is part of \\(W_i\\).\nAnd what’s our treatment, and what’s our outcome? Let’s say the outcome is the response to the life satisfaction question, and the treatment is UK region, with the South East excluding London as the ‘control’ region.\nSo, how do matching methods work? Well, they can of course only work with the data available to them, \\(D\\). The basic approach is as follows:\n\nFor each person like Billy, who’s in the ‘treatment’ group \\(Z = 1\\) (‘treated’ to living in the North of England), we know various recorded characteristics about them \\(X_j^*\\), and so we want to look for one or more people on the ‘control’ group \\(Z=0\\) who are like the treated individual.\nSo, for Billy, we’re looking for someone in the part of the dataset where \\(Z=0\\) whose characteristics other than treatment assignment, i.e. \\(X^*\\) not \\(Z\\), are similar to Billy’s. Let’s say that, on paper, the person who’s most similar to Billy in the dataset is Mike, who’s 73 (just one year older), also owns his own home, also married, has a BMI of 26.3 (Billy’s is 26.1), and also diagnosed with hypertension. But, whereas Billy lives in the North of England, Mike lives in the South East.\nWe then compare the recorded response for Billy (6 - agree) with the recorded response for Mike (5 - mildly agree), to get an estimated treatment effect for Billy. 7\nWe then repeat the exercise for everyone else who, like Billy, is in the treatment/exposure group, trying to match them up with one or more individuals in the control group pool.\nOnce we’ve done that, we then average up the paired differences in responses - between each treated individual, and each person the’ve been paired up with - to produce an average treatment effect on the treated (ATT) estimate.\n\nHow do we go about about matchmaking Billy and other treated individuals? There are a variety of approaches, and as with using regression to ‘control for’ variables quite a lot of researcher degrees of freedom, different ways of matching, that can lead to different causal effect estimates. These include:\n\nExact matching: Find someone for all available characteristics other than assignment is exactly the same as the individual in the treated group to be matched. Obviously this is seldom possible, so an alternative is:\nCoarsened Exact Matching: Lump the characteristics into broader groups, such as 10 year age groups rather than age in single years, and match on someone who’s exactly roughly the same, i.e. matches the target within the more lumped/aggregated categories rather than exactly the same to the finest level of data resolution agailable.\nPropensity Score Matching: Use the known characteristics of individiduals to predict their probability of being in the treatment group, then use these predicted probabilities to try to balance the known characteristics of the populations in both treatment and control arms.\nSynthetic Controls: Combine and ‘mix’ observed characteristics from multiple untreated/unexposed individuals so that their average/admixed/combined characteristics is closely similar to those of individuals in the treated/exposed population.\n\nThese approaches are neither exhaustive nor mutually exclusive, and there are a great many ways that they could be applied in practice. One of the general aims of matching approaches is to reduce the extent to which ATT or ATE estimates depend on the specific modelling approach adopted, 8 and for Propensity Score Matching, it’s often to try to break the \\(X^* \\rightarrow Z\\) link, and so achieve orthogonality (\\(X^* \\perp Z\\)). However, it can’t necessarily do the same with unobserved characteristics (\\(W \\rightarrow Z\\))."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#method-three-utilise-natural-experiments",
    "href": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#method-three-utilise-natural-experiments",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Method Three: Utilise ‘natural experiments’",
    "text": "Method Three: Utilise ‘natural experiments’\nThe idea with a ‘natural experiment’ is that something happens in the world that just happens to break the links between individual characteristics and assignment to exposure/treatment. The world has therefore created a situation for us where the orthogonality assumptions \\(W \\perp Z\\) and \\(X^* \\perp Z\\) which are safe to assume when working with RCT data can also, probably, possibly, be made with certain types of observational data too. When such factors are proposed and used by economists, they tend to call them instrumental variables. Some examples include:\n\nLottery winnings to estimate the effect of money on happiness: A lottery win is an increase in money available to someone that ‘just happens’ (at least amongst lottery players). Do lottery winners’ subjective wellbeing scores increase following a win? If so for how long? Why is this preferable to just looking at the relationship between income/assets and happiness? Well, the causality could go the other way: perhaps happier people work harder, increasing their income. Or perhaps a common underlying personality factor - something like ‘conscientious stoicism’, which isn’t measured - affects both income and happiness. By utilising the randomness of a big win allocation to just a small minority of players, 9 such alternative explanations for why there are differences between populations being compared can be more safely discounted.\nComparing educational outcomes for pupils who only just got into, and only just got rejected from, selective schools and universities: Say a selective school runs its own standardised entry exam, for which a pass mark of 70 or higher is required to be accepted. An applicant who achieves a mark of 69 isn’t really that different in their aptitude than one who achieves a of 70, but this one point difference sadly appears to make the world of difference for the applicant with a 69, and gladly appears to make the world of difference for the applicant with a 70. For years afterwards, the 70-scoring applicant will have access to a fundemntally different educational environment than the 69-scoring applicant. And presumably both applicants 10 both applied because they thought the selective educational institution really would make a substantial and positive difference for their long-term educational outcomes. But does it really? By following the actual educational outcomes of pupils just north of the selection boundary, and of non-pupils just south of the selection boundaries, we have something like a treatment and control group, whose only main difference is that some are in the selective school and some are not.\n\nNote that neither of these examples are perfect substitutes for an RCT. Perhaps the people who win lotteries, or win big, are different enough from those who don’t that the winner/non-winner group’s aren’t similar in important ways. And perhaps the way people process and feel about money they get through lottery winnings isn’t the same as they they receive through earnings or social security, so the idea of there being a single money-to-happiness pathway isn’t valid. For the second example there are other concerns: of course applicants only one mark apart won’t be very different to each other, but there won’t be many of these, meaning the precision of the estimate will tend to be low. So how about expanding the ‘catchment’ to each arm, either side of the boundary line, to 2 marks, 3 marks, 5 marks? Now there should be more people in both the control and treatment arms, but they’ll also be more different to each other. 11\nAs you might expect, if using instrumental variables, the quality of the instrument matters a lot. But generally the quality of the instrument isn’t something that can be determined through any kind of formal or statistical test. It tends to be, for want of a better term, a matter of story telling. If the story the researcher can tell their audience, about the instrument and why it’s able to break the causal links it needs to break, is convincing to the audience, then the researcher and audience will both be more willing to assume that the estimates produced at the end of the analysis are causal."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#summing-up",
    "href": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#summing-up",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Summing up",
    "text": "Summing up\nSo, three methods for trying to do something technically impossible: using observational data to estimate causal effects. These methods aren’t mutually exclusive, nor are they likely to be exhaustive, and nor are any of them failsafe.\nIn the absence of being able to really know, to peak behind the veil and see the causal chains working their magic, a good pragmatic strategy tends to be to try multiple approaches. At its extreme, this can mean asking multiple teams of researchers the same question, and giving them access to the same dataset, and encouraging each team to not contact any other teams until they’ve finished their analysis, then compare the results they produce. If many different teams, with many different approaches, all tend to produce similar estimates, then maybe the estimates are really tapping into genuine causal effects, and not just reflecting some of the assumptions and biases built into the specific models and methods we’re using?"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#coming-up",
    "href": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#coming-up",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Coming up",
    "text": "Coming up\nThe next post attempts to apply matching methods to a relatively complex dataset on an economic intervention, using the MatchIt package. The post largely follows an introductory example from the package, but at some points goes ‘off piste’. I hope it does so, however, in ways that are interesting, useful, and help bridge the gaps between the theoretical discussions in this and previous posts, with the practical challenges involved in applying such theory."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#footnotes",
    "href": "posts/glms/causal-inference/lms-are-glms-part-16/index.html#footnotes",
    "title": "Part Sixteen: Causal Inference: How to try to do the impossible",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npronounced ‘unobtainium’↩︎\nAKA ‘exposed’↩︎\nAKA ‘unexposed’↩︎\nOr multivariable, if we wish to reserve the term multivariate to models with multiple response columns.↩︎\nLogically, we should assume there is also an Average Treatment Effect on the Untreated (ATU), but this is seldom discussed in practice.↩︎\nThis might be represented as something like \\(D^{(T)} \\subset D \\iff Z_i = 1\\), i.e. the data used are filtered based on the value of \\(Z\\) matching a condition.↩︎\nThis data is really ordinal, meaning we know ‘agree’ is higher than ‘mildly agree’, but don’t know how much higher, so should really be modelled as such, with something like an ordered logit or ordered probit model specification. However it’s often either treated as cardinal - 1, 2, 3, 4, 5, 6, 7 - with something like a linear regression, or collapsed into two categories (agree/ don’t agree) so standard logit or probit regression could be used.↩︎\nEven B-A is a modelling approach, to an extent.↩︎\nIt could be you. But it probably won’t be.↩︎\nOr their pushy parents…↩︎\nAn example of a bias/variance trade-off↩︎"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-17/index.html",
    "href": "posts/glms/causal-inference/lms-are-glms-part-17/index.html",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "",
    "text": "The previous post (re)introduced three ways to try to allow causal effect estimation using observational data: i) ‘controlling for’ variables using multiple regression; ii) matching methods; iii) Identifying possible ‘natural experiments’ in observational datasets. The fundamental challenge of using observational data to estimate causal effects is that we cannot be sure either the observed (\\(X^*\\)) or unobserved (\\(W\\)) characteristics of observations do not influence allocation to exposure/treatment, i.e. cannot rule out \\(X^* \\rightarrow Z\\) or \\(W \\rightarrow Z\\), meaning that statistical estimates of the effect of Z on the outcome \\(Z \\rightarrow y_i\\) may be biased.\nThe first two approaches will, within limits, generally attenuate the link between \\(X^*\\) and \\(Z\\), but can do little to break the link between \\(W\\) and \\(Z\\), as \\(W\\) is by definition those features of observational units that are not contained in the dataset \\(D\\), and so any statistical method will be ‘blind’ to. The last approach, if the instrumental variable possesses the properties we expect and hope it will, should be able to break the \\(W \\rightarrow Z\\) link too. But unfortunately that can be a big if: the instrument may not have the properties we hope it does.\nThis post will go explore some application of the first two approaches: controlling for variables using multiple regression; and using matching methods. A fuller consideration of the issues is provided in Ho et al. (2007), and the main package and dataset used will be that of the associated MatchIt package Ho et al. (2011) and vignette using the lalonde dataset."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#recap-and-aim",
    "href": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#recap-and-aim",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "",
    "text": "The previous post (re)introduced three ways to try to allow causal effect estimation using observational data: i) ‘controlling for’ variables using multiple regression; ii) matching methods; iii) Identifying possible ‘natural experiments’ in observational datasets. The fundamental challenge of using observational data to estimate causal effects is that we cannot be sure either the observed (\\(X^*\\)) or unobserved (\\(W\\)) characteristics of observations do not influence allocation to exposure/treatment, i.e. cannot rule out \\(X^* \\rightarrow Z\\) or \\(W \\rightarrow Z\\), meaning that statistical estimates of the effect of Z on the outcome \\(Z \\rightarrow y_i\\) may be biased.\nThe first two approaches will, within limits, generally attenuate the link between \\(X^*\\) and \\(Z\\), but can do little to break the link between \\(W\\) and \\(Z\\), as \\(W\\) is by definition those features of observational units that are not contained in the dataset \\(D\\), and so any statistical method will be ‘blind’ to. The last approach, if the instrumental variable possesses the properties we expect and hope it will, should be able to break the \\(W \\rightarrow Z\\) link too. But unfortunately that can be a big if: the instrument may not have the properties we hope it does.\nThis post will go explore some application of the first two approaches: controlling for variables using multiple regression; and using matching methods. A fuller consideration of the issues is provided in Ho et al. (2007), and the main package and dataset used will be that of the associated MatchIt package Ho et al. (2011) and vignette using the lalonde dataset."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#getting-started",
    "href": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#getting-started",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Getting started",
    "text": "Getting started\nWe start by loading the Matchit package and exploring the lalonde dataset.\n\n\nCode\nlibrary(tidyverse)\nlibrary(MatchIt)\nunmatched_data &lt;- tibble(lalonde)\n\nunmatched_data\n\n\n# A tibble: 614 × 9\n   treat   age  educ race   married nodegree  re74  re75   re78\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1     1    37    11 black        1        1     0     0  9930.\n 2     1    22     9 hispan       0        1     0     0  3596.\n 3     1    30    12 black        0        0     0     0 24909.\n 4     1    27    11 black        0        1     0     0  7506.\n 5     1    33     8 black        0        1     0     0   290.\n 6     1    22     9 black        0        1     0     0  4056.\n 7     1    23    12 black        0        0     0     0     0 \n 8     1    32    11 black        0        1     0     0  8472.\n 9     1    22    16 black        0        0     0     0  2164.\n10     1    33    12 white        1        0     0     0 12418.\n# ℹ 604 more rows"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#data",
    "href": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#data",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Data",
    "text": "Data\nThe description of the lalonde dataset is as follows:\n\n\nCode\nhelp(lalonde)\n\n\n\nDescription\nThis is a subsample of the data from the treated group in the National Supported Work Demonstration (NSW) and the comparison sample from the Population Survey of Income Dynamics (PSID). This data was previously analyzed extensively by Lalonde (1986) and Dehejia and Wahba (1999).\nFormat\nA data frame with 614 observations (185 treated, 429 control). There are 9 variables measured for each individual.\n\n“treat” is the treatment assignment (1=treated, 0=control).\n“age” is age in years.\n“educ” is education in number of years of schooling.\n“race” is the individual’s race/ethnicity, (Black, Hispanic, or White). Note previous versions of this dataset used indicator variables black and hispan instead of a single race variable.\n“married” is an indicator for married (1=married, 0=not married).\n“nodegree” is an indicator for whether the individual has a high school degree (1=no degree, 0=degree).\n“re74” is income in 1974, in U.S. dollars.\n“re75” is income in 1975, in U.S. dollars.\n“re78” is income in 1978, in U.S. dollars.\n\n“treat” is the treatment variable, “re78” is the outcome, and the others are pre-treatment covariates.\n\nLet’s look at the data to get a sense of it:\n\n\nCode\nunmatched_data |&gt;\n    mutate(treat = as.factor(treat)) |&gt;\n    filter(re78 &lt; 25000) |&gt;\n    ggplot(aes(y = re78, x = re75, shape = treat, colour = treat)) + \ngeom_point() + \ngeom_abline(intercept = 0, slope = 1) +\ncoord_equal() + \nstat_smooth(se = FALSE, method = \"lm\")\n\n\n\n\n\nClearly this is quite complicated data, where the single implied control, wages in 1975 (re75) is not sufficient. There are also a great many observations where wages in either of both years were 0, hence the horizontal and vertical streaks apparent.\nThe two lines are the linear regression lines for the two treatment groups as a function of earlier wage. The lines are not fixed to have the same slope, so the differences in any crude treatment effect estimate vary by earlier wage, but for most previous wages the wages in 1978 appear to be lower in the treatment group (blue), than the control group (red). This would suggest either that the treatment may be harmful to wages… or that there is severe imbalance between the characteristics of persons in both treatment conditions.\nLet’s now start to use a simple linear regression to estimate an average treatment effect, before adding more covariates to see how these model-derived estimates change\n\n\nCode\n# Model of treatment assignment only\nmod_01 &lt;- lm(re78 ~ treat, unmatched_data)\nsummary(mod_01) \n\n\n\nCall:\nlm(formula = re78 ~ treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -6984  -6349  -2048   4100  53959 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   6984.2      360.7  19.362   &lt;2e-16 ***\ntreat         -635.0      657.1  -0.966    0.334    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7471 on 612 degrees of freedom\nMultiple R-squared:  0.001524,  Adjusted R-squared:  -0.0001079 \nF-statistic: 0.9338 on 1 and 612 DF,  p-value: 0.3342\n\n\nOn average the treated group had (annual?) wages $635 lower than the control group. However the difference is not statistically significant.\nNow let’s add previous wage from 1975\n\n\nCode\nmod_02 &lt;- lm(re78 ~ re75 + treat, unmatched_data)\nsummary(mod_02)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-15918  -5457  -2025   3824  54103 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5547.63718  412.84637  13.438  &lt; 2e-16 ***\nre75           0.58242    0.08937   6.517  1.5e-10 ***\ntreat        -90.79498  641.40291  -0.142    0.887    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7230 on 611 degrees of freedom\nMultiple R-squared:  0.06642,   Adjusted R-squared:  0.06336 \nF-statistic: 21.73 on 2 and 611 DF,  p-value: 7.611e-10\n\n\nPreviously observed wage is statistically significant and positive. The point estimate on treatment is smaller, and even less ‘starry’.\nNow let’s add all possible control variables and see what the treatment effect estimate produced is:\n\n\nCode\nmod_03 &lt;- lm(re78 ~ re75 + age + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_03)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + age + educ + race + married + nodegree + \n    re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13595  -4894  -1662   3929  54570 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -1.174e+03  2.456e+03  -0.478   0.6328    \nre75         2.315e-01  1.046e-01   2.213   0.0273 *  \nage          1.298e+01  3.249e+01   0.399   0.6897    \neduc         4.039e+02  1.589e+02   2.542   0.0113 *  \nracehispan   1.740e+03  1.019e+03   1.708   0.0882 .  \nracewhite    1.241e+03  7.688e+02   1.614   0.1071    \nmarried      4.066e+02  6.955e+02   0.585   0.5590    \nnodegree     2.598e+02  8.474e+02   0.307   0.7593    \nre74         2.964e-01  5.827e-02   5.086 4.89e-07 ***\ntreat        1.548e+03  7.813e+02   1.982   0.0480 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6948 on 604 degrees of freedom\nMultiple R-squared:  0.1478,    Adjusted R-squared:  0.1351 \nF-statistic: 11.64 on 9 and 604 DF,  p-value: &lt; 2.2e-16\n\n\nWith all of these variables as controls, the effect of treatment is now statistically significant and positive, associated with on average an increase of $155 over the control group.\nHowever, we should probably be concerned about how dependent this estimate is on the specific model specification we used. For example, it is fairly common to try to ‘control for’ nonlinearities in age effects by adding a squared term. If modeller decisions like this don’t make much difference, then its addition shouldn’t affect the treatment effect estimate. Let’s have a look:\n\n\nCode\nmod_04 &lt;- lm(re78 ~ re75 + poly(age, 2) + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_04)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + poly(age, 2) + educ + race + married + \n    nodegree + re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13692  -4891  -1514   3884  54313 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -5.395e+02  2.172e+03  -0.248   0.8039    \nre75           2.190e-01  1.057e-01   2.072   0.0387 *  \npoly(age, 2)1  3.895e+03  7.994e+03   0.487   0.6262    \npoly(age, 2)2 -6.787e+03  7.918e+03  -0.857   0.3917    \neduc           3.889e+02  1.599e+02   2.432   0.0153 *  \nracehispan     1.682e+03  1.021e+03   1.648   0.0999 .  \nracewhite      1.257e+03  7.692e+02   1.634   0.1028    \nmarried        2.264e+02  7.267e+02   0.312   0.7555    \nnodegree       3.185e+02  8.504e+02   0.375   0.7081    \nre74           2.948e-01  5.832e-02   5.055 5.73e-07 ***\ntreat          1.369e+03  8.090e+02   1.692   0.0911 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6949 on 603 degrees of freedom\nMultiple R-squared:  0.1488,    Adjusted R-squared:  0.1347 \nF-statistic: 10.54 on 10 and 603 DF,  p-value: &lt; 2.2e-16\n\n\nThe inclusion of the squared term to age has changed the point estimate of treatment from around $1550 to $1370. However it has also changed the statistical significance of the effect from p &lt; 0.05 to p &lt; 0.10, i.e. from ‘statistically significant’ to ‘not statistically significant’. If we were playing the stargazing game, this might be the difference between a publishable finding and an unpublishable finding.\nAnd what if we excluded age, because none of the terms are statistically significant at the standard level?\n\n\nCode\nmod_05 &lt;- lm(re78 ~ re75 + educ + race + married + nodegree + re74 + treat, unmatched_data)\nsummary(mod_05)\n\n\n\nCall:\nlm(formula = re78 ~ re75 + educ + race + married + nodegree + \n    re74 + treat, data = unmatched_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-13681  -4912  -1652   3877  54648 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -676.43048 2115.37702  -0.320   0.7493    \nre75           0.22705    0.10395   2.184   0.0293 *  \neduc         389.00786  154.33865   2.520   0.0120 *  \nracehispan  1710.16654 1015.15590   1.685   0.0926 .  \nracewhite   1241.00510  768.22972   1.615   0.1067    \nmarried      478.55017  671.28910   0.713   0.4762    \nnodegree     201.04497  833.99164   0.241   0.8096    \nre74           0.30209    0.05645   5.351 1.24e-07 ***\ntreat       1564.68896  779.65173   2.007   0.0452 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6943 on 605 degrees of freedom\nMultiple R-squared:  0.1475,    Adjusted R-squared:  0.1363 \nF-statistic: 13.09 on 8 and 605 DF,  p-value: &lt; 2.2e-16\n\n\nNow the exclusion of this term, which the coefficient tables suggested wasn’t statistically significant, but intuitively we recognise as an important determinant of labour market activity, has led to yet another point estimate. It’s switched back to ‘statistically significant’ again, but now the point estimate is about $1565 more. Such estimates aren’t vastly different, but they definitely aren’t the same, and come from just a tiny same of the potentially hundreds of different model specifications we could have considered and decided to present to others."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#matching-with-matchit",
    "href": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#matching-with-matchit",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Matching with MatchIt",
    "text": "Matching with MatchIt\nAs the title of Ho et al. (2007) indicates, matching methods are presented as a way of preprocessing the data to reduce the kind of model dependence we’ve just started to explore. Let’s run the first example they present in the MatchIt vignette then discuss what it means:\n\n\nCode\nm.out0 &lt;- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = NULL, distance = \"glm\")\nsummary(m.out0)\n\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = NULL, distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       429     185\nUnmatched       0       0\nDiscarded       0       0\n\n\nWith method = NULL, the matchit function presents some summary estimates of differences in characteristics between the Treatment and Control groups. For example, the treated group has an average age of around 25, compared with 28 in the control group, have a slightly higher education score, are more likely to be Black, less likely to be Hispanic, and much less likely to be White (all important differences in the USA context, especially perhaps of the 1970s). They are also less likely to be married, more likely to have no degree, and have substantially earlier wages in both 1974 and 1975. Clearly a straightforward comparision between average outcomes is far from a like-with-like comparisons between groups. The inclusion of other covariates (\\(X^*\\)) does seem to have made a difference, switching the reported direction of effect and its statistical significance, but if we could find a subsample of the control group whose characteristics better match those of the treatment groups, we would hopefully get a more precise and reliable estimate of the effect of the labour market programme.\nThe next part of the vignette shows MatchIt working with some fairly conventional settings:\n\n\nCode\nm.out1 &lt;- matchit(treat ~ age + educ + race + married + \n                   nodegree + re74 + re75, data = lalonde,\n                 method = \"nearest\", distance = \"glm\")\nm.out1\n\n\nA matchit object\n - method: 1:1 nearest neighbor matching without replacement\n - distance: Propensity score\n             - estimated with logistic regression\n - number of obs.: 614 (original), 370 (matched)\n - target estimand: ATT\n - covariates: age, educ, race, married, nodegree, re74, re75\n\n\nThe propensity score, i.e. the probability of being in the treatment group, has been predicted using the other covariates, and using logistic regression. For each individual in the treatment group, a ‘nearest neighbour’ in the control group has been identified with the most similar propensity score, which we hope also will also mean the characteristics of the treatment group, and matched pairs from the control group, will be more similar too.\nWe can start to see what this means in practice by looking at the summary of the above object\n\n\nCode\nsummary(m.out1)\n\n\n\nCall:\nmatchit(formula = treat ~ age + educ + race + married + nodegree + \n    re74 + re75, data = lalonde, method = \"nearest\", distance = \"glm\")\n\nSummary of Balance for All Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.1822          1.7941     0.9211    0.3774\nage              25.8162       28.0303         -0.3094     0.4400    0.0813\neduc             10.3459       10.2354          0.0550     0.4959    0.0347\nraceblack         0.8432        0.2028          1.7615          .    0.6404\nracehispan        0.0595        0.1422         -0.3498          .    0.0827\nracewhite         0.0973        0.6550         -1.8819          .    0.5577\nmarried           0.1892        0.5128         -0.8263          .    0.3236\nnodegree          0.7081        0.5967          0.2450          .    0.1114\nre74           2095.5737     5619.2365         -0.7211     0.5181    0.2248\nre75           1532.0553     2466.4844         -0.2903     0.9563    0.1342\n           eCDF Max\ndistance     0.6444\nage          0.1577\neduc         0.1114\nraceblack    0.6404\nracehispan   0.0827\nracewhite    0.5577\nmarried      0.3236\nnodegree     0.1114\nre74         0.4470\nre75         0.2876\n\nSummary of Balance for Matched Data:\n           Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\ndistance          0.5774        0.3629          0.9739     0.7566    0.1321\nage              25.8162       25.3027          0.0718     0.4568    0.0847\neduc             10.3459       10.6054         -0.1290     0.5721    0.0239\nraceblack         0.8432        0.4703          1.0259          .    0.3730\nracehispan        0.0595        0.2162         -0.6629          .    0.1568\nracewhite         0.0973        0.3135         -0.7296          .    0.2162\nmarried           0.1892        0.2108         -0.0552          .    0.0216\nnodegree          0.7081        0.6378          0.1546          .    0.0703\nre74           2095.5737     2342.1076         -0.0505     1.3289    0.0469\nre75           1532.0553     1614.7451         -0.0257     1.4956    0.0452\n           eCDF Max Std. Pair Dist.\ndistance     0.4216          0.9740\nage          0.2541          1.3938\neduc         0.0757          1.2474\nraceblack    0.3730          1.0259\nracehispan   0.1568          1.0743\nracewhite    0.2162          0.8390\nmarried      0.0216          0.8281\nnodegree     0.0703          1.0106\nre74         0.2757          0.7965\nre75         0.2054          0.7381\n\nSample Sizes:\n          Control Treated\nAll           429     185\nMatched       185     185\nUnmatched     244       0\nDiscarded       0       0\n\n\nPreviously, there were 185 people in the treatment group, and 429 people in the control group. After matching there are 185 people in the treatment group… and also 185 people in the control group. So, each of the 185 people in the treatment group has been matched up with a ‘data twin’ in the control group, so the ATT should involve more of a like-with-like comparison.\nThe summary presents covariate-wise differences between the Treatment and Control groups for All Data, then for Matched Data. We would hope that, in the Matched Data, the differences are smaller for each covariate, though this isn’t necessarily the case. After matching, for example, we can see that the Black proportion in the Control group is now 0.47 rather than 0.20, and that the earlier income levels are lower, in both cases bringing the values in the Control group closer to, but not identical to, those in the Treatment group. Another way of seeing how balancing has changed things is to look at density plots:\n\n\nCode\nplot(m.out1, type = \"density\", interactive = FALSE,\n     which.xs = ~age + married + re75+ race + nodegree + re74)\n\n\n\n\n\n\n\n\nIn these density charts, the darker lines indicate the Treatment group and the lighter lines the Control groups. The matched data are on the right hand side, with All data on the left. We are looking to see if, on the right hand side, the two sets of density lines are more similar than they are on the right. Indeed they do appear to be, though we can also tell they are far from identical."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#estimating-treatment-effect-sizes-after-matching",
    "href": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#estimating-treatment-effect-sizes-after-matching",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Estimating Treatment Effect Sizes after matching",
    "text": "Estimating Treatment Effect Sizes after matching\nHistorically, the MatchIt package was designed to work seamlessly with Zelig, which made it much easier to use a single library and framework to produce ‘quantities of interest’ using multiple model structures. However Zelig has since been deprecated, meaning the vignette now recommends using the marginaleffects package. We’ll follow their lead:\nFirst the vignette recommends extracting matched data from the matchit output:\n\n\nCode\nm.data &lt;- match.data(m.out1)\n\nm.data &lt;- as_tibble(m.data)\nm.data\n\n\n# A tibble: 370 × 12\n   treat   age  educ race   married nodegree  re74  re75   re78 distance weights\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n 1     1    37    11 black        1        1     0     0  9930.   0.639        1\n 2     1    22     9 hispan       0        1     0     0  3596.   0.225        1\n 3     1    30    12 black        0        0     0     0 24909.   0.678        1\n 4     1    27    11 black        0        1     0     0  7506.   0.776        1\n 5     1    33     8 black        0        1     0     0   290.   0.702        1\n 6     1    22     9 black        0        1     0     0  4056.   0.699        1\n 7     1    23    12 black        0        0     0     0     0    0.654        1\n 8     1    32    11 black        0        1     0     0  8472.   0.790        1\n 9     1    22    16 black        0        0     0     0  2164.   0.780        1\n10     1    33    12 white        1        0     0     0 12418.   0.0429       1\n# ℹ 360 more rows\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nWhereas the unmatched data contains 614 observations, the matched data contains 370 observations. Note that the Treatment group contained 185 observations, and that 370 is 185 times two. So, the matched data contains one person in the Control group for each person in the Treatment group.\nWe can also see that, in addition to the metrics originally included, the matched data contains three additional variables: ‘distance’, ‘weights’ and ‘subclass’. The ‘subclass’ field is perhaps especially useful for understanding the intuition of the approach, because it helps show which individual in the Control group has been paired with which individual in the Treatment group. Let’s look at the first three subgroups:\n\n\nCode\nm.data |&gt; filter(subclass == '1')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race  married nodegree   re74  re75  re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt;    &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    37    11 black       1        1     0      0 9930.    0.639       1\n2     0    22     8 black       1        1 16961.     0  959.    0.203       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nSo, for the first subclass, a 37 year old married Black person with no degree has been matched to a 22 year old Black married person with no degree.\n\n\nCode\nm.data |&gt; filter(subclass == '2')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race  married nodegree  re74  re75   re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;   &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    33    12 white       1        0    0      0 12418.   0.0429       1\n2     0    39    12 white       1        0 1289.     0  1203.   0.0430       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFor the second subclass a 33 year old married White person with a degree has been paired with a 39 year old White person with a degree.\n\n\nCode\nm.data |&gt; filter(subclass == '3')\n\n\n# A tibble: 2 × 12\n  treat   age  educ race   married nodegree  re74  re75   re78 distance weights\n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;    &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     1    31     9 hispan       0        1     0    0  26818.    0.250       1\n2     0    16    10 white        0        1     0  190.  2137.    0.105       1\n# ℹ 1 more variable: subclass &lt;fct&gt;\n\n\nFor the third subclass, a 31 year old unmarried Hispanic person with no degree has been paired with a 16 year old White person with no degree.\nIn each case, we can see the pairings are similar in some ways but (as with the last example) quite dissimilar in others. The matching algorithm is trying to do the best it can with the data available, especially with the constraint1 that once a person in the Control group has been paired up once to someone in the Treatment group, they can’t be paired up again with someone else in the Treatment group.\nThe identification of these specific pairings suggests we can used a fairly crude strategy to produce an estimate of the ATT: namely just compare the outcome across each of these pairs. Let’s have a look at this:\n\n\nCode\ntrt_effects &lt;- \n    m.data |&gt;\n        group_by(subclass) |&gt;\n        summarise(\n            ind_treat_effect = re78[treat == 1] - re78[treat == 0]\n        ) |&gt; \n        ungroup()\n\ntrt_effects |&gt;\n    ggplot(aes(ind_treat_effect)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = mean(trt_effects$ind_treat_effect), colour = \"red\") + \n    geom_vline(xintercept = 0, colour = 'lightgray', linetype = 'dashed')\n\n\n\n\n\nThis crude paired comparison suggests an average difference that’s slightly positive, of $894.37.\nThis is not a particularly sophisticated or ‘kosher’ approach however. Instead the vignette suggests calculating the treatment effect estimate as follows:\n\n\nCode\nlibrary(\"marginaleffects\")\n\nfit &lt;- lm(re78 ~ treat * (age + educ + race + married + nodegree + \n             re74 + re75), data = m.data, weights = weights)\n\navg_comparisons(fit,\n                variables = \"treat\",\n                vcov = ~subclass,\n                newdata = subset(m.data, treat == 1),\n                wts = \"weights\")\n\n\n\n  Term Contrast Estimate Std. Error    z Pr(&gt;|z|)   S 2.5 % 97.5 %\n treat    1 - 0     1121        752 1.49    0.136 2.9  -354   2596\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \nType:  response \n\n\nUsing the recommended approach, the ATT estimate is now $1121. Not statistically significant at the conventional 95% threshold, but also more likely to be positive than negative."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#summary",
    "href": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#summary",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Summary",
    "text": "Summary\nIn this post we have largely followed along with the introductionary vignette from the MatchIt package, in order to go from the fairly cursory theoretical overview in the previous post, to showing how some of the ideas and methods relating to multiple regression and matching methods work in practice. There are a great many ways that both matching, and multiple regression, can be implemented in practice, and both are likely to affect any causal effect estimates we produce. However, the aspiration of using matching methods is to somewhat reduce the dependency that causal effect estimates have on the specific model specifications we used."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#coming-up",
    "href": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#coming-up",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Coming up",
    "text": "Coming up\nThe next post concludes this series on causal inference, by discussing in more detail a topic many users of causal inference will assume I should have started with: the Pearlean school of causal inference. In brief: the approach to causal inference I’m used to interprets the problem, fundamentally, as a missing data problem; whereas the Pearlean approach interprets it more as a modelling problem. I see value in both sides, as well as some points of overlap, but in general I’m both more used to, and more comfortable with, the missing data interpretation."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#footnotes",
    "href": "posts/glms/causal-inference/lms-are-glms-part-17/index.html#footnotes",
    "title": "Part Seventeen: Causal Inference: Controlling and Matching Approaches",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI think this is implied by the use of method = \"nearest\", which is the default, meaning ‘greedy nearest neighbour matching’.↩︎"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html",
    "title": "Part Seven: Feeling Uncertain",
    "section": "",
    "text": "In the previous post we managed to use numerical optimisation, with the optim() function, to good \\(\\beta\\) estimates for linear regression model fit to some toy data. In this post, we will explore how the optim() function can be used to produce estimates of uncertainty about these \\(\\beta\\) coefficients, and how these relates to measures of uncertainty presented in the standard lm and glm summary functions."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#aim",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#aim",
    "title": "Part Seven: Feeling Uncertain",
    "section": "",
    "text": "In the previous post we managed to use numerical optimisation, with the optim() function, to good \\(\\beta\\) estimates for linear regression model fit to some toy data. In this post, we will explore how the optim() function can be used to produce estimates of uncertainty about these \\(\\beta\\) coefficients, and how these relates to measures of uncertainty presented in the standard lm and glm summary functions."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#prereqs",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#prereqs",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Prereqs",
    "text": "Prereqs\nAs before, we’ll be using the same toy dataset, and same log likelihood function, as in the last two posts in this series. Let’s create these again:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nLet’s also run and save our parameter estimates produced both ‘the hard way’ (using optim), and ‘the easier way’ (using ‘glm’)\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nCode\npars_optim &lt;- optim_results$par\n\nnames(pars_optim) &lt;- c(\"beta0\", \"beta1\", \"eta\")\n\npars_optim\n\n\n    beta0     beta1       eta \n 2.460571  1.375421 -1.336209 \n\n\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- tibble(x = x, y = y)\nmod_glm &lt;- glm(y ~ x, data = df, family = gaussian(link=\"identity\"))\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nSo, both optim and the summary to mod_glm report \\(\\{\\beta_0 = 2.36, \\beta_1 = 1.38\\}\\), so both approaches appear to arrive at the same point on the log likelihood surface.\nHowever, note that the glm summary reports not just the estimates themselves (in the Estimate column of coefficients), but also standard errors (the Std. Error column) and derived quantities (t value, Pr(&gt;|t|), and the damnable stars at the very right of the table). How can these measures of uncertainty about the true value of the \\(\\beta\\) coefficients be derived from optim?"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#barefoot-and-blind-a-weird-analogy-for-a-complicated-idea",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#barefoot-and-blind-a-weird-analogy-for-a-complicated-idea",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Barefoot and Blind: A weird analogy for a complicated idea",
    "text": "Barefoot and Blind: A weird analogy for a complicated idea\nImagine optim, your hill-finding robo-chauffeur, has taken you to the top of a likelihood surface. Then it leaves you there…\n… and you’re blind, and have no shoes. (You also have an uncanny sense of your orientation, whether north-south, east-west, or some other angle.)\nSo, you know you’re at the top of the hill, but you can’t see what the landscape around you looks like. However, you still want to get a sense of this landscape, and how it varies around the spot you’re standing on.\nWhat do you do?\nIf you’re playing along with this weird thought experiment, one approach would be to use your feet as depth sensors. You make sure you never stray from where you started, and to always keep one foot planted on this initial spot (which you understand to be the highest point on the landscape). Then you use your other foot to work out how much further down the surface is from the highest point as you venture away from the highest point in different directions.\nSay you keep your left foot planted on the highest point, and make sure your right foot is always positioned (say) 10 cm horizontally from your left foot. Initially your two feet are arranged east-west; let’s call this 0 degrees. When you put your right foot down, you notice it needs to travel 2 cm further down to reach terra ferma relative to your left foot.\n2cm at 0 degrees. You’ll remember that.\nNow you rotate yourself 45 degrees, and repeat the same right foot drop. This time it needs to travel 3cm down relative to your left foot.\n3cm at 45 degrees. You remember that too.\nNow you rotate another 45 degrees, north-south orientation, place your right foot down; now it falls 5cm down relative to your left foot.\n2cm at 0 degrees; 3cm at 45 degrees; 5cm at 90 degrees.\nNow with this information, you try to construct the landscape you’re on top of with your mind’s eye, making the assumption that the way it has to have curved from the peak you’re on to lead to the drops you’ve observed is consistent all around you; i.e. that there’s only one hill, you’re on top of it, and it’s smoothly curved in all directions.\nIf you could further entertain the idea that your feet are infinitely small, and the gap between feet is also infinitely small (rather than the 10cm above), then you have the intuition behind this scary-looking but very important formula from King (1998) (p. 89):\n\\[\n\\widehat{V(\\hat{\\theta})} = - \\frac{1}{n}[\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]^{-1}_{\\tilde{\\theta} = \\hat{\\theta}}\n\\]\nWhat this is saying, in something closer to humanese, is something like:\n\nOur best estimate of the amount of uncertainty we have in our estimates is a function of how much the likelihood surface curves at the highest point on the surface. (It also gets less uncertain, the more observations we have)."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#information-and-uncertainty",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#information-and-uncertainty",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Information and uncertainty",
    "text": "Information and uncertainty\nAmongst the various bells, whistles and decals in the previous formula is the superscript \\((.)^{-1}\\). This means invert, which for a single value means \\(\\frac{1}{.}\\) but for a matrix means something conceptually the same but technically not.\nAnd what’s being inverted in the last formula? A horrible-looking expression, \\([\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]_{\\tilde{\\theta} = \\hat{\\theta}}\\), that’s basically an answer to the question of how curvy is the log likelihood surface at its peak position?\nWithin King (1998) (p.89, eq. 4.18), this expression (or rather the negative of the term) is defined as \\(I(\\hat{\\theta} | y)\\), where \\(I(.)\\) stands for information.\nSo, the algebra are saying\n\nUncertainty is inversely related to information\n\nOr perhaps even more intuitively\n\nThe more information we have, the less uncertain we are\n\nOf course this makes sense. If you ask someone “How long will this task take?”, and they say “Between one hour and one month”, they likely have less information about how long the task will actually than if they had said “Between two and a half and three hours”. More generally:\n\nShallow gradients mean wide uncertainty intervals mean low information\nSharp gradients mean narrow uncertaintly intervals mean high information\n\nThis is, fundamentally, what the blind and barefoot person in the previous analogy is trying to achieve: by feeling out the local curvature around the highest point, they are trying to work out how much information they have about different pieces of the model. The curvature along any one dimension of the surface (equivalent to the 0 and 90 degree explorations) indicates how much information there is about any single coefficient, and the curvature along the equivalent of a 45 degree plane gives a measure of how associated any two coefficients tend to be.\nWith these many analogies and equations spinning in our heads, let’s now see how these concepts can be applied in practice."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#optimal-uncertainty",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#optimal-uncertainty",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Optimal uncertainty",
    "text": "Optimal uncertainty\nHaving reminded myself of the particular options for optim that are typically used to report parameter uncertainty, let’s run the follows:\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nWe have used a slightly different algorithm (‘BFGS’), and a different way of specifying the function to search over (using fnscale = -1 to invert the likelihood), but we have the same par estimates as before: \\(\\beta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\). So the changes we’ve made to the optim arguments haven’t changed what it estimates.\nOne new argument we’ve set in optim is hessian = TRUE. Hessian is a kind of coarse fabric made from vegetable waste, typically woven in a criss-crossing, grid-like pattern. Hessian matrices are matrices of second derivatives, as described in the wikipedia article. 1 If you can bear to recall the really complex expression above, for calculating the curvature around a point on a surface, you’ll recall it’s also about second derivatives.\nNone of this is a coincidence. The hessian component of the optim output above contains what we need.\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\nhess\n\n\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nYou might notice that the Hessian matrix is square, with as many columns as rows. And, that the number of columns (or rows) is equal to the number of parameters we have estimated, i.e. three in this case.\nYou might also notice that the values are symmetrical about the diagonal running from the top left to the bottom right.\nAgain, this is no accident.\nRemember that variation is inversely related to information, and that \\((.)^{-1}\\) is the inversion operator on \\(I(.)\\), the Information Matrix. Well, this Hessian is (pretty much) \\(I(.)\\). So let’s see what happens when we invert it (using the solve operator):\n\n\nCode\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\nAs with hess, inv_hess is symmetric around the top-left to bottom-right diagonal. For example, the value on row 2 and column 1 is the same as on row 1, column 2.\nWe’re mainly interested in the first two columns and rows, as these contain the values most comparable with the glm summary reports\n\n\nCode\ninv_hess_betas &lt;- inv_hess[1:2, 1:2]\n\ninv_hess_betas\n\n\n             [,1]         [,2]\n[1,]  0.033577455 -0.004379668\n[2,] -0.004379668  0.004379668\n\n\nWhat the elements of the above matrix provide are estimates of the variances of a single parameter \\(\\beta_j\\), and/or the covariances between any two parameters \\(\\{\\beta_0, \\beta_1\\}\\). In this example:\n\\[\n\\begin{bmatrix}\nvar(\\beta_0) & cov(\\beta_0, \\beta_1) \\\\\ncov(\\beta_1, \\beta_0) & var(\\beta_1)\n\\end{bmatrix}\n\\]\nIt’s because the on-diagonal terms are variances of uncertaintly for a single term, that it can be useful to take the square root of these terms to get estimates of the standard errors:\n\n\nCode\nsqrt(diag(inv_hess_betas))\n\n\n[1] 0.18324152 0.06617906\n\n\nCompare with the Std Err term in the following:\n\n\nCode\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe estimates from the Hessian in optim, of \\(\\{0.18, 0.07\\}\\), are not exactly the same as the \\(\\{0.21, 0.08\\}\\) reported for mod_glm; the methods employed are not identical. But they are hopefully similar enough to demonstrate they provide similar information about similar quantities of uncertainty."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#summary",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#summary",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Summary",
    "text": "Summary\nThis is probably the most difficult single section so far. Don’t worry: it’s likely to get easier from here on in."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#coming-up",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#coming-up",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Coming up",
    "text": "Coming up\nThe next part of the series goes into more detail about how numerical optimisation works."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#footnotes",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-07/index.html#footnotes",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough I had assumed Hessian matrices are called Hessian matrices because they sort-of resemble the criss-crossing grids of Hessian bags, they’re actually named after Otto Hesse, who proposed them.↩︎"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "",
    "text": "The previous post, perhaps the toughest of the series, showed how some special settings within R’s numerical optimisation optim() function can be used to estimate how much uncertainty there is in our estimates of the the model parameters \\(\\beta\\). We covered the concept that information and uncertainty are inversely related: the more information we have, the less uncertain we are, and vice versa. We estimated parameter uncertainty around the point that maximised (log) likelihood by asking the algorithm to take small steps from this highest point in different directions (dimensions, in effect variables), and report how steep the fall is in different directions. Steeper falls along a dimension imply less uncertainty and so more more information and narrower confidence intervals; as usual, the converse is also true. The component returned by optim() which reports the results of this ‘stepping out’ is a square matrix called the Hessian, which can be inverted to produce estimates of the variances and covarainces of each of the parameters being estimated in our model."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#recap",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#recap",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "",
    "text": "The previous post, perhaps the toughest of the series, showed how some special settings within R’s numerical optimisation optim() function can be used to estimate how much uncertainty there is in our estimates of the the model parameters \\(\\beta\\). We covered the concept that information and uncertainty are inversely related: the more information we have, the less uncertain we are, and vice versa. We estimated parameter uncertainty around the point that maximised (log) likelihood by asking the algorithm to take small steps from this highest point in different directions (dimensions, in effect variables), and report how steep the fall is in different directions. Steeper falls along a dimension imply less uncertainty and so more more information and narrower confidence intervals; as usual, the converse is also true. The component returned by optim() which reports the results of this ‘stepping out’ is a square matrix called the Hessian, which can be inverted to produce estimates of the variances and covarainces of each of the parameters being estimated in our model."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#aim",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#aim",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Aim",
    "text": "Aim\nThe aims of this post are to show how estimates of uncertainty around the point estimates produced from the Hessian, based around the curvature measured around the point of maximum likelihood, are similar to those produced using a much more extensive (and computationally intensive) interrogation of the likelihood surface using a grid-search approach. It will also show how representations of joint uncertainty for parameter values can be generated using the multivariate normal distribution."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#comparing-inferred-and-observed-likelihood-surfaces",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#comparing-inferred-and-observed-likelihood-surfaces",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Comparing inferred and observed likelihood surfaces",
    "text": "Comparing inferred and observed likelihood surfaces\nLet’s return once again to the toy dataset used in the last two posts, whose true parameters we know because we made them up; and also the log likelihood function:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nTo estract estimates of uncertainty about the uncertainty of each of these parameters, we used optim() with the options shown below, and then inverted the matrix to go from information to uncertainty.\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\nWe were especially interested in the first two rows and columns of this matrix, as they correspond to uncertainty in \\(\\beta = \\{ \\beta_0, \\beta_1 \\}\\).\n\n\nCode\ninv_hess_betas &lt;- inv_hess[1:2, 1:2]\n\ninv_hess_betas\n\n\n             [,1]         [,2]\n[1,]  0.033577455 -0.004379668\n[2,] -0.004379668  0.004379668\n\n\nBack in part five, we used this same dataset to show how the log likelihood varies for various, equally spaced, candidate values for \\(\\beta_0\\) and \\(\\beta_1\\) (having fixed \\(\\eta = \\exp({\\sigma^2})\\) at its true value). This led to the followng map of the landscape1\n\n\nCode\nlibrary(tidyverse)\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-15, 15, by = 0.05),\n    beta_1 = seq(-15, 15, by = 0.05)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nWithin the above we can see that the log likelihood landscape for these two parameters looks like a bivariate normal distribution, we can also see a bit of a slant in this normal distribution. This implies a correlation between the two candidate values. The direction of the slant is downwards from left to right, implying the correlation is negative.\nFirstly let’s check that the correlation between \\(\\beta_0\\) and \\(\\beta_1\\) implied by the Hessian is negative. These are the off-diagonal elements, either first row, second column, or second row, first column:\n\n\nCode\ninv_hess_betas[1,2]\n\n\n[1] -0.004379668\n\n\nCode\ninv_hess_betas[2,1]\n\n\n[1] -0.004379668\n\n\nYes they are!\nAs mentioned previously, the likelihood surface produced by the gridsearch method involves a lot of computations, so a lot of steps, and likely a lot of trial and error, if it were to be used to try to find the maximum likelihood value for the parameters. By contrast, the optim() algorithm typically involves far fewer steps, ‘feeling’ its way up the hill until it reaches a point where there’s nowhere higher. 2 When it then reaches this highest point, it then ‘feels’ the curvature around this point in multiple directions, producing the Hessian. The algorithm doesn’t see the likelihood surface, because it hasn’t travelled along most of it. But the Hessian can be used to infer the likelihood surface, subject to subject (usually) reasonable assumptions.\nWhat are these (usually) reasonable assumptions? Well, that the likelihood surface can be approximated by a multivariate normal distribution, which is a generalisation of the standard Normal distribution over more than one dimensions.3\nWe can use the mvrnorm function from the MASS package, alongside the point estimates and Hessian from optim, in order to produce estimates of \\(\\theta = \\{ \\beta_0, \\beta_1, \\eta \\}\\) which represent reasonable uncertainty about the true values of each of these parameters. Algebraically, this can be expressed as something like the following:\n\\[\n\\tilde{\\theta} \\sim Multivariate Normal(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere \\(\\dot{\\theta}\\) are the point estimates from optim() and \\(\\Sigma\\) is the implied variance-covariance matrix recovered from the Hessian.\nLet’s create this MVN model and see what kinds of outputs it produces.\n\n\nCode\nlibrary(MASS)\n\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\nhead(param_draws)\n\n\n        beta0    beta1         eta\n[1,] 2.564978 1.375636 -0.30407255\n[2,] 2.440111 1.367774 -1.16815288\n[3,] 2.775332 1.338583 -0.05574937\n[4,] 2.283011 1.481799 -0.26095101\n[5,] 2.695635 1.228565 -1.18369341\n[6,] 2.686818 1.483601 -0.44262363\n\n\nWe can see that mvrnorm(), with these inputs from optim() produces three columns: one for each parameter being estimated \\(\\{ \\beta_0, \\beta_1, \\eta \\}\\). The n argumment indicates the number of draws to take; in this case, 10000. This number of draws makes it easier to see how much variation there is in each of the estimates.\n\n\nCode\ndf_param_draws &lt;- \nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    mutate(\n        sig2 = exp(eta)\n    ) |&gt;\n    pivot_longer(\n        -draw, \n        names_to = \"param\",\n        values_to = \"value\"\n    ) \n    \ndf_param_draws |&gt;\n    ggplot(aes(x = value)) + \n    geom_density() + \n    facet_grid(param ~ .) + \n    geom_vline(xintercept=0)\n\n\n\n\n\nThere are a number of things to note here: firstly, that the average of the \\(\\beta_0\\) and \\(\\beta_1\\) values appear close to their known ‘true’ values of 2.5 and 1.4 respectively. Secondly, that whereas the \\(\\eta\\) values are normally distributed, the \\(\\sigma^2\\) values derived from them are not, and are never below zero; this is the effect of the exponential link between quantities. Thirdly, that the implied values of \\(\\sigma^2\\) do appear to be centred around 0.25, as they should be as \\(\\sigma\\) was set to 0.50 in the model.\nAnd forthly, that the density around \\(\\beta_1\\) is more peaked than around \\(\\beta_0\\). This concords with what we saw previously in the filled contour map: both the horizontal beta0 axis and vertical beta1 axis are on the same scale, but the oval is broader along the horizontal axis than the vertical axis. This in effect implies that we have more information about the true value of \\(\\beta_1\\), the slope, than about the true value of \\(\\beta_0\\), the intercept.\nWe can also use these draws to reproduce something similar to, but not identical to, 4 the previous filled contour map:\n\n\nCode\n# param_draws |&gt;\n#     as_tibble(\n#         rownames = 'draw'\n#     ) |&gt;\n#     ggplot(aes(x = beta0, y = beta1)) + \n#     geom_point(alpha = 0.1) + \n#     coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 10))\n\nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    ggplot(aes(x = beta0, y = beta1)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nOnce again, we see the same qualities as the contour map produced by interrogating the likelihood surface exhaustively: the distribution appears bivariate normal; there is a greater range in the distribution along the beta0 than the beta1 axis; and there is evidence of some negative correlation between the two parameters."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#summary",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#summary",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Summary",
    "text": "Summary\nThis post has shown how optim(), which in its vanilla state only returns point estimates, can be configured to also calculater and report the Hessian, a record of instantaneous curvature around the point estimates. Even without a fine-grained and exhausive search throughout the likelihood surface, this measure of curvature can be used to produce similar measures of uncertainty to the more exhausive approach, in a fraction of the number of computations.\nMore importantly, it can be used to generate draws of plausible combinations of parameter values, something denoted as \\(\\tilde{\\theta}\\) earlier. This is something especially useful for producing honest quantities of interest, which both tell users of models something they want to know, while also representing how uncertain we are in this knowledge.\nWe’ll cover that in the next post… 5"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#footnotes",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-08/index.html#footnotes",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’ve narrowed the space between values slightly, and increased the range of permutations of values to search through, for an even more precise recovery of the likelihood landscape.↩︎\nIn practice, the algorithm seeks to minimise the value returned by the function, not maximise it, hence the negative being applied through the argument fnscale = -1 in the control argument. But the principle is identical.↩︎\nThis means that, whereas the standard Normal returns a single output, the Multivariate Normal returns a vector of outputs, one for each parameter in \\(\\theta\\), which should also be the length of the diagonal (or alternatively either the number of rows or columns) of \\(\\Sigma\\).↩︎\nThe values will not be identical because the values for \\(\\eta\\), and so \\(\\sigma^2\\), have not been fixed at the true value in this example.↩︎\nI was expecting to cover it in the current post, but this is probably enough content for now!↩︎"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-10/index.html",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-10/index.html",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "",
    "text": "Within this series, parts 1-4 formed what we might call ‘section one’, and part 5-9 ‘section two’.\nSection one (re)introduced statistical models as siblings, children of a mother model which combines a systematic component (an equation with a \\(=\\) symbol in it) and a stochastic component (an equation with a \\(\\sim\\) in it, which can largely be read as ‘drawn from’). Part one provided a graphical representation of the challenge of model fitting from an algorithmic perspective, in which the parameters that go into the two component are tweaked and tweaked until some condition is met: usually that the discrepency between model predictions and observed outcomes are minimised some way. The two component mother model is largely equivalent to the concept of the generalised linear model: parts two and three explored this association a bit more. Part four demonstrated how, for statistical models other than standard linear regression, the kinds of answer one usually wants from a model are not readily apparent from the model coefficients themselves, and so careful use of model predictions, and calibration of the questions, are required to use models to answer substantivelly meaningful questions.\nSection two aimed to show how likelihood theory is used in practice in order to justify a loss function that algorithms can be used to try to ‘solve’.1 These loss functions and optimisation algorithms are usually called implicitly by statistical model functions, but we did things the hard way by building the loss function from scratch, and evoking the algorithms more directly, using R’s optim() function. As well as the pedagogical value (and bragging rights) of being able to create and fit statistical models directly, an additional benefit of using optim() (with some of its algorithms) is that it returns something called the Hessian. The Hessian is what allows us to be honest when making model predictions and projections, showing how our uncertainty about the true value of the model parameters (the multiple inputs that optim() algorithms try to tweak until they’re good enough) leads to uncertainty in what we’re predicting and projecting.\nUnfortunately, we’re still in section two. The material below aims to repeat the same kind of exercise performed for standard linear regression, but using logistic regression instead."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-10/index.html#recap",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-10/index.html#recap",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "",
    "text": "Within this series, parts 1-4 formed what we might call ‘section one’, and part 5-9 ‘section two’.\nSection one (re)introduced statistical models as siblings, children of a mother model which combines a systematic component (an equation with a \\(=\\) symbol in it) and a stochastic component (an equation with a \\(\\sim\\) in it, which can largely be read as ‘drawn from’). Part one provided a graphical representation of the challenge of model fitting from an algorithmic perspective, in which the parameters that go into the two component are tweaked and tweaked until some condition is met: usually that the discrepency between model predictions and observed outcomes are minimised some way. The two component mother model is largely equivalent to the concept of the generalised linear model: parts two and three explored this association a bit more. Part four demonstrated how, for statistical models other than standard linear regression, the kinds of answer one usually wants from a model are not readily apparent from the model coefficients themselves, and so careful use of model predictions, and calibration of the questions, are required to use models to answer substantivelly meaningful questions.\nSection two aimed to show how likelihood theory is used in practice in order to justify a loss function that algorithms can be used to try to ‘solve’.1 These loss functions and optimisation algorithms are usually called implicitly by statistical model functions, but we did things the hard way by building the loss function from scratch, and evoking the algorithms more directly, using R’s optim() function. As well as the pedagogical value (and bragging rights) of being able to create and fit statistical models directly, an additional benefit of using optim() (with some of its algorithms) is that it returns something called the Hessian. The Hessian is what allows us to be honest when making model predictions and projections, showing how our uncertainty about the true value of the model parameters (the multiple inputs that optim() algorithms try to tweak until they’re good enough) leads to uncertainty in what we’re predicting and projecting.\nUnfortunately, we’re still in section two. The material below aims to repeat the same kind of exercise performed for standard linear regression, but using logistic regression instead."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-10/index.html#log-likelihood-for-logistic-regression",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-10/index.html#log-likelihood-for-logistic-regression",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "Log likelihood for logistic regression",
    "text": "Log likelihood for logistic regression\nPreviously we focused on the log likelihood for standard linear regression. Let’s now do the same for logistic regression. According to the relevant section of the Zelig website:\nStochastic component \\[\nY_i \\sim Bernoulli(y_i | \\pi_i )\n\\]\n\\[\nY_i = \\pi_i^{y_i}(1 - \\pi_i)^{1-y_i}\n\\]\nwhere \\(\\pi_i = P(Y_i = 1)\\)\nAnd\nSystematic Component\n\\[\n\\pi_i = \\frac{1}{1 + \\exp{(-x_i \\beta)}}\n\\]\nThe likelihood is the product of the above for all observations in the dataset \\(i \\in N\\)\n\\[\nL(.) = \\prod{\\pi_i^{y_i}(1 - \\pi_i)^{1-y_i}}\n\\]\nThe effect of logging the above2:\n\\[\n\\log{L(.)} = \\sum{[y_i \\log{\\pi_i} + (1-y_i)\\log{(1-y_i)}]}\n\\]\nThis can now be implemented as a function:\n\n\nCode\nllogit &lt;- function(par, y, X){\n    xform &lt;- function(z) {1 / (1 + exp(-z))}\n    p &lt;- xform(X%*%par)\n    sum(y * log(p) + (1-y) * log(1 - p))\n}\n\n\nLet’s pick an appropriate dataset. How about… picking a Palmer Penguin!?\n\n\nCode\nlibrary(tidyverse)\npalmerpenguins::penguins\n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nLet’s say we want to predict whether a penguin is of the Chinstrap species\n\n\nCode\npalmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    ggplot(aes(x = bill_length_mm, y = bill_depth_mm, colour = is_chinstrap, shape = sex)) + \n    geom_point()\n\n\n\n\n\nNeither bill length nor bill depth alone appears to distinguish between chinstrap and other species. But perhaps the interaction (product) of the two terms would do:\n\n\nCode\npalmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    mutate(bill_size = bill_length_mm * bill_depth_mm) |&gt;\n    ggplot(aes(x = bill_size, fill = is_chinstrap)) + \n    facet_wrap(~sex) + \n    geom_histogram()\n\n\n\n\n\nThe interaction term isn’t great at separating the two classes, but seems to be better than either length or size alone. So I’ll include it in the model.\n\n\nCode\ndf &lt;- palmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    mutate(bill_size = bill_length_mm * bill_depth_mm) |&gt;\n    mutate(is_male = as.numeric(sex == \"male\"))\n\ny &lt;- df$is_chinstrap\n\nX &lt;- cbind(1, df[,c(\"bill_length_mm\", \"bill_depth_mm\", \"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\n\nSo, including the intercept term, our predictor matrix \\(X\\) contains 5 columns, including the interaction term bill_size. 3\nLet’s try now to use the above in optim()\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = rep(0, 5), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] 82.9075239 -2.4368673 -6.4311531  0.1787047 -6.4900678\n\n$value\n[1] -33.31473\n\n$counts\nfunction gradient \n     137       45 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n             [,1]         [,2]          [,3]         [,4]         [,5]\n[1,]   -12.103063    -550.0621    -209.30944    -9674.925    -3.700623\n[2,]  -550.062097  -25256.3082   -9500.55848  -443670.225  -184.360139\n[3,]  -209.309443   -9500.5585   -3650.65107  -168517.417   -68.158844\n[4,] -9674.924703 -443670.2251 -168517.41718 -7846293.352 -3464.964868\n[5,]    -3.700623    -184.3601     -68.15884    -3464.965    -3.700623\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n            [,1]         [,2]         [,3]          [,4]         [,5]\n[1,] 41.95816335 -0.156192235 -0.309892876 -4.036895e-02  9.329019450\n[2,] -0.15619224 -0.005017392 -0.024806420  1.070652e-03 -0.139430425\n[3,] -0.30989288 -0.024806420 -0.042869947  2.854565e-03 -0.337480429\n[4,] -0.04036895  0.001070652  0.002854565 -7.331214e-05  0.003098092\n[5,]  9.32901945 -0.139430425 -0.337480429  3.098092e-03  1.202424836\n\n\nNow let’s compare with glm()\n\n\nCode\nmod_glm &lt;- glm(is_chinstrap ~ bill_length_mm * bill_depth_mm +is_male, data = df, \nfamily = binomial())\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = is_chinstrap ~ bill_length_mm * bill_depth_mm + \n    is_male, family = binomial(), data = df)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  365.2924    88.3341   4.135 3.54e-05 ***\nbill_length_mm                -8.9312     2.0713  -4.312 1.62e-05 ***\nbill_depth_mm                -23.6184     5.5003  -4.294 1.75e-05 ***\nis_male                      -11.8725     2.6121  -4.545 5.49e-06 ***\nbill_length_mm:bill_depth_mm   0.5752     0.1292   4.452 8.53e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 337.113  on 332  degrees of freedom\nResidual deviance:  49.746  on 328  degrees of freedom\nAIC: 59.746\n\nNumber of Fisher Scoring iterations: 9\n\n\nUh oh! On this occasion it appears one or both approaches have become confused. A five dimensional search space might be too much for the algorithms to cope with, especially with collinearity 4 between some of the terms. Let’s simplify the task a bit, and just use intercept, bill size, and is_male as covariates. First with the standard package:\n\n\nCode\nmod_glm_simpler &lt;- glm(is_chinstrap ~ bill_size +is_male,   data = df, \nfamily = binomial())\nsummary(mod_glm_simpler)\n\n\n\nCall:\nglm(formula = is_chinstrap ~ bill_size + is_male, family = binomial(), \n    data = df)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -32.815339   4.325143  -7.587 3.27e-14 ***\nbill_size     0.043433   0.005869   7.400 1.36e-13 ***\nis_male      -7.038215   1.207740  -5.828 5.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 337.11  on 332  degrees of freedom\nResidual deviance:  90.60  on 330  degrees of freedom\nAIC: 96.6\n\nNumber of Fisher Scoring iterations: 7\n\n\nAnd now with the bespoke function and optim\n\n\nCode\nX &lt;- cbind(1, df[,c(\"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\nfuller_optim_output &lt;- optim(\n    par = rep(0, 3), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] -32.60343219   0.04314546  -6.98585077\n\n$value\n[1] -45.30114\n\n$counts\nfunction gradient \n      73       18 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]         [,2]         [,3]\n[1,]    -13.008605   -10662.078    -5.201308\n[2,] -10662.078251 -8846787.584 -4846.390833\n[3,]     -5.201308    -4846.391    -5.201308\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n             [,1]          [,2]         [,3]\n[1,] -536.7022079  0.7206703142 -134.7923170\n[2,]    0.7206703 -0.0009674672    0.1807806\n[3,] -134.7923170  0.1807806218  -33.4602664\n\n\nThe estimates from the two approaches are now much closer, even if they aren’t as close to each other as in the earlier examples. Using optim(), we have parameter estimates \\(\\beta = \\{\\beta_0 = -32.60, \\beta_1 = 0.04, \\beta_2 = -6.99\\}\\), and using glm(), we have estimates \\(\\beta = \\{\\beta_0 = -32.82, \\beta_1 = 0.04, \\beta_2 = -7.04 \\}\\)\nIf we cheat a bit, and give the five dimensional version starting values closer to the estimates from glm(), we can probably get similar estimates too.\n\n\nCode\nX &lt;- cbind(1, df[,c(\"bill_length_mm\", \"bill_depth_mm\", \"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\nfuller_optim_output &lt;- optim(\n    par = c(300, -10, -29, 0.5, -10), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] 299.5512512  -7.3684567 -19.3951742   0.4747209  -9.7521255\n\n$value\n[1] -25.33208\n\n$counts\nfunction gradient \n     153       22 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n             [,1]          [,2]          [,3]         [,4]         [,5]\n[1,]    -8.378918    -370.41592    -140.86865    -6342.301    -1.800406\n[2,]  -370.415921  -16580.87909   -6238.75358  -284403.350   -91.239716\n[3,]  -140.868648   -6238.75358   -2387.19776  -107598.410   -33.018551\n[4,] -6342.300809 -284403.34960 -107598.40987 -4906697.476 -1685.235507\n[5,]    -1.800406     -91.23972     -33.01855    -1685.236    -1.800406\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n            [,1]         [,2]        [,3]          [,4]         [,5]\n[1,] -59.5448267  2.316365876  5.14842594 -0.1737609491 10.383684649\n[2,]   2.3163659 -0.064512887 -0.16844980  0.0044962968 -0.166413655\n[3,]   5.1484259 -0.168449797 -0.33888931  0.0106735535 -0.387558164\n[4,]  -0.1737609  0.004496297  0.01067355 -0.0002712683  0.004068597\n[5,]  10.3836846 -0.166413655 -0.38755816  0.0040685965  1.904433768\n\n\nWell, they are closer, but they aren’t very close. As mentioned, the glm() model produced warnings, and some of the variables are likely to be collinear, so this initial specification may have been especially difficult to fit. Both approaches found an answer, but neither seem happy about it!"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-10/index.html#summary",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-10/index.html#summary",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "Summary",
    "text": "Summary\nIn the exercise above we did for logistic regression what the previous few posts in section two did for standard regression: i.e. we derived the log likelihood, applied it using optim, and compared with results from the glm() package. We saw in this case that fitting models isn’t always straightforward. We were - well, I was - overly ambitious in building and applying an overly parameterised model specification. But we eventually got to similar parameter values using both approaches.\nThough this wasn’t as straightforward as I was hoping for, I’m presenting it warts-and-all. In principle, the log-likelihood maximisation approach generalises to a great many model specifications, even if in practice some model structures aren’t as straightforward to fit as others."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-10/index.html#coming-up",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-10/index.html#coming-up",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "Coming up",
    "text": "Coming up\nIn the next post, I’ll finally be moving off ‘section two’, with its algebra and algorithms, and showing some tools that can be used to make honest prediction and projections with models, but without all the efforts undertaken here and in the last few posts."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-10/index.html#footnotes",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-10/index.html#footnotes",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBy ‘loss function’ I mean a function that takes one or more numeric inputs and returns a single numeric output. The aim of the algorithm is to find the combination of inputs that minimises (or maximises) the function’s output.↩︎\nThanks to this post. My calculus is a bit rusty these days.↩︎\nAn important point to note is that, though bill_size is derived from other variables, it’s its own variable, and so has another distinct ‘slot’ in the vector of \\(\\beta\\) parameters. It’s just another dimension in the search space for optim to search through.↩︎\nThis is fancy-speak for when two terms aren’t independent, or both adding unique information. For example, length in mm, length in cm, and length in inches would all be perfectly collinear, so shouldn’t all be included in the model.↩︎"
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-01/index.html",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-01/index.html",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-01/index.html#tldr",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-01/index.html#tldr",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-01/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-01/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "Part 1: What are statistical models and how are they fit?",
    "text": "Part 1: What are statistical models and how are they fit?\nIt’s common for different statistical methods to be taught as if they’re completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\nSomething I’ve found immensely helpful over the years is the following pair of equations:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nIn words, the above is saying something like:\n\nThe predicted response \\(Y_i\\) for a set of predictors \\(X_i\\) is assumed to be drawn from (the \\(\\sim\\) symbol) a stochastic distribution (\\(f(.,.)\\))\nThe stochastic distribution contains both parameters we’re interested in, and which are determined by the data \\(\\theta_i\\), and parameters we’re not interested in and might just have to assume, \\(\\alpha\\).\nThe parameters we’re interested in determining from the data \\(\\theta_i\\) are themselves determined by a systematic component \\(g(.,.)\\) which take and transform two inputs: The observed predictor data \\(X_i\\), and a set of coefficients \\(\\beta\\)\n\nAnd graphically this looks something like:\n\n\n\n\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --&gt; g\n  beta --&gt; g\n  g --&gt; theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt; Y\n\n\n\n\n\n\n\nTo understand how this fits into the ‘whole game’ of modelling, it’s worth introducing another term, \\(D\\), for the data we’re using, and to say that \\(D\\) is partitioned into observed predictors \\(X_i\\), and observed responses, \\(y_i\\).\nFor each observation, \\(i\\), we therefore have a predicted response, \\(Y_i\\), and an observed response, \\(y_i\\). We can compare \\(Y_i\\) with \\(y_i\\) to get the difference between the two, \\(\\delta_i\\).\nNow, obviously can’t change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the \\(\\beta\\) parameters that feed into the systematic component \\(g\\). Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n\n\n\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D --&gt;|partition| X\n  D --&gt;|partition| y\n  X --&gt; g\n  beta --&gt;|rerun| g\n  g --&gt;|transform| theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt;|predict| Y\n  \n  Y --&gt;|compare| diff\n  y --&gt;|compare| diff\n  \n  diff --&gt;|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n\n\n\n\n\nPretty much all statistical model fitting involves iterating along this \\(g \\to \\beta\\) and \\(\\beta \\to g\\) feedback loop until some kind of condition is met involving minimising \\(\\delta\\).\nI’ll expand on this idea further in part 2."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-03/index.html",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-03/index.html",
    "title": "Part Three: glm is just fancy lm",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-03/index.html#tldr",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-03/index.html#tldr",
    "title": "Part Three: glm is just fancy lm",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-03/index.html#part-3-how-to-express-a-linear-model-as-a-generalised-linear-model",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-03/index.html#part-3-how-to-express-a-linear-model-as-a-generalised-linear-model",
    "title": "Part Three: glm is just fancy lm",
    "section": "Part 3: How to express a linear model as a generalised linear model",
    "text": "Part 3: How to express a linear model as a generalised linear model\nIn the last part, we introduced two types of generalised linear models, with two types of transformation for the systematic component of the model, g(.), the logit transformation, and the identity transformation. This post will show how this framework is implemented in practice in R.\nIn R, there’s the lm function for linear models, and the glm function for generalised linear models.\nI’ve argued previously that the standard linear regression is just a specific type of generalised linear model, one that makes use of an identity transformation I(.) for its systematic component g(.). Let’s now demonstrate that by producing the same model specification using both lm and glm.\nWe can start by being painfully unimaginative and picking using one of R’s standard datasets\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\niris |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\nIt looks like, where the petal length is over 2.5, the relationship with sepal length is fairly linear\n\niris |&gt; \n  filter(Petal.Length &gt; 2.5) |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\nSo, let’s make a linear regression just of this subset\n\niris_ss &lt;- \n  iris |&gt; \n  filter(Petal.Length &gt; 2.5) \n\nWe can produce the regression using lm as follows:\n\nmod_lm &lt;- lm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\nAnd we can use the summary function (which checks the type of mod_lm and evokes summary.lm implicitly) to get the following:\n\nsummary(mod_lm)\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09194 -0.26570  0.00761  0.21902  0.87502 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3731 on 98 degrees of freedom\nMultiple R-squared:  0.6864,    Adjusted R-squared:  0.6832 \nF-statistic: 214.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nWoohoo! Three stars next to the Petal.Length coefficient! Definitely publishable!\nTo do the same using glm.\n\nmod_glm &lt;- glm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\nAnd we can use the summary function for this data too. In this case, summary evokes summary.glm because it knows the class of mod_glm contains glm.\n\nsummary(mod_glm)\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nSo, the coefficients are exactly the same. But there’s also some additional information in the summary, including on the type of ‘family’ used. Why is this?\nIf we look at the help for glm we can see that, by default, the family argument is set to gaussian.\nAnd if we delve a bit further into the help file, in the details about the family argument, it links to the family help page. The usage statement of the family help file is as follows:\nfamily(object, ...)\n\nbinomial(link = \"logit\")\ngaussian(link = \"identity\")\nGamma(link = \"inverse\")\ninverse.gaussian(link = \"1/mu^2\")\npoisson(link = \"log\")\nquasi(link = \"identity\", variance = \"constant\")\nquasibinomial(link = \"logit\")\nquasipoisson(link = \"log\")\nEach family has a default link argument, and for this gaussian family, this link is the identity function.\nWe can also see that, for both the binomial and quasibinomial family, the default link is logit, which transforms all predictors onto a 0-1 scale, as shown in the last post.\nSo, by using the default family, the Gaussian family is selected, and by using the default Gaussian family member, the identity link is selected.\nWe can confirm this by setting the family and link explicitly, showing that we get the same results\n\nmod_glm2 &lt;- glm(Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), data = iris_ss)\nsummary(mod_glm2)\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), \n    data = iris_ss)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nIt’s the same!\nHow do these terms used in the glm function, family and link, relate to the general framework in King, Tomz, and Wittenberg (2000)?\n\nfamily is the stochastic component, f(.)\nlink is the systematic component, g(.)\n\nThey’re different terms, but it’s the same broad framework.\nLinear models are just one type of general linear model!\n\nComing up\nIn the next part of this series, we will delve into the differences between linear regression models and logistic regression models, with a focus on how to get meaningful effect estimates from both types of model."
  },
  {
    "objectID": "posts/glms/hacker-stats/permutation-with-base-r/index.html",
    "href": "posts/glms/hacker-stats/permutation-with-base-r/index.html",
    "title": "Permutation Testing, and the intuition of the Null hypothesis, with Base R",
    "section": "",
    "text": "In this post I’ll cover the intuition of permutation tests through a little toy example. In a follow-up post I’ll discuss how this intuition can be implemented (and made a bit easier) using the infer package."
  },
  {
    "objectID": "posts/glms/hacker-stats/permutation-with-base-r/index.html#core-intuition-for-permutation-tests",
    "href": "posts/glms/hacker-stats/permutation-with-base-r/index.html#core-intuition-for-permutation-tests",
    "title": "Permutation Testing, and the intuition of the Null hypothesis, with Base R",
    "section": "Core intuition for permutation tests",
    "text": "Core intuition for permutation tests\nLet’s try to understand the intuition of permutation tests using a (rather boring) story:\n\nImagine you have two types of index cards: red cards and blue cards.\nSay there are 12 red cards and 8 blue cards, so a total of 20 cards.\nOn each of the cards is a value. Let’s say it’s a binary value: 1 (maybe for a ‘success’) or 0 (a ‘failure’).\nLet’s say the values from the red card came from flipping a specific coin, Coin A, 12 times, and writing a 1 on a blank red index card if the coin came up heads, and 0 on a blank red index card if the coin came up tails.\nThe values on the blue cards came from flipping a different coin, Coin B, 8 times, and doing the same thing, but with blue cards instead of red cards.\n\nWhat you want to know is whether Coin A or Coin B are different, i.e. one has a different probability of producing heads than the other one. However, you don’t have access either Coin A or Coin B. The only information you have to go on is the 20 index cards: 12 red, 8 blue.\nHow do you go about determining if the two coins are different, when you don’t have access to either coin, and all you have are the 20 index cards?\nOne approach is to perform permutation tests. This is a way of using computation to produce a Null Distribution, meaning a distribution of some kind of summary statistic that you would expect to observe if there were really no difference between Coin A and Coin B. This Null Distribution is a distribution of summary values that you would expect to observe if the Null Hypothesis were true, where the Null Hypothesis is that Coin A and Coin B behave in exactly the same way. You then compare the corresponding summary statistic from the observed data against this Null Distribution. If the observed summary statistic is far from the range of summary statistics, then you have more reason to Reject the Null Hypothesis, which generally corresponds to evidence for the Alternative Hypothesis, which in this case is that Coin A and Coin B are different.\nThe way you would manually perform a permutation test (without computers) in this example is as follows:\n\nYou get a big box of only red index cards, and a big box of blue index cards, all of which are blank.\nFrom the big box of red index cards, you take 12 cards, and put them into a little pile.\nFrom the big box of blue index cards, you take 8 cards, and put them into the same pile containing the 12 red index cards.\nYou then randomly shuffle the 20 cards with values written on them (your data), and place this randomly shuffled pile face down.\nYou take the top card from the data pile, turn it over, and write its value on the first card in the small pile of 20 blank cards you’ve just made. You then take this now-not-blank card from the small pile, and place it next to the pile of now 19 blank cards.\nYou then repeat with the next card in the data pile, and the next card in the small blank card pile, until all cards in the blank card pile have had a value (1 or 0) written onto them.\nYou then repeat steps 2 through 6 a large number of times: say another 999 times. At the end of this, you now have one real dataset, comprising 20 index cards - 12 red, 8 blue - and 1000 ‘fake datasets’, i.e. 1000 piles of 20 index cards each - 12 red, 8 blue - which also each have 1 or 0 written on them.\nAfter you have done this, you calculate a summary statistic for both the one real dataset, and the 1000 ‘fake datasets’. Say this is the difference in the proportions of 1 in the red subset of cards, and the blue subset in cards. You calculate this for the real dataset, and call it the observed statistic. And you also calculate it for each of the 1000 fake datasets, which provides your Null distribution for this same statistic.\nFinally, you compare the observed statistic (from the real dataset), with the Null distribution of summary statistics. If the observed statistic is somewhere in the middle of the Null distribution, there’s little reason to reject the Null Hypothesis; if it’s quite far from the Null distribution, there’s much more reason to reject the Null Hypothesis.\n\nAs you can tell from the description above, this would be quite a slow approach to making a Null distribution if we were to follow the steps manually. This is why historically many of the approaches for producing Null distributions that you might be familiar with involve algebra-based theoretical distributions. In the example above a classic way of calculating the Null distribution would be using the Chi-Squared distribution. Historically, it was much quicker for one person to figure out the algebra once, and perform calculation based on the algebraic solution, than to perform a permutation test. These days, even if we have an algebraic solution, it can still be as quick or quicker to perform a permutation test.\nLet’s actually make the dataset I’ve described above (using a random number seed so the answers don’t change). Let’s say in our example the true proportion for Coin A is 0.55, and for Coin B it’s 0.50. (Something we’d never know in practice.)\n\n\nCode\nset.seed(7) # Random number set.seed\n\ndraws_A &lt;- rbinom(n=12, size=1, prob=0.55)\ndraws_B &lt;- rbinom(n=8, size=1, prob=0.50)\n\ncard_colour &lt;- c(\n    rep(\"red\", 12),\n    rep(\"blue\", 8)\n)\n\nreal_data &lt;- data.frame(\n    card_colour = card_colour,\n    outcome = c(draws_A, draws_B)\n)\n\nreal_data\n\n\n   card_colour outcome\n1          red       0\n2          red       1\n3          red       1\n4          red       1\n5          red       1\n6          red       0\n7          red       1\n8          red       0\n9          red       1\n10         red       1\n11         red       1\n12         red       1\n13        blue       1\n14        blue       0\n15        blue       0\n16        blue       0\n17        blue       1\n18        blue       0\n19        blue       1\n20        blue       0\n\n\nIn this example, what is the proportion of 1s in the red card subgroup, and the blue card subgroup?\n\n\nCode\nprop_in_red &lt;- real_data$outcome[real_data$card_colour == \"red\"] |&gt;\n    mean()\n\nprop_in_blue &lt;- real_data$outcome[real_data$card_colour == \"blue\"] |&gt;\n    mean()\n\ndiff_in_props &lt;- prop_in_red - prop_in_blue\n\ndiff_in_props\n\n\n[1] 0.375\n\n\nIn this example the proportion ‘heads’ in the red subgroup (from coin A) is 0.750, and in the blue subgroup (from coin B) happens to be exactly 0.375. This means the difference in proportions is 0.375.\nHow would we use a permutation test to produce a Null distribution of differences in proportions between the two groups?\nHere’s one approach:\n\n\nCode\nnReps &lt;- 1000 # We'll perform 1000 replications/resamples\n\nnullVector &lt;- vector(mode = \"numeric\", length = 1000)\n\n\noutcomes &lt;- real_data$outcome\nlabels &lt;- real_data$card_colour\n\nnObs &lt;- length(outcomes)\n\nfor (i in 1:nReps){\n\n    random_draw_of_outcomes &lt;- sample(outcomes, size = nObs, replace = FALSE)\n\n    fake_prop_red &lt;- mean(\n        random_draw_of_outcomes[labels == \"red\"]\n    )\n\n    fake_prop_blue &lt;- mean(\n        random_draw_of_outcomes[labels == \"blue\"]\n    )\n\n    fake_diff_outcomes &lt;- fake_prop_red - fake_prop_blue\n\n    nullVector[i] &lt;- fake_diff_outcomes\n}\n\nhead(nullVector)\n\n\n[1] -0.25000000 -0.04166667  0.16666667 -0.04166667 -0.25000000 -0.04166667\n\n\nWhat does the distribution of differences look like?\n\n\nCode\nhist(nullVector)\n\n\n\n\n\nHere we can see quite a wide range of differences in proportions are generated by the permutation-based Null distribution. We can use the quantile function to get a sense of the range:\n\n\nCode\nquantile(nullVector, prob = c(0.025, 0.050, 0.25, 0.50, 0.75, 0.95, 0.975))\n\n\n       2.5%          5%         25%         50%         75%         95% \n-0.45833333 -0.45833333 -0.25000000 -0.04166667  0.16666667  0.37500000 \n      97.5% \n 0.37500000 \n\n\nHere the median value of the proportion of differences is -0.042. Half of the values are between -0.025 and 0.0167; 90% of the values are between -0.458 and 0.375, and 95% of values are between -0.458 and 0.375.\nFor reference, the real observed difference in proportions is 0.375. This seems to be at the far right end of the Null distribution. We can calculate what is in effect a p-value, of the probability of seeing a value as or more extreme than the observed value from the Null distribution, by counting up the proportion of Null distribution values that were as or more extreme than the observed value:\n\n\nCode\nsum(nullVector &gt;= diff_in_props) / length(nullVector)\n\n\n[1] 0.103\n\n\nSo, the proportion of times the Null distribution generates a value as great or greater than the observed value is about 10%. This wouldn’t meet conventional thresholds of statistical significance, which would be less than 5% of values being this or more extreme. However it does seem from the data that it’s more likely than not the two coins may be different. (And we know, as a fact, the two coins are different, because we made them to be!)\nFinally, let’s use the Chi-squared test to try to answer the same sort of question1:\nFirst we make a cross-tab out of the real data:\n\n\nCode\nxtab &lt;- xtabs(~card_colour + outcome, data = real_data)\nxtab\n\n\n           outcome\ncard_colour 0 1\n       blue 5 3\n       red  3 9\n\n\nAnd then we pass the cross-tab to the function chisq.test:\n\n\nCode\nchisq.test(xtab)\n\n\nWarning in chisq.test(xtab): Chi-squared approximation may be incorrect\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  xtab\nX-squared = 1.467, df = 1, p-value = 0.2258\n\n\nHere the function produces a p-value that’s even larger than the approximately 0.10 value from the permutation approach, giving even less confidence that there may be a difference between the two groups. However it also gives a warnings that the assumptions made in producing this p-value may not be appropriate. In particular, two of the four cells (so 50% of the cells) in the cross-tab have values less than 5, whereas a rule-of-thumb when calculating a Chi-squared statistic is that no more than 20% of cells shoudl have values less than 5.\nAn alternative to the Chi-Square test, when there are small sample sizes, is the Fisher Exact test. This is more computationally intensive than the Chi-Square test, but can be more appropriate when there are small sample sizes. Unlike with the Chi-Square test, we can perform one sided as well as two sided tests using this method, with the default being two sided. Let’s see what this produces:\n\n\nCode\nfisher.test(xtab)\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  xtab\np-value = 0.1675\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  0.5182846 52.4512095\nsample estimates:\nodds ratio \n  4.564976 \n\n\nHere the p-value is slightly smaller than for the Chi-squared test, but slightly larger than for the (one-sided) permutation based p-value. Let’s see what the corresponding p-value is if we specify we want a one-sided test, by setting the alternative argument to \"greater\":\n\n\nCode\nfisher.test(xtab, alternative = \"greater\")\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  xtab\np-value = 0.1132\nalternative hypothesis: true odds ratio is greater than 1\n95 percent confidence interval:\n 0.6835727       Inf\nsample estimates:\nodds ratio \n  4.564976 \n\n\nThis time, we get a p value of 0.113, which is much closer to the permutation-based one-sided p-value of 0.103 we derived previously."
  },
  {
    "objectID": "posts/glms/hacker-stats/permutation-with-base-r/index.html#summary",
    "href": "posts/glms/hacker-stats/permutation-with-base-r/index.html#summary",
    "title": "Permutation Testing, and the intuition of the Null hypothesis, with Base R",
    "section": "Summary",
    "text": "Summary\nIn this post we’ve used only Base R functions to understand the intuition and implementation of permutation based tests for trying to either reject or not reject the Null hypothesis. Permutation methods, like bootstrapping, fall under the broader umbrella of resampling methods, and are immensely versatile and applicable to a great many types of data and question.\nApproaches like these are sometimes referred to as ‘Hacker Stats’, as being able to implement them correctly depends much more on having some computer science knowledge - such as for loops or equivalent - than much knowledge of statistical methods and tests. In this example I happened to know of a couple of classic conventional statistical tests that were broadly appropriate to the type of question we were trying to answer, but a reasonable programmer, once they understand the intuition behind the approach, would be able to produce a p-value and Null distribution in the way I did, and get to roughly the right answer even without knowing or implementing either of the classical statistical methods shown here.\nFrom my perspective, I don’t think it’s a case of either-or when it comes to which kind of approach we use - Hacker Stats or ‘Proper’ Stats. Indeed, I think it’s from these simulation based examples, where we can run a little experiment and see what happens, that we can develop the kind of deep intuition about the Null hypothesis - and so p-values, statistical significance, and the bread-and-butter of a lot of conventional statistical learning - that we need to be effective statisticians. It’s likely only by historical accident, in my view, that Hacker Stats are often only taught later in courses, and classical approaches taught first. Resampling methods can be both the Alpha of statistics, because they help to develop the deep intuitions through clear examples that don’t rely on much algebra, and also the Omega of statistics, because some quantities of interest just aren’t easy (and in some cases may be impossible) to derive analytic solutions to."
  },
  {
    "objectID": "posts/glms/hacker-stats/permutation-with-base-r/index.html#footnotes",
    "href": "posts/glms/hacker-stats/permutation-with-base-r/index.html#footnotes",
    "title": "Permutation Testing, and the intuition of the Null hypothesis, with Base R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, the Chi-Squared test here is two sided, looking for much smaller and much higher values than the Null distribution, whereas in the example below where we used a one-sided test.↩︎"
  },
  {
    "objectID": "posts/glms/hacker-stats/resampling-approaches-intro/index.html",
    "href": "posts/glms/hacker-stats/resampling-approaches-intro/index.html",
    "title": "Hacker Stats: Intro and overview",
    "section": "",
    "text": "This is the first post in a small series on resampling approaches to statistical inference. 1 Resampling approaches are a powerful and highly adaptable set of approaches for trying to get ‘good enough’ estimates of how statistically significant some observed value or summary of observed values is likely to be, or equivalently how likely what one’s observed is to have been observed by chance. They can also be extended and applied to performing post-stratification, which allows samples of the population with known biases to be adjusted in ways that aim to mitigate such biases, and so produce summary estimates more representative of the population of interest."
  },
  {
    "objectID": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#introduction",
    "href": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#introduction",
    "title": "Hacker Stats: Intro and overview",
    "section": "",
    "text": "This is the first post in a small series on resampling approaches to statistical inference. 1 Resampling approaches are a powerful and highly adaptable set of approaches for trying to get ‘good enough’ estimates of how statistically significant some observed value or summary of observed values is likely to be, or equivalently how likely what one’s observed is to have been observed by chance. They can also be extended and applied to performing post-stratification, which allows samples of the population with known biases to be adjusted in ways that aim to mitigate such biases, and so produce summary estimates more representative of the population of interest."
  },
  {
    "objectID": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#resampling-as-hacker-stats",
    "href": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#resampling-as-hacker-stats",
    "title": "Hacker Stats: Intro and overview",
    "section": "Resampling as Hacker Stats",
    "text": "Resampling as Hacker Stats\nResampling methods are sometimes called Hacker Stats, which might be a slightly derogatory term, but is also an informative one. Broadly, Resampling Methods:\n\nSubstitute meat brain effort (deriving and recalling analytic solutions) for silicon brain effort (i.e. they’re computationally intensive rather than human knowledge and reasoning intensive).\nAre theoretically and methodologically thin rather than theoretically and methodologically fat.\nAre approximate, stochastic and general; rather than precise, deterministic and specialist.\n\nPut another way, Hacker Stats are methods that data scientists and more casual users of statistics can use to get good enough approximations of the kinds of careful, analytic solutions and tests that, with many years of specialist training and memorisation, a degree in statistics would provide. They’re a good example of the 80:20 Principle: part of the 20% of stats know-how that’s used for 80% of the tasks."
  },
  {
    "objectID": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#types-of-permutation-method",
    "href": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#types-of-permutation-method",
    "title": "Hacker Stats: Intro and overview",
    "section": "Types of permutation method",
    "text": "Types of permutation method\nThe following flowchart shows the ‘family tree’ of types of resampling method:\n\n\n\n\nflowchart TB\n    sd[Sample Data]\n    us(Uniform Sampling)\n    nus(Non-Uniform Sampling)\n    pt[Permutation Testing]\n    bs[Bootstrapping]\n    ps[Post-Stratification]\n\n    pw[Population Weights]\n\n    dec1{Equal Probability?}\n    dec2{With Replacement?}\n\n    sd --sampling--&gt; dec1\n\n    us --&gt; dec2\n\n    dec1 --Yes--&gt; us\n    dec1 --No--&gt; nus\n    nus --&gt; ps\n\n    dec2 --Yes--&gt; bs\n    dec2 --No--&gt; pt\n\n    pw --&gt; nus\n\n\n\n\n\n\n\n\nn.b. Bootstrapping and permutation testing can be applied to post-stratified data too!"
  },
  {
    "objectID": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#the-thin-but-deep-theories",
    "href": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#the-thin-but-deep-theories",
    "title": "Hacker Stats: Intro and overview",
    "section": "The thin-but-deep theories",
    "text": "The thin-but-deep theories\nBoth Bootstrapping, which is resampling with replacement, and Permutation Testing, which is resampling without replacement, use computation to explore the implications of two distinct, simple, and important theories about the sample data, and any observations we may think we’ve observed within it. Let’s try to talk through these two thin-but-deep theories:\n\nBootstrapping\nBootstrapping starts and ends with something like the following claim:\n\nEvery observation in our dataset is equally likely.\n\nWhy is this?\n\nBecause each specific observation in our dataset has been observed the same number of times.\n\nWhy do you say that?\n\nBecause each observation in the dataset has been observed exactly one time, and 1=1!\n\nAnd why does this matter?\n\nBecause, if we can accept the above, we can say that another dataset, made up by resampling the real sample data, so that each observation (row) is as likely to be picked as every other one, is as likely as the dataset we actually observed. And so long as this other dataset has the same number of observations as the original dataset, then it’s also as precise as the original dataset.\n\nIt’s this line of reasoning - and the two conditions for another dataset: equally likely; and equally precise - which lead to the justification, in bootstrapping, for resampling with replacement.\n\n\nPermutation Tests\nSay we have a sample dataset, \\(D\\), which is a big rectangle of data with rows (observations) and columns (variables). To simplify, imagine \\(D\\) comprises five observations and two variables, so it looks like this:\n\\[\nD =\n\\begin{pmatrix}\nd_{1,1} & d_{1,2} \\\\\nd_{2,1} & d_{2,2} \\\\\nd_{3,1} & d_{3,2} \\\\\nd_{4,1} & d_{4,2} \\\\\nd_{5,1} & d_{5,2}  \n\\end{pmatrix}\n\\]\nThere are a number of different ways of describing and thinking about this kind of data, which is really just a structured collection of elements. One approach is to think about from the perspective of observations, which leads to a row-wise interpretation of the dataset:\n\\[\nD =\n\\begin{pmatrix}\nd_{1} = \\{d_{1,1} , d_{1,2}\\} \\\\\nd_{2} = \\{d_{2,1} , d_{2,2}\\} \\\\\nd_{3} = \\{d_{3,1} , d_{3,2}\\} \\\\\nd_{4} = \\{d_{4,1} , d_{4,2}\\} \\\\\nd_{5} = \\{d_{5,1} , d_{5,2}\\}  \n\\end{pmatrix}\n\\]\nAnd another way of thinking about the data is from the perspective of variables, which leads to a column-wise interpretation of the data:\n\\[\nD = \\{X, Y\\}\n\\]\n\\[\nX = \\{d_{1,1}, d_{2,1}, d_{3, 1}, d_{4, 1}, d_{5, 1}\\}\n\\]\n\\[\nY = \\{d_{1,2}, d_{2,2}, d_{3, 2}, d_{4, 2}, d_{5, 2}\\}\n\\]\nNow, imagine we’ve looked at our dataset, and we think there’s an association between the two variables \\(X\\) and \\(Y\\). What would be a very generalisable way of testing for whether we’re correct in assuming this association?\nThe key piece of reasoning behind resampling without replacement for permutation testing is as follows:\n\nIf there is a real association between the variables then the way values are paired up as observations matters, and should be preserved. If there’s no real association between the variables then the pairing up of values into observations doesn’t matter, so we can break this pairing and still get outcomes similar to what we actually observed.\n\nThere’s another term for resampling with replacement: shuffling. We can break-up the observational pairing seen in the dataset by shuffling one or both of the variables, then putting back the data into the same kind of rectangular structure it was before.\nFor instance, say we shuffle variable \\(Y\\), and end up with the following new vector of observations:\n\\[\nY^{shuffled} = \\{ d_{2,2}, d_{5, 2}, d_{3, 2}, d_{1,2}, d_{4, 2} \\}\n\\]\nWe could then make a new fake dataset, with all the same values as in the original dataset, but not necessarily in the same order:\n\\[\nX = \\{d_{1,1}, d_{2,1}, d_{3, 1}, d_{4, 1}, d_{5, 1}\\}\n\\]\n\\[\nY^{shuffled} = \\{d_{4,2}, d_{2,2}, d_{1, 2}, d_{3, 2}, d_{5, 2}\\}\n\\]\n\\[\nD^{fake} = \\{X, Y^{shuffled}\\}\n\\]\n\\[\nD^{fake} =\n\\begin{pmatrix}\nd_{1}^{fake} = \\{d_{1,1} , d_{4,2}\\} \\\\\nd_{2}^{fake} = \\{d_{2,1} , d_{2,2}\\} \\\\\nd_{3}^{fake} = \\{d_{3,1} , d_{1,2}\\} \\\\\nd_{4}^{fake} = \\{d_{4,1} , d_{3,2}\\} \\\\\nd_{5}^{fake} = \\{d_{5,1} , d_{5,2}\\}  \n\\end{pmatrix}\n\\]\nSo, in \\(D^{fake}\\) the observed (row-wise) association between each \\(X\\) and corresponding \\(Y\\) value has broken, even though the same values \\(d_{i,j}\\) are present.\nHowever, if the assumption/‘hunch’ about there being an association between \\(X\\) and \\(Y\\) from the real dataset \\(D\\) was justified through some kind of summary statistic, such as a correlation coefficient, \\(r(X, Y)\\), then we calculate the same summary statistic for the fake dataset too, \\(r(X, Y^{fake})\\).\nIn fact (and in practice) we can repeat the fakery, permuting the values again and again, and each time calculating the summary statistic of interest. This produces a distribution of values for this summary statistic, against which we can compare the observed value of this summary statistic.\nThis distribution of summary statistics produced from a large selection of permutated (fake) datasets is the distribution we would expect to see under the Null Hypothesis, which is that the apparent association is illusionary, and that no real association exists: the appearance of association comes from chance alone."
  },
  {
    "objectID": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#post-stratification",
    "href": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#post-stratification",
    "title": "Hacker Stats: Intro and overview",
    "section": "Post-stratification",
    "text": "Post-stratification\nResampling methods can also be used as a method for post-stratification, reweighting sample data to try to make it more representative of the population of interest. Consider two scenarios where this might be important:\n\nIntentional Oversampling: Say we know that 95% of people working in a particular occupation tend to be female, and 5% male. We are interested both in the typical characteristics of people who work in this occupation, but also in properly understanding the characteristics of males and females separately, and the differences between males and females within the occupation. And we know that, if we take a purely random sample of the population, we’ll only get, on average, 5% of the sample being males, which won’t give us enough precision/resolution to properly understand males in the population. So, we intentionally oversample from the male population, meaning our sample contains 20% males and 80% females, even though this isn’t representative of the population as a whole.\n\n\nUnintentional Undersampling: Say we are interested in political party voting intentions at an upcoming election. However for reasons of convenience we decide only to poll people who play console games, by asking someone about to play a game if they’re more likely to vote for the Blue Party or the Red Party. We know that our sample has very different characteristics to the population at large. However we also know so many people play console games that we have a reasonably large (and so sufficiently precise) set of estimates for each of the main demographic stratas of interest to us. So what do we do to convert the very biased sample data into unbiased population estimates? 2\n\nIn either case resampling methods can be applied. Just go from equal probability sampling to weighted probability sampling, in which samples from our dataset is more likely to be selected if they are under-represented in the sample dataset compared with the population, and less likely to be selected if they are under-represented in the sample dataset compared with the population."
  },
  {
    "objectID": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#summary",
    "href": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#summary",
    "title": "Hacker Stats: Intro and overview",
    "section": "Summary",
    "text": "Summary\nIn this post we’ve discussed the key ideas behind resampling methods, AKA Hacker Stats. These approaches are computationally intensive as compared with analytical solutions, which would have been a big barrier to their use until, perhaps, the mid 1980s. However computationally intensive these days might just mean it takes five seconds to perform many times, whereas the analytic solution takes five microseconds: still a large relative difference in computing time, but practically both kinds of approaches are similarly fast to perform.\nThese days, whether you know an analytic approximation for performing the test or calculation of interest, or whether you don’t, the Hacker Stats approach is still worth trying out. Even at their slowest, the worst case scenario with Hacker Stats is your computer might whirr a bit more loudly than usual, and you’ll finally have a good excuse to get that much-deserved tea- or coffee-break!"
  },
  {
    "objectID": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#footnotes",
    "href": "posts/glms/hacker-stats/resampling-approaches-intro/index.html#footnotes",
    "title": "Hacker Stats: Intro and overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough it wasn’t written as the first post in the series, so a challenge for me is to figure out how to present these in something other than date order!↩︎\nThis isn’t a made-up example, but broadly the approach used by Wang et al 2014 to produce pretty accurate estimates of a then-upcoming US election↩︎"
  },
  {
    "objectID": "posts/glms/hacker-stats/infer-introduction/index.html",
    "href": "posts/glms/hacker-stats/infer-introduction/index.html",
    "title": "Getting started with the infer package",
    "section": "",
    "text": "This post continues a short series on resampling methods, sometimes also known as ‘Hacker Stats’, for hypothesis testing. To recap: resampling with replacement is known as bootstrapping. Resampling without replacement can be used for permutation tests: testing whether apparent patterns in the data, including apparent associations between variables in the data, could likely have emerged from the Null distribution.\nIn a previous post introducing bootstrapping, I showed how the approach can be used to perform something like hypothesis tests for quantities of interest that aren’t as easily amenable as means to being assessed parametrically, such as differences in medians. In the next post, on resampling and permutation tests, I described the intuition and methodology behind resampling with replacement to produce Null distributions, and how to implement the procedure using base R.\nIn this post, I show how the infer package, can be used to perform both bootstrapping and permutation testing in a way that’s slightly easier, and more declarative in the context of a general hypothesis testing framework."
  },
  {
    "objectID": "posts/glms/hacker-stats/infer-introduction/index.html#introduction",
    "href": "posts/glms/hacker-stats/infer-introduction/index.html#introduction",
    "title": "Getting started with the infer package",
    "section": "",
    "text": "This post continues a short series on resampling methods, sometimes also known as ‘Hacker Stats’, for hypothesis testing. To recap: resampling with replacement is known as bootstrapping. Resampling without replacement can be used for permutation tests: testing whether apparent patterns in the data, including apparent associations between variables in the data, could likely have emerged from the Null distribution.\nIn a previous post introducing bootstrapping, I showed how the approach can be used to perform something like hypothesis tests for quantities of interest that aren’t as easily amenable as means to being assessed parametrically, such as differences in medians. In the next post, on resampling and permutation tests, I described the intuition and methodology behind resampling with replacement to produce Null distributions, and how to implement the procedure using base R.\nIn this post, I show how the infer package, can be used to perform both bootstrapping and permutation testing in a way that’s slightly easier, and more declarative in the context of a general hypothesis testing framework."
  },
  {
    "objectID": "posts/glms/hacker-stats/infer-introduction/index.html#setting-up",
    "href": "posts/glms/hacker-stats/infer-introduction/index.html#setting-up",
    "title": "Getting started with the infer package",
    "section": "Setting up",
    "text": "Setting up\nLet’s install the infer packge and try a couple of examples from the documentation.\n\n# install.packages(\"infer\") # First time around\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(infer)"
  },
  {
    "objectID": "posts/glms/hacker-stats/infer-introduction/index.html#the-infer-package",
    "href": "posts/glms/hacker-stats/infer-introduction/index.html#the-infer-package",
    "title": "Getting started with the infer package",
    "section": "The infer package",
    "text": "The infer package\nFrom the vignette page we can see that infer’s workflow is framed around four verbs:\n\nspecify() allows you to specify the variable, or relationship between variables, that you’re interested in.\nhypothesize() allows you to declare the null hypothesis.\ngenerate() allows you to generate data reflecting the null hypothesis.\ncalculate() allows you to calculate a distribution of statistics from the generated data to form the null distribution.\n\nThe package describes the problem of hypothesis testing as being somewhat generic, regardless of the specific test, hypothesis, or dataset being used:\n\nRegardless of which hypothesis test we’re using, we’re still asking the same kind of question: is the effect/difference in our observed data real, or due to chance? To answer this question, we start by assuming that the observed data came from some world where “nothing is going on” (i.e. the observed effect was simply due to random chance), and call this assumption our null hypothesis. (In reality, we might not believe in the null hypothesis at all—the null hypothesis is in opposition to the alternate hypothesis, which supposes that the effect present in the observed data is actually due to the fact that “something is going on.”) We then calculate a test statistic from our data that describes the observed effect. We can use this test statistic to calculate a p-value, giving the probability that our observed data could come about if the null hypothesis was true. If this probability is below some pre-defined significance level \\(\\alpha\\), then we can reject our null hypothesis."
  },
  {
    "objectID": "posts/glms/hacker-stats/infer-introduction/index.html#the-gss-dataset",
    "href": "posts/glms/hacker-stats/infer-introduction/index.html#the-gss-dataset",
    "title": "Getting started with the infer package",
    "section": "The gss dataset",
    "text": "The gss dataset\nLet’s look through - and in some places adapt - the examples used. These mainly make use of the gss dataset.\n\ndata(gss)\n\n\nglimpse(gss)\n\nRows: 500\nColumns: 11\n$ year    &lt;dbl&gt; 2014, 1994, 1998, 1996, 1994, 1996, 1990, 2016, 2000, 1998, 20…\n$ age     &lt;dbl&gt; 36, 34, 24, 42, 31, 32, 48, 36, 30, 33, 21, 30, 38, 49, 25, 56…\n$ sex     &lt;fct&gt; male, female, male, male, male, female, female, female, female…\n$ college &lt;fct&gt; degree, no degree, degree, no degree, degree, no degree, no de…\n$ partyid &lt;fct&gt; ind, rep, ind, ind, rep, rep, dem, ind, rep, dem, dem, ind, de…\n$ hompop  &lt;dbl&gt; 3, 4, 1, 4, 2, 4, 2, 1, 5, 2, 4, 3, 4, 4, 2, 2, 3, 2, 1, 2, 5,…\n$ hours   &lt;dbl&gt; 50, 31, 40, 40, 40, 53, 32, 20, 40, 40, 23, 52, 38, 72, 48, 40…\n$ income  &lt;ord&gt; $25000 or more, $20000 - 24999, $25000 or more, $25000 or more…\n$ class   &lt;fct&gt; middle class, working class, working class, working class, mid…\n$ finrela &lt;fct&gt; below average, below average, below average, above average, ab…\n$ weight  &lt;dbl&gt; 0.8960034, 1.0825000, 0.5501000, 1.0864000, 1.0825000, 1.08640…\n\n\nLet’s go slightly off piste and say we are interested in seeing if there is a relationship between age, a cardinal variable, and sex, a categorical variable. We can start by stating our null and alternative hypotheses explicitly:\n\nNull hypothesis: There is no difference between age and sex\nAlt hypothesis: There is a difference between age and sex\n\nLet’s see if we can start by just looking at the data to see if, informally, it looks like it might better fit the Null or Alt hypothesis.\n\ngss |&gt; \n    ggplot(aes(x=age, group = sex, colour = sex)) + \n    geom_density()\n\n\n\n\nIt looks like the densities of age distributions are similar for both sexes. However, they’re not identical. Are the differences more likely to be due to chance, or are they more structural?\nWe can start by calculating, say, the differences in average ages between males and females:\n\ngss |&gt;\n    group_by(sex) |&gt;\n    summarise(n = n(), mean_age = mean(age))\n\n# A tibble: 2 × 3\n  sex        n mean_age\n  &lt;fct&gt;  &lt;int&gt;    &lt;dbl&gt;\n1 male     263     40.6\n2 female   237     39.9"
  },
  {
    "objectID": "posts/glms/hacker-stats/infer-introduction/index.html#our-first-testable-hypothesis-using-permutation-testingsampling-without-replacement",
    "href": "posts/glms/hacker-stats/infer-introduction/index.html#our-first-testable-hypothesis-using-permutation-testingsampling-without-replacement",
    "title": "Getting started with the infer package",
    "section": "Our first testable hypothesis (using permutation testing/sampling without replacement)",
    "text": "Our first testable hypothesis (using permutation testing/sampling without replacement)\nThe mean age is 40.6 for males and 39.9 for females, a difference of about 0.7 years of age. Could this have occurred by chance?\nThere are 263 male observations, and 237 female observations, in the dataset. Imagine that the ages are values, and the sexes are labels that are added to these values.\nOne approach to operationalising the concept of the Null Hypothesis is to ask: If we shifted around the labels assigned to the values, so there were still as many male and female labels, but they were randomly reassigned, what would the difference in mean age between these two groups be? What would happen if we did this many times?\nThis is the essence of building a Null distribution using a permutation test, which is similar to a bootstrap except it involves resampling with replacement rather than without replacement.\nWe can perform this permutation test using the infer package as follows:\n\nmodel &lt;- gss |&gt;\n    specify(age ~ sex) |&gt;\n    hypothesize(null = 'independence') |&gt;\n    generate(reps = 10000, type = 'permute')\n\nmodel\n\nResponse: age (numeric)\nExplanatory: sex (factor)\nNull Hypothesis: independence\n# A tibble: 5,000,000 × 3\n# Groups:   replicate [10,000]\n     age sex    replicate\n   &lt;dbl&gt; &lt;fct&gt;      &lt;int&gt;\n 1    28 male           1\n 2    52 female         1\n 3    53 male           1\n 4    31 male           1\n 5    18 male           1\n 6    42 female         1\n 7    57 female         1\n 8    48 female         1\n 9    20 female         1\n10    41 female         1\n# ℹ 4,999,990 more rows\n\n\nThe infer package has now arbitrarily shifted around the labels assigned to the age values 10000 times. Each time is labelled with a different replicate number. Let’s take the first nine replicates and show what the densities by sex look like:\n\nmodel |&gt;\n    filter(replicate &lt;= 9) |&gt;\n    ggplot(aes(x=age, group = sex, colour = sex)) + \n    geom_density() + \n    facet_wrap(~replicate)\n\n\n\n\nWhat if we now look at the differences in means apparent in each of these permutations\n\nmodel |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) |&gt;\n    visualize()\n\n\n\n\nHere we can see the distribution of differences in means follows broadly a normal distribution, which appears to be centred on 0.\nLet’s now calculate and save the observed difference in means.\n\ntmp &lt;- gss |&gt;\n    group_by(sex) |&gt;\n    summarise(mean_age = mean(age))\n\ntmp \n\n# A tibble: 2 × 2\n  sex    mean_age\n  &lt;fct&gt;     &lt;dbl&gt;\n1 male       40.6\n2 female     39.9\n\ndiff_means &lt;- tmp$mean_age[tmp$sex == \"male\"] - tmp$mean_age[tmp$sex == \"female\"]\n\ndiff_means\n\n[1] 0.7463541\n\n\n\nA two-sided hypothesis\nLet’s now show where the observed difference in means falls along the distribution of differences in means generated by this permutation-based Null distribution:\n\nmodel |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) |&gt;\n    visualize() +\n    shade_p_value(obs_stat = diff_means, direction = \"two-sided\")\n\n\n\n\nThe observed difference in means appears to be quite close to the centre of mass for the distribution of differences in means generated by the Null distribution. So it appears very likely that this observed difference could be generated from a data generating process in which there’s no real difference in mean ages between the two groups. We can formalise this slightly by calcuating a p-value:\n\nmodel |&gt;\n    calculate(stat = \"diff in means\", order = c(\"male\", \"female\")) |&gt;\n    get_p_value(obs_stat = diff_means, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.535\n\n\nThe p value is much, much greater than 0.05, suggesting there’s little evidence to reject the Null hypothesis, that in this dataset age is not influenced by sex."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "",
    "text": "In the last post we reached the end of a winding journey. This post will show how Bayesian approaches to model fitting, rather than the frequentist approaches more commonly used, can reach the intended destination of this journey more quickly, despite being a bit more conceptually challenging to start with."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#aim",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#aim",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "",
    "text": "In the last post we reached the end of a winding journey. This post will show how Bayesian approaches to model fitting, rather than the frequentist approaches more commonly used, can reach the intended destination of this journey more quickly, despite being a bit more conceptually challenging to start with."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#recap",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#recap",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "Recap",
    "text": "Recap\nThe start of this blog series aimed to do two things:\n\nReintroduce statistical models via a generalised model formulae, comprising a systematic component and a stochastic component.\nReintroduce the fitting of statistical models from the perspective of algorithmic optimisation, in which the gap between what the model predicts and what’s observed is minimised in some way.\n\nThe rest of the first section of the series - posts two, three and four - added more context to the first post, and introduced the concept of using models for prediction - and the types of quantities of interest they can predict. The first section ended with post four, which illustrated some of the complexities of getting meaningful effect estimates - the overall effect of one specific predictor variable on the outcome being predicted - for model structures under than standard linear regression.\nThe second section - covering posts five to ten - delved into a lot more detail about how statistical models are fit. It introduced the concept of likelihood as a means of deciding what the target of a statistical optimisation algorithm should be. And it also showed - in sometimes excruciating detail - how to perform numeric optimisation based on likelihood in order to extract not just the best set of model parameters, but estimates of joint uncertainty in the best estimated set of model parameters. It’s this joint uncertainty in parameter estimates, estimated via the Hessian from the optim() function, which allowed uncertainty in model parameter estimates to be propagated and percolated through specific ‘what-if?’ questions - i.e. specific configurations of predictor variables passed through to the model - in order to produce honest answers to these ‘what-if?’ questions, which provide a range of answers, rather than a single answer, in order to show how model parameter estimation uncertainty leads to uncertainty in the answers the model provides.\nThe third section - posts 10-12 - completed the journey, showing how many of the concepts and ideas learned through considerable effort in sections one and (especially) two allow more intelligent and effective use of standard statistical model outputs - produced using R’s lm() and glm() functions - for honest prediction.\nThis post will extend the third section to show why the kind of honest prediction which we managed to produce using the kind of frequentist modelling framework used by lm() and glm() are, in fact, easier to produce using Bayesian models."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#on-marbles-and-jumping-beans",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#on-marbles-and-jumping-beans",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "On marbles and jumping beans",
    "text": "On marbles and jumping beans\nPost five introduced Bayes’ Rule and the Likelihood axiom. It pointed out that, at heart, Bayes’ Rule is a way of expressing that given this in terms of this given that; and that Likelihood is also a claim about how that given this relates to this given that. More specifically, the claim of Likelihood is:\n\nThe likelihood of the model given the data is proportional to the probability of the data given the model.\n\nThere are two aspects to the model: firstly its structure; secondly its parameters. The structure includes the type of statistical model - whether it is a standard linear regression, negative binomial regression, logistic regression, Poisson regression model and so on - and also the specific types of columns from the dataset selected as either predictor variables (\\(X\\)) or response variables (\\(Y\\)). It is only after both the higher level structure of the model family, and the lower level structure of the data inputs (what’s being regressed on what?) have been decided that the Likelihood theory is used.\nAnd how is Likelihood theory used? Well, it defines a landscape over which an algorithm searches. This landscape has as many dimensions as there are parameters to fit. Where there are just two parameters, \\(\\beta_0\\) and \\(\\beta_1\\) to fit, we can visualise this landscape using something like a contour plot, with \\(\\beta_0\\) as latitude, \\(\\beta_1\\) as longitude, and the likelihood at this position its elevation or depth. Each possible joint value \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which the algorithm might wish to propose leads to a different long-lat coordinate over the surface, and each coordinate has a different elevation or depth. Although we can’t see beyond three dimensions (latitude, longitude, and elevation/depth), mathematics has no problem extending the concept of multidimensional space into far more dimensions than we can see or meaningfully comprehenend. If a model has ten parameters to fit, for example, the likelihood search space really is ten dimensional, and so on.\nNoticed I used elevation and depth interchangably in the description above. Well, this is because it really doesn’t matter whether an optimisation algorithm is trying to find the greatest elevation over a surface, or the greatest depth over the surface. The aim of maximum likelihood estimation is to find the configuration of parameters that maximises the likelihood, i.e. finds the top of the surface. However we saw that when passing the likelihood function to optim() we often inverted the function by multiplying it by -1. This is because the optimisation algorithms themselves seek to minimise the objective function they’re passed, not maximise it. By multiplying the likelihood function by -1 we made what we were trying to seek compatible with what the optimisation algorithms seek to do: find the greatest depth over a surface, rather than the highest elevation over the surface.\nTo make this all a bit less abstract let’s develop the intuition of an algorithm that seeks to minimise a function by way of a(nother) weird little story:\n\nImagine there is a landscape made out of transparent perspex. It’s not just transparent, it’s invisible to the naked eye. And you want to know where the lowest point of this surface is. All you have to do this is a magical leaking marble. The marble is just like any other marble, except every few moments, at regular intervals (say every tenth of a second), it dribbles out a white dye that you can see. And this dye sticks on and stains the otherwise invisible landscape whose lowest point you wish to find.\n\n\nNow, you drop the marble somewhere on the surface. You see the first point it hits on the surface - a white blob appears. The second blob appears some distance away from the first blob; and the third blob slightly less far away from the second blob as the second was to the second. After a few seconds, a trail of white spots is visible, the first few of which form something like a straight line, each consecutive point slightly less closer to the previous one. A second or two later, and the rumbling sounds of the marble rolling over the surface cease; the marble has clearly run out of momentum. And as you look at the trail of dots it’s generated, and is still generating, and you see it keeps highlighting the same point on the otherwise invisible surface, again and again.\n\nPreviously I used the analogy of a magical robo-chauffer, taking you to the top of a landscape. But the falling marble is probably a closer analogy to how many of optim()’s algorithms actually work. Using gravity and its shape alone, it finds the lowest point on the surface, and with its magical leaking dye, it tells you where this lowest point is.\nNow let’s extend the story to convert the analogy of the barefoot-and-blind person from part seven as well:\n\nThe marble has now ‘told’ you where the lowest point on the invisible surface is. However you also want to know more about the shape of the depression it’s in. You want to know if it’s a steep depression, or a shallow depression. And you want to know if it’s as steep or shallow in every direction, or if it’s steeper in some ways than the other.\n\n\nSo you now have to do a bit more work. You move your hand to just above the marble, and with your forefinger ‘flick’ it in a particular direction (say east-west): you see it move in the direction you flick it briefly, before rolling back towards (and beyond, and then towards) the depression point. As it does so, it leaks dye onto the surface, revealing a bit more about the landscape’s steepness or shallowness in this dimension. Then you do the same, but along a different dimension (say, north-south). After you’ve done this enough times, you are left with a collection of dyed points on the part of the surface closest to its deepest depression. The spacing and shape of these points tells you something about the nature of the depression and the part of the landscape it’s surrounding.\n\nNotice in this analogy you had to do extra work to get the marble to reveal more information about the surface. By default, the marble tells you the specific location of the depression, but not what the surface is like around this point. Instead, you need to intervene twice: firstly by dropping the marble onto the surface; secondly by flicking it around once it’s reached the lowest point on the surface.\nNow, let’s imagine swapping out our magical leaking marble for something even weirder: a magical leaking jumping bean.\n\nThe magical jumping bean does two things: it leaks and it jumps. (Okay, it does three things: when it leaks it also sticks to the surface it’s dying). When the bean is first dropped onto the surface, it marks the location it lands on. Then, it jumps up and across in a random direction. After jumping, it drops onto another part of the surface, marks it, and the process starts again. Jumping, sticking, marking; jumping, sticking, marking; jumping, sticking, marking… potentially forever.\n\n\nBecause of the effect of gravity, though the jumping bean jumps in a random direction, after a few jump-stick-mark steps it’s still, like the marble, very likely to move towards the depression. However, unlike the marble, even when it gets towards the lowest point in the depression, it’s not going to just rest there. The magical jumping bean is never at rest. It’s forever jump-stick-marking, jump-stick-marking.\n\n\nHowever, once the magical bean has moved towards the depression, though it keeps moving, it’s likely never to move too far from the depression. Instead, it’s likely to bounce around the depression. And as it does so, it drops ever more marks on the surface, which keep showing what the surface looks like around the depression in ever more detail.\n\nSo, because of the behaviour of the jumping bean, you only have to act on it once, by choosing where to drop it, rather than twice as with the marble: first choosing where to drop it, then flicking it around once it’s reached the lowest point on the surface."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#so-what",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#so-what",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "So what?",
    "text": "So what?\nIn the analogies above, the marble is to frequentist statistics as the jumping bean is to Bayesian statistics. A technical distinction between the marble and the jumping bean is that the marble converges towards a point (meaning it reaches a point of rest on the surface) whereas the jumping bean converges towards a distribution (meaning it never rests).\nIt’s Bayesian statistics’ 1 property of converging to a distribution rather than a point that makes the converged posterior distribution of parameter estimates Bayesian models produce ideal for the kind of honest prediction so much of this blog series has been focused on.\nLet’s now do some Bayesian modelling to compare…"
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#bayesian-modelling-now-significantly-less-terrifying-than-it-used-to-be",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#bayesian-modelling-now-significantly-less-terrifying-than-it-used-to-be",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "Bayesian modelling: now significantly less terrifying than it used to be",
    "text": "Bayesian modelling: now significantly less terrifying than it used to be\nThere are a lot of packages and approaches for building Bayesian models. In fact there are whole statistical programming languages - like JAGS, BUGS 2 and Stan - dedicated to precisely describing every assumption the statistician wants to make about how a Bayesian model should be built. For more complicated and bespoke models these are ideal.\nHowever there are also an increasingly large number of Bayesian modelling packages that abstract away some of the assumptions and complexity apparent in the above specialised Bayesian modelling languages, and allow Bayesian versions of the kinds of model we’re already familiar with to be specified using formulae interfaces almost identical to what we’ve already worked with. Let’s look at one of them, rstanarm, which allows us to use stan, a full Bayesian statistical programming language, without quite as much thinking and set-up being required on our part.\nLet’s try to use this to build a Bayesian equivalent of the hamster tooth model we worked on in the last couple of posts.\n\nData Preparation and Frequentist modelling\nLet’s start by getting the dataset and building the frequentist version of the model we’re already familiar with:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nCode\nbest_model_frequentist &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model_frequentist)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nBuilding the Bayesian equivalent\nNow how would we build a Bayesian equivalent of this? Firstly let’s load (and if necessary install3) rstanarm.\n\n\nCode\nlibrary(rstanarm)\n\n\nWhereas for the frequentist model we used the function lm(), rstanarm has what looks like a broadly equivalent function stan_lm(). However, as I’ve just discovered, it’s actually more straightforward with stan_glm instead:\n\n\nCode\nbest_model_bayesian &lt;- stan_glm(len ~ log(dose) * supp, data = df)\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000209 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.09 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.029 seconds (Warm-up)\nChain 1:                0.034 seconds (Sampling)\nChain 1:                0.063 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.04 seconds (Warm-up)\nChain 2:                0.034 seconds (Sampling)\nChain 2:                0.074 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 9e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.035 seconds (Warm-up)\nChain 3:                0.028 seconds (Sampling)\nChain 3:                0.063 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 7e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.033 seconds (Warm-up)\nChain 4:                0.038 seconds (Sampling)\nChain 4:                0.071 seconds (Total)\nChain 4: \n\n\nCode\nsummary(best_model_bayesian)\n\n\n\nModel Info:\n function:     stan_glm\n family:       gaussian [identity]\n formula:      len ~ log(dose) * supp\n algorithm:    sampling\n sample:       4000 (posterior sample size)\n priors:       see help('prior_summary')\n observations: 60\n predictors:   4\n\nEstimates:\n                   mean   sd   10%   50%   90%\n(Intercept)      20.6    0.7 19.8  20.6  21.5 \nlog(dose)         9.3    1.2  7.7   9.3  10.8 \nsuppVC           -3.7    1.0 -5.0  -3.7  -2.4 \nlog(dose):suppVC  3.8    1.7  1.7   3.8   6.0 \nsigma             3.8    0.4  3.4   3.8   4.3 \n\nFit Diagnostics:\n           mean   sd   10%   50%   90%\nmean_PPD 18.8    0.7 17.9  18.8  19.7 \n\nThe mean_ppd is the sample average posterior predictive distribution of the outcome variable (for details see help('summary.stanreg')).\n\nMCMC diagnostics\n                 mcse Rhat n_eff\n(Intercept)      0.0  1.0  2675 \nlog(dose)        0.0  1.0  2514 \nsuppVC           0.0  1.0  3076 \nlog(dose):suppVC 0.0  1.0  2439 \nsigma            0.0  1.0  3294 \nmean_PPD         0.0  1.0  3700 \nlog-posterior    0.0  1.0  1767 \n\nFor each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1).\n\n\nSome parts of the summary for the Bayesian model look fairly familiar compared with the frequentist model summary; other bits a lot more exotic. We’ll skip over a detailed discussion of these outputs for now, though it is worth comparing the estimates section of the summary directly above, from the Bayesian approach, with the frequentist model produced earlier.\nThe frequentist model had point estimates of \\(\\{20.7, 9.3, -3.7, 3.8\\}\\). The analogous section of the Bayesian model summary is the mean column of the estimates section. These are reported to fewer decimal places by default - Bayesians are often more mindful of spurious precision - but are also \\(\\{20.7, 9.3, -3.7, 3.8\\}\\), so the same to this number of decimal places.\nNote also the Bayesian model reports an estimate for an additional parameter, sigma. This should be expected if we followed along with some of the examples using optim() for linear regression: the likelihood function required the ancillary parameters (referred to as \\(\\alpha\\) in the ‘mother model’ which this series started with, and part of the stochastic component \\(f(.)\\)) be estimated as well as the primary model parameters (referred to as \\(\\beta\\) in the ‘mother model’, and part of the systematic component \\(g(.)\\)). The Bayesian model’s coefficients (Intercept), log(dose), suppVC and the interaction term log(dose):suppVC are all part of \\(\\beta\\), whereas the sigma parameter is part of \\(\\alpha\\). The Bayesian model has just been more explicit about exactly which parameters it’s estimated from the data.\nFor the \\(\\beta\\) parameters, the Std. Error column in the Frequentist model summary is broadly comparable with the sd column in the Bayesian model summary. For the \\(\\beta\\) parameters these values are \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Frequentist model, and \\(\\{0.7, 1.2, 1.0, 1.7\\}\\) in the Bayesian model the summary. i.e. they’re the same to the degree of precision offered in the Bayesian model summary.\nBut let’s get to the crux of the argument: with Bayesian models honest predictions are easier.\nAnd they are, with the posterior_predict() function, passing what we want to predict on through the newdata argument, much as we did with the predict() function with frequentist models.\n\n\nScenario modelling\nLet’s recall the scenarios we looked at previously:\n\npredicted and expected values: length when dosage is 1.25mg and supplement is OJ\nfirst difference difference between OJ and VC supplement when dosage is 1.25mg\n\nLet’s start with the first question:\n\n\nCode\npredictors &lt;- data.frame(supp = \"OJ\", dose = 1.25)\n\npredictions &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nhead(predictions)\n\n\n            1\n[1,] 28.76876\n[2,] 17.77913\n[3,] 18.66405\n[4,] 24.47681\n[5,] 24.58427\n[6,] 25.81011\n\n\nCode\ndim(predictions)\n\n\n[1] 4000    1\n\n\nBy default posterior_predict() returns a matrix, which in this case has 4000 rows and just a single column. Let’s do a little work on this and visualise the distribution of estimates it produces:\n\n\nCode\npreds_df &lt;- tibble(estimate = predictions[,1])\n\n# lower, median, upper\nlmu &lt;- quantile(preds_df$estimate, c(0.025, 0.500, 0.975))\n\nlwr &lt;- lmu[1]\nmed &lt;- lmu[2]\nupr &lt;- lmu[3]\n\npreds_df |&gt;\n    mutate(\n        in_range = between(estimate, lwr, upr)\n    ) |&gt;\n    ggplot(aes(x = estimate, fill = in_range)) + \n    geom_histogram(bins = 100) + \n    scale_fill_manual(\n        values = c(`FALSE` = 'lightgray', `TRUE` = 'darkgray')\n    ) +\n    theme(legend.position = \"none\") + \n    geom_vline(xintercept = med, linewidth = 1.2, colour = \"steelblue\")\n\n\n\n\n\nThe darker-shaded parts of the histogram show the 95% uncertainty interval, and the blue vertical line the median estimate. This 95% interval range is 15.14 to 30.31.\nRemember we previously estimated both the expected values and the predicted values for this condition. Our 95% range for the expected values were 20.27 to 24.19 (or thereabouts), whereas our 95% range for the predicted values were (by design) wider, at 15.34 to 30.11. The 95% uncertainty interval above is therefore of predicted values, which include fundamental variation due to the ancillary parameters \\(\\sigma\\), rather than expected values, which result from parameter uncertainty alone.\nThere are a couple of other functions in rstanarm we can look at: predictive_error() and predictive_interval()\nFirst here’s predictive_interval. It is a convenience function that the posterior distribution generated previously, predictions, and returns an uncertainty interval:\n\n\nCode\npredictive_interval(\n    predictions\n)\n\n\n        5%      95%\n1 16.25049 29.08077\n\n\nWe can see by default the intervals returned are from 5% to 95%, i.e. are the 90% intervals rather than the 95% intervals considered previously. We can change the intervals requested with the prob argument:\n\n\nCode\npredictive_interval(\n    predictions, \n    prob = 0.95\n)\n\n\n      2.5%    97.5%\n1 15.13993 30.30754\n\n\nAs expected, this requested interval returns an interval closer to (but not identical to) the interval estimated using the quantile function.\nLet’s see if we can also use the model directly, specifying newdata directly to predictive_interval:\n\n\nCode\npredictive_interval(\n    best_model_bayesian,\n    newdata = predictors, \n    prob = 0.95\n)\n\n\n      2.5%    97.5%\n1 14.87043 30.34887\n\n\nYes. This approach works too. The values aren’t identical as, no doubt, a more sophisticated approach is used by predictive_interval to estimate the interval than simply arranging the posterior estimates in order using quantile.\nFor producing expected values we can use the function posterior_epred:\n\n\nCode\nepreds &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors\n)\n\nexp_values &lt;- epreds[,1]\n\nquantile(exp_values, probs = c(0.025, 0.500, 0.975))\n\n\n    2.5%      50%    97.5% \n21.28228 22.70410 24.20022 \n\n\nFor comparison, the expected value 95% interval we obtained from the Frequentist model was 21.3 to 24.2 when drawing from the quasi-posterior distribution, and 22.7 to 24.2 when using the predict() function with the interval argument set to \"confidence\".\nNow, finally, let’s see if we can produce first differences: the estimated effect of using VC rather than OJ as a supplement when the dose is 1.25mg\n\n\nCode\npredictors_x0 &lt;- data.frame(supp = \"OJ\", dose = 1.25)\npredictors_x1 &lt;- data.frame(supp = \"VC\", dose = 1.25)\n\npredictors_fd &lt;- rbind(predictors_x0, predictors_x1)\n\npredictions_fd &lt;- rstanarm::posterior_predict(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd)\n\n\n            1        2\n[1,] 22.72454 18.36237\n[2,] 23.06037 14.41203\n[3,] 16.54353 24.49811\n[4,] 26.94058 22.87904\n[5,] 21.53167 15.95980\n[6,] 25.31222 10.34290\n\n\nThe newdata argument to posterior_predict now has two rows, one for the OJ supplement and the other for the VC supplement scenario. And the predictions matrix returned by posterior_predict now has two columns: one for each scenario (row) in predictors_fd. We can look at the distribution of both of these columns, as well as the rowwise comparisions between columns, which will give our distribution of first differences for the predicted values:\n\n\nCode\npreds_fd_df &lt;- \n    predictions_fd |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\n\nTo reiterate, these are predicted values for the two scenarios, not the expected values shown in the first differences section of post 12. This explains why there is greater overlap between the two distributions. Let’s visualise and calculate the first differences in predicted values:\n\n\nCode\npreds_fd_df |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nWe can see that the average of the distribution is below 0, but as we are looking at predicted values the range of distributions is much higher. Let’s get 95% intervals:\n\n\nCode\nquantile(preds_fd_df$fd, probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-13.734532  -3.032998   7.924211 \n\n\nThe 95% intervals for first differences in predicted values is from -13.6 to +7.9, with the median estimate at -3.0. As expected, the median is similar to the equivalent value from using expected values (-2.9) but the range is wider.\nNow let’s use posterior_epred to produce estimates of first differences in expected values, which will be more directly comparable to our first differences estimates in part 12:\n\n\nCode\npredictions_fd_ev &lt;- posterior_epred(\n    best_model_bayesian,\n    newdata = predictors_fd\n)\n\nhead(predictions_fd_ev)\n\n\n          \niterations        1        2\n      [1,] 21.75636 20.58615\n      [2,] 22.54271 19.43571\n      [3,] 23.31800 20.14816\n      [4,] 22.24563 19.63095\n      [5,] 24.02259 18.45190\n      [6,] 23.33564 19.46180\n\n\n\n\nCode\npreds_fd_df_ev &lt;- \n    predictions_fd_ev |&gt;\n        as_tibble(rownames = \"draw\") |&gt;\n        rename(x0 = `1`, x1 = `2`) |&gt;\n        mutate(fd = x1 - x0)\n\npreds_fd_df_ev |&gt; \n    select(-fd) |&gt;\n    pivot_longer(cols = c(\"x0\", \"x1\"), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    facet_wrap(~ scenario, nrow = 2)\n\n\n\n\n\nThis time, as the stochastic variation related to the \\(\\sigma\\) term has been removed, the distributions of the expected values are more distinct, with less overlap. Let’s visualise and compare the first differences of the expected values:\n\n\nCode\npreds_fd_df_ev |&gt;\n    select(fd) |&gt;\n    ggplot(aes(x = fd)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\n\n\nCode\nquantile(preds_fd_df_ev$fd, probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-4.8978035 -2.8342469 -0.7296937 \n\n\nWe now have a 95% interval for the first difference in expected values of -4.9 to -0.7. By contrast, the equivalent range estimated using the Frequentist model in part 12 was -4.8 to -0.8. So, although they’re not identical, they do seem to be very similar."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#summing-up",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#summing-up",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "Summing up",
    "text": "Summing up\nUp until now we’ve been using Frequentist approaches to modelling. However the simulation approach required to produce honest uncertainty depends on ‘tricking’ Frequentist models into producing something like the converged posterior distributions which, in Bayesian modelling approaches, come ‘for free’ from the way in which Bayesian frameworks estimate model parameters.\nAlthough Bayesian models are generally more technically and computationally demanding than Frequentist models, we have shown the folllowing:\n\nThat packages like rstanarm abstract away some of the challenges of building Bayesian models from scratch;\nThat the posterior distributions produced by Bayesian models produce estimates of expected values, predicted values, and first differences - our substantive quantities of interest - that are similar to those produced previously from Frequentist models\nThat for the estimation of these quantities of interest, the posterior distributions Bayesian models generate make it more straightforward, not less, to produce using Bayesian methods than using Frequentist methods.\n\nThanks for reading, and congratulations on getting this far through the series."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#footnotes",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-13/index.html#footnotes",
    "title": "Part Thirteen: On Marbles and Jumping Beans",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOr perhaps more accurately Bayesian statistical model estimation rather than Bayesian statistics more generally? Bayes’ Rule can be usefully applied to interpret results derived from frequentist models. But the term Bayesian Modelling generally implies that Bayes’ Rule is used as part of the model parameter estimation process, in which a prior distribution is updated according to some algorithm, and then crucially the posterior distribution produced then forms the prior distribution at the next step in the estimation. The specific algorithm that works as the ‘jumping bean’ is usually something like Hamiltonian Monte Carlo, HMC, and the general simulation framework in which a posterior distribution generated from applying Bayes’ Rule is repeatedly fed back into the Bayes’ Rule equation as the prior distribution is known as Markov Chain Monte Carlo, MCMC.↩︎\nOminously named.↩︎\nrstanarm has a lot of dependencies. It’s the friendly, cuddly face of a beast!↩︎"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-23/index.html",
    "href": "posts/glms/time-series/lms-are-glms-part-23/index.html",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "",
    "text": "In previous posts on time series, we decomposed then applied a common general purpose modelling strategy for working with time series data called ARIMA. ARIMA model can involve autoregressive components (AR(p)), integration/differencing components (I(d)), and moving average components (MA(q)). As we saw, the time series data can also be pre-transformed, in ways other than just differencing; the example of this we saw was the application of the Box-Cox transformation for regularising the variance of the outcome, and includes logging of values as one possible transformation within the framework.\nThe data we used previous was annual data, showing the numbers of airmiles travelled in the USA by year up to the 1960s. Of course, however, many types of time series data are sub-annual, reported not just by year, but by quarter, or month, or day as well. Data disaggregated into sub-annual units often exhibit seasonal variation, patterns that repeat themselves at regular intervals within a 12 month cycle. 1\nIn this post we will look at some seasonal data, and consider two strategies for working with this data: STL decomposition; and Seasonal ARIMA (SARIMA)."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-23/index.html#recap-and-introduction",
    "href": "posts/glms/time-series/lms-are-glms-part-23/index.html#recap-and-introduction",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "",
    "text": "In previous posts on time series, we decomposed then applied a common general purpose modelling strategy for working with time series data called ARIMA. ARIMA model can involve autoregressive components (AR(p)), integration/differencing components (I(d)), and moving average components (MA(q)). As we saw, the time series data can also be pre-transformed, in ways other than just differencing; the example of this we saw was the application of the Box-Cox transformation for regularising the variance of the outcome, and includes logging of values as one possible transformation within the framework.\nThe data we used previous was annual data, showing the numbers of airmiles travelled in the USA by year up to the 1960s. Of course, however, many types of time series data are sub-annual, reported not just by year, but by quarter, or month, or day as well. Data disaggregated into sub-annual units often exhibit seasonal variation, patterns that repeat themselves at regular intervals within a 12 month cycle. 1\nIn this post we will look at some seasonal data, and consider two strategies for working with this data: STL decomposition; and Seasonal ARIMA (SARIMA)."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-23/index.html#an-example-dataset",
    "href": "posts/glms/time-series/lms-are-glms-part-23/index.html#an-example-dataset",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "An example dataset",
    "text": "An example dataset\nLet’s continue to use the examples and convenience functions from the forecast package used in the previous post, and for which the excellent book Forecasting: Principles and Practice is available freely online.\nFirst some packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(fpp3)\nlibrary(forecast)\nlibrary(fable)\n\n\nNow some seasonal data\n\n\nCode\n# Using this example dataset: https://otexts.com/fpp3/components.html\ndata(us_employment)\nus_retail_employment &lt;- us_employment |&gt;\n  filter(Title == \"Retail Trade\")\n\nus_retail_employment\n\n\n# A tsibble: 969 x 4 [1M]\n# Key:       Series_ID [1]\n      Month Series_ID     Title        Employed\n      &lt;mth&gt; &lt;chr&gt;         &lt;chr&gt;           &lt;dbl&gt;\n 1 1939 Jan CEU4200000001 Retail Trade    3009 \n 2 1939 Feb CEU4200000001 Retail Trade    3002.\n 3 1939 Mar CEU4200000001 Retail Trade    3052.\n 4 1939 Apr CEU4200000001 Retail Trade    3098.\n 5 1939 May CEU4200000001 Retail Trade    3123 \n 6 1939 Jun CEU4200000001 Retail Trade    3141.\n 7 1939 Jul CEU4200000001 Retail Trade    3100 \n 8 1939 Aug CEU4200000001 Retail Trade    3092.\n 9 1939 Sep CEU4200000001 Retail Trade    3191.\n10 1939 Oct CEU4200000001 Retail Trade    3242.\n# ℹ 959 more rows\n\n\nThere are two differences we can see with this dataset compared with previous time series data we’ve looked at.\nFirstly, the data looks like a data.frame object, or more specifically a tibble() (due to the additional metadata at the top). In fact they are of a special type of tibble called a tsibble, which is basically a modified version of a tibble optimised to work with time series data. We can check this by interrogating the class attributes of us_employment:\n\n\nCode\nclass(us_retail_employment)\n\n\n[1] \"tbl_ts\"     \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nThese class attributes go broadly from the most specific type of object class: tbl_ts (the tsibble); to the most general type of object class: the data.frame.\nSecondly, we can see that the data are disaggregated not by year as in the last post’s example, but also by month. So, what does this monthly data actually look like?\n\n\nCode\nautoplot(us_retail_employment, Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nThis data looks… spikey. There’s clearly both a long-term trend - including periods of faster and slower growth, and occasionally some falls - but there’s also what looks like a series of near-vertical spikes along this trend, at what may be regular intervals. What happens if we zoom into a smaller part of the time series?\n\n\nCode\nautoplot(\n    us_retail_employment |&gt;\n        filter(year(Month) &gt;=1990), \n    Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nHere we can start to see there’s not just a single repeating ‘vertical spike’, but a pattern that appears to repeat within each year, for each year. Let’s zoom in even further, for just three years:\n\n\nCode\nautoplot(\n    us_retail_employment |&gt;\n        filter(between(year(Month), 1994, 1996)), \n    Employed) +\n  labs(y = \"Persons (thousands)\",\n       title = \"Total employment in US retail\")\n\n\n\n\n\nAlthough each of these three years is different in terms of the average number of persons employed in retail, they are similar in terms of having a spike in employment towards the end of the year, then a drop off at the start of the year, then a relative plateau for the middle of the year.\nThis is an example of a seasonal pattern, information that gets revealed about a time series when we use a sub-annual resolution that might not be apparent it we used only annual data. How do we handle this kind of data?"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-23/index.html#approach-one-reannualise",
    "href": "posts/glms/time-series/lms-are-glms-part-23/index.html#approach-one-reannualise",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "Approach one: reannualise",
    "text": "Approach one: reannualise\nOf course we could simply reaggregate the data to an annual series:\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month)\n    ) |&gt;\n    ungroup() |&gt;\n    index_by(year) |&gt;\n    summarise(\n        Employed = sum(Employed)\n    ) %&gt;%\n    autoplot(., Employed)\n\n\n\n\n\nOne thing we can notice with this is that there appears to be a big drop in total employment for the last year. This is likely because the last year is incomplete, so whereas previous years are summing up 12 months’ observations, for the last year a smaller number of months are being summed up. We could then drop the last year:\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month)\n    ) |&gt;\n    ungroup() |&gt;\n    index_by(year) |&gt;\n    summarise(\n        Employed = sum(Employed)\n    ) |&gt;\n    filter(year != max(year)) %&gt;%\n    autoplot(., Employed)\n\n\n\n\n\nBut then we are losing some data that we really have. Even if we don’t have the full year, we might be able to get a sense from just the first few months worth of data whether the overall values for the last year are likely to be up or down compared to the same month in the previous years. We could even turn this single annual time series into 12 separate series: comparing Januaries with Januaries, Februaries with Februaries, and so on.\n\n\nCode\nus_retail_employment |&gt;\n    mutate(\n        year = year(Month), \n        month = month(Month, label = TRUE )\n    ) |&gt;\n    ggplot(\n        aes(year, Employed)\n    ) + \n    facet_wrap(~month) + \n    geom_line()\n\n\n\n\n\nHere we can see that comparing annual month-by-month shows a very similar trend overall. It’s as if each month’s values could be thought of as part of an annual ‘signal’ (an underlying long-term trend) plus a seasonal adjustment up or down: compared with the annual trend, Novembers and Decembers are likely to be high, and Januaries and Februaries to be low; and so on.\nIt’s this intuition - That we have a trend component, and a seasonal component - which leads us to our second strategy: decomposition."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-23/index.html#approach-two-seasonal-composition",
    "href": "posts/glms/time-series/lms-are-glms-part-23/index.html#approach-two-seasonal-composition",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "Approach Two: Seasonal Composition",
    "text": "Approach Two: Seasonal Composition\nThe basic intuition of decomposition is to break sub-annual data into a series of parts: The underling long term trend component; and repeating (usually) annual seasonal component.\nA common method for performing this kind of decomposition is known as STL. This actually stands for Seasonal and Trend Decomposition using Loess (Where Loess is itself another acronym). However it’s heuristically easier to imagine it stands for Season-Trend-Leftover, as it tends to generate three outputs from a single time-series input that correspond to these three components. Let’s regenerate the example in the forecasting book and then consider the outputs further:\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 7) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nThe plotted output contain four rows. These are, respectively:\n\nTop Row: The input data from the dataset\nSecond Row: The trend component from STL decomposition\nThird Row: The seasonal component from the STL decomposition\nBottom Row: The remainder (or leftover) component from the STL decomposition.\n\nSo, what’s going on?\nSTL uses an algorithm to find a repeated sequence (the seasonal component) in the data that, once subtracted from a long term trend, leaves a remainder (set of errors or deviations from observations) that is minimised in some way, and ideally random like white noise.\nIf you expanded the code chunk above, you will see two parameters as part of the STL model: the window argument for a trend() function; and the window argument for a season() function. This implies there are ways of setting up STL differently, and these would produce different output components. What happens if we change the window argument to 1 (which I think is its smallest allowable value)?\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n    filter(year(Month) &lt;= 2017) |&gt;\n  model(\n    STL(Employed ~ trend(window = 1) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nHere the trend component becomes, for want of a better term, ‘wigglier’. And the remainder term, except for a strange data artefact at the end, appears much smaller. So what does the window argument do?\nConceptually, what the window argument to trend() does is adjust the stiffness of the curve that the trendline uses to fit to the data. A longer window, indicated by a higher argument value, makes the curve stiffer, and a shorter window, indicated by a lower argument value, makes the curve less stiff. We’ve adjusted from the default window length of 7 to a much shorter length of 1, making it much less stiff.2 Let’s look at the effect of increasing the window length instead:\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 31) +\n                   season(window = \"periodic\"),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nHere we can see that, as well as the trend term being somewhat smoother than when a size 7 window length was used, the remainder term, though looking quite noisy, doesn’t really look random anymore. In particular, there seems to be a fairly big jump in the remainder component in the late 2000s. The remainder series also does not particularly stationary, lurching up and down at particular points in the series.\nIn effect, the higher stiffness of the trend component means it is not able to capture and represent enough signal in the data, and so some of that ‘signal’ is still present in the remainder term, when it should be extracted instead.\nNow what happens if we adjust the window argument in the season() function instead?\n\n\nCode\nus_retail_employment |&gt;\n    filter(year(Month) &gt;= 1990) |&gt;\n  model(\n    STL(Employed ~ trend(window = 7) +\n                   season(window = 5),\n    robust = TRUE)) |&gt;\n  components() |&gt;\n  autoplot()\n\n\n\n\n\nIn the above I’ve reduced the season window size (by default it’s infinite). Whereas before this seasonal pattern was forced to be constant for the whole time period, this time we an see that it changes, or ‘evolves’, over the course of the time series. We can also see that the remainder component, though looking quite random, now looks especially ‘spiky’, suggesting that the kinds of residuals left are somewhat further from Guassian white noise than in the first example.\n\nSection concluding thoughts\nSTL decomposition is one of a number of strategies for decomposition available to us. Other examples are described here. However the aims and principles of decomposition are somewhat similar no matter what approach is used.\nHaving performed a decomposition on time series data, we could potentially apply something like an ARIMA model to the trend component of the data alone for purposes of projection. If using a constant seasonal component, we could then add this component onto forecast values from the trend component, along with noise consistent with the properties of the remainder component. However, there is a variant of the ARIMA model specification that can work with this kind of seasonal data directly. Let’s look at that now"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-23/index.html#approach-three-sarima",
    "href": "posts/glms/time-series/lms-are-glms-part-23/index.html#approach-three-sarima",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "Approach Three: SARIMA",
    "text": "Approach Three: SARIMA\nSARIMA stands for ‘Seasonal ARIMA’ (where of course ARIMA stands for Autoregressive-Integrated-Moving Average). Whereas an ARIMA model has a specification shorthand ARIMA(p, d, q), a SARIMA model has an extended specification: SARIMA(p, d, q) (P, D, Q)_S. This means that whereas ARIMA has three parameters to specify, a SARIMA model has seven. This might appear like a big jump in model complexity, but the gap from ARIMA to SARIMA is smaller than it first appears.\nTo see this it’s first noticing that, as well as terms p, d and q, there are also terms P, D and Q. This would suggest that whatever Autoregressive (p), integration (d) and moving average (q) processes are involved in standard ARIMA are also involved in another capacity in SARIMA. And what’s this other capacity? The clue to this is in the S term.\nS 3 stands for the seasonal component of the model, and specifies the number of observations that are expected to include a repeating seasonal cycle. As most seasonal cycles are annual, this means S will be 12 if the data are monthly, 4 if the data are quarterly, and so on.\nThe UPPERCASE P, D and Q terms then specify which standard ARIMA processes should be modelled as occurring every S steps in the data series. Although algebraically this means SARIMA models may look a lot more complicated than standard ARIMA models, it’s really the same process, and the same intuition, applied twice: to characterising the seasonal ‘signals’ in the time series, and to characteristing the non-seasonal ‘signals’ in the time series.\nAlthough there are important diagnostic charts and heuristics to use when determining and judging which SARIMA specification may be most appropriate for modelling seasonal data, such as the PACF and ACF, we can still use the auto.arima() function to see if the best SARIMA specification can be identified algorithmically:\n\n\nCode\nbest_sarima_model &lt;- auto.arima(as.ts(us_retail_employment, \"Employed\"))\nbest_sarima_model\n\n\nSeries: as.ts(us_retail_employment, \"Employed\") \nARIMA(1,1,2)(2,1,2)[12] \n\nCoefficients:\n         ar1      ma1     ma2     sar1     sar2    sma1     sma2\n      0.8784  -0.8428  0.1028  -0.6962  -0.0673  0.2117  -0.3873\ns.e.  0.0374   0.0481  0.0332   0.0977   0.0691  0.0937   0.0776\n\nsigma^2 = 1442:  log likelihood = -4832.08\nAIC=9680.16   AICc=9680.31   BIC=9719.06\n\n\nHere auto.arima() produced an ARIMA(1, 1, 2) (2, 1, 2)_12 specification, meaning p=1, d=1, q=2 for the non-seasonal part; and P=2, D=1, Q=2 for the seasonal part.\nWhat kind of forecasts does this produce?\n\n\nCode\nbest_sarima_model |&gt; \n  forecast(h=48) |&gt;\n  autoplot()\n\n\n\n\n\nWe can see the forecasts tend to repeat the seasonal pattern apparent throughout the observed data, and also widen in the usual way the further we move from the observed data."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-23/index.html#summing-up",
    "href": "posts/glms/time-series/lms-are-glms-part-23/index.html#summing-up",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "Summing up",
    "text": "Summing up\nIn this post we have looked at three approaches for working with seasonal data: aggregating seasonality away; decomposition; and SARIMA. These are far from an exhaustive list, but hopefully illustrate some common strategies for working with this kind of data."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-23/index.html#footnotes",
    "href": "posts/glms/time-series/lms-are-glms-part-23/index.html#footnotes",
    "title": "Part Twenty Three: Time series and seasonality",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOccasionally, we might also see repeated patterns over non-annual timescales. For example, we might see the apparent population size of a country shifting abruptly every 10 years, due to information from national censuses run every decade being incorporated into the population estimates. Or if we track sales by day we might see a weekly cycle, because trade during the weekends tends to be different than during the weekdays.↩︎\nHow this works is due to the acronym-in-the-acronym: LOESS, meaning local estimation. Effectively for each data point a local regression slope is calculated based on values a certain number of observations ahead and behind the value in question. The number of values ahead and behind considered is the ‘window’ size.↩︎\nSometimes m is used instead of S.↩︎"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-25/index.html",
    "href": "posts/glms/time-series/lms-are-glms-part-25/index.html",
    "title": "Time series: Some closing remarks",
    "section": "",
    "text": "In the last few posts I’ve walked through some of the key concepts in time series, but focused on a particular modelling framework: the ARIMA model specification. Let’s recap what we’ve covered:\n\nIn post one, we discussed autoregression, and more generally the way that data including repeated measures of a single observation, or just a few observations, can still be treated largely like other types of data suitable for statistical modelling, through the inclusion of enough previous values as predictors that, once included, the observations can be considered as approximately independent of each other.\nIn post two, we discussed differencing and integration: an operation for trying to make non-stationary data stationary; and a reverse operation for starting to build forecasts based on such differenced data.\nIn post three, we discussed the intuition behind the moving average model: a way of thinking about time series as something analogous to a noisy singing bowl: a system that ‘wants’ to return to a position of either rest or a fundamental ‘tone’, but which is forever being subjected both to contemporary disturbances, and the influence of past disturbances.\nIn post four, we integrated the components of the first three posts - autoregression AR(p), integration I(d), and moving averages MA(q) - to produce the basic ARIMA(p, d, q) model specification, and saw some examples of trying to run and use this specification in practice.\nIn post five, we covered the topic of seasonality: repeated patterns over time that repeat in predictable ways over known and predictable time periods. We looked at both seasonal decomposition of such data into seasonal, trend, and (heuristically) ‘leftover’ components using the STL approach; and extending the ARIMA model specification to incorporate seasonality using the Seasonal ARIMA, or SARIMA, modelling framework.\nIn post six, we took some of the ideas covered in posts one and two and extended then in another way, to build the intuitions behind a vector autoregressive (or VAR) model specification. This post can be considered both an extension of some of the aspects covered elsewhere in the time series series, but also an extension of the discussions in our series on generalised modelling in general, and using models for prediction and simulation, as it was the first time we encountered multivariate models, i.e. models in which we aim to fit more than one outcome or response at a single time."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-25/index.html#recap",
    "href": "posts/glms/time-series/lms-are-glms-part-25/index.html#recap",
    "title": "Time series: Some closing remarks",
    "section": "",
    "text": "In the last few posts I’ve walked through some of the key concepts in time series, but focused on a particular modelling framework: the ARIMA model specification. Let’s recap what we’ve covered:\n\nIn post one, we discussed autoregression, and more generally the way that data including repeated measures of a single observation, or just a few observations, can still be treated largely like other types of data suitable for statistical modelling, through the inclusion of enough previous values as predictors that, once included, the observations can be considered as approximately independent of each other.\nIn post two, we discussed differencing and integration: an operation for trying to make non-stationary data stationary; and a reverse operation for starting to build forecasts based on such differenced data.\nIn post three, we discussed the intuition behind the moving average model: a way of thinking about time series as something analogous to a noisy singing bowl: a system that ‘wants’ to return to a position of either rest or a fundamental ‘tone’, but which is forever being subjected both to contemporary disturbances, and the influence of past disturbances.\nIn post four, we integrated the components of the first three posts - autoregression AR(p), integration I(d), and moving averages MA(q) - to produce the basic ARIMA(p, d, q) model specification, and saw some examples of trying to run and use this specification in practice.\nIn post five, we covered the topic of seasonality: repeated patterns over time that repeat in predictable ways over known and predictable time periods. We looked at both seasonal decomposition of such data into seasonal, trend, and (heuristically) ‘leftover’ components using the STL approach; and extending the ARIMA model specification to incorporate seasonality using the Seasonal ARIMA, or SARIMA, modelling framework.\nIn post six, we took some of the ideas covered in posts one and two and extended then in another way, to build the intuitions behind a vector autoregressive (or VAR) model specification. This post can be considered both an extension of some of the aspects covered elsewhere in the time series series, but also an extension of the discussions in our series on generalised modelling in general, and using models for prediction and simulation, as it was the first time we encountered multivariate models, i.e. models in which we aim to fit more than one outcome or response at a single time."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-25/index.html#whats-missing",
    "href": "posts/glms/time-series/lms-are-glms-part-25/index.html#whats-missing",
    "title": "Time series: Some closing remarks",
    "section": "What’s missing?",
    "text": "What’s missing?\nIn choosing to focus on the ARIMA modelling specification, we necessarily didn’t cover some other approaches to time series. This is similar to our series on causal inference, which stuck largely to one of the two main frameworks for thinking about the problems of causal inference - Rubin’s Missing Data framework - and only gave some brief coverage and attempt at consiliation of the other framework - the Pearlean graph-based framework - in the final post.\nSo, like the final post of the Causal Inference series, let’s discuss a couple of key areas that I’ve not covered in the series so far: state space models; and demographic models."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-25/index.html#state-space-models-and-ets",
    "href": "posts/glms/time-series/lms-are-glms-part-25/index.html#state-space-models-and-ets",
    "title": "Time series: Some closing remarks",
    "section": "State Space Models and ETS",
    "text": "State Space Models and ETS\nBorrowing concepts from control theory - literally rocket science! - and requiring an unhealthy level of knowledge of linear algebra to properly understand and articulate, state space models represent a 1990s statistical formalisation of - mainly - a series of model specifications first developed in the 1950s. In this sense they seem to bookend ARIMA model specifications, which were first developed in the 1970s. The 1950s models were based around the concept of exponential smoothing: the past influences the present, but the recent past influences the present more than the distant past. 1\nState space models involve solving (or getting a computer to solve) a series of simultaneous equations that link observed values over time \\(y_t\\) to a series of largely unobserved and unobservable model parameters. The key conceptual link between state space models and control theory is that these observed parameters are allowed to change/evolve over time, in response to the degree of error between predicted and observed values at different points in time.\nTo conceptualise what’s going on, think of a rocket trying to hit another moving target: the target the rocket’s guidance system is tracking keeps changing, as does the position of the rocket relative to its target. So, the targetting parameters used by the rocket to ensure it keeps track of the target need to keep getting updated too. And more recent observations of the target’s location are likely to be more important to determining where the rocket should move, and so how its parameters should be updated, than older observations. Also, the older observations are already in a sense incorporated into the system, as they influenced past decisions about the rocket’s parameters, and so its trajectory in the past, and so its current position. So, given the rocket’s current position, the most recent target position may be the only information needed, now, to decide how much to update the parameters.\nTo add 2 to the analogy (which in some use-cases may have not been an analogy), we could imagine having some parameters which determine how quickly or slowly other parameters get updated. Think of a dial that affects the stiffness of another dial: when this first dial is turned down, the second dial can be moved clockwise or counterclockwise very quickly in response to the moving target; when the first dial is turned up, the second dial is more resistant to change, so takes longer to turn: this is another way of thinking about what exponential smoothing means in practice. There’s likely to be a sweet spot when it comes to this first type of dial: too stiff is bad, as it means the system takes a long time to adjust to the moving target; but too responsive is bad too, as it means the system could become very unstable very quickly. Both excess stiffness and excess responsiveness can contribute to the system (i.e. our model predictions) getting further and further away from its target, and so to greater error.\nIn practice, with the excellent packages associated with Hyndman and Athanasopoulos’s excellent Forecasting book, we can largely ignore some of the more technical aspects of using state space models for time series forecasting with exponential smoothing, and just think of such models by analogy with ARIMA models, as model frameworks with a number of parameters to either select, or use heuristic or algorithmic methods to select for us. With ARIMA with have three parameters: for autoregression (p), differencing (d), and moving average (q); and with Seasonal ARIMA each of these receives a seasonal pair: p pairs with its seasonal analogue, P; d pairs with its seasonal analogue D; and q with its seasonal analogue Q.\nThe exponential smoothing analogue of the ARIMA framework is known as ETS, which stands for error-trend-season. Just as ARIMA model frameworks take three ‘slots’ - an AR() slot, an I() slot, and a MA() slot - ETS models also have three slots to be filled. However, these three slots don’t take numeric values, but the following arguments:\n\nThe error slot E(): takes N for ‘none’, A for ‘additive’, or M for ‘multiplicative’\nThe trend slot T(): takes N for ‘none’, A for ‘additive’, or A_d for ‘additive-damped’\nThe seasonal slot S(): takes N for ‘none’, A for ‘additive’, or M for ‘multiplicative’.\n\nSo, regardless of their different backgrounds, we can use ETS models as an alternative to, and in a very similar way to, ARIMA and SARIMA models.3"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-25/index.html#demographic-forecasting-models",
    "href": "posts/glms/time-series/lms-are-glms-part-25/index.html#demographic-forecasting-models",
    "title": "Time series: Some closing remarks",
    "section": "Demographic forecasting models",
    "text": "Demographic forecasting models\nAnother (more niche) type of forecasting model I’ve not covered in this series are those used in demographic forecasting, such as the Lee-Carter model specification and its derivatives. When forecasting life expectancy, we could simply model life expectancy directly… or we could model each of its components together: the mortality rates at each specific year of age, and then derive future life expectancies from what the forecast age specific life expectancies at different ages imply the resultant life expectancy should be (i.e. perform a life table calculation on the projected/forecast values for a given future year, much as we do for observed data). This is our second example of multivariate regression, which we were first introduced to in the post on vector autoregression VAR. However, Lee-Carter style models can involve projecting forward dozens, if not over a hundred, response values at the same time, whereas in the VAR model example we just had two response values.\nLee-Carter style models represent an intersection between forecasting and factor analysis, due to the way the observed values of many different age-specific mortality rates are assumed to be influenced by, and provide information that can inform, an underlying latent variable known as the drift parameter. Once (something like) factor analysis is used to determine this drift parameter, the (logarithm of the) mortality rates at each specific age are assumed to follow this drift approximately, subject to some random variation, meaning the time series specification they follow is random-walk-with-drift (RWD), which (I think) is an ARIMA(0, 1, 0) specification. This assumption of a single underlying latent drift parameter influencing all ages has been criticised as perhaps too strong a structural assumption, leading both to the suggestion that each age-specific mortality rate should be forecast independently with RWD, or that the Lee-Carter assumptions implicit in its specification be relaxed in a more piecemeal fashion, leading to some of the alternative longevity forecasting models summarised and evaluated in this paper, and this paper.\nThe overlap between general time series and demographic forecasting is less tenuous than it might first appear, when you consider that one of the authors of the first of the two papers above is none other than Rob Hyndman, whose forecasting book I’ve already referenced and made use of many times in this series. Hyndman is also the maintainer of R’s demography package. So, much of what can be learned about time series can be readily applied to demography too.\nFinally, demographic forecasting models in which age-specific mortality over time is modelled open up another way of thinking about the problem: that of spatial statistics. Remember that with exponential smoothing models the assumed information value of models declines exponentially with time? As mentioned in an earlier footnote this is largely what’s known as Tobler’s First Law of Geography, but applied to just a single dimension (time) rather than two spatial dimensions such as latitude and longitude. Well, with spatial models there are two spatial dimensions, both of which have the same units (say, metres, kilometres, etc). Two points can be equally far apart, but along different spatial dimensions. Point B could be 2km east of point A, or 2km north of point A, or 2km north-east of point A.4 In each case, the distance between points is the same, and so the amount of downweighting of the information value of point B as to the true value of point A would be the same.\nWell, with the kind of data used by Lee-Carter style models, we have mortality rates that are double indexed: \\(m_{x,t}\\), where \\(x\\) indexes the age in years, and \\(t\\) indexes the time in years. Note the phrase in years: both age and time are in the same unit, much as latitude and longitude are in the same unit. So it makes sense to think of two points on the surface - \\(m_{a,b}\\) and \\(m_{c,d}\\) - as being a known distance apart from each other,5 and so for closer observations to be more similar or informative about each other than more distant values. This kind of as-if-spatial reasoning leads to thinking about how a surface of mortality rates might be smoothed appropriately, with estimates for each point being a function of the observed value of that point, plus some kind of exponentially weighted average of more and less proximately neighbouring points. (See, for example, Girosi & King’s framework)"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-25/index.html#summing-up",
    "href": "posts/glms/time-series/lms-are-glms-part-25/index.html#summing-up",
    "title": "Time series: Some closing remarks",
    "section": "Summing up",
    "text": "Summing up\nIn this last post, we’ve scratched the surface on a couple of areas related to time series that the main post series didn’t cover: state space modelling and demographic forecasting models. Although hopefully we can agree that they’re both interesting topics, they’ve also helped to illustrate why the main post series has been anchored around the ARIMA modelling framework. Time series is a complex area, and so by sticking with ARIMA, we’ve managed to avoid getting too far adrift."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-25/index.html#footnotes",
    "href": "posts/glms/time-series/lms-are-glms-part-25/index.html#footnotes",
    "title": "Time series: Some closing remarks",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn some ways, this idea seems equivalent to Tobler’s First Law of Geography, but applied to temporal rather than spatial distance.↩︎\nPossibly confusion↩︎\nA complicating coda to this is that state space modelling approaches can also be applied to ARIMA models.↩︎\nUsing a little trigonometry, this would be about 1.41 km east of point A, and 1.41km north of point A.↩︎\nAgain, a little trigomometry tells us that the Cartesian distance between two points \\(m_{a,b}\\) and \\(m_{c,d}\\) should be \\(d = \\sqrt{(c-a)^2 + (d-b)^2}\\). In practice with spatial statistics two elements are often encoded in terms of adjacency: 1 if two elements are contiguous (next to each other); 0 if they are not.↩︎"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-20/index.html",
    "href": "posts/glms/time-series/lms-are-glms-part-20/index.html",
    "title": "Part Twenty: Time Series: Integration",
    "section": "",
    "text": "In the last part of this series, I discussed why time series data are both a bit dissimilar to many other types of data we try to model, and also ‘one weird trick’ - autoregression - which allows the standard generalised linear model ‘chasis’ - that two part equation - to be used with time series data.\nWithin the last part, I said autoregression was just one of three common tools used for working with time series data, with the other two being integration and moving averages. Let’s now cover those two remaining tools:"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-20/index.html#recap",
    "href": "posts/glms/time-series/lms-are-glms-part-20/index.html#recap",
    "title": "Part Twenty: Time Series: Integration",
    "section": "",
    "text": "In the last part of this series, I discussed why time series data are both a bit dissimilar to many other types of data we try to model, and also ‘one weird trick’ - autoregression - which allows the standard generalised linear model ‘chasis’ - that two part equation - to be used with time series data.\nWithin the last part, I said autoregression was just one of three common tools used for working with time series data, with the other two being integration and moving averages. Let’s now cover those two remaining tools:"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-20/index.html#integration",
    "href": "posts/glms/time-series/lms-are-glms-part-20/index.html#integration",
    "title": "Part Twenty: Time Series: Integration",
    "section": "Integration",
    "text": "Integration\nConsider the following time series data:\n\n\nCode\nlibrary(tidyverse)\nset.seed(8)\nt &lt;- 0:30\n\nintercept &lt;- 2.35\nslope &lt;- 0.15\n\ny &lt;- intercept + slope * t + rnorm(31, mean = 0, sd = 0.2)\n\ndf &lt;- tibble(\n    t = t,\n    y = y\n)\n\ndf |&gt;\n    ggplot(aes(t, y)) + \n    geom_point() + \n    geom_line() + \n    expand_limits(y = 0)\n\n\n\n\n\nThis time series data is an example of a non-stationary time series. This term means that its value drifts in a particular direction over time. In this case, upwards, meaning values towards the end of the series tend to be higher than values towards the start of the series.\nWhat this drift means is that the order of the observations matters, i.e. if we looked at the same observations, but in a random order, we wouldn’t see something that looks similar to what we’re seeing here.\n\n\nCode\ndf |&gt;\n    mutate(rand_selection = sample(0:30)) |&gt;\n    ggplot(aes(rand_selection, y)) + \n    geom_point() + \n    geom_line() + \n    expand_limits(y = 0)\n\n\n\n\n\nAs it’s clear the order of the sequence matters, the standard simplifying assumptions for statistical models of IID (independent and identically distributed) does not hold, so the extent to which observations from the same time series dataset can be treated like new pieces of information for the model is doubtful. We need a way of making the observations that go into the model (though not necessarily what we do with the model after fitting it) more similar to each other, so these observations can be treated as IID. How do we do this?\nThe answer is something that’s blindingly obvious in retrospect. We can transform the data that goes into the model by taking the differences between consecutive values. So, if the first ten values of our dataset look like this:\n\n\nCode\nhead(df, n=10 )\n\n\n# A tibble: 10 × 2\n       t     y\n   &lt;int&gt; &lt;dbl&gt;\n 1     0  2.33\n 2     1  2.67\n 3     2  2.56\n 4     3  2.69\n 5     4  3.10\n 6     5  3.08\n 7     6  3.22\n 8     7  3.18\n 9     8  2.95\n10     9  3.58\n\n\nThen we can take the differences between consecutive values and get the following:\n\n\nCode\ndf |&gt; \n    arrange(t) |&gt;\n    mutate(diff_y = y - lag(y, 1)) |&gt;\n    head(n=11)\n\n\n# A tibble: 11 × 3\n       t     y  diff_y\n   &lt;int&gt; &lt;dbl&gt;   &lt;dbl&gt;\n 1     0  2.33 NA     \n 2     1  2.67  0.335 \n 3     2  2.56 -0.111 \n 4     3  2.69  0.133 \n 5     4  3.10  0.407 \n 6     5  3.08 -0.0188\n 7     6  3.22  0.138 \n 8     7  3.18 -0.0336\n 9     8  2.95 -0.235 \n10     9  3.58  0.634 \n11    10  3.70  0.117 \n\n\nSo, as with autoregression (AR), with integration we’ve arranged the data in order, then used the lag operator. The difference between the use of lagging as a tool for AR, and lagging as a tool for integration (I), is that, whereas for autoregression, we’re using lagging to construct one or more variables to use as predictor terms for the model, within integration we’re using lagging to construct new variables for use in either the response or the predictor sides of the model equation.\nWhat does our differenced data look like?\n\n\nCode\ndf |&gt; \n    arrange(t) |&gt;\n    mutate(diff_y = y - lag(y, 1)) |&gt;\n    ggplot(aes(t, diff_y)) + \n    geom_hline(yintercept = 0) + \n    geom_point() + \n    geom_line()\n\n\n\n\n\nIn the above I’ve added a reference line at y=0. Note that the average of this series appears to be above the zero line. Let’s check this assumption:\n\n\nCode\ndy &lt;- df |&gt;\n    arrange(t) |&gt;\n    mutate(diff_y = y - lag(y, 1)) |&gt;\n    pull(diff_y) \n\nprint(paste0(\"The mean dy is \", mean(dy, na.rm = TRUE) |&gt; round(2)))\n\n\n[1] \"The mean dy is 0.16\"\n\n\nCode\nprint(paste0(\"The corresponding SE is \", (sd(dy, na.rm=TRUE) / sqrt(length(dy)-1)) |&gt; round(2)))\n\n\n[1] \"The corresponding SE is 0.06\"\n\n\nThis mean value of the differences values, \\(dy\\), is about 0.16. This is the intercept of the differenced data. As we made up the original data, we also know that its slope is 0.15, i.e. except for estimation uncertainty, the intercept of the differenced data is the slope of the original data. 1\nImportantly, when it comes to time series, whereas our original data were not stationary, our differenced data are. This means they are more likely to meet the IID conditions, including that the order of observations no longer really matters as to its value.\nOne way of demonstrating this is with a statistical identity parade.\nHere are nine versions of the undifferenced data, eight of which have been randomly shuffled. Can you tell which is the original, unshuffled data?\n\n\nCode\nset.seed(9)\n\npermute_randomly &lt;- function(id, df){\n    df |&gt; \n        mutate(y = sample(y))\n}\n\ndf_parade &lt;- tibble(\n    id = LETTERS[1:9]\n) |&gt;\n    mutate(data = map(id, permute_randomly, df = df))\n\ndf_parade$data[[5]] &lt;- df\n\ndf_parade |&gt;\n    unnest(data) |&gt; \n    ggplot(aes(t, y)) + \n    geom_point() + \n    geom_line() + \n    facet_wrap(~id)\n\n\n\n\n\nHere it seems fairly obvious which dataset is the original, unshuffled version of the data, again illustrating that the original time series are not IID, and not a stationary series.\nBy contrast, let’s repeat the same exercise with the differenced data:\n\n\nCode\nd_df &lt;- df |&gt; \n    arrange(t) |&gt;\n    mutate(diff_y = y - lag(y, 1)) |&gt;\n    select(t, y = diff_y) %&gt;%\n    filter(complete.cases(.))\n\ndiff_df_parade &lt;- tibble(\n    id = LETTERS[1:9]\n) |&gt;\n    mutate(data = map(id, permute_randomly, df = d_df))\n\ndiff_df_parade$data[[5]] &lt;- d_df\n\ndiff_df_parade |&gt;\n    unnest(data) |&gt; \n    ggplot(aes(t, y)) + \n    geom_point() + \n    geom_line() + \n    facet_wrap(~id)\n\n\n\n\n\nHere it’s much less obvious which of the series is the original series, rather than a permuted/shuffled version of the same series. This should give some reassurance that, after differencing, the data are now IID."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-20/index.html#why-is-integration-called-integration-not-differencing",
    "href": "posts/glms/time-series/lms-are-glms-part-20/index.html#why-is-integration-called-integration-not-differencing",
    "title": "Part Twenty: Time Series: Integration",
    "section": "Why is integration called integration not differencing?",
    "text": "Why is integration called integration not differencing?\nIn the above we have performed what in time series parlance would be called an I(1) operation, differencing the data once. But why is this referred to as integration, when we’re doing the opposite?\nWell, when it comes to transforming the time series data into something with IID properties, we are differentiating rather than integrating. But the flip side of this is that, if using model outputs based on differenced data for forecasting, we have to sum up (i.e. integrate) the values we generate in the order in which we generate them. So, the model works on the differenced data, but model forecasts work by integrating the random variables generated by the model working on the differenced data.\nLet’s explore what this means in practice. Let’s generate 10 new values from a model calibrated on the mean and standard deviation of the differenced data:\n\n\nCode\nnew_draws &lt;- rnorm(\n    10, \n    mean = mean(d_df$y, na.rm = TRUE),\n    sd = sd(d_df$y, na.rm = TRUE)\n)\n\nnew_draws\n\n\n [1]  0.053763359  0.638791031 -0.106726422  0.470132841 -0.084156880\n [6] -0.041865610  0.002496487  0.481433544 -0.082119365  0.086125038\n\n\nWe can append these to the end of our differenced data to see how this forecast series compared with the observed series:\n\n\nCode\nmax_t &lt;- max(d_df$t)\n\nforecast_df &lt;- tibble(\n    t = seq(from = max_t+1, to = max_t + length(new_draws)),\n    y = new_draws,\n    type = \"forecast\"\n)\n\nobs_forecast_df &lt;- bind_rows(\n    d_df |&gt; mutate(type = 'observed'),\n    forecast_df\n)\n\nobs_forecast_df |&gt; \n    ggplot(aes(t, y)) + \n    geom_point(aes(shape = type, colour = type)) +\n    geom_line(aes(linetype = type)) + \n    scale_linetype_manual(values = c(\"observed\" = 'solid', 'forecast' = 'dashed'))\n\n\n\n\n\nSo we can see that the forecast sequence of values looks quite similar to the differenced observations before it.\nIn order to use this for forecasting values, rather than differences, we therefore have to take the last observed value, and keep adding the consecutive forecast values.\n\n\nCode\nlast_obs_y &lt;- df |&gt; filter(t == max(t)) |&gt; pull(y)\n\naccumulated_new_draws &lt;- cumsum(new_draws)\n\naccumulated_new_draws\n\n\n [1] 0.05376336 0.69255439 0.58582797 1.05596081 0.97180393 0.92993832\n [7] 0.93243481 1.41386835 1.33174898 1.41787402\n\n\nCode\nforecast_values &lt;- last_obs_y + accumulated_new_draws\n\nforecast_df &lt;- tibble(\n    t = seq(from = max_t+1, to = max_t + length(new_draws)),\n    y = forecast_values,\n    type = \"forecast\"\n)\n\nobs_forecast_df &lt;- bind_rows(\n    df |&gt; mutate(type = 'observed'),\n    forecast_df\n)\n\nobs_forecast_df |&gt; \n    ggplot(aes(t, y)) + \n    geom_point(aes(shape = type, colour = type)) +\n    geom_line(aes(linetype = type)) + \n    scale_linetype_manual(values = c(\"observed\" = 'solid', 'forecast' = 'dashed'))\n\n\n\n\n\nSo, after integrating (accumulating or summing up) the modelled differenced values, we now see the forecast values continuing the upwards trend observed in the original data.\nOf course, there’s nothing special about the specific sequence of draws generated from the model. We could run the same exercise multiple times and each time get a different sequence of model draws, and so a different forecast path. Let’s see ten draws, for example:\n\n\nCode\nmake_multiple_paths &lt;- function(path_length, n_reps, start_value, mu, sigma, start_t){\n\n    make_path &lt;- function(start_t, mu, sigma, path_length, start_value) {\n        draws &lt;- rnorm(path_length, mean = mu, sd = sigma)\n\n        summed_values &lt;- cumsum(draws)\n        forecast_values &lt;- summed_values + start_value\n\n        out &lt;- tibble(\n            t = seq(from = start_t, to = start_t + path_length - 1),\n            y = forecast_values\n        )\n        return(out)\n    }\n\n    paths &lt;- replicate(\n        n_reps, \n        make_path(\n                    start_t = start_t, \n                    mu = mu, sigma = sigma, path_length = path_length, start_value = start_value\n            )\n    ) |&gt; \n        apply(2, as.data.frame)\n\n    out &lt;- tibble(\n        rep_num = 1:n_reps,\n        data = paths\n    ) |&gt;\n      unnest(data)\n\n    return(out)\n}\n\n\npaths &lt;- make_multiple_paths(\n    path_length = 10, \n    n_reps = 10, \n    mu = mean(d_df$y, na.rm = TRUE),\n    sigma = sd(d_df$y, na.rm = TRUE),\n    start_value = last_obs_y, \n    start_t = max(d_df$t) + 1\n)\n\ndf |&gt;\n    ggplot(aes(t, y)) + \n    geom_point() + geom_line() +\n    geom_line(aes(t, y, group = rep_num), inherit.aes = FALSE, data = paths, alpha = 0.5, colour = \"blue\")\n\n\n\n\n\nThis gives a sense of the kinds of upwards-drifting walks are compatible with the amount of variation observed in the original data series. If we ran the experiment another 10 times, we would get another ten paths.\nIn fact, we could generate a much larger number of simulations, say 10,000, and then report the range of values within which (say) 50% or 90% of the values for each time period are contained:\n\n\nCode\nmany_paths &lt;- make_multiple_paths(\n    path_length = 10, \n    n_reps = 10000, \n    mu = mean(d_df$y, na.rm = TRUE),\n    sigma = sd(d_df$y, na.rm = TRUE),\n    start_value = last_obs_y, \n    start_t = max(d_df$t) + 1\n)\n\nmany_paths_summary &lt;- many_paths |&gt;\n    group_by(t) |&gt; \n    summarise(\n        med = median(y), \n        lq = quantile(y, probs = 0.25), uq = quantile(y, probs = 0.75), l5 = quantile(y, probs = 0.05), u5 = quantile(y, probs = 0.95)) |&gt; \n    ungroup()\n\ndf |&gt;\n    ggplot(aes(t, y)) + \n    geom_point() + geom_line() +\n    geom_line(\n        aes(t, med), inherit.aes = FALSE, data = many_paths_summary, colour = \"blue\", linewidth = 1.2\n    ) +\n    geom_ribbon(\n        aes(t, ymin = lq, ymax = uq), \n        inherit.aes = FALSE, data = many_paths_summary,\n        colour = NA, alpha = 0.25\n    ) + \n    geom_ribbon(\n        aes(t, ymin = l5, ymax = u5),\n        inherit.aes = FALSE, data = many_paths_summary,\n        colour = NA, alpha = 0.25\n    )\n\n\n\n\n\nThese produce the kinds of ‘fans of uncertainty’ we might be used to seeing from a time series forecast. Because of the large numbers of simulations run, the shape of the fans appear quite smooth, and close to the likely analytical solution."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-20/index.html#summing-up",
    "href": "posts/glms/time-series/lms-are-glms-part-20/index.html#summing-up",
    "title": "Part Twenty: Time Series: Integration",
    "section": "Summing up",
    "text": "Summing up\nIn this post we’ve explored the second of the three main tools in the most common time series analytical toolkit: Integration. We’ve differenced our data once, which in time series parlance is represented by the shorthand d=1. Then we’ve integrated estimates we’ve produced from a model after differencing to represent random paths projecting forward from the observed data into a more uncertain future. Doing this multiple times has allowed us to represent uncertainty about these projections, and the ways that uncertainty increases the further we move from the observed data."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-20/index.html#coming-up",
    "href": "posts/glms/time-series/lms-are-glms-part-20/index.html#coming-up",
    "title": "Part Twenty: Time Series: Integration",
    "section": "Coming up",
    "text": "Coming up\nIn the next post, we will look at the final of the three tools in the standard time series toolkit: the moving average."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-20/index.html#footnotes",
    "href": "posts/glms/time-series/lms-are-glms-part-20/index.html#footnotes",
    "title": "Part Twenty: Time Series: Integration",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHurray! We’ve demonstrated something we should know from school, namely that if \\(y = \\alpha + \\beta x\\), then \\(\\frac{\\partial y}{\\partial x} = \\beta\\).↩︎"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-19/index.html",
    "href": "posts/glms/time-series/lms-are-glms-part-19/index.html",
    "title": "Part Nineteen: Time Series: Introduction and Autoregression",
    "section": "",
    "text": "A few weeks ago, I polled both LinkedIn and (what’s left of) Twitter for statistical topics to cover next in this series. By a small to moderate margin, time series came out on top. So, after a longer-than-usual delay, here’s an introduction to time series modelling."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-19/index.html#introduction",
    "href": "posts/glms/time-series/lms-are-glms-part-19/index.html#introduction",
    "title": "Part Nineteen: Time Series: Introduction and Autoregression",
    "section": "",
    "text": "A few weeks ago, I polled both LinkedIn and (what’s left of) Twitter for statistical topics to cover next in this series. By a small to moderate margin, time series came out on top. So, after a longer-than-usual delay, here’s an introduction to time series modelling."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-19/index.html#time-series-an-exception-that-proves-the-rule",
    "href": "posts/glms/time-series/lms-are-glms-part-19/index.html#time-series-an-exception-that-proves-the-rule",
    "title": "Part Nineteen: Time Series: Introduction and Autoregression",
    "section": "Time Series: An exception that proves the rule?",
    "text": "Time Series: An exception that proves the rule?\nThroughout this series I’ve returned many times to the same ‘mother formula’ for a generalised linear model:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nSo, we have a system that takes a series of inputs, \\(X\\), and returns an output, \\(Y\\), and we have two sets of parameters, \\(\\beta\\) in \\(g(.)\\) and \\(\\alpha\\) in \\(f(.)\\), which are calibrated based on the discrepancy between what the model predicted output \\(Y\\) and the observed output \\(y\\).\nThere are two important things to note: Firstly, that the choice of parts of the data go into the inputs \\(X\\) and the output(s) \\(Y\\) is ultimately our own. A statistical model won’t ‘know’ when we’re trying to predict the cause of something based on its effect, for example. Secondly, that although the choice of input and output for the model are ultimately arbitrary, they cannot be the same. i.e., we cannot do this:\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\n\\[\n\\theta_i = g(Y_i, \\beta)\n\\]\nThis would be the model calibration equivalent of telling a dog to chase, or a snake to eat, its own tail. It doesn’t make sense, and so the parameter values involved cannot be calculated.\nFor time series data, however, this might appear to be a fundamental problem, given our observations may comprise only of ‘outcomes’, which look like they should be in the output slot of the formulae, rather than determinants, which look like they should be in the input slot of the formulae. i.e. we might have data that looks as follows:\n\n\n\n\\(i\\)\n\\(Y_{T-2}\\)\n\\(Y_{T-1}\\)\n\\(Y_{T}\\)\n\n\n\n\n1\n4.8\n5.0\n4.9\n\n\n2\n3.7\n4.1\n4.3\n\n\n3\n4.3\n4.1\n4.3\n\n\n\nWhere \\(T\\) indicates an index time period, and \\(T-k\\) a fixed difference in time ahead of or behind the index time period. For example, \\(T\\) might be 2019, \\(T-1\\) might be 2018, \\(T-2\\) might be 2017, and so on.\nAdditionally, for some time series data, the dataset will be much more wide than long, perhaps with just a single observed unit, observed at many different time points:\n\n\n\n\n\n\n\n\n\n\n\n\n\\(i\\)\n\\(Y_{T-5}\\)\n\\(Y_{T-4}\\)\n\\(Y_{T-3}\\)\n\\(Y_{T-2}\\)\n\\(Y_{T-1}\\)\n\\(Y_{T}\\)\n\n\n\n\n1\n3.9\n5.1\n4.6\n4.8\n5.0\n4.9\n\n\n\nGiven all values are ‘outcomes’, where’s the candidate for an ‘input’ to the model, i.e. something we should consider putting into \\(X\\)?\nDoesn’t the lack of an \\(X\\) mean time series is an exception to the ‘rule’ about what a statistical model looks like, and so everything we’ve learned so far is no longer relevant?\nThe answer to the second question is no. Let’s look at why."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-19/index.html#autoregression",
    "href": "posts/glms/time-series/lms-are-glms-part-19/index.html#autoregression",
    "title": "Part Nineteen: Time Series: Introduction and Autoregression",
    "section": "Autoregression",
    "text": "Autoregression\nInstead of looking at the data in the wide format above, let’s instead rearrange it in long format, so the time variable is indexed in its own column:\n\n\nCode\nlibrary(tidyverse)\n\ndf &lt;- tribble(\n    ~i, ~t, ~y,\n    1, 2008, 3.9,\n    1, 2009, 5.1,\n    1, 2010, 4.6,\n    1, 2011, 4.8,\n    1, 2012, 5.0,\n    1, 2013, 4.9\n)\ndf\n\n\n# A tibble: 6 × 3\n      i     t     y\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     1  2008   3.9\n2     1  2009   5.1\n3     1  2010   4.6\n4     1  2011   4.8\n5     1  2012   5  \n6     1  2013   4.9\n\n\nAs there’s only one type of observational unit, \\(i=1\\) in all cases, there’s no variation in this variable, so it can’t provide any information in a model. Let’s look at the series and think some more:\n\n\nCode\ndf |&gt; \n    ggplot(aes(t, y)) + \n    geom_line() + \n    geom_point()\n\n\n\n\n\nWe could, of course, regress the outcome against time:\n\n\nCode\ndf |&gt;\n    ggplot(aes(t, y)) + \n    geom_line() + \n    geom_point() + \n    stat_smooth(method = \"lm\")\n\n\n\n\n\nIs this reasonable? It depends on the context. We obviously don’t have that many observations, but the regression slope appears to have a positive gradient, meaning values projected into the future will likely be higher than the observed values, and values projected into the past will likely have lower than the observed values.\n\n\nCode\ndf |&gt;\n    ggplot(aes(t, y)) + \n    geom_line() + \n    geom_point() + \n    scale_x_continuous(limits = c(2000, 2020)) + \n    stat_smooth(method = \"lm\", fullrange = TRUE)\n\n\n\n\n\nMaybe this kind of extrapolation is reasonable. Maybe it’s not. As usual it depends on context. Although it’s a model including time as a predictor, it’s not actually a time series model. Here’s an example of a time series model:\n\n\nCode\nlm_ts_ar0 &lt;- lm(y ~ 1, data = df)\n\nsummary(lm_ts_ar0)\n\n\n\nCall:\nlm(formula = y ~ 1, data = df)\n\nResiduals:\n       1        2        3        4        5        6 \n-0.81667  0.38333 -0.11667  0.08333  0.28333  0.18333 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   4.7167     0.1778   26.53 1.42e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4355 on 5 degrees of freedom\n\n\nThis is an example of a time series model, even though it doesn’t have a time component on the predictor side. In fact, it doesn’t have anything as a predictor. Its formula is y ~ 1, meaning there’s just an intercept term. It’s saying “assume new values are just like old values: all just drawn from the same normal distribution”.\nI called this model ar0. Why? Well, let’s look at the following:\n\n\nCode\nlm_ts_ar1 &lt;- df |&gt;\n    arrange(t) |&gt;\n    mutate(y_lag1 = lag(y, 1)) %&gt;%\n    lm(y ~ y_lag1, data = .)\n\nsummary(lm_ts_ar1)\n\n\n\nCall:\nlm(formula = y ~ y_lag1, data = .)\n\nResiduals:\n        2         3         4         5         6 \n-0.005066 -0.158811 -0.103084  0.154626  0.112335 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   6.2304     0.7661   8.132  0.00389 **\ny_lag1       -0.2885     0.1630  -1.770  0.17489   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1554 on 3 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.5108,    Adjusted R-squared:  0.3477 \nF-statistic: 3.133 on 1 and 3 DF,  p-value: 0.1749\n\n\nFor this model, I included one new predictor: y_lag1. For this, I created a new column in the dataset:\n\n\nCode\ndf |&gt;\n    arrange(t) |&gt;\n    mutate(y_lag1 = lag(y, 1))\n\n\n# A tibble: 6 × 4\n      i     t     y y_lag1\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1     1  2008   3.9   NA  \n2     1  2009   5.1    3.9\n3     1  2010   4.6    5.1\n4     1  2011   4.8    4.6\n5     1  2012   5      4.8\n6     1  2013   4.9    5  \n\n\nThe lag() operator takes a column and a lag term parameter, in this case 1 1. For each row, the value of y_lag1 is the value of y one row above. 2\nThe first simple model was called AR0, and the second AR1. So what does AR stand for?\nIf you’re paying attention to the section title, you already have the answer. It means autoregressive. The AR1 model is the simplest type of autoregressive model, by which I don’t mean this is a type of regression model that formulates itself, but do mean it includes its own past states (at time T-1) as predictors of its current state (at time T).\nAnd what does \\(T\\) and \\(T-1\\) refer to in the above? Well, for row 6 \\(T\\) refers to \\(t\\), which is 2013; and so \\(T-1\\) refers to 2012. But for row 5 \\(T\\) refers to 2012, so \\(T-1\\) refers to 2011. This continues back to row 2, where \\(T\\) is 2009 so \\(T-1\\) must be 2008. (The first row doesn’t have a value for y_lag1, so can’t be included in the regression).\nIsn’t this a bit weird, however? After all, we only have one real observational unit, \\(i\\), but for the AR1 model we’re using values from this unit five times. Doesn’t this violate some kind of rule or expectation required for model outputs to be legitimate?\nWell, it might. A common shorthand when describing the assumptions that we make when applying statistical models is \\(IID\\), which stands for ‘independent and identically distributed’. As with many unimaginative discussions of statistics, we can illustrate something that satifies both of these properties, independent and identically distributed by looking at some coin flips:\n\n\nCode\nset.seed(7)\n\nuniform_values &lt;- runif(10)\ncoin_df &lt;- tibble(\n    flip_number = 1:10, \n    is_head = uniform_values &gt;= 0.5\n)\n\ncoin_df\n\n\n# A tibble: 10 × 2\n   flip_number is_head\n         &lt;int&gt; &lt;lgl&gt;  \n 1           1 TRUE   \n 2           2 FALSE  \n 3           3 FALSE  \n 4           4 FALSE  \n 5           5 FALSE  \n 6           6 TRUE   \n 7           7 FALSE  \n 8           8 TRUE   \n 9           9 FALSE  \n10          10 FALSE  \n\n\nThe above series of coin flips is identically distributed because the order in which the flips occur doesn’t matter to the value generated. The dataframe could be permutated in any order and it wouldn’t matter to the data generation process at all. The series is independent because the probability of getting, say, a sequence of three heads is just the product of getting one head, three times, i.e. i.e. \\(\\frac{1}{2} \\frac{1}{2} \\frac{1}{2}\\) or \\([\\frac{1}{2}]^{3}\\). Without going into too much detail, both of these assumptions are necessary to make in order for likelihood estimation, which relies on multiplying sequences of numbers 3 to ‘work’.\nThe central assumption and hope with an autoregressive model specification is that, conditional on the autoregressive terms being included on the predictor side of the model, the data can assumed to have been generated from an IID data generating process (DGP).\nThe intuition behind this is something like the following: say you wanted to know if I’ll have the flu tomorrow. It would obviously be useful to know if I have the flu today, because symptoms don’t change very quickly. 4 Maybe it would also be good to know if I had the flu yesterday too, maybe even two days ago as well. But would it be good to know if I had the flu two weeks ago, or five weeks ago? Probably not. At some point, i.e. some number of lag terms from a given time, more historical data stops being informative. i.e., beyond a certain number of lag periods, the data series can be assumed to be IID (hopefully).\nWhen building autoregressive models, it is common to look at a range of specifications, each including different numbers of lag terms. I.e. we can build a series of AR specification models as follows:\n\nAR(0): \\(Y_T \\sim 1\\)\nAR(1): \\(Y_T \\sim Y_{T-1}\\)\nAR(2): \\(Y_T \\sim Y_{T-1} + Y_{T-2}\\)\nAR(3): \\(Y_T \\sim Y_{T-1} + Y_{T-2} + Y_{T-3}\\)\n\nAnd so on. Each successive AR(.) model contains more terms than the last, so is a more complicated and data hungry model than the previous one. We should already by this point in the series be familiar with standard approaches for trying to find the best trade off between model complexity and model fit. The above models are in a sense nested, so for example F-tests can be used to compare these models. Another approach, which ‘works’ for both nested and non-nested model specifications, is AIC, and indeed this is commonly used to select the ‘best’ number of autoregressive terms to include.\nFor future reference, the number of AR terms is commonly denoted with the letter ‘p’, meaning that if p is 3, for example, then we are talking about an AR(3) model specification."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-19/index.html#summing-up",
    "href": "posts/glms/time-series/lms-are-glms-part-19/index.html#summing-up",
    "title": "Part Nineteen: Time Series: Introduction and Autoregression",
    "section": "Summing up",
    "text": "Summing up\nThe other important thing to note is that autoregression is a way of fitting time series data within the two component ‘mother formulae’ at the start of this post (and many others), by operating on the systematic component of the model framework, \\(g(.)\\). At this stage, nothing unusual is happening with the stochastic component of the model framework \\(f(.)\\).\nWith autoregression, denoted by the formula shorthand \\(AR(.)\\) and the parameter shorthand \\(p\\), we now have one of the three main tools in the modeller’s toolkit for handling time series data."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-19/index.html#coming-up",
    "href": "posts/glms/time-series/lms-are-glms-part-19/index.html#coming-up",
    "title": "Part Nineteen: Time Series: Introduction and Autoregression",
    "section": "Coming up",
    "text": "Coming up\nIn the next post in this mini-series, we’ll start to look at the other two main components of time series modelling, integration and moving averages, before looking at how they’re combined and applied in a general model specification called ARIMA."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-19/index.html#footnotes",
    "href": "posts/glms/time-series/lms-are-glms-part-19/index.html#footnotes",
    "title": "Part Nineteen: Time Series: Introduction and Autoregression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhich is also the default, so not strictly necessary this time.↩︎\nTwo further things to note: Firstly, that if there were more than one observational unit i then the data frame should be grouped by the observational unit, then arranged by time. Secondly, that because the first time unit has no time unit, its lagged value is necessarily missing, hence NA.↩︎\nOr equivalently and more commonly summing up the log of these numbers↩︎\nThis is in some ways a bad example, as influenza is of course highly seasonal, and one flu episode may be negatively predictive of another flu episode in the same season. But I’m going with it for now…↩︎"
  },
  {
    "objectID": "posts/handdrawn-stats/statistics-as-circuits/index.html",
    "href": "posts/handdrawn-stats/statistics-as-circuits/index.html",
    "title": "Statistics as circuit boards",
    "section": "",
    "text": "The general approach I advocate for thinking about statistics in my ever-expanding series is, for me, predicated on a series of related mental models for thinking about statistical inference and what we can do with statistical models. And these mental models are both graphical, and have some similarity with circuit board schematics, or more generally graphical representations of complex systems. To the extent these mental models have been useful for me, I hope they’ll be useful to others as well.\nIn the handwritten notes below I’ll try to show some of these mental models, and how they can help demystify some of the processes and opportunities involved in statistical inference"
  },
  {
    "objectID": "posts/handdrawn-stats/statistics-as-circuits/index.html#introduction",
    "href": "posts/handdrawn-stats/statistics-as-circuits/index.html#introduction",
    "title": "Statistics as circuit boards",
    "section": "",
    "text": "The general approach I advocate for thinking about statistics in my ever-expanding series is, for me, predicated on a series of related mental models for thinking about statistical inference and what we can do with statistical models. And these mental models are both graphical, and have some similarity with circuit board schematics, or more generally graphical representations of complex systems. To the extent these mental models have been useful for me, I hope they’ll be useful to others as well.\nIn the handwritten notes below I’ll try to show some of these mental models, and how they can help demystify some of the processes and opportunities involved in statistical inference"
  },
  {
    "objectID": "posts/handdrawn-stats/statistics-as-circuits/index.html#terminology",
    "href": "posts/handdrawn-stats/statistics-as-circuits/index.html#terminology",
    "title": "Statistics as circuit boards",
    "section": "Terminology",
    "text": "Terminology\n\nData for the model\nAs I recently discussed in a post relating to multivariate models, ultimately almost all models work with a big rectangle of data: each row an observation, each column a variable. We can call this big rectangle \\(D\\). Then, we need to imagine a way of splitting out this rectangle into two pieces: the model inputs (or predictor matrix), which we call \\(X\\), and the model outputs (or response matrix) which we call \\(y\\).\nTo try to represent this, I first thought about a big piece of square-lined paper, with a vertical perforation on it. Tear along this vertical perforation, and one of the pieces of paper is the output \\(y\\), and the other the input \\(X\\).\nHowever, I then realised the \\(X\\)/\\(y\\) distinction is probably clearer to express symbolically as two complementary shapes: the input \\(X\\) being basically a rectangle with a right-facing chevron, and the output \\(y\\) being a rectangle with a chevron-shaped section missing from it on the left.\n\n\n\nSymbols\n\n\n\n\nModel components\nAt a high enough level of generalisation, there are basically two component types that statistical models contain: stochastic components, denoted \\(f(.)\\), and deterministic components, denoted \\(g(.)\\). Within the figure, I’m referring to the deterministic components as transformers and the stochastic components as noisemakers. I decided to draw the transformers as triangles, and the noisemakers as ribbons."
  },
  {
    "objectID": "posts/handdrawn-stats/statistics-as-circuits/index.html#model-fittingcalibration",
    "href": "posts/handdrawn-stats/statistics-as-circuits/index.html#model-fittingcalibration",
    "title": "Statistics as circuit boards",
    "section": "Model fitting/calibration",
    "text": "Model fitting/calibration\nBoth transformers and noisemakers require specific parameters. Imagine these as a series of dials on two separate panels. The parameters for the transformers are usually referred to as \\(\\beta\\) (‘beta’), which can be either (and rarely) a single value, or a vector of values. And the parameters for the noisemakers are usually referred to as \\(\\alpha\\) (‘alpha’). There are many possible values of \\(\\beta\\) and \\(\\alpha\\) that a model can accept - many different ways the dials on the two panels can be set - and the main challenge of fitting a statistical model is to decide on the best configuration of \\(\\beta\\) and \\(\\alpha\\) to set the model to.\nAnd what does ‘best’ mean? Broadly, that the discrepancy between what comes out of the model, \\(Y\\), and the corresponding outcome values in the dataset, \\(y\\), is minimised in some way. Essentially, this discrepancy, \\(\\delta\\), is calculated with the current configuration of \\(\\beta\\) and \\(\\alpha\\), and then some kind of algorithm is applied to make a decision about how to adjust the dials. With the dials now adjusted, new model predictions \\(Y\\) are produced, leading to a new discrepancy value \\(\\delta\\). If necessary, then the calibration algorithm is applied once again, so the \\(\\alpha\\) and \\(\\beta\\) parameters adjusted, and so the parameter operationalisation loop is repeated, until some kind of condition is met defining when the parameters identified are good enough.\n\n\n\nModel fitting\n\n\nThere are a number of possible ways of arriving at the ‘best’ parameter configuration. One approach is to employ an analytical solution, such as with the least-squares or generalised least-squares methods. In these cases some kind of algebraic ‘magic’ is performed and - poof! - the parameters just drop out instantly from the solution, meaning no repeated interation. In all other cases, however, it’s likely the predict-compare-calibrate cycle will be repeated many times, either until the error is deemed small enough, or until some kind of resource-based stopping condition - such as “stop after 10,000 tries” - has been reached.\nThis iterative cyclic quality of model fitting applies regardless of whether frequentist models - using maximum likelihood estimation - or Bayesian models - employing something like Hamiltonian Monte-Carlo estimation - have been employed. Both involve trying to minimise a loss function, i.e. the error \\(\\delta\\) though updating the current best estimate of the parameter set \\(\\beta\\) and \\(\\alpha\\). 1"
  },
  {
    "objectID": "posts/handdrawn-stats/statistics-as-circuits/index.html#model-simulation",
    "href": "posts/handdrawn-stats/statistics-as-circuits/index.html#model-simulation",
    "title": "Statistics as circuit boards",
    "section": "Model simulation",
    "text": "Model simulation\n\nSimple simulation\nOnce the model \\(M\\) has been calibrated, i.e. the best possible set of parameters \\(\\beta\\) (for the transformer, \\(g(.)\\)) and \\(\\alpha\\) (for the noisemaker, \\(f(.)\\)) have been identified in the calibration set, the model can now be used for prediction, projection, interpolation, extrapolation, and simulation more generally.\nThe challenges are two-fold: knowing how to ‘ask the model questions’; and knowing how to interpret the answers the model gives.\n\n\n\nModel prediction\n\n\nTo ‘ask the model questions’, we need to specify some input data, \\(X\\), to put into the model. This input data could be taken from the same dataset used to calibrate the model in the first place. But it doesn’t have to be. We could ask the model to produce a prediction for configurations of input the model has never seen before. In post 11 of the statistics series, we asked the model to predict levels of tooth growth where the dosage was between the dosage values in the dataset; this is an example of interpolation. In the same dataset we also saw examples of extrapolation, including dosage levels that were predicted/projected to lead to negative tooth growth, i.e. impossible values. So, a key challenge in asking questions of the model, through proposing an input predictor matrix \\(X\\), is to know which questions are and are not sensible to ask.\nKnowing how to interpret the answers from the model is the second part of the challenge. If we run the model in its entirety, the values from the systematic component (transformer) are passed to the stochastic component (noisemaker), meaning we’ll get different answers each time.2 For some types of model, we can just extract the results of the transformer part alone, and so produce expected values. If we pass the values from the transformer to the noisemaker, however, we’ll end up with a distribution of values from the model, even though the calibration parameters \\(\\beta\\) and \\(\\alpha\\), and input data \\(X\\) are non-varying. So, we may need to choose a way to summarise this distribution. For example, we may want to know the proportion of occasions/draws that exceed a particular threshold value \\(\\tau\\). Or may want to calculate the median and a prediction interval.\n\n\nSimulating ‘trials’\nOnce the fundamentals of simulation using statistical model simulations are understood, it’s just a small step to producing hypothetical simulation-based ‘trials’. Just apply two different input datasets \\(X_0\\) and \\(X_1\\) to the same model \\(M\\). These two datasets should differ only in terms of a specific exposure variable of interest \\(Z\\), with all other inputs kept the same. This is equivalent to magicking up the hypothetical ‘platinum standard’3 discussed in this post on causal inference: imagine exactly the same individual being observed in two different worlds, where only one thing (exposed/not exposed, or treated/not treated) is different.\n\n\n\nSimulated ‘trial’\n\n\nWhen it comes to intepretating the outputs, the job is now to compare between the outputs generated when \\(X_0\\) is passed to \\(M\\), and when \\(X_1\\) is passed to \\(M\\). Call these outputs \\(Y_1\\) and \\(Y_0\\) respectively; our treatment or exposure effect estimate is therefore the difference: \\(Y_1 - Y_0\\).\n\n\nSimulating with honest uncertainty\nOnce you’re familiar with the last couple of steps - how to ‘ask models questions’, and how to ‘perform simulated trials’, the last challenge is how to do so honestly.\nBy honestly, I mean with appropriate acknowledgement of the effect that parameter uncertainty has on uncertainty in model outputs. We don’t really know the ‘true’ values of the parameter values \\(\\beta\\) and \\(\\alpha\\); we’ve just estimated them. And because they’re estimated, we’re not certain of their true value.\n\n\n\nSimulation with uncertainty\n\n\nSo, in order to represent the effect this parameter uncertainty has on the model outputs, there needs to be a way of generating and passing lots of ‘plausible parameter values’, \\(\\theta = \\{ \\beta, \\alpha \\}\\), to the model. This means there’s a collection, or ensemble, of parameter values for the model, \\(\\tilde{\\theta}\\), and so an ensemble of models - each with a slightly different parameter configuration - that the predictor matrix \\(X\\) goes into.\nAnd this then means that there’s an ensemble of model outputs, and so again a need to think about to summarise the distribution of outputs. Note that, because the variation in outputs comes about because of variation in the model parameters, a distribution of \\(Y\\) is generated even if the noisemakers (\\(f(.)\\)) are turned off, i.e. even if estimating for expected values rather than predicted values.\nAnd how is the ensemble of parameter values produced? There are basically three approaches:\n\nAnalytical approximate solutions using something called the Delta Method: Not discussed here\nSimulation methods involving normal approximations for frequentist-based models\nUse the converged Bayesian posterior distribution.\n\nIn the figure, the Bayesian approach is shown on the right, and the simulation approach is shown on the left. A Bayesian converged posterior distribution is a distribution of plausible parameter values after the calibration process has been run enough times. It’s ideal for doing simulation with honest uncertainty, and was discussed back in the marbles-and-jumping-beans post. The downside is Bayesian models can take longer to run, and require more specialist software and algorithms to be installed/used.\nThe simulation approach for frequentist statistics was the focus of the 12th post of the series, with its rationale developed over a few earlier posts. The basic aim of this approach is to generate an approximate analogue to the Bayesian posterior distribution, and this usually involves using a model to make inputs to feed into another model. It’s not quite models-all-the-way-down, but is a bit more meta than it may first appear!"
  },
  {
    "objectID": "posts/handdrawn-stats/statistics-as-circuits/index.html#conclusion",
    "href": "posts/handdrawn-stats/statistics-as-circuits/index.html#conclusion",
    "title": "Statistics as circuit boards",
    "section": "Conclusion",
    "text": "Conclusion\nThis post aimed to re-introduce many of the key intuitions I’ve tried to develop in my main stats post series, but with a focus on the graphical intuition and concepts involved in statistical simulation, rather than just the algebra and R code examples. I hope this provides a useful complementary set of materials for thinking about statistical inference and statistical simulation. As mentioned at the start of the post, this is largely how I tend to think about statistical modelling, and so hopefully this way of thinking is useful for others who want to use statistical models more effectively too."
  },
  {
    "objectID": "posts/handdrawn-stats/statistics-as-circuits/index.html#footnotes",
    "href": "posts/handdrawn-stats/statistics-as-circuits/index.html#footnotes",
    "title": "Statistics as circuit boards",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne crucial difference between the frequentist and Bayesian approaches is that, in the frequentist approach, a final set of parameter estimates is identified, equivalent to the dials on the panels being set a particular way and then never touched again. By contrast with the Bayesian approach the parameter set never quite stops changing, though it does tend to change less than it did at the start. The Bayesian approach is like a music producer who’s never quite satisfied with his desk, always tweaking this and that dial, though usually not by much. The technical definition is that frequentist parameter estimation converges to a point (hopefully), whereas Bayesian parameter estimation converges to a distribution (hopefully). This is what I was trying to express through the marble/jumping bean distinction back in post 13 of my main stats series. The marble finds a position of rest; the jumping bean does not.↩︎\nThough we can set a random number seed to make sure the different answers are the same each time.↩︎\nIf you’re a fan of the niche genre of sci-fi-rom-coms, you could also think of these as “sliding door moments”.↩︎"
  },
  {
    "objectID": "posts/link-breaking-spring-clean/index.html",
    "href": "posts/link-breaking-spring-clean/index.html",
    "title": "Link-breaking Spring Clean",
    "section": "",
    "text": "As frequent/occasional viewers of this site might notice, the contents and header structure of this blog have changed: Instead of a single statisics listing page, I’ve now created a ‘Statistics & Data Science’ listing button, which then expands to reveal three specific listing pages:\n\nStatistical Theory & Applications: My series on re-then-de-mystifying statistical inference, reintroducing almost all models as ‘types of’ generalised linear model, and then showing the implications of this framing for better understanding how models work and what we can do with them.\nTardy Tuesday: My growing collaborative series where, each week, my colleagues and I try to get information out of the latest TidyTuesday dataset from Posit.\nHand Drawn Statistics: Largely a series of notes I occasionally make for myself to try to think through different aspects of statistics and statistical models, hand drawn in my overpriced (but still worth it) gridded Moleskine notebook, and then scanned from my phone as a series of images (using including my looming shadow and shaky-hand camera artefacts). As I tend to think and reason in quite a graphical way this is in some ways a more intuitive expression of statistical reasoning for me than more formally written material; I hope this material is useful to others too.\n\nIf you look in the statistical theory & Applications section, you’ll also see that I’ve attempted to break up the posts into sections: A core section and an additional section, and within the core section some further subdivision into three subsections: Introduction; Likelihood and simulation theory; a complete example.\nI hope this spring clean makes these resources easier to navigate and use than before.\nUnfortunately a number of internal links broke as a result of moving the posts around. I’ve fixed quite a lot of these, but if you see any more broken links please leave a message as a comment in this post (github ID required), or contact me some other way."
  },
  {
    "objectID": "posts/threw-missiles/index.html",
    "href": "posts/threw-missiles/index.html",
    "title": "Threw missiles: a concealed tautology?",
    "section": "",
    "text": "It’s probably amongst the least important thing to reflect on in the context of the reported violence and disorder that’s occurred in the last couple of weeks, but in reports on the BBC (for example), I often hear phrases along the following lines:\n\n[X] threw missiles\n\n\nmissiles were thrown\n\nThis term ‘missile’ is something that’s long struck me as unnecessarily obscure, and an excellent example of prioritising formality over clarity. Obviously it seems unlikely that what’s meant by a missile is something produced by international arms manufacturers. It’s unlikely to be something laser guided, heat seeking, carrying an explosive payload and so on.\nNo, in this context a ‘missile’ is defined as:\n\nAn object that is thrown.\n\nWhich means the above phrases actually mean:\n\n[X] threw [objects that were thrown]\n\nOr\n\n[thrown things] were thrown\n\nWhat’s wrong with saying “things were thrown” when things were thrown? Why, instead, the convention to indirectly say “thrown things were thrown” using a term that on the face of it sounds more precise but which always seems never to convey any additional information in the context of the phrases in which it’s almost invariably used? (As if it weren’t thrown, it wouldn’t by definition be a missile.)\nAre there any contexts or cases where the term ‘objects’ could not be used in place of ‘missiles’? I can’t think of any."
  },
  {
    "objectID": "posts/brian-may-gentleman-ecoscientist/index.html",
    "href": "posts/brian-may-gentleman-ecoscientist/index.html",
    "title": "Brian May: Gentleman Ecoscientist",
    "section": "",
    "text": "Brian May: The Badgers, The Farmers and Me\nI managed to find an interesting programme on BBC iPlayer last night (not always easy): Brian May: The Badgers, the Farmers and me. Though it contains distressing scenes, I found it much more hopeful than I was expecting.\nAround fifteen or more years ago, Anne Brunner, a conservationist, appeared on Brian May’s property with a hedgehog, asking whether this and other rescued wild animals could be released on his property. This led, for May, to an interest in local ecology and animal welfare, and through this to particular concerns for the plight of tbe badger in England & Wales."
  },
  {
    "objectID": "posts/brian-may-gentleman-ecoscientist/index.html#existing-transmission-model",
    "href": "posts/brian-may-gentleman-ecoscientist/index.html#existing-transmission-model",
    "title": "Brian May: Gentleman Ecoscientist",
    "section": "Existing Transmission Model",
    "text": "Existing Transmission Model\nIn England & Wales, more so in the south than the north, Bovine TB affects cattle, and through this the livelihood of farmers who raise livestock. Farmers are both mandated to conduct a skin test on their cows every 60 days and - if the test is positive for a cow - to declare the cow a ‘reactor’ and (let’s not use euphamism) kill the animal.\nA long held belief amongst farmers and government agricultural scientists was that Bovine TB, like TB in people, is primarily a respiratory infection, and transmitted primarily through air and water.\n\n\n\n\nflowchart LR\nreactor[\"Infectious cow ('Reactor')\"]\ns[\"Susceptible cow\"]\n\nreactor --air--&gt; s\nreactor --water--&gt; s\n\n\n\n\n\n\nHowever, even once all cows that test positive for TB using the standard skin test are killed, bovine TB still, sadly, often seems to recur in farms previous afflicted. This led to the assumption that another vector of infection was responsible, wild badgers, who get close to cows, share water and air with them, and so are presumed to infect them\n\n\n\n\nflowchart LR\ncw[Cows]\nreactor[\"Infectious cow ('Reactor')\"]\nr2[\"newly infectious cow (hidden reactor)\"]\nsc[\"Susceptible cow\"]\nib[\"Infectious badger\"]\nst{\"Skin test\"}\ncw --&gt; st\nkl&gt;killed]\nst --positive--&gt;reactor --&gt; kl\nib --infects--&gt; sc\nst --negative--&gt;sc\nsc --becomes--&gt; r2\nr2 --&gt; cw"
  },
  {
    "objectID": "posts/brian-may-gentleman-ecoscientist/index.html#badger-culling",
    "href": "posts/brian-may-gentleman-ecoscientist/index.html#badger-culling",
    "title": "Brian May: Gentleman Ecoscientist",
    "section": "Badger Culling",
    "text": "Badger Culling\nBased on this transmission, it follows that culling (killing) only infectious cows isn’t enough to stop herds becoming infected. Badgers also have to be culled (killed). And because testing is expensive, and the farmers don’t own or profit from the badgers, the approach taken was to assume all badgers were potentially infected and infectious, and cull (kill) any found near farms ‘just in case’.\nThis mass culling (killing) of badgers was something that struck May as immensely inhumane and unfair, a presumption of guilt and imposition of a death sentence on an entire class of animal. In May’s words:\n\nAs a species that inhabited Britain long before humans, and who have an equal right to be here, they are now being brutally persecuted, for a crime - I believe - they didn’t commit."
  },
  {
    "objectID": "posts/brian-may-gentleman-ecoscientist/index.html#first-response-activism",
    "href": "posts/brian-may-gentleman-ecoscientist/index.html#first-response-activism",
    "title": "Brian May: Gentleman Ecoscientist",
    "section": "First Response: Activism",
    "text": "First Response: Activism\nOver a decade ago, May’s response was to petition vocally for the rights of badgers to exist, and for the government to change their policy regarding the culling of badgers. This brought him allies and attentions, but also set him in opposition to government, and to farmers, who both believed in the model of transmission outlined above. This approach engorged hearts that were already bleeding, but ultimately did little to change policy, and so the extent of badger culling in England & Wales."
  },
  {
    "objectID": "posts/brian-may-gentleman-ecoscientist/index.html#second-response-scientific-research",
    "href": "posts/brian-may-gentleman-ecoscientist/index.html#second-response-scientific-research",
    "title": "Brian May: Gentleman Ecoscientist",
    "section": "Second Response: Scientific Research",
    "text": "Second Response: Scientific Research\nSo, May changed tack. He sought to find flaws in the above disease transmission model, to become - in effect - an epidemiologist, and as part of this to work with farmers.\nMay’s hope was that a new disease transmission model could be demonstrated, one in which badgers do not constitute a predominant vector of transmission. Of course, hope is just that, and May is clear in stating that - even if he wrong about this - he would still oppose the mass culling of badgers on ethical grounds."
  },
  {
    "objectID": "posts/brian-may-gentleman-ecoscientist/index.html#approach",
    "href": "posts/brian-may-gentleman-ecoscientist/index.html#approach",
    "title": "Brian May: Gentleman Ecoscientist",
    "section": "Approach",
    "text": "Approach\nSo, what did May do? He worked with a farmer, and a vet. The vet devised an enhanced test, actually a composite of three tests: the skin test currently used, and two other - more expensive (and intrusive) - tests. The suspicion was that the skin test alone was not, from a diagnostic perspective, the Gold Standard it was assumed to be: some cows who test positive may not be infected or infectious (a false positive), and some cows who test negative may be infected or infectious (a false negative). By using enhanced testing, May sought to get closer the true Gold Standard we might hope for (especially when a positive test is a death sentence for a cow), rather than the Fool’s Gold Standard May, and the vet, were starting to suspect the Skin Test was in reality.\nAnd why does this matter? Because if there are false negatives in a tested herd, then there are hidden reactors, i.e. cows within the stock that, despite testing negative, are still infectious to other cows. This would mean that an apparently ‘clean’ herd, in which all apparently infectious cows have been removed, actually still contained some infectious animals, who could then infect the rest of the herd.\nThe difference in implication is massive: if the Skin Test were a true Gold Standard then any new infections in cows would have to be due to an exogenous cause (hence, badgers). If instead the Skin Test were a Fool’s Gold Standard, then the cause of new apparent infection could be endogenous: the cows were infecting the other cows, and no exogenous infection from badgers was needed to be assumed.\nBy performing more enhanced testing of the cows, a much higher proportion of the infectious cows (‘reactors’) were identified, and so there were far fewer reactors left in the herd to infect other cows. Sadly this of course meant more cows were killed as a result of this test in the short term, but - if the endogenous infection hypothesis were true - would mean far fewer would be killed prematurely in the longer term.\nUsing PCR tests, May and the vet were also able to successfully test for the pathogen responsible for Bovine TB, M Bovis, in various candidate transmission vectors. In particular, they were able to identify that cow faeces contains much higher levels of M Bovis than previously thought, and so that faeces was a much more potent transmission vector than previously assumed.\nBy first enhancing the tests for Bovine TB at the test farm, and then by increasing the standards of hygiene for the herd - in particular greatly reducing the scope for faecal transmission - May, the vet, and the farmer were able to stop transmission at the test farm within a decade.\nA slightly unfortunate natural experiment may have also helped their case: they were unable to acquire permission to vaccinate the local wild badger population, meaning - when tested - they still carried Bovine TB. So, even though a potentially infectious pool of M Bovis still existed in the local wild badger population, the test farm itself still did not become infected."
  },
  {
    "objectID": "posts/brian-may-gentleman-ecoscientist/index.html#changed-model",
    "href": "posts/brian-may-gentleman-ecoscientist/index.html#changed-model",
    "title": "Brian May: Gentleman Ecoscientist",
    "section": "Changed Model",
    "text": "Changed Model\nIn short, they had developed fairly strong evidence against the following transmission model:\n\n\n\n\nflowchart LR\n\nbadger --air--&gt; cow\nbadger --water--&gt; cow\n\n\n\n\n\n\nAnd for the following transimission model:\n\n\n\n\nflowchart LR\n\ncow --faeces--&gt; cow\ncow --faeces--&gt; trough\ntrough --water--&gt; cow"
  },
  {
    "objectID": "posts/brian-may-gentleman-ecoscientist/index.html#gentleman-ecoscientism",
    "href": "posts/brian-may-gentleman-ecoscientist/index.html#gentleman-ecoscientism",
    "title": "Brian May: Gentleman Ecoscientist",
    "section": "Gentleman Ecoscientism",
    "text": "Gentleman Ecoscientism\nThe epilogue of the documentary suggests that, by collecting the above evidence, running the above tests, engaging with farmers - especially skeptical and previously hostile farmers - May was successfully able to change the minds of those whose minds needed to be changed to prevent further mass cullings. This research took around a decade to produce, and by the sounds of it came at some substantial cost to May, both financially, and reputationally amongst those ‘hearts’ who had previously supported him as an activist, but were wary of his efforts to engage with those farmers that - some activists at least - saw as ‘the enemy’.\nBy the sounds of it, May’s efforts as scientist may now finally turn out to be successful, not despite but because, though motivated by heart, they spoke to mind. At the age of 77, May may now finally be successful in preventing the killing of hundreds of thousands of wild animals unnecessarily, and stopping the potential extinction of badgers in large swathes of England & Wales.\nBut of course May was only able to engage in such a long-standing effort because of three things: his passion, his scientific acumen, and his wealth.\nSo, in this sense, May’s research seems almost like a return to the Gentleman Scientists of the Victorian and previous eras, a kind of atavistic return to the inequitable, but sometimes productive, roots of scientific research more generally, in which independently wealthy eccentrics devoted decades of their otherwise unstructured lives to pursuing personal interests and obsessions, with the birth of scientific journals themselves emerging as collections of correspondence between similarly minded Gentlemen Scientists, collated by elite institutions (like the Royal Society) where such (mostly) Gentlemen could meet, fraternise, and share their passions and interests with similarly minded (and well-heeled) folk.\nI don’t know quite what to make of this from a broader socioeconomic perspective… but at least, thanks to May, the badgers’ future in England & Wales is now a bit more secure!"
  },
  {
    "objectID": "posts/wrapping-guide/index.html",
    "href": "posts/wrapping-guide/index.html",
    "title": "How to wrap presents",
    "section": "",
    "text": "Here’s the key information I’ve learned from watching too many Youtube videos on how to wrap presents over the years.\n\n\n\n\nflowchart TB\n\nstart[Presents to Wrap]\ndecision{Are they cuboid?}\nbox(Put them in a cuboid)\naction(Wrap them)\nfinish[Presents are wrapped]\n\nstart --&gt; decision\ndecision --&gt;|yes| action\ndecision --&gt;|no| box\nbox --&gt; action\naction --&gt; finish\n\n\n\n\n\n\nYou’re welcome!"
  },
  {
    "objectID": "posts/psycho-logical-arithmetic/index.html",
    "href": "posts/psycho-logical-arithmetic/index.html",
    "title": "Psycho-logical Arithmetic",
    "section": "",
    "text": "Dairy Milk PLUS Oreo\n\n\nWithin this blog I think I’ve got out of the habit of writing short posts apropos of nothing, so here’s an attempt to get back into the habit.\nSomething that’s intrigued me over the last few years, perusing the confectionary aisles at newsagents and supermarkets, is the apparent proliferation of the range of Cadbury’s Dairy Milk-branded chocolate bars. Looking through the Cadbury’s Dairy Milk website here I can see:\n\nCadbury Dairy Milk Fruit & Nut\nCadbury Dairy Milk\nCadbury Dairy Milk Caramel\nCadbury Dairy Milk Mint\nCadbury Dairy Milk Oreo\nCadbury Dairy Milk Daim\nCadbury Dairy Milk Marvellous Creations Jelly Popping Chocolate Candy Bar\nCadbury Dairy Milk Wholenut\nCadbury Dairy Milk Salted Caramel\nCadbury Dairy Milk Fruitier and Nuttier\nCadbury Dairy Milk &more Caramel Nut Crunch\nCadbury Dairy Milk &more Nutty Praline Crisp\nCadbury Dairy Milk Crunchie Bits\n\nWhat intrigues me about this is that, from a marketing perspective, the majority of these chocolate bars are marketed as:\n\nCadbury Dairy Milk, plus something extra!\n\nBut at the same time, the weights of the bars all seem to be identical, along with the price: if the standard Dairy Milk bar were 25g, for example, I don’t think the Cadbury Dairy Milk Crunchie Bits is 30g: the 25g of the standard Dairy Milk bar, plus another 5g of crunchie bits. So, what the other lines of bars offer is actually something like:\n\nCadbury Dairy Milk, minus some Cadbury Dairy Milk, plus an equal amount of something extra!\n\nI.e. it’s really a substitution, which involves both subtraction and addition.\nSo, is a substitution a good thing or a bad thing from the perspective of the customer? From a purely economic perspective, I guess the answer depends - so long as the price of the good isn’t changed - on whether what’s taken away as part of the substitution is worth more or less than what it’s replaced by.\nNow, I can’t be bothered trying to work out the materials costs of what the subtracted chocolate is being replaced with, but according to the website tradingeconomics.com, here’s how the trade price of cocoa has changed over the last decade:\n\n\n\nCocoa Price\n\n\nSo, almost regardless of what’s been added to the chocolate bars, it seems likely that it costs Cadbury’s less than the chocolate that’s been taken away from the bars!\nIn the marketing psychology book Alchemy: The Surprising Power of Ideas that Don’t Make Sense, advertising guru Rory Sutherland makes the distinction between what’s logical, and what’s psycho-logical. Of course, logically, it doesn’t make sense to conflate subtraction with addition. But psycho-logically, for Cadbury Dairy Milk, it seems to make perfect sense!"
  },
  {
    "objectID": "posts/socatic-dialogue-part-01/index.html",
    "href": "posts/socatic-dialogue-part-01/index.html",
    "title": "Nerdy Dialogues on Life and Death",
    "section": "",
    "text": "Two cats, Emu and Goose. Goose is invading Emu’s personal space slightly\nHere’s an attempt to think some more about how and why standardised rates are used to compare populations. I’m doing so via the medium of a Socratic Dialogue1."
  },
  {
    "objectID": "posts/socatic-dialogue-part-01/index.html#why-age-standardise-a-dialogue",
    "href": "posts/socatic-dialogue-part-01/index.html#why-age-standardise-a-dialogue",
    "title": "Nerdy Dialogues on Life and Death",
    "section": "Why age standardise? A dialogue",
    "text": "Why age standardise? A dialogue\nMore than twice as many deaths are reported in population A than population B\nFirst question: Does population A have about twice the population as population B?\nOkay. I’ve got the number of deaths in population A, \\(n_A\\), and the population size in population A, \\(N_A\\). So, I’ve calculated the rate \\(r_A = \\frac{n_A}{N_A}\\) for population A, and done the same for population B, \\(r_B = \\frac{n_B}{N_B}\\).\nOkay… so what’s the ratio of \\(r_A\\) to \\(r_B\\)?\nIt’s 1.4. So, the mortality rate in population A is 40% higher than in population B\nAnd what does that mean?\nPopulation A are exposed to more of something bad, or maybe less to something good, than those in population B.\nPossibly. What if I told you that the mortality rate in a care home was 40% higher than the mortality rate in a combat unit fighting on the front line? If the differences in rates is just due to differences in exposures, surely if we were to move the people in the care home into the combat unit, the differences in mortality rates between the two populations should disappear?\nThat doesn’t sound right. I think the mortality rates of the care home population would be even higher if they were in the combat unit.\nAnd what does that imply?\nDifferences in health outcomes between populations can be due to differences in the characteristics of the populations being compared, as well as differences in the exposures the two populations encounter\nExactly. And what are the main differences in characteristics between the care home population and the combat unit population likely to be?\nI’d expect the combat unit population to be much younger than the care home population. I’d also expect the combat unit population to be overwhelmingly male, whereas the care home population might be more mixed, but perhaps skewed more towards females than males.\nGood. So what does this mean for methodology?\nWe need to look to compare like-with-like when trying to work out how much of a difference in health outcome is due to differences in exposures. At the very least, we should try to compare like-with-like on age and sex, as these are very important determinants of mortality risk.\nGreat. So, instead of just a single ratio to compare between populations, we can compare a load of ratios, one for each combination of age and sex we’ve got common data for. So, the ratio of mortality rates in 25 year old females, 37 year old males, 60 year old females, 82 year old males, and so on…\nMaybe…\nIf we’ve got males and females, each by age in single year up to age 90, that means we have almost 200 such ratios to compare. Any difficulties with that?\nI guess that makes it hard to see the wood for the trees, one or two numbers is easier to convey than one hundred or two hundred.\n…\nSo I guess we need some way of summarising this further, making sure the summary measure presented is a reasonable summary of all of the like-with-like comparisons we’ve got?\nYes. What might be some ways of doing this?\nI guess we could do something like the mean or median value of these age-sex specific ratios??\nFrom first principles, that doesn’t seem like a terrible idea. However it would have some problems.\nSuch as?\nFor example, if a few ratios are based on small numbers of deaths and population counts, they could be very big or very small due to sample estimation issues alone. This would be more of an issue if using the mean than the median.\nAlso all subpopulations’ estimates would contribute equally to such a summary measure, even if some subpopulations contribute much more to the overall health outcome in the population than others.\nSo, what are some better alternatives?\nWell, I guess, for overall mortality, you could use life expectancy.\nAh, we’re all familiar with that.\nMaybe not as familiar as you think you are. It’s less straightforward to calculate and interpret than you might think. For example, imagine it’s 1890, and life expectancy is 51 years of age. You’re 31 years old. How long can you expect to live?\nTwenty more years? More if I’m careful\nProbably quite a bit longer. ‘Life expectancy’ \\(e\\) is usually used as shorthand for ‘period life expectancy from birth’, \\(e_0\\), or ‘unconditional period life expectancy’. Historically, the first year of life was one of the most dangerous ages to be alive.2 It’s a tall and weighty hurdle to cross. But as a 31 year old you’ve already crossed it. Your life expectancy isn’t ‘unconditional’ life expectancy from birth, \\(e_0\\), but ‘conditional’ life expectancy from age 31, \\(e_{31}\\).\nWhat does this mean?\nSay there were 10,000 people running the obstacle course of life, and they’re starting at the start of the course. The further you follow the course along, the fewer people reach each stage. Life expectancy at birth \\(e_0\\), heuristically, answers the question “at what stage in the course should you expect there’ll only be 5,000 of the original contestants remaining, on average?”\nAnd \\(e_{31}\\)?\nAlmost the same, except instead of the 10,000 people starting at the start of the course, they’re all allowed to start at stage 31 instead.\nAh! I think I see now why \\(e_{31}\\) should be greater than \\(e_0\\)!\nYes, and as mentioned it used to be a lot higher, because the first stage used to be one of the toughest.\nOkay. I think I understand why life expectancy isn’t a completely straightforward concept. Let’s go into the weeds even further. Why did you refer to life expectancy as period life expectancy? What’s the alternative?\nWell, let’s keep with the obstacle course analogy. Imagine two more things…\nOkay, but you’re hurting my brain.\n… Firstly, that the 10,000 contestants aren’t all the contestants. Instead, they’re just one of a series of cohorts of contestants. Every fifteen minutes, say, another 10,000 contestants are lined up at the starting block and, when their starting pistol goes, they start the course.\nSounds pretty crowded…\nYes. None of this is practically possible; we also have to assume everyone runs at the same rate for the analogy to work.\nAnyway, the second big thing to imagine is that the designers of the obstacle course are constantly redesigning it. They’re making some of the hurdles higher, and other hurdles lower, and they’re doing this all the time.\nAh, so the obstacle course is never the same for any two cohorts who traverse it?\nExactly! You’ve got it!3\nSo, it’s cohorts who traverse the course, but you said life expectancy is usually period life expectancy. What does this mean?\nA period life expectancy is like taking a snapshot, or just a few seconds-long clip, of people who are currently on the obstacle course, at all stages, and using this information to try to work out how far a cohort, starting at the start of the course, would likely get along the course, if the obstacle course never changed while the cohort is traversing the course.\nThat sounds like an important caveat, given you just said the course is always changing.\nIt certainly is. It’s for this reason a period life expectancy is sometimes also called a synthetic cohort life expectancy, because the cohort imagined to traverse the obstacle course doesn’t actually exist, but is made up of different pieces of different cohorts at different stages of the course.\nSo, why not use a real cohort?\nTwo reasons: Relevance, and data.\nGo on..\nEither you have a completed cohort, where there’s the data, but not the relevance. Or you have an incomplete cohort, where there’s the relevance, but not the data.\nI think I understand: The completed cohort completed the obstacle course, especially the start of it, when it was very different to how it is now, so although the data’s complete, much of it doesn’t speak to the current challenges on the course. And for the incomplete cohort, as they’ve not yet reached all the hurdles, we can’t yet know how many of them will reach each stage.\nCorrect, and correct. Two reasons why period life expectancies are usually used, even though they’re based on some pretty weird fictions.\nThis whole dialogue is a weird fiction. Shall we call it a night?\nIndeed we shall!"
  },
  {
    "objectID": "posts/socatic-dialogue-part-01/index.html#footnotes",
    "href": "posts/socatic-dialogue-part-01/index.html#footnotes",
    "title": "Nerdy Dialogues on Life and Death",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e. two people talking about an idea, one of whom thinks they know more than the other one, though, very annoyingly, insists they don’t.↩︎\nExisting starts with a boss fight.↩︎\nMy Socrates doesn’t intend this to sound condescending, but it still does a bit.↩︎"
  },
  {
    "objectID": "posts/edinburgh-js-talk/index.html",
    "href": "posts/edinburgh-js-talk/index.html",
    "title": "A Quarto blog post on a Quarto presentation about the Quarto blog",
    "section": "",
    "text": "I have the privilege of giving a short talk on Tuesday, at the always-excellent Edinburgh JS meetup, on using Quarto for blogging. My presentation, made in Quarto using reveal.js, is available here.\nAs part of a community talks evening, the presentation time is short: between five and ten minutes, with five as the target. A key point I want to get across is that, around a year ago, I completed one of Codeclan‘s last Professional Software Development courses (E63), in large part to address a key ’known unknown’ when it comes to developing dashboards and web content via R.\nThis known unknown: I knew that, when I make a dashboard in Shiny, or compile to html with rmarkdown, I’m telling R to generate a bundle of HTML, CSS and Javascript; and that by pressing a button to deploy I’m having this bundle sent and hosted by some server somewhere, and having it associated with a url. However, until I took the software development course, I was largely in the dark as to what the HTML, CSS and Javascript looked like in practice, and how it worked, and definitely would struggled to build something directly in HTML, CSS and JS. So, for example, if I wanted to customise a page or app so it doesn’t look so obviously ‘Shiny’, I’d have struggled, because I just had to rely on Shiny as an intermediary - writing the web bundle at a distance, rather than directly.\nWith the knowledge of the software development course, however, I was able to both make a Dash-based equivalent of my mortality database explorer app; and also to use React, with semantic HTML and CSS written from scratch to produce and style my website. 1\nHowever, I’m mindful of a quote I half remember from a documentary I saw - on the history of the internet - back in the 1990s. It’s something like, “A nerd is someone who uses a telephone just to talk to 2 people about how telephones work”. More broadly: the message needs to be more than just the medium.\nSo, there’s a balance to be struck: between too much abstraction, and too little abstraction. Too much abstraction led to the known unknown issue mentioned above: becoming too dependent on one tool to handle other tools on my behalf, and not knowing how to use these tools myself to make any finishing touches. But too little abstraction can lead to an excess focus on coalface concerns about exactly how to go about implementation, and towards a preoccupation with the minutiae, which can be at the expense of thinking deeply and clearly about exactly what you want to say.\nAnd for now, and for blogging, I think Quarto strikes a great balance. It definitely abstracts away a lot more of the mechanics of website development and deployment than, say, React. And it includes default features and styles that definitely look good enough with low effort. But on the other hand, through things like fences and more advanced frontomatter configuration, it allows quite a lot of additional customisation opportunity if needed.\nWithin this blog, I’ve aimed to focus largely on one post at a time, and when writing these posts to focus on the content. Whether writing about socioeconomic matters, or pop culture, or statistical modelling and data science (now over three hours of content), I’ve aimed to think deeply about what I’m writing, not how I’m writing it. And for this purpose, Quarto has continued to be great.\nSo, in a sense, with Quarto I’ve returned to development-at-a-distance, using one tool to instruct others to make content. But at least now I’m doing so out of choice, not necessity borne of ignorance.\nAnd on deployment: Well, you can see the presentation itself is deployed via github pages, something I don’t think I’d have attempted before the Codeclan course."
  },
  {
    "objectID": "posts/edinburgh-js-talk/index.html#footnotes",
    "href": "posts/edinburgh-js-talk/index.html#footnotes",
    "title": "A Quarto blog post on a Quarto presentation about the Quarto blog",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt took about six months after the course had finished, and poring through - in particular - Dave Gray’s excellent 11 hour tutorial on CSS, and then to build my own styling rules from scrach using a BEM format, that I finally learned not to treat CSS, and I guess styling more generally, with the usual indifference and contempt I do for most matters sartorial and sylistic!↩︎\nOr possibly ‘at’↩︎"
  },
  {
    "objectID": "posts/utterences-comments/index.html",
    "href": "posts/utterences-comments/index.html",
    "title": "New blog feature: comments",
    "section": "",
    "text": "I think I’ve managed to set up a blog comment feature for each of the posts.\nAs usual, the quarto documentation is great, though information on comments is in the HTML basics section rather than the website or blog section, so takes a bit of hunting. Albert Rapp’s very comprehensive blogpost is a great resource, covering this and much else.\nThe Quarto documentation gives three options for comments:\n\nHypothes.is, which allows comments and annotations to be provided line-by-line, a bit like non-editable tracked changes.\nutterances, which is a lightweight interface based on the discussion feature in github.\ngiscus, which seems to be built on utterances, but a bit more heavy-weight/opinionated.\n\nI’ve attempted, and think I’ve managed to implement, utterances.\nIn order to make a comment on a post, you need to have a Github username, and log in.\nFrom my end, I needed to do the following:\n\nSet up a public Github repo for blog comments. I unimaginately called this BlogComments\nInstall utterances on github and associate it with this repo\nWithin the file posts/_metadata.yml, add the following declaration\n\ncomments: \n  utterances: \n    repo:  JonMinton/BlogComments\nParameters in posts/_metadata.yml are applied to all posts within the posts subdirectory. This should mean that each post will now contain a comment box at the bottom.\nWhen a comment is added by a registered Github user, metadata from the specific post being commented on should be appended to an issue/discussion post within the JonMinton/BlogComments directory. And whenever a post is rendered, all associated discussion/issue items in the BlogComments repo should be fetched and shown at the bottom of the post.\nI’ve said should because I’ve only just set this up, and there are currently no comments.\nWhy not try to add a comment and see what happens?!"
  },
  {
    "objectID": "posts/unattended-deaths/index.html",
    "href": "posts/unattended-deaths/index.html",
    "title": "A Deathly Silence",
    "section": "",
    "text": "Trends in R98/R99 deaths since 1990\n\n\nWhat does it mean when someone dies, and no one notices for days, weeks, or months on end?\nThe bodies, once found, will be decomposed to such an extent that no effective autopsy can be performed, and so no cause of death can be identified. Such deaths are then likely to be coded either as R98 (‘Unattended death’) or R99 (‘Other ill-defined and unknown causes of mortality’). Far from being ‘junk codes’, wouldn’t a sudden and sustained change in deaths coded this way (absent an obvious explanation, such as a change in coding practice) signal that something broader is afoot?\nWorking with Lu Hiam, an Oxford PhD student and former GP, and Theodore Estrin-Serlui, a histopathologist, I analysed trends in deaths with these codes, as compared with mortality trends overall in England & Wales.\nSuch codes are rarely used, but in England & Wales they sadly became many times more common over the 1990s and 2000s. Standardised mortality rates in the R98/R99 category became more than three and a half times a common between 1990 and 2010, even as general standardised mortality rates fell by around a third.\nFor every body found so decomposed that the R98/R99 category had to be used, there are usually many more that have been unattended for a few days, have started to decompose, but for which autopsy can still be successfully performed. If these deaths are the tip of the iceberg, the base of this iceberg may be a growing epidemic of loneliness and social isolation, of ever more people with connections to friends and family, with no one to turn to in times of crisis.\nOur paper, A Deathly Silence, has been published in the Journal of the Royal Society of Medicine, and received press coverage from a number of outlets."
  },
  {
    "objectID": "posts/tolerating-intolerance-paradox/index.html",
    "href": "posts/tolerating-intolerance-paradox/index.html",
    "title": "The Paradox of Tolerating Intolerance: Position A and Position B",
    "section": "",
    "text": "Here’s something I’ve been thinking about for some time.\nIt’s something I call The Paradox of Tolerating Intolerance, and arises because there seem to be two correct, but mutually exclusive, ways of completing the following sentence:\nThe two positions:\nLet’s say there’s a conversation between Alice and Bob.\nAlice says something that expresses intolerance towards some kind of outgroup, of which neither Alice nor Bob are members. Bob does not hold the intolerant position towards this outgroup that Alice expressed.\nIf Bob objects to Alice’s intolerance, he would be acting according to Position B.\nIf Bob does not object to Alice’s intolerance, he would be acting according to Position A.\nIn not objecting, Bob would himself be acting with tolerance, which could be seen as a promoting tolerance. But at the same time the lack of objection could lead Alice to believe her intolerant views are more widely held than they are, to implicitly be seen to condone and normalise such views, meaning Alice is more likely to continue to hold and express such views in the future. So Bob’s action (or inaction) of not objecting could also be seen to promote intolerance (Position B).\nBy contrast, if Bob objects to Alice, the direct act of objecting to an intolerant view being expressed is itself an act of intolerance: Alice believed Bob was someone with whom she could speak freely and honestly. But Bob’s act of censuring Alice’s expression of her views means she may now no longer consider Bob someone about whom she can speak freely and honestly. At the same time, the fact and experience of being censured over her views may cause Alice to question her views - and her rationale for intolerance - more carefully, as well as reassess how commonly held she considers her position to be. If Bob is one of Alice’s friends, but Bob does not hold the same intolerant position that Alice holds, perhaps other friends of Alice, Bob, or both will tend not to hold that position too?\nSo, Position A seems to be something like a first order act of tolerance, because Bob is accepting of (if not supporting of) Alice’s position; but potentially also a second order act of intolerance, because it causes Alice to feel more confident and assured both in having and expressing her intolerant view.\nBy contrast Position B seems the opposite: a first order act of intolerance, because Bob explicitly opposes Alice, but potentially a second order act of tolerance, if it causes Alice to reevaluate her positions on both having and expressing an intolerant viewpoint.\nTo re-iterate: this really does seem to be a paradox. There’s no right answer. But at the same time, whichever position a person holds - Position A or Position B - that person would not really be acting with any kind of moral consistency if they were to switch positions randomly: there are probably Position A people, and Position B people.\nSo, if both positions are internally consistent, and there’s no outside factor that ‘proves’ one position is better than the other, then what determines whether someone tends towards Position A or Position B?\nPersonality is surely going to be a factor. For example, the act of openly disagreeing with someone may be easier for someone with higher disagreeability (almost by definition), lower neuroticism 1, and possibly higher conscientiousness than the converse. On the other hand, the (in)act of not disagreeing when encountering views that are both intolerant and contrary to one’s own could be due to higher openness - more curiousity about how others think and reason about the world. Or it could due to more long-term/strategic thinking about how best to engage with someone who holds intolerant positions: Instead of openly disagreeing and censuring someone immediately for saying something intolerant, it may be (from experience) more effective to initially say nothing, but then ask a series of questions that, hopefully, lead the person expressing such views to be more introspective and self-questioning about why they hold such views, which may then lead them to genuinely change their mind. This is what might be called the third order effect of adopting Position A as opposed to Position B when encountering intolerance.\nAnother complex of factors is likely to be cultural. In particular, the changing implicit norms and mores that change and shift over time (period effects), and the norms and mores that were predominant around formative periods in individuals’ cultural and attitudinal development: such as childhood and (in particular) early adulthood (i.e. cohort effects). As well as age effects (there may be views that are easier to hold while young than while old, and vice versa), period and cohort effects in cultural exposure could also be important factors in shaping whether someone tends more towards Position A or Position B, and the relative share of Position A and Position B behaviours expressed within the overall population, and population strata.\nMy own default is quite strongly towards Position A: Tolerating intolerance promotes tolerance. But anecdotally I sense there’s been a cultural shift towards Position B: Tolerating intolerance promotes intolerance. My guess is such a shift occurred around the 2010s, and has both period and cohort (generational) components to it, with those who entered adulthood over the 2010s showing perhaps the most pronounced shift towards Position B tendencies as compared with earlier generations of young adults.\nFrom my own Position A default a shift towards a Position B default has felt like something of a retrograde step towards genuinely addressing and ameliorating intolerance and prejudice. My intuition is that Position A is not only acting tolerantly from a first order perspective (I am acting with tolerance), but also from a third order perspective: allowing a place for intolerance to be expressed is giving a space for the roots, reasoning and habits of thought that gave rise to intolerant attitudes having developed, and so a space for critical enquiry along broadly Socratic lines - as to why someone believes what they believe, and what would have to change for them to believe (and act) otherwise - to take place.\nHowever, just as I believe much of the evolving default towards Position B may have been culturally influenced - the position held being a function of the salient norms of time and place - I can’t claim my own strong preference towards Position A is really derived deductively from first principles, or objectively proven to be superior, either. I’m just as much a product of nature and nurture, and the complex interplay and ongoing negotiation between the two, as anyone else. I believe and hope that acting according to my Position A instincts tends to be the better strategy when it comes to promoting tolerance, especially when considering higher order effects, but ultimately, that’s all it is: belief and hope."
  },
  {
    "objectID": "posts/tolerating-intolerance-paradox/index.html#footnotes",
    "href": "posts/tolerating-intolerance-paradox/index.html#footnotes",
    "title": "The Paradox of Tolerating Intolerance: Position A and Position B",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe opposite argument could also be made regarding this factor: someone may find an intolerant position held by someone so inherently noxious that they will be willing to do anything to avoid being exposed to it, whether that be walking out (or logging off) from a conversation with a proponent of such a postion, through to speaking out almost in instinct against it. And so the driver of speaking out could be high neuroticism, not low, where in this case (and many others) neuroticism should really be thought of as something like ‘threat sensitivity’, and for the listener the speaker’s intolerant position is genuinely perceived as an acute threat, akin to a violent act.↩︎"
  },
  {
    "objectID": "posts/talk-at-edinbr/index.html",
    "href": "posts/talk-at-edinbr/index.html",
    "title": "EdinbR talk on modelling economic (in)activity transitions",
    "section": "",
    "text": "Yesterday I had the great privilege of being one of two speakers at the Edinburgh R Users group, called EdinbR. (Difficult to say without sounding like a pirate.)\nI spoke through some of the modelling and conceptual challenges involved in trying to model the effect that various drivers/factors/exposures have on how many people in the UK become economically inactive, especially economically inactive for reasons of long-term sickness.\nThe talk seemed to go well (though perhaps the speaker’s always the last person qualified to judge), even though some of the algebra didn’t render correctly. (Which unfortunately means I also used algebra.)\nLike this blog, the presentation also made use of Quarto, but in the presentation’s case using reveal.js.\nThe presentation is available, for those intrepid souls interested in seeing something with R code and algebra, here."
  },
  {
    "objectID": "tardy-tuesday.html",
    "href": "tardy-tuesday.html",
    "title": "Tardy Tuesdays",
    "section": "",
    "text": "For the last couple of years, I’ve run an informal weekly R training session to support people who want to develop and maintain their R skills in matters of data tidying and analysis. By default the sessions are based around the weekly TidyTuesday data challenges developed to support people using R through the tidyverse paradigm promoted in R for Data Science. Each week, TidyTuesday makes a new open source dataset available; we give ourselves one hour, with no preparation, to see what we can learn from the data.\nBecause, due to scheduling conflicts, Tuesdays tend to be especially busy, the sessions actually take place first thing on Wednesday mornings. Hence, Tardy Tuesdays! The scripts developed since I’ve been using Quarto for blogging, with attendees listed as authors (generally in alphabetical order) are shown below:"
  },
  {
    "objectID": "tardy-tuesday.html#introduction",
    "href": "tardy-tuesday.html#introduction",
    "title": "Tardy Tuesdays",
    "section": "",
    "text": "For the last couple of years, I’ve run an informal weekly R training session to support people who want to develop and maintain their R skills in matters of data tidying and analysis. By default the sessions are based around the weekly TidyTuesday data challenges developed to support people using R through the tidyverse paradigm promoted in R for Data Science. Each week, TidyTuesday makes a new open source dataset available; we give ourselves one hour, with no preparation, to see what we can learn from the data.\nBecause, due to scheduling conflicts, Tuesdays tend to be especially busy, the sessions actually take place first thing on Wednesday mornings. Hence, Tardy Tuesdays! The scripts developed since I’ve been using Quarto for blogging, with attendees listed as authors (generally in alphabetical order) are shown below:"
  },
  {
    "objectID": "hand-drawn-statistics.html",
    "href": "hand-drawn-statistics.html",
    "title": "Hand-drawn Statistics",
    "section": "",
    "text": "Sometimes it can be easiest to think about complex ideas without having to think too much about the tooling to use to express and represent these ideas. Instead of having to deal with R code, mermaid uml syntax, latex equations and so on, sometimes the right tool for expressing an idea is just a pen and paper, and the right tool for presenting it to others is just take a picture with my camera phone.\nAt some point these hand drawn notes may become the basis of ‘proper’ blog posts. But for now I hope they’re legible and comprehensible enough to be useful resources to others in the mean time.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\nMay 4, 2024\n\n\nFactor analysis with ordinal variables\n\n\n\n\nMay 5, 2024\n\n\nHow factor analysis is used in testing\n\n\n\n\nMay 28, 2024\n\n\nStatistics as circuit boards\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html",
    "href": "posts/edinbr-pair-programming/index.html",
    "title": "Edinbr Pair Programming",
    "section": "",
    "text": "This is a blog post where we’ve covered a pair programming exercise completed as part of the EdinbR group.\n\nThis session [was]… led by Dr Brittany Blankinship and Dr Kasia Banas, two academics from the Data Driven Innovation for Health and Social Care Talent Team, based at the Usher Institute. Brittany and Kasia are avid R programmers and data science educators.\n\nThe link to the dataset and exercise description is here\nThere are two datasets. We are looking at one related to Madrid (and maybe why we shouldn’t go there as it’s polluted?!)"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#intro",
    "href": "posts/edinbr-pair-programming/index.html#intro",
    "title": "Edinbr Pair Programming",
    "section": "",
    "text": "This is a blog post where we’ve covered a pair programming exercise completed as part of the EdinbR group.\n\nThis session [was]… led by Dr Brittany Blankinship and Dr Kasia Banas, two academics from the Data Driven Innovation for Health and Social Care Talent Team, based at the Usher Institute. Brittany and Kasia are avid R programmers and data science educators.\n\nThe link to the dataset and exercise description is here\nThere are two datasets. We are looking at one related to Madrid (and maybe why we shouldn’t go there as it’s polluted?!)"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#packages",
    "href": "posts/edinbr-pair-programming/index.html#packages",
    "title": "Edinbr Pair Programming",
    "section": "Packages",
    "text": "Packages\nWe will be using the tidyverse set of packages:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-1",
    "href": "posts/edinbr-pair-programming/index.html#task-1",
    "title": "Edinbr Pair Programming",
    "section": "Task 1",
    "text": "Task 1\nTo begin with, load the madrid_pollution.csv data set into your R environment. Assign the data to an object called madrid.\n\ndata_url &lt;- \"https://raw.githubusercontent.com/bblankinship/EdinbRTalk-2024-02-23/main/madrid_pollution.csv\"\n\ndta &lt;- read_tsv(data_url)\n\nRows: 51864 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr   (1): mnth\ndbl  (15): BEN, CO, EBE, MXY, NMHC, NO_2, NOx, OXY, O_3, PM10, PXY, SO_2, TC...\ndttm  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndta\n\n# A tibble: 51,864 × 17\n   date                  BEN     CO   EBE   MXY   NMHC  NO_2   NOx   OXY   O_3\n   &lt;dttm&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 2001-08-01 01:00:00 1.5   0.340  1.49   4.10 0.07    56.2  75.2 2.11  42.2 \n 2 2001-08-01 02:00:00 0.870 0.0600 0.880  2.41 0.01    29.7  31.4 1.20  56.5 \n 3 2001-08-01 03:00:00 0.660 0.02   0.610  1.60 0.01    22.8  22.5 0.800 64.1 \n 4 2001-08-01 04:00:00 0.470 0.0400 0.410  1    0.02    31.6  34.8 0.470 60.8 \n 5 2001-08-01 05:00:00 0.600 0.0400 0.670  1.68 0.01    30.9  32.5 0.740 65.6 \n 6 2001-08-01 06:00:00 0.520 0.0900 0.460  1.27 0.01    66.7  78.0 0.590 41.7 \n 7 2001-08-01 07:00:00 0.540 0.120  0.510  1.33 0.01    69.7  85.3 0.630 32.6 \n 8 2001-08-01 08:00:00 0.910 0.430  0.730  1.91 0.0600  97.8 139.  0.970 17.0 \n 9 2001-08-01 09:00:00 1.62  0.75   1.29   3.40 0.120  108   177   1.65   8.36\n10 2001-08-01 10:00:00 2.85  1.65   2.60   6.89 0.210  110.  202.  3.27   8.38\n# ℹ 51,854 more rows\n# ℹ 7 more variables: PM10 &lt;dbl&gt;, PXY &lt;dbl&gt;, SO_2 &lt;dbl&gt;, TCH &lt;dbl&gt;, TOL &lt;dbl&gt;,\n#   year &lt;dbl&gt;, mnth &lt;chr&gt;\n\n\nFirst thing we realised, it’s not really a CSV."
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-2",
    "href": "posts/edinbr-pair-programming/index.html#task-2",
    "title": "Edinbr Pair Programming",
    "section": "Task 2",
    "text": "Task 2\nNow that the data is loaded in R, create a scatter plot that compares ethylbenzene (EBE) values against the date they were recorded. This graph will showcase the concentration of ethylbenzene in Madrid over time. As usual, label your axes:\n\nx = Date\ny = Ethylbenzene (μg/m³)\n\nAssign your answer to an object called EBE_pollution.\n\nEBE_pollution &lt;- dta |&gt;\n  ggplot(aes(date, EBE)) +\n  geom_point(alpha = 0.4) +\n  scale_x_datetime() +\n  scale_y_log10() +\n  labs(x = \"Date\", y = \"Ethylbenzene (μg/m³)\")\n\nWhat is your conclusion about the level of EBE over time?\nLooks like it has a seasonal pattern."
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-3",
    "href": "posts/edinbr-pair-programming/index.html#task-3",
    "title": "Edinbr Pair Programming",
    "section": "Task 3",
    "text": "Task 3\nThe question above asks you to write out code that allows visualization of all EBE recordings in the dataset - which are taken every single hour of every day. Consequently the graph consists of many points and appears densely plotted. In this question, we are going to clean up the graph and focus on max EBE readings from each month. Create a new data set with maximum EBE reading from each month in each year. Save your new data set as madrid_pollution.\n\nmadrid_pollution &lt;-\n    dta |&gt;\n        mutate(\n            mnth = month(date, label = TRUE),\n            yr   = year(date)\n        ) |&gt;\n        group_by(\n            yr, mnth\n        ) |&gt;\n        summarise(\n            max_ebe = max(EBE, na.rm = TRUE)\n        ) |&gt;\n        ungroup()\n\n`summarise()` has grouped output by 'yr'. You can override using the `.groups`\nargument.\n\nmadrid_pollution\n\n# A tibble: 72 × 3\n      yr mnth  max_ebe\n   &lt;dbl&gt; &lt;ord&gt;   &lt;dbl&gt;\n 1  2001 Jan     11.7 \n 2  2001 Feb     18.9 \n 3  2001 Mar     15.6 \n 4  2001 Apr     12.5 \n 5  2001 May     14.7 \n 6  2001 Jun     12.0 \n 7  2001 Jul     73.0 \n 8  2001 Aug      8.39\n 9  2001 Sep     10.1 \n10  2001 Oct     39.8 \n# ℹ 62 more rows"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-4",
    "href": "posts/edinbr-pair-programming/index.html#task-4",
    "title": "Edinbr Pair Programming",
    "section": "Task 4",
    "text": "Task 4\nPlot the new maximum EBE values versus the month they were recorded, split into side-by-side plots for each year.\nAssign your answer to an object called madrid_plot.\n\nmadrid_plot &lt;- madrid_pollution |&gt;\n  ggplot(aes(mnth, max_ebe)) +\n  geom_col() +\n  scale_y_log10() +\n  labs(x = \"Month\", y = \"Maximum Ethylbenzene (μg/m³)\") +\n  facet_wrap(~ yr) + \n  coord_flip()\n\nmadrid_plot"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-5",
    "href": "posts/edinbr-pair-programming/index.html#task-5",
    "title": "Edinbr Pair Programming",
    "section": "Task 5",
    "text": "Task 5\nNow we want to see which of the pollutants has decreased the most in 2001.\nAssign your answer to an object called pollution_2001.\n\ndta |&gt;\n    filter(year(date) == 2001) |&gt;\n    select(-year, -mnth) |&gt;\n    pivot_longer(-date, names_to = \"pollutant\", values_to = \"value\") |&gt;\n    group_by(pollutant) |&gt;\n    summarise(\n        max_val = max(value, na.rm = TRUE),\n        min_val = min(value, na.rm = TRUE)\n    ) |&gt;\n    mutate(diff = max_val - min_val) |&gt; \n    arrange(desc(diff))\n\n# A tibble: 14 × 4\n   pollutant max_val min_val    diff\n   &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 NOx       1416      5.13  1411.  \n 2 NO_2       271.     5.03   266.  \n 3 PM10       266.     0.820  265.  \n 4 TOL        243.     0.360  243.  \n 5 O_3        173.     4.98   168.  \n 6 SO_2       137.     0.190  137.  \n 7 PXY        103      0.150  103.  \n 8 OXY        103      0.190  103.  \n 9 MXY         93.1    0.230   92.9 \n10 EBE         77.3    0.140   77.1 \n11 BEN         49.9    0.100   49.8 \n12 CO          10.4    0.01    10.4 \n13 TCH          4.77   0.760    4.01\n14 NMHC         2.42   0        2.42\n\n\nThoughts:\nThis is an ambiguous question. We’ve chosen to answer it in a way that doesn’t make sense but is quick to solve!"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-6",
    "href": "posts/edinbr-pair-programming/index.html#task-6",
    "title": "Edinbr Pair Programming",
    "section": "Task 6",
    "text": "Task 6\nNow repeat what you did for Task 5, but filter for 2006 instead. Assign your answer to an object called pollution_2006.\n\npollution_2006 &lt;- dta |&gt;\n    filter(year(date) == 2006) |&gt;\n    select(-year, -mnth) |&gt;\n    pivot_longer(-date, names_to = \"pollutant\", values_to = \"value\") |&gt;\n    group_by(pollutant) |&gt;\n    summarise(\n        max_val = max(value, na.rm = TRUE),\n        min_val = min(value, na.rm = TRUE)\n    ) |&gt;\n    mutate(diff = max_val - min_val) |&gt; \n    arrange(desc(diff))\n\npollution_2006\n\n# A tibble: 14 × 4\n   pollutant  max_val min_val     diff\n   &lt;chr&gt;        &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1 NOx       1274      6.68   1267.   \n 2 NO_2       287.     5.45    282.   \n 3 PM10       269.     0.660   268.   \n 4 O_3        132      3.20    129.   \n 5 TOL         64.8    0.160    64.7  \n 6 SO_2        66.2    5.59     60.6  \n 7 MXY         54.9    0.260    54.6  \n 8 OXY         31.4    0.310    31.1  \n 9 PXY         27.0    0.300    26.7  \n10 EBE         20.0    0.210    19.8  \n11 BEN         16.9    0.150    16.7  \n12 CO           3.48   0.140     3.34 \n13 TCH          2.84   1.08      1.76 \n14 NMHC         0.970  0.0800    0.890"
  },
  {
    "objectID": "posts/edinbr-pair-programming/index.html#task-7",
    "href": "posts/edinbr-pair-programming/index.html#task-7",
    "title": "Edinbr Pair Programming",
    "section": "Task 7",
    "text": "Task 7\nWhich pollutant decreased by the greatest magnitude between 2001 and 2006? Come up with a programmatic solution\nWe’ll interpret this as the average for the year for each pollutant in both years.\n\ndta |&gt; \n    filter(year %in% c(2001, 2006)) |&gt;\n    select(-date, -mnth) |&gt;\n    pivot_longer(-year, names_to = \"pollutant\", values_to = \"value\") |&gt;\n    group_by(year, pollutant) |&gt;\n    summarise(mean_value = mean(value, na.rm = TRUE)) |&gt;\n    ungroup() |&gt; \n    group_by(pollutant) |&gt;\n    summarise(\n        change = mean_value[year == 2006] - mean_value[year == 2001]\n    ) |&gt;\n    ungroup() |&gt;\n    arrange(change)\n\n`summarise()` has grouped output by 'year'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 14 × 2\n   pollutant   change\n   &lt;chr&gt;        &lt;dbl&gt;\n 1 NOx       -52.4   \n 2 NO_2      -11.2   \n 3 TOL        -9.53  \n 4 SO_2       -4.18  \n 5 MXY        -3.70  \n 6 BEN        -1.76  \n 7 PXY        -1.63  \n 8 EBE        -1.59  \n 9 OXY        -1.55  \n10 CO         -0.445 \n11 TCH        -0.103 \n12 NMHC        0.0363\n13 O_3         1.66  \n14 PM10        3.14  \n\n\nNOTE: This worksheet has been adapted from Data Science: First Introduction Worksheets, by Tiffany Timbers, Trevor Campbell and Melissa Lee, available at https://worksheets.datasciencebook.ca/"
  },
  {
    "objectID": "posts/rwanda-information-warfare/index.html",
    "href": "posts/rwanda-information-warfare/index.html",
    "title": "Information Warfare: Proportion of Rwandans who are Muslims",
    "section": "",
    "text": "In trying to write up some thoughts about the recent riots and protests, I decided to try to confirm what proportion of the population of Rwanda practice Islam, as one of the few details known about the knife attack perpetrator is that his family is from Rwanda. But also that many of the riots and protests seem to be anti-Islamic and to be targetting mosques as well as migrants.\nAnd here’s where things get weird.\nWhen I look on Google for ‘religion in Rwanda’, I get some preview text for the Wikipedia page which states that Rwanda is a majority Christian nation:\n\n\n\nGoogle Search Results and Preview\n\n\nHowever, when I clicked on the Wikipedia article link a few minutes ago, whose preview text stated that the majority of the population is Christian, I found an article stating that the majority of the population is Muslim.\n\n\n\nWikipedia a few minutes ago\n\n\nI stated a few minutes ago, because when I went back to confirm just now, the article’s text had reverted back to stating Rwanda is a mainly Christian nation.\n\n\n\nWikipedia reverted\n\n\nSo, it appears there have been concerted efforts to rewrite history to be more compatible with anti-Islamic sentiment in the UK. Fortunately, Wikipedia seems to itself be quite quite to repair itself from this kind of damage, and the additional sources of information all suggested something was amiss."
  },
  {
    "objectID": "posts/circular-reasoning/index.html",
    "href": "posts/circular-reasoning/index.html",
    "title": "I got permanently banned from a politics forum for mentioning how circles work",
    "section": "",
    "text": "A month or so back, I was skimming a far-left-wing online politics forum, where a visual metaphor was presented to illustrate why it’s important that nominally left-wing parties and institutions be genuinely left-wing, and not the insidious pretenders to the left disparagingly known as ‘liberals’. Verbally, the argument was something as follows:\nSo, in this story, both the right and the liberal faux left are complicit in the horrible direction that modern society is taking. The liberals are at least as much to blame as the media, and so we hate them both.\nBut let’s first try to draw this visual metaphor, starting with just a unit circle:\nCode\nlibrary(tidyverse)\norigin &lt;- c(x= 0, y = 0)\n\nradius &lt;- 1\n\nangles &lt;- seq(0, 2*pi, length.out = 200)\n\nxpos &lt;- cos(angles)\nypos &lt;- sin(angles)\n\ndf &lt;- tibble(angle = angles, x = xpos, y = ypos)\n\ndf |&gt;\n    ggplot(aes(x=x, y = y)) + \n        geom_path(linewidth = 1) + \n        coord_equal() +\n        theme_minimal()\nNow let’s think about how to draw the teeth:\nCode\n# For a sawtooth circle, the radius will be made of two components: \n# r_main, for the inner part of the circle, which does not change\n# r_saw, the amount the circle extends beyond r_main at different angles\n\ngen_coords &lt;- function(angles, r_main = 1, r_saw_max = 1, n_teeth = 36) {\n\n    gen_saw_protrusion &lt;- function(r_saw_max, angles, n_teeth) {\n        r_saw_max  * angles %% (2*pi / n_teeth)\n    }\n\n    x &lt;- (r_main + gen_saw_protrusion(r_saw_max, angles, n_teeth) ) * cos(angles)\n    y &lt;- (r_main + gen_saw_protrusion(r_saw_max, angles, n_teeth) ) * sin(angles)\n\n    tibble(angle = angles, x = x, y = y)\n}\n\nangles &lt;- seq(0, 2*pi, length.out = 2000)\n\nsawcircle_coords &lt;- gen_coords(angles)\n\n\nsawcircle_coords |&gt;\n    ggplot(aes(x=x, y = y)) + \n        geom_path(linewidth = 1) + \n        coord_equal() +\n        theme_minimal()\nNow let’s label it:\nCode\nrw_angles &lt;- seq(20 * pi / 180, 70 * pi / 180, length.out = 100) \nrw_curve &lt;- tibble(\n    x = 1.1 * cos(rw_angles),\n    y = 1.1 * sin(rw_angles)\n)\n\nsawcircle_coords |&gt;\n    ggplot(aes(x=x, y = y)) + \n        geom_path(linewidth = 1) + \n        coord_equal() +\n        theme_minimal() + \n        annotate('rect', xmin = -1.2, xmax=0, ymin=0, ymax=1.2, fill = 'red', alpha = 0.1) + \n        annotate('rect', xmin=0, xmax = 1.2, ymin=0, ymax=1.2, fill = 'blue', alpha = 0.1) + \n        annotate('rect', xmin=0, xmax=1.2, ymin = -1.2, ymax = 0, fill = 'blue', alpha = 0.2) + \n        annotate('rect', xmin=-1.2, xmax=0, ymin=-1.2, ymax=0, fill = 'red', alpha = 0.2 ) + \n        annotate('text', x = -0.5, y = 0.25, label = \"centre left\") + \n        annotate('text', x = 0.5, y = 0.25, label = 'centre right') + \n        annotate('text', x = 0.5, y = -0.25, label = 'far right') + \n        annotate('text', x = -0.5, y = -0.25, label = 'far left') + \n        annotate('rect', xmin=-0.70, xmax=-0.95, ymin=0.70, ymax = 0.95, fill = 'red') + \n        annotate('text', x = -0.45, y = 0.58, colour = 'red', label = 'LIBERAL\\n BLOCKERS!') + \n        geomtextpath::geom_labelpath(aes(x = x, y = y), inherit.aes = FALSE, \n            data = rw_curve, \n            label = \"RW Media!\",\n            color = \"blue\", \n            arrow = arrow(ends = \"first\")\n\n        )\nYes. That’s pretty much how the image looked."
  },
  {
    "objectID": "posts/circular-reasoning/index.html#banning-offence",
    "href": "posts/circular-reasoning/index.html#banning-offence",
    "title": "I got permanently banned from a politics forum for mentioning how circles work",
    "section": "Banning offence…",
    "text": "Banning offence…\nSo, why did I get permanently banned from the forum that promoted this visual metaphor?\nWell, I tend to take metaphors very seriously. So I asked something like the following:\n\nDoesn’t this model suggest that some on the far right will become far left?\n\nWhich, if this political circle works like any other kind of circle, would seem to be the case…\n\n\nCode\nor_angles &lt;- seq(240 * pi / 180, 300 * pi / 180, length.out = 100) \nor_curve &lt;- tibble(\n    x = 0.95 * cos(or_angles),\n    y = 0.95 * sin(or_angles)\n)\n\n\nsawcircle_coords |&gt;\n    ggplot(aes(x=x, y = y)) + \n        geom_path(linewidth = 1) + \n        coord_equal() +\n        theme_minimal() + \n        annotate('rect', xmin = -1.2, xmax=0, ymin=0, ymax=1.2, fill = 'red', alpha = 0.1) + \n        annotate('rect', xmin=0, xmax = 1.2, ymin=0, ymax=1.2, fill = 'blue', alpha = 0.1) + \n        annotate('rect', xmin=0, xmax=1.2, ymin = -1.2, ymax = 0, fill = 'blue', alpha = 0.2) + \n        annotate('rect', xmin=-1.2, xmax=0, ymin=-1.2, ymax=0, fill = 'red', alpha = 0.2 ) + \n        annotate('text', x = -0.5, y = 0.25, label = \"centre left\") + \n        annotate('text', x = 0.5, y = 0.25, label = 'centre right') + \n        annotate('text', x = 0.5, y = -0.25, label = 'far right') + \n        annotate('text', x = -0.5, y = -0.25, label = 'far left') + \n        annotate('rect', xmin=-0.70, xmax=-0.95, ymin=0.70, ymax = 0.95, fill = 'red') + \n        annotate('text', x = -0.45, y = 0.58, colour = 'red', label = 'LIBERAL\\n BLOCKERS!') + \n        geomtextpath::geom_labelpath(aes(x = x, y = y), inherit.aes = FALSE, \n            data = or_curve, \n            label = \"Radicalised!\",\n            color = \"black\", \n            arrow = arrow(ends = \"first\")\n        ) + \n        geomtextpath::geom_labelpath(aes(x = x, y = y), inherit.aes = FALSE, \n            data = rw_curve, \n            label = \"RW Media!\",\n            color = \"blue\", \n            arrow = arrow(ends = \"first\")\n        ) + \n        geom_point(aes(x = x, y = y), inherit.aes= FALSE, \n        data = or_curve[1,], colour = \"darkred\", shape = 15, size = 3) + \n        geom_point(aes(x = x, y = y), inherit.aes= FALSE, \n        data = or_curve[nrow(or_curve),], colour = \"darkblue\", shape = 16, size = 3)\n\n\n\n\n\nYup. That’s how circles work.\nBut apparently mentioning this gets you banned!"
  },
  {
    "objectID": "posts/glm-series/index.html",
    "href": "posts/glm-series/index.html",
    "title": "GLMs: My first series",
    "section": "",
    "text": "I now have four fairly technical posts that form part of a series on understanding statistical modelling from a generalised linear regression (GLM) perspective. This series is far from complete, but is complete enough that it should be fairly useful to intrepid readers.\nTo make it easier to find this series, I’ve now created a page for links just to entries in this series. To see this, just look on the top left of this site, and click on ‘generalised linear models’."
  },
  {
    "objectID": "posts/anti-victories-and-velvet-mousetraps/index.html",
    "href": "posts/anti-victories-and-velvet-mousetraps/index.html",
    "title": "On Anti-victories and Velvet Mousetraps",
    "section": "",
    "text": "I was speaking with someone recently who started a new job a few months ago, after an unexpected redundancy from a job they were both good at and found engaging and meaningful.\nDespite many surface similarities between the previous job and the current one, after over half a year it’s become clear the new job does not have those same qualities that brought a sense of purpose, engagement and meaning that the previous job brought. The work environment, the organisational culture, and various other aspects of the new job are all subtly but substantially different in a way that provides, overall, a much less engaging and purposive experience of work. And from this a yearning to stop has been growing, and to look for opportunities and experiences that are altogether different: to pursue travelling, music and other artistic avenues of expression, and so on, but without necessarily a clear plan for how to make a sustainable livelihood of this. To perhaps seek not to find something more similar to the old job that brought so much purpose, but something altogether different.\nI found myself unironically using the term Anti-Victories in this conversation, a term taken from the game Cultist Simulator, which I wrote about previously. This of course led to a brief detour into the game itself, and the surprisingly deep and complex ways it attempts to simulate key aspects of human purpose and motivation. Despite its otherworldly theme, Cultist Simulator really does seem to speak to genuine aspects of the various needs that people have, and the complexity of trying to balance such needs in a sustainable way.\nAnother term that speaks to the kind of predicament sketched out above is Velvet Mousetrap, a term offered to me by someone I spoke to a few years ago when discussing my own job situation, regarding the role I left around a month ago.\nA more bloodless and technical term for the same underlying concept is local optima, the concept taken from the mechanics of statistical inference that I’ve perhaps found most philosophically meaningful and insightful, and appears a useful concept across a great many domains of life.\nTo understand what’s meant by a local optima it’s first necessary to imagine something called a fitness landscape. This is a continuous space of possibilities over which a function of that space, called its fitness, also continuously varies.\nIf this continuous space of possibilities is imagined as a single dimension, x, then the fitness landscape may look as follows:\n\n\n\nFitness Landscape\n\n\nIn this image, the fitness function \\(f(x)\\) of the search space (\\(x\\)) is the vertical axis, and labelled ‘amount of badness’. Basically, the idea is to find the position along the search space with the least amount of badness, meaning the value of x that corresponds to the lowest value of y.\nIn the image, there’s also what we can call an agent, represented by the small ball.\nThe figure shows that the fitness landscape is nonlinear: more x doesn’t always lead to better y, or vice versa. The landscape also shows two troughs, positions of x for which any small change in the value of x will lead to increases rather than decreases in y.\nCurrently the agent (the ball) is in one of these two troughs. As we are seeking to find the least amount of badness, each trough is referred to as an optima. (If we were seeking to maximise something, the optima would be the highest peaks, rather than the deepest troughs.)\nIn the figure, the two optima are labelled: the more shallow of the two troughs, where the agent currently resides, is called the local optima, whereas to the right of it we see a deeper trough, which is labelled the global optima.\nTo my mind, the concepts of both the Anti-victory and Velvet Mousetrap are very similar: a realisation of being in a local optima. The Anti-victory, however, seems more fatalistic: a sense of resignation and deep about this realisation. What I’m doing now: it’s fine. It’s not great, but it’s not terrible either; it could definitely be worse. So let’s stick with what I know. I’m okay with that… I really am.\nFrom the agent’s perspective (and all too often, we are the agents), there can be two fundamental challenges in reaching the global optima:\n\nWe cannot see the entire landscape;\nEven if we can see a preferable optima to the one we are closest to, we might not have the energy to reach it.\n\nThere’s a simpler term for this: settling. (A related psychological concept: satisficing. Though this relates more to transient decisions, such as which brand of toothpaste to buy, rather than more permanent states.) There are advantages and disadvantages to being and becoming settled.\nIf we take our little green agent in the figure, we can see the dilemma: almost all of the journey from the local optima, where the agent currently resides, to global optima, where they might hope to be, is worse (has more badness) than where they are now. Additionally, for much of this journey they will be working against, rather than with, gravity. If they cannot sustain the ascent to and beyond the worst point in the path from local to global, they will just roll back to where they started.\nTo my mind, that’s why people settle."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "First Post",
    "section": "",
    "text": "Hi, this is my first blog post. I’m making this using Quarto, starting off by slavishly following the tutorial, then incrementally adapting it to suit my preferences.\n\nI’m even keeping the default image of the first blog post. It’s not dissimilar to what I’m actually looking at!"
  },
  {
    "objectID": "posts/x-minute-neighbourhoods/index.html",
    "href": "posts/x-minute-neighbourhoods/index.html",
    "title": "Why such pushback against 20 minute neighbourhoods?",
    "section": "",
    "text": "I had the privilege yesterday of hearing a series of talks by researchers at the University of Glasgow on 20 minute neighbourhoods.1 The talks covered areas like evidence surveys of health associations, GIS methods,2 and engagement with historical amenities.\nI was vaguely aware that this kind of initiative is sometimes conflated with (ultra) low emission zones, and in recent years sometimes receives a hostile response from some audiences. So, in the Q&A, I asked if this had been their experience, what they think the causes of the hositility were, and what (if anything) is best to do about it.\nThe researchers had encountered such responses, and the coordinator sent a link to a youtube video introducing the research they were involved in. Only around 2% of those who viewed the video decided to comment on it, but it surprised me that the vast majority did express the kind of hostility I was thinking about. The top few comments are indicative:\nOnly by the fifth ranked comment is there a response broadly supportive of the initiative, though is more lamenting than hopeful:\nOn the possible reasons for such responses, the researchers suggests that COVID may be a factor. On what to do about it, there was less clarity, except to be mindful that many people may not change their mind on such issues, so engaging with them might not be worth the time involved.\nThe COVID explanation definitely seems part of it, and is evident in some of the examples above. 2020 and 2021 was a confusing time, and the popularity of conspiracy theories which offered ‘answers’ seems to have grown as a result. Within the conspiracy theory linking Lockdown to ULEZ and walkable neighbourhoods, Lockdown was a dress rehearsal, an attempt to understand just how pliant and willing to give up on hard earned freedoms the populace at large would be when told such restrictions were necessary and temporary. Initiatives like Walkable Neighbourhoods are then framed as something like ‘the next phase’, initiatives which curtail freedom on a permanent rather than temporary basis, with the ultimate endpoint being something like ‘prison cities’, where everyone is controlled and monitored at all times in some kind of Orwellian nightmare.\nClearly, there seems to be a lot of imputation and extrapolation involved in getting from ‘being able to walk to school while passing some nice buildings’ to 1984. But perhaps having a preexisting set of assumptions, which link driving to freedom and so walking to tyranny, is something that makes people more susceptible to the conspiratorial way of thinking outlined above. Let’s consider this some more.\nIn surveys of household affluence from decades gone by, my understanding3 is that some surveys used to ask UK adults questions along the lines of: “How many cooked meals with meat did you eat in the last week?” The idea of such questions was that, if people could afford to eat more meat, they would do. Such questions were considered unobtrusive measures of individual and household means, because the individual wants, to eat as much meat as one could afford to do so, was simply taken as given.\nA few years ago Gapminder generalised something like this principle to international development, providing simple but graphic illustrations of how what people eat, drink, and use as transport varies across four very broad income levels. I’ve made this illustration the main image for this blog post.\nIf we look at income level 1, under $2 a day, people are obligate walkers, and they’re likely to be obligate vegans, relying on a simple grain to survive. As they reach higher levels, they start to be able to afford to augment their simple stable dish with vegetables, spices and meat. And they start to move from walking, to being able to afford a bicycle, then a motorcycle, then finally a car. The changing transport mode is presented as what people move onto when they can afford to do so, with each form presenting new found physical freedoms to go along with the new found financial freedoms their higher income level now affords them.\nMy suspicion is that many people who adopted the kind of conspiracy theory sketched above, which leads to the kind of hostile comments to the kind of walkability initiatives being discussed, did so because they internalised something like the Gapminder model of development both too deeply and too crudely. In particular, they conflate driving with money and freedom, and so not driving with poverty and restriction. I suspect it’s easier to subscribe to the car=freedom equation when you have direct experience of not being able to afford to own or run a car, of driving being a genuine hard-won freedom. In social epi parlance, I suspect there’s likely to be a socioeconomic gradient in hostility to walkability initiatives, as for many poorer people the idea of not being allowed to do something you can only just afford to do (and want to do in large part because you can only just afford to do), would seem inherently perverse.\nPersonally, as a city-dwelling vegetarian, my intuitions are all in support of Walkability initiatives. I’m just trying to be mindful of how those with different circumstances may look at the same things, but see something very different!"
  },
  {
    "objectID": "posts/x-minute-neighbourhoods/index.html#footnotes",
    "href": "posts/x-minute-neighbourhoods/index.html#footnotes",
    "title": "Why such pushback against 20 minute neighbourhoods?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis generalises to ‘x minute neighbourhoods’, e.g. 15 minute neighbourhoods, 30 minute neighbourhoods, 10 minute neighbourhoods.↩︎\nIn practice this seemed to be the identification of whether and how many common amenities are within 800 metres of someone’s home.↩︎\nPlease correct me if I’m wrong, or provide an example or two if I’m not. I wasn’t able to find an example dataset before, so might be confabulating this!↩︎"
  },
  {
    "objectID": "posts/changing-tenure-in-scotland/index.html",
    "href": "posts/changing-tenure-in-scotland/index.html",
    "title": "Changing tenure in Scotland",
    "section": "",
    "text": "As it’s something I’ve done already for work, but it’s all using public domain data, here’s a link to some charts showing how housing tenure in Scotland has changed over time. As Social Housing stock went down, private rental stock went up."
  },
  {
    "objectID": "posts/stats-website/index.html",
    "href": "posts/stats-website/index.html",
    "title": "Statistics Website",
    "section": "",
    "text": "I’ve now taken a lot of my statistics blog material and turned it into a website, available here.\nBasically, think of this blog as a ‘staging area’ for statistics material in development, and the stats website as a place where this material is deployed once entire ‘courses’ on particular topics have been completed, rather than in development.\nFeedback on typos, errors and omissions very welcome!"
  },
  {
    "objectID": "posts/economic-inactivity-modelling-package-readme/index.html",
    "href": "posts/economic-inactivity-modelling-package-readme/index.html",
    "title": "My Economic Inactivity Modelling Package: Informative Readme File!",
    "section": "",
    "text": "A few weeks ago, as I wasn’t using any personal or sensitive data, I decided to make the main repository where I keep my economic inactivity modelling work public, meaning in theory anyone could take a look.\nHowever (much like this blog), I didn’t tell anyone about it.\nThe repo is a bit of a mess, but it works. It’s both an R package, containing various convenience functions and lookup files, and a series of notebooks, presentations and now draft papers which make use of such functions and files through quarto. In due course, it may be a good idea to separate the package side of things from the ‘working’ repo which makes use of the package. Any suggestions how best to do this are welcome.\nThe main thing I’ve changed recently is the readme.md file. The economic inactivity project makes extensive use of Understanding Society, in order to populate the models with information on transitions from one wave to the next between the seven mutually exclusive economic inactivity states. Now, the readme.md contains information about how and where to add the relevant Understanding Society1 dataset to a local clone of the repo in order to try out the functions and package.\nTo reiterate, caveat emptor, the repo is what it is. But if you’re interested in taking a look, creating your own fork of it, cloning it, and adding the requisite data, then it’s available from this link"
  },
  {
    "objectID": "posts/economic-inactivity-modelling-package-readme/index.html#footnotes",
    "href": "posts/economic-inactivity-modelling-package-readme/index.html#footnotes",
    "title": "My Economic Inactivity Modelling Package: Informative Readme File!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe particular version of the dataset I plumped for includes British Household Panel Survey data as well, the predecessor to Understanding Society, which began in 1991. So there’s the potential to use the package to explore changes in transition probabilities and drivers thereof between states for a much longer period than I’m using to calibrate the model and explore the trends.↩︎"
  },
  {
    "objectID": "posts/handdrawn-stats/factor-analysis-ordinal-variables-analogue/index.html",
    "href": "posts/handdrawn-stats/factor-analysis-ordinal-variables-analogue/index.html",
    "title": "Factor analysis with ordinal variables",
    "section": "",
    "text": "For all of the software and tooling available on a computer, sometimes the best way to start thinking and describing complex ideas is with a notepad and pen. No need to think through which graphics software to use: it’s a pen. No need to think about typography and formatting: it’s a pen! Undo buttons? A strike through with a pen. And what if it looks crap? Doesn’t matter. With a pen (and my handwriting) it’s guaranteed to look crap. Just don’t worry about it. Just focus on getting the idea out, from brain to pen to paper.\nRecently I’ve been thinking about an application of factor analysis with ordinal variables. With R’s lavaan package, for instance, if the manifest items (rectangles) which go into determining a latent factor (ovals) are ordinal factors, then a method known as polychoric correlation is used. This allows a relaxation of the assumption made implicitly when using ordinal variables, such as those using a Likert scale, which is that each response level is an equal distance apart from the adjacent levels. i.e. something like:\n\nEqual distance treatment of Likert scale\n\n\nResponse Category\nValue\n\n\n\n\nStrongly Disagree\n-2\n\n\nDisagree\n-1\n\n\nNeither Disagree nor Agree\n0\n\n\nAgree\n1\n\n\nStrongly Agree\n2\n\n\n\nInstead, with enough good indicators for a latent factor, and enough data, the equal distance assumption can be dropped.\nHere’s my handwritten introduction to how:\n\n\n\nPage 1\n\n\n\n\n\nPage 2\n\n\n\n\n\nPage 3\n\n\n\n\n\nPage 4\n\n\n\n\n\nPage 5\n\n\n\n\n\nPage 6"
  },
  {
    "objectID": "posts/handdrawn-stats/how-factor-analysis-is-used-in-testing/index.html",
    "href": "posts/handdrawn-stats/how-factor-analysis-is-used-in-testing/index.html",
    "title": "How factor analysis is used in testing",
    "section": "",
    "text": "I mentioned in some links to my previous post on factor analysis with ordinal variables that there’s a link between the approach detailed there, and the approach used to construct standardised aptitude and knowledge tests. In some ways the testing example is even simpler than the introductory example. Here are my notes about how it seems to work:1"
  },
  {
    "objectID": "posts/handdrawn-stats/how-factor-analysis-is-used-in-testing/index.html#footnotes",
    "href": "posts/handdrawn-stats/how-factor-analysis-is-used-in-testing/index.html#footnotes",
    "title": "How factor analysis is used in testing",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf anyone has any direct experience of building or working with such tests, and wants to correct or add further nuance to my notes here, please do! I’m just reasoning from first principles here, so expect there are some additional complexities in practice.↩︎"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-21/index.html",
    "href": "posts/glms/time-series/lms-are-glms-part-21/index.html",
    "title": "Part Twenty One: Time Series: The Moving Average Model",
    "section": "",
    "text": "In the last couple of posts, we looked first at autoregression (the AR(p) model), then integration (represented by the term d), as part of a more general strategy for modelling time series data. In this post, we’ll complete the trilogy, by looking at the Moving Average (MA(q)) model."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-21/index.html#recap",
    "href": "posts/glms/time-series/lms-are-glms-part-21/index.html#recap",
    "title": "Part Twenty One: Time Series: The Moving Average Model",
    "section": "",
    "text": "In the last couple of posts, we looked first at autoregression (the AR(p) model), then integration (represented by the term d), as part of a more general strategy for modelling time series data. In this post, we’ll complete the trilogy, by looking at the Moving Average (MA(q)) model."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-21/index.html#the-moving-average-model-in-context-of-arima",
    "href": "posts/glms/time-series/lms-are-glms-part-21/index.html#the-moving-average-model-in-context-of-arima",
    "title": "Part Twenty One: Time Series: The Moving Average Model",
    "section": "The Moving Average Model in context of ARIMA",
    "text": "The Moving Average Model in context of ARIMA\nAs with the post on autoregression, it’s worth returning to what I’ve been calling The Mother Model (a general way of thinking about statistical models), and how AR and I relate to it:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nTo which we might also imagine adding a transformation or preprocessing step, \\(h(.)\\) on the dataset \\(D\\)\n\\[\nZ = h(D)\n\\]\nThe process of differencing the data is the transformation step employed most commonly in time series modelling. This changes the types of values that go into both the data input slot of the mother model, \\(X\\), and the output slot of the mother model, \\(Y\\). The type of data transformer, however, is deterministic, hence the use of the \\(=\\) symbol. This means an inverse transform function, \\(h^{-1}(.)\\) can be applied to the transformed data to convert it back to the original data:\n\\[\nD = h^{-1}(Z)\n\\]\nThe process of integrating the differences, including a series of forecast values from the time series models, constitutes this inverse transform function in the context of time series modelling, as we saw in the last post, on integration.\nAutoregression is a technique for working with either the untransformed data, \\(D\\), or the transformed data \\(Z\\), which operates on the systematic component of the mother model. For example, an AR(3) autoregressive model, working on data which have been differenced once (\\(Z_t = Y_t - Y_{t-1}\\)), may look as follows:\n\\[\nZ_t \\sim N(\\mu, \\sigma^2)\n\\] \\[\n\\mu = \\beta_0 + \\beta_1 Z_{t-1} + \\beta_2 Z_{t-2} + \\beta_3 Z_{t-3}\n\\]\nWhich more commonly will look like something like:\n\\[\nZ_t =  \\mu + \\beta_1 Z_{t-1} + \\beta_2 Z_{t-2} + \\beta_3 Z_{t-3} + \\epsilon\n\\]\nNote that these two approaches, AR and I, have involved operating on the systematic component and the preprocessing step, respectively. This gives us a clue about how the Moving Average (MA) modelling strategy is fundamentally different. Whereas AR models work on the systematic component (\\(g(.)\\)), MA models work on the stochastic component (\\(f(.)\\)). The following table summarises the distinct roles each technique plays in the general time series modelling strategy:\n\nAR, I, and MA in the context of ‘the Mother Model’\n\n\n\n\n\n\n\nTechnique\nWorks on…\nARIMA letter shorthand\n\n\n\n\nIntegration (I)\nData Preprocessing \\(h(.)\\)\nd\n\n\nAutoregression (AR)\nSystematic Component \\(g(.)\\)\np\n\n\nMoving Average (MA)\nStochastic Component \\(f(.)\\)\nq\n\n\n\nIn the above, I’ve spoiled the ending! The Autoregressive (AR), Integration (I), and Moving Average (MA) strategies are commonly combined into a single model framework, called ARIMA. ARIMA is a framework for specifying a family of models, rather than a single model, which differ by the amount of differencing (d), or autoregression terms (p), or moving average terms (q) which the model contains.\nAlthough in the table above, I’ve listed integration/differencing first, as it’s the data preprocessing step, the more conventional way of specifying an ARIMA model is in the order indicated in the acronym:\n\nAR: p\nI: d\nMA: q\n\nThis means ARIMA models are usually specified with a three value shorthand ARIMA(p, d, q). For example:\n\nARIMA(1, 1, 0): AR(1) with I(1) and MA(0)\nARIMA(0, 2, 2): AR(0) with I(2) and MA(2)\nARIMA(1, 1, 1): AR(1) with I(1) and MA(1)\n\nEach of these models is fundamentally different. But each is a type of ARIMA model.\nWith this broader context, about how MA models fit into the broader ARIMA framework, let’s now look at the Moving Average model:"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-21/index.html#the-sound-of-a-moving-average",
    "href": "posts/glms/time-series/lms-are-glms-part-21/index.html#the-sound-of-a-moving-average",
    "title": "Part Twenty One: Time Series: The Moving Average Model",
    "section": "The sound of a Moving Average",
    "text": "The sound of a Moving Average\nThe intuition of a MA(q) model is in some ways easier to develop by starting not with the model equations, but with the following image:\n\n\n\nTibetan Singing Bowl\n\n\nThis is a Tibetan Singing Bowl, available from all good stockists (and Amazon), whose product description includes:\n\nErgonomic Design: The 3 inch singing bowl comes with a wooden striker and hand-sewn cushion which flawlessly fits in your hand. The portability of this bowl makes it possible to carry it everywhere you go.\nHolistic Healing : Our singing bowls inherently produces a deep tone with rich quality. The resonance emanated from the bowl revitalizes and rejuvenates all the body, mind and spirit. It acts as a holistic healing tool for overall well-being.\n\nNotwithstanding the claims about health benefits and ergonomics, the bowl is something meant to be hit by a wooden striker, and once hit makes a sound. This sound sustains over time (‘sings’), but as time goes on the intensity decays. As a sound wave, this might look something like the following:\n\n\nCode\nlibrary(tidyverse)\n\nA0 = 5\ndecay_rate = 1/12\nperiodicity &lt;- 5\ndelay &lt;- 7\n\ntibble(\n    t = seq(0, 100, by = 0.001)\n) |&gt;\n    mutate(\n        H_t = ifelse(t &lt; delay, 0, 1),\n        A_t = A0 * exp(-(decay_rate * (t - delay))),\n        c_t = cos((1/periodicity) * 2 * pi * (t - delay)),\n        f_t = H_t * A_t * c_t\n    ) |&gt;\n    ggplot(aes(t, f_t)) + \n    geom_line() + \n    labs(\n        y = \"f(t)\",\n        x = \"t\",\n        title = \"Intensity over time\"\n    ) + \n    geom_vline(xintercept = delay, linetype = \"dashed\", colour = \"red\")\n\n\n\n\n\nIn this figure, the red dashed line indicates when the wooden striker strikes the bowl. Before this time, the bowl makes no sound. Directly after the strike, the bowl is loudest, and over time the intensity of the sound waves emanating from the bowl decays. The striker can to some extent determine the maximum amplitude of the bowl, whereas it’s largely likely to be the properties of the bowl itself which determines how quickly or slowly the sound decays over time.\nHow does this relate to the Moving Average model? Well, if we look at the Interpretation section of the moving average model wikipedia page, we see the cryptic statement “The moving-average model is essentially a finite impulse response filter applied to white noise”. And if we then delve into the finite impulse response page we get the definition “a finite impulse response (FIR) filter is a filter whose impulse response (or response to any finite length input) is of finite duration, because it settles to zero in finite time”. Finally, if we go one level deeper into the wikirabbit hole, and enter the impulse response page, we get the following definition:\n\n[The] impulse response, or impulse response function (IRF), of a dynamic system is its output when presented with a brief input signal, called an impulse.\n\nIn the singing bowl example, the striker is the impulse, and the ‘singing’ of the bowl is its response."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-21/index.html#the-moving-average-equation",
    "href": "posts/glms/time-series/lms-are-glms-part-21/index.html#the-moving-average-equation",
    "title": "Part Twenty One: Time Series: The Moving Average Model",
    "section": "The moving average equation",
    "text": "The moving average equation\nNow, finally, let’s look at the general equation for a moving average model:\n\\[\nX_t = \\mu + \\sum_{i=1}^{q} \\theta_i \\epsilon_{t-i} + \\epsilon_t\n\\]\nFor a MA(q=2) model, for example, this would look like:\n\\[\nX_t = \\mu + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2}\n\\]\nHere is something like the fundamental value or quality of the time series system. For the singing bowl, \\(\\mu = 0\\), but it can take any value. \\(\\epsilon_t\\) is intended to capture something like a ‘shock’ or impulse now which would cause its manifested value to differ from its fundamental, \\(\\epsilon_{t-1}\\) a ‘shock’ or impulse one time unit ago, and \\(\\epsilon_{t-2}\\) a ‘shock’ or impulse two time units ago.\nThe values \\(\\theta_i\\) are similar to the way the intensity of the singing bowl’s sound decays over time. They are intended to represent how much influence past ‘shocks’, from various recent points in history, have on the present value manifested. Larger values of \\(\\theta\\) indicate past ‘shocks’ that have larger influence on the present value, and smaller \\(\\theta\\) values less influence."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-21/index.html#autoregression-and-moving-average-the-long-and-the-short-of-it",
    "href": "posts/glms/time-series/lms-are-glms-part-21/index.html#autoregression-and-moving-average-the-long-and-the-short-of-it",
    "title": "Part Twenty One: Time Series: The Moving Average Model",
    "section": "Autoregression and moving average: the long and the short of it",
    "text": "Autoregression and moving average: the long and the short of it\nAutoregressive and moving average models are intended to be complementary in their function in describing a time series system: whereas Autoregressive models allow for long term influence of history, which can change the fundamentals of the system, Moving Average models are intended to represent transient, short term disturbances to the system. For an AR model, the system evolves in response to its past states, and so to itself. For a MA model, the system, fundamentally, never changes. It’s just constantly being ‘shocked’ by external events."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-21/index.html#concluding-remarks",
    "href": "posts/glms/time-series/lms-are-glms-part-21/index.html#concluding-remarks",
    "title": "Part Twenty One: Time Series: The Moving Average Model",
    "section": "Concluding remarks",
    "text": "Concluding remarks\nSo, that’s the basic intuition and idea of the Moving Average model. A system fundamentally never changes, but external things keep ‘happening’ to it, meaning it’s almost always different to its true, fundamental value. A boat at sea will fundamentally have a height of sea level. But locally sea level is always changing, waves from every direction, and various intensities, buffetting the boat up and down - above and below the fundamental sea level average at every moment.\nAnd the ‘sound’ of a moving average model is almost invariably likely to be less sonorous than that of a singing bowl. Instead of a neat sine wave, each shock is a random draw from a noisemaker (typically the Normal distribution). In this sense a more accurate analogy might not be a singing bowl, but a guitar amplifier, with a constant hum, but also with dodgy connections, constantly getting moved and adjusted, with each adjustment causing a short belt of white noise to be emanated from the speaker. A moving average model is noise layered upon noise."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-22/index.html",
    "href": "posts/glms/time-series/lms-are-glms-part-22/index.html",
    "title": "Part Twenty Two: Time Series - ARIMA in practice",
    "section": "",
    "text": "The last three posts have covered three of the main techniques - autoregression, integration, and moving average modelling - which combine to form the ARIMA model framework for time series analysis.\nThe purpose of this post is to look at an example (or maybe two) showing how ARIMA models are fit and employed in practice."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-22/index.html#recap-and-purpose-of-this-post",
    "href": "posts/glms/time-series/lms-are-glms-part-22/index.html#recap-and-purpose-of-this-post",
    "title": "Part Twenty Two: Time Series - ARIMA in practice",
    "section": "",
    "text": "The last three posts have covered three of the main techniques - autoregression, integration, and moving average modelling - which combine to form the ARIMA model framework for time series analysis.\nThe purpose of this post is to look at an example (or maybe two) showing how ARIMA models are fit and employed in practice."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-22/index.html#setup",
    "href": "posts/glms/time-series/lms-are-glms-part-22/index.html#setup",
    "title": "Part Twenty Two: Time Series - ARIMA in practice",
    "section": "Setup",
    "text": "Setup\nFor this post I’ll make use of R’s forecast package.\n\n\nCode\nlibrary(tidyverse)\nlibrary(forecast)"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-22/index.html#dataset-used-airmiles",
    "href": "posts/glms/time-series/lms-are-glms-part-22/index.html#dataset-used-airmiles",
    "title": "Part Twenty Two: Time Series - ARIMA in practice",
    "section": "Dataset used: Airmiles",
    "text": "Dataset used: Airmiles\nThe dataset I’m going to use is airmiles, an example dataset from the datasets package, which is included in most R sessions by default\n\n\nCode\nairmiles\n\n\nTime Series:\nStart = 1937 \nEnd = 1960 \nFrequency = 1 \n [1]   412   480   683  1052  1385  1418  1634  2178  3362  5948  6109  5981\n[13]  6753  8003 10566 12528 14760 16769 19819 22362 25340 25343 29269 30514\n\n\nThe first thing we notice with this dataset is that it is not in the kind of tabular format we may be used to. Let’s see what class the dataset is:\n\n\nCode\nclass(airmiles)\n\n\n[1] \"ts\"\n\n\nThe dataset is of class ts, which stands for time series. A ts data object is basically a numeric vector with various additional pieces of metadata attached. We can see these metadata fields are start date, end date, and frequency. The documentation for ts indicates that if frequency is 1, then the data are annual. As the series are at fixed intervals, with the start date and frequency specified, along with the length of the numeric vector, the time period associated with each value in the series can be inferred.1\n\nVisual inspection of airmiles\nWe can look at the data using the base graphics plot function:\n\n\nCode\nplot(airmiles)\n\n\n\n\n\nWe can see this dataset is far from stationary, being much higher towards the end of the series than at the start. This implies we should consider differencing the data to make it stationery. We can use the diff() function for this:\n\n\nCode\nplot(diff(airmiles))\n\n\n\n\n\nThis differenced series still doesn’t look like IID data. Remember that differencing is just one of many kinds of transformation (data pre-processing) we could consider. Also we can difference more than once.\nAs there cannot be negative airmiles, and the series looks exponential since the start of the series, we can can consider using a log transform:\n\n\nCode\nplot(log(airmiles))\n\n\n\n\n\nHere the data look closer to a straight line. Differencing the data now should help us get to something closer to stationary:\n\n\nCode\nplot(diff(log(airmiles)))\n\n\n\n\n\nMaybe we should also look at differencing the data twice:\n\n\nCode\nplot(diff(log(airmiles), differences = 2))\n\n\n\n\n\nMaybe this is closer to the kind of stationary series that ARIMA works best with?\n\n\nARIMA fitting for airmiles\nThe visual inspection above suggested the dataset definitely needs at least one differencing term applied to it, and might need two; and might also benefit from being pre-transformed by being logged. With the forecast package, we can pass the series to the auto.arima() function, which will use an algorithm to attempt to identify the best combination of p, d and q terms to use. We can start by asking auto.arima() to determine the best ARIMA specification if the only transformation allowed is that of differencing the data, setting the trace argument to TRUE to learn more about which model specifications the algorithm has considered:\n\n\nCode\nbest_arima_nolambda &lt;- auto.arima(\n    y = airmiles, \n    trace = TRUE\n)\n\n\n\n ARIMA(2,2,2)                    : Inf\n ARIMA(0,2,0)                    : 384.231\n ARIMA(1,2,0)                    : 375.735\n ARIMA(0,2,1)                    : 375.3\n ARIMA(1,2,1)                    : 376.9756\n ARIMA(0,2,2)                    : 377.1793\n ARIMA(1,2,2)                    : Inf\n\n Best model: ARIMA(0,2,1)                    \n\n\nCode\nsummary(best_arima_nolambda)\n\n\nSeries: airmiles \nARIMA(0,2,1) \n\nCoefficients:\n          ma1\n      -0.7031\ns.e.   0.1273\n\nsigma^2 = 1234546:  log likelihood = -185.33\nAIC=374.67   AICc=375.3   BIC=376.85\n\nTraining set error measures:\n                   ME    RMSE      MAE      MPE     MAPE      MASE       ACF1\nTraining set 268.7263 1039.34 758.5374 4.777142 10.02628 0.5746874 -0.2848601\n\n\nWe can see from the trace that a range of ARIMA specifications were considered, starting with the ARIMA(2,2,2). The selection algorithm used is detailed here, and employs a variation of AIC, called ‘corrected AIC’ or AICc, in order to compare the model specifications.\nThe algorithm arrives at ARIMA(0, 2, 1) as the preferred specification. That is: no autorgression (p=0), twice differenced (d=2), and with one moving average term (MA=1).\nThe Forecasting book linked to above also has a recommended modelling procedure for ARIMA specifications, and cautions that the auto.arima() function only performs part of this proceudure. In particular, it recommends looking at the residuals\n\n\nCode\ncheckresiduals(best_arima_nolambda)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,2,1)\nQ* = 4.7529, df = 4, p-value = 0.3136\n\nModel df: 1.   Total lags used: 5\n\n\nThe three plots show the model residuals as a function of time (top), the distribution of residuals (bottom right), and the auto-correlation function, ACF (bottom-left), which indicates how the errors at different lags are correlated with each other. It also returns a test score, where high P-values (substantially above 0.05) should be considered evidence that the residuals appear like white noise, and so (something like) no further substantial systematic information in the data exists to be represented in the model.\nIn this case, the test statistic p-value is 0.31, which should be reassuring as to the appropriateness of the model specification identified.\nFinally, we can use this model to forecast a given number of periods ahead. Let’s take this data to the 1990s, even though this is a dangerously long projection.\n\n\nCode\nbest_arima_nolambda |&gt; forecast(h=35) |&gt; autoplot()\n\n\n\n\n\nThe central projection (dark blue line) is almost linear, but the projection intervals are wide and growing, and include projection scenarios where the number of flights in the 1990s are somewhat lower than those in the 1960s. These wide intervals should be considered a feature rather than a bug with the approach, as the further into the future we project, the more uncertain we should become.\n\n\nARIMA modelling with an additional transformation.\nAnother option to consider within the auto.arima() function is to allow another parameter to be estimated. This is known as the lambda parameter and represents an additional possible transformation of the data before the differencing step. This lambda parameter is used as part of a Box-Cox Transformation, intended to stabilise the variance of the series. If the lambda parameter is 0, then this becomes equivalent to logging the data. We can allow auto.arima to select a Box-Cox Transformation by setting the parameter lambda = \"auto\"\n\n\nCode\nbest_arima_lambda &lt;- auto.arima(\n    y = airmiles, \n    trace = TRUE,\n    lambda = \"auto\"\n)\n\n\n\n ARIMA(2,1,2) with drift         : Inf\n ARIMA(0,1,0) with drift         : 190.0459\n ARIMA(1,1,0) with drift         : 192.1875\n ARIMA(0,1,1) with drift         : 192.1483\n ARIMA(0,1,0)                    : 212.0759\n ARIMA(1,1,1) with drift         : 195.1062\n\n Best model: ARIMA(0,1,0) with drift         \n\n\nCode\nsummary(best_arima_lambda)\n\n\nSeries: airmiles \nARIMA(0,1,0) with drift \nBox Cox transformation: lambda= 0.5375432 \n\nCoefficients:\n        drift\n      18.7614\ns.e.   2.8427\n\nsigma^2 = 194.3:  log likelihood = -92.72\nAIC=189.45   AICc=190.05   BIC=191.72\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE      ACF1\nTraining set 123.1317 934.6956 724.6794 -5.484572 12.91378 0.5490357 -0.169863\n\n\nIn this case, a lambda value of about 0.54 has been identified, and a different ARIMA model specification selected. This specification is listed as ARIMA(0,1,0) with drift. This with drift term means the series are recognised as non-stationary, but where (after transformation) there is an average (in this case) constant amount upwards drift in the values as we progress through the series. 2 Let’s check the residuals for this model:\n\n\nCode\ncheckresiduals(best_arima_lambda)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0) with drift\nQ* = 3.9064, df = 5, p-value = 0.563\n\nModel df: 0.   Total lags used: 5\n\n\nThe test P-value is even higher in this case, suggesting the remaining residuals appear to behave even more like white noise than in the previous specification.\nNow to look at projections from the model into the 1990s:\n\n\nCode\nbest_arima_lambda |&gt; forecast(h=35) |&gt; autoplot()\n\n\n\n\n\nUsing this specification we get a qualitatively different long-term projection with, on the identity scale of the data itself, a much narrower long-term projection interval.\n\n\nComparing model specifications\nSo, the two different ARIMA specifications arrived at - one with additional pre-transformation of the data before differencing; the other without - lead to qualitatively different long-term projections. Do we have any reason to presume one specification is better than the other?\nI guess we could look at the AIC and BIC of the two models:\n\n\nCode\nAIC(best_arima_nolambda, best_arima_lambda)\n\n\n                    df      AIC\nbest_arima_nolambda  2 374.6684\nbest_arima_lambda    2 189.4459\n\n\nCode\nBIC(best_arima_nolambda, best_arima_lambda)\n\n\n                    df      BIC\nbest_arima_nolambda  2 376.8505\nbest_arima_lambda    2 191.7169\n\n\nHere the lower scores for the model with a Box-Cox transformation suggest it should be preferred. However, as both functions warn, the number of observations differ between the two specifications. This is likely because the no-lambda version differences the data twice, whereas the with-lambda specification differences the data once, and so the no-lambda version should have one fewer observation. Let’s check this:\n\n\nCode\nn_obs_nolambda &lt;- summary(best_arima_nolambda)$nobs\n\nn_obs_lambda &lt;- summary(best_arima_lambda)$nobs\n\nprint(paste(\"Observations for no lambda:\", n_obs_nolambda))\n\n\n[1] \"Observations for no lambda: 22\"\n\n\nCode\nprint(paste(\"Observations for with-lambda:\", n_obs_lambda))\n\n\n[1] \"Observations for with-lambda: 23\"\n\n\nYes. This seems to be the cause of the difference.\nAnother way of comparing the models is by using the accuracy() function, which reports a range of accuracy measures:\n\n\nCode\nprint(\"No lambda specification: \")\n\n\n[1] \"No lambda specification: \"\n\n\nCode\naccuracy(best_arima_nolambda)\n\n\n                   ME    RMSE      MAE      MPE     MAPE      MASE       ACF1\nTraining set 268.7263 1039.34 758.5374 4.777142 10.02628 0.5746874 -0.2848601\n\n\nCode\nprint(\"With-lambda specification: \")\n\n\n[1] \"With-lambda specification: \"\n\n\nCode\naccuracy(best_arima_lambda)\n\n\n                   ME     RMSE      MAE       MPE     MAPE      MASE      ACF1\nTraining set 123.1317 934.6956 724.6794 -5.484572 12.91378 0.5490357 -0.169863\n\n\nWhat’s returned by accuracy() comprises one row (labelled Training set) and seven columns, each for a different accuracy metric. A common (and relatively easy-to-understand) accuracy measure is RMSE, which stands for (square) root mean squared error. According to this measure, the Box-Cox transformed ARIMA model outperforms the untransformed (by double-differenced) ARIMA model, so perhaps it should be preferred.\nHowever, as the act of transforming the data in effect changes (by design) the units of the data, perhaps RMSE is not appropriate to use for comparison. Instead, there is a measure called MAPE, which stands for “mean absolute percentage error”, that might be more appropriate to use because of the differences in scales. According to this measure, the Box-Cox transformed specification has a higher error score than the no-lambda specification (around 13% instead of around 10%), suggesting instead the no-lambda specification should be preferred instead.\nSo what to do? Once again, the ‘solution’ is probably just to employ some degree of informed subjective judgement, along with a lot of epistemic humility. The measures above can help inform our modelling decisions, but they cannot make these decisions for us."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-22/index.html#discussion-and-coming-up",
    "href": "posts/glms/time-series/lms-are-glms-part-22/index.html#discussion-and-coming-up",
    "title": "Part Twenty Two: Time Series - ARIMA in practice",
    "section": "Discussion and coming up",
    "text": "Discussion and coming up\nFor the first three posts in this time-series miniseries, we looked mainly at the theory of the three components of the ARIMA time series modelling approach. This is the first approach where we’ve used ARIMA in practice. Hopefully you got a sense of two different things:\n\nThat because of packages like forecast, getting R to produce and forecast from an ARIMA model is relatively quick and straightforward to do in practice.\nThat even in this brief applied example of applied time series, we started to learn about a range of concepts - such as the auto-correlation function (ACF), the Box-Cox transformation, and alternative measures of accuracy - which were not mentioned in the previous three posts on ARIMA.\n\nIndeed, if you review the main book associated with the forecasting package, you can see that ARIMA comprises just a small part of the overall time series toolkit. There’s a lot more that can be covered, including some methods that are simpler to ARIMA, some methods (in particular SARIMA) which are further extensions of ARIMA, some methods that are alternatives to ARIMA, and some methods that are decidedly more complicated than ARIMA. By focusing on the theory of ARIMA in the last three posts, I’ve aimed to cover something in the middle-ground of the overall toolbox.\nComing up: to be determined!"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-22/index.html#footnotes",
    "href": "posts/glms/time-series/lms-are-glms-part-22/index.html#footnotes",
    "title": "Part Twenty Two: Time Series - ARIMA in practice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe information appears to be ‘overdeterimined’, as one of the metadata fields should be inferrable given the other pieces of information. I suspect this works as something like a ‘checksum’ test, to ensure the data are as intended.↩︎\nAutoregressive terms p can be included as part of non-stationary series, and there can be an arbitrary number of differencing operations d, but the moving average term q is only suitable for stationary series. So, for example, ARIMA(1,0,0) with drift can be possible, as can ARIMA(1,1,0) with drift, but ARIMA(0,0, 1) with drift is not a legitimate specification.↩︎"
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-24/index.html",
    "href": "posts/glms/time-series/lms-are-glms-part-24/index.html",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "",
    "text": "So far in this short series on time series, we’ve looked at time series modelling from some first principles, learning how the types of data and challenge in time series analysis both are similar and different from those of statistical modelling more generally. We started by looking at the concept of auto-regression, then differentiation and integration, and then the moving average model specification, before combining these three components - AR, I, and MA - to produce the ARIMA model specification common in time series analysis. Afterwards, we then extended the ARIMA specification slightly to deal with seasonally varying data, the ARIMA specification begetting the Seasonal ARIMA modelling framework, or SARIMA. As part of the post on Seasonality, we also looked at time series decomposition, using the STL decomposition framework."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-24/index.html#time-series-recap",
    "href": "posts/glms/time-series/lms-are-glms-part-24/index.html#time-series-recap",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "",
    "text": "So far in this short series on time series, we’ve looked at time series modelling from some first principles, learning how the types of data and challenge in time series analysis both are similar and different from those of statistical modelling more generally. We started by looking at the concept of auto-regression, then differentiation and integration, and then the moving average model specification, before combining these three components - AR, I, and MA - to produce the ARIMA model specification common in time series analysis. Afterwards, we then extended the ARIMA specification slightly to deal with seasonally varying data, the ARIMA specification begetting the Seasonal ARIMA modelling framework, or SARIMA. As part of the post on Seasonality, we also looked at time series decomposition, using the STL decomposition framework."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-24/index.html#aim-of-this-post",
    "href": "posts/glms/time-series/lms-are-glms-part-24/index.html#aim-of-this-post",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "Aim of this post",
    "text": "Aim of this post\nIn this post, we’ll take time series in a different direction, to show an application of multivariate regression common in time series, called vector autoregression (VAR). VAR is both simpler in some ways, and more complex in other ways, than SARIMA modelling. It’s simpler in that, as the name suggests, moving average (MA) terms tend to not be part of VAR models; we’ll also not be considering seasonality either. But it’s more complicated in the sense that we are jointly modelling two outcomes at the same time."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-24/index.html#model-family-tree",
    "href": "posts/glms/time-series/lms-are-glms-part-24/index.html#model-family-tree",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "Model family tree",
    "text": "Model family tree\nThe following figure aims to show the family resemblances between model specifications and challenges:\n\n\n\n\nflowchart TB \n    uvm(univariate models)\n    mvm(multivariate models)\n\n    ar(\"AR(p)\")\n    i(\"I(d)\")\n    ma(\"MA(q)\")\n    arima(\"ARIMA(p, d, q)\")\n    sarima(\"ARIMA(p, d, q)[P, D, Q]_s\")\n    var(VAR)\n\n    ar --&gt; var\n    mvm --&gt; var\n\n    i -.-&gt; var\n\n    uvm -- autoregression --&gt; ar\n    uvm -- differencing --&gt; i\n    uvm -- moving average --&gt; ma\n    ar & i & ma --&gt; arima\n\n    arima -- seasonality --&gt;  sarima\n\n\n\n\n\n\nSo, the VAR model is an extension of the autoregressive component of a standard, univariate AR(p) specification models to multivariate models. It can also include both predictor and response variables that are differenced, hence the the dashed line from I(d) to VAR."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-24/index.html#so-what-is-a-multivariate-model",
    "href": "posts/glms/time-series/lms-are-glms-part-24/index.html#so-what-is-a-multivariate-model",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "So what is a multivariate model?",
    "text": "So what is a multivariate model?\nYou might have seen the term multivariate model before, and think you’re familiar with what it means.\nIn particular, you might have been taught that whereas a univariate regression model looks something like this:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]\nA multivariate regression model looks more like this:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon\n\\]\ni.e. You might have been taught that, if the predictors include one term for the intercept (the \\(\\beta_0\\) term) and one term for the slope (the \\(\\beta_1\\) term), then this is a univariate model. But if there are two or more terms that can claim to be ‘the slope’ then this is a multivariate model.\nHowever, this isn’t the real distinction between a univariate model and a multivariate model. To see this distinction we have to return, for the umpeenth time, to the ‘grandmother model’ specification first introduced at the start of the very first post:\nStochastic Component\n\\[\nY \\sim f(\\theta, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta = g(X, \\beta)\n\\]\nNow, both the response data, \\(Y\\), and the predictor data, \\(X\\), are both taken from the same rectangular dataset, \\(D\\). Let’s say this dataset, \\(D\\), has six rows and five columns. As a matrix it would look something like this:\n\\[\nD =\n\\begin{pmatrix}\nd_{1,1} & d_{1,2} & d_{1,3} & d_{1, 4} & d_{1,5} \\\\\nd_{2,1} & d_{2,2} & d_{2,3} & d_{2, 4} & d_{2,5} \\\\\nd_{3,1} & d_{3,2} & d_{3,3} & d_{3, 4} & d_{3,5} \\\\\nd_{4,1} & d_{4,2} & d_{4,3} & d_{4, 4} & d_{4,5} \\\\\nd_{5,1} & d_{5,2} & d_{5,3} & d_{5, 4} & d_{5,5} \\\\\nd_{6,1} & d_{6,2} & d_{6,3} & d_{6, 4} & d_{6,5}\n\\end{pmatrix}\n\\]\nHere the dataset \\(D\\) is made up of a whole series of elements \\(d_{i,j}\\), where the first subset value indicates the row number \\(i\\) and the second subset value indicates the column number \\(j\\). So, for example, \\(d_{5, 2}\\) indicates the value of the 5th row and 2nd column, whereas \\(d_{2, 5}\\) indicates the value of the 2nd row and 5th column.\nFundamentally, the first challenge in building a model is deciding which columns from \\(D\\) we put in the predictor matrix \\(X\\), and which parts we put into the response matrix \\(Y\\). For example, if we wanted to predict the third column \\(j=3\\) given the fifth column \\(j=5\\) our predictor and response matrices would look as follows:\n\n\n\\[\nY = \\begin{pmatrix}\nd_{1,3} \\\\\nd_{2,3} \\\\\nd_{3,3} \\\\\nd_{4,3} \\\\\nd_{5,3} \\\\\nd_{6,3}  \n\\end{pmatrix}\n\\]\n\n\\[\nX = \\begin{pmatrix}\n1 & d_{1,5} \\\\\n1 & d_{2,5} \\\\\n1 & d_{3,5} \\\\\n1 & d_{4,5} \\\\\n1 & d_{5,5} \\\\\n1 & d_{6,5}  \n\\end{pmatrix}\n\\]\n\n\nWhere does the column of 1s come from? This is how we specify, in matrix notation, that we want an intercept term to be calculated. Models don’t have to have intercept terms, but in almost all cases we’re likely to be familiar with, they tend to.\nLet’s say we now want to include two columns, 2 and 5, from \\(D\\) in the predictor matrix, leading to what’s commonly (and wrongly) called a ‘multivariate regression’. This means that \\(Y\\) stays the same, but X is now as follows:\n\\[\nX = \\begin{pmatrix}\n1 & d_{1,2}  & d_{1,5}\\\\\n1 & d_{2,2}  & d_{2,5}\\\\\n1 & d_{3,2}  & d_{3,5}\\\\\n1 & d_{4,2}  & d_{4,5}\\\\\n1 & d_{5,2}  & d_{5,5}\\\\\n1 & d_{6,2}  & d_{6,5}\n\\end{pmatrix}\n\\]\nNo matter now many columns we include in the predictor matrix, X, however, we still don’t have a real multivariate regression model specification. Even if X had a hundred columns, or a thousand, it would still not be a multivariate regression in the more technical sense of the term.\nInstead, here’s an example of a multivariate regression model:\n\n\n\\[\nY = \\begin{pmatrix}\nd_{1,1} & d_{1,3} \\\\\nd_{2,1} & d_{2,3} \\\\\nd_{3,1} & d_{3,3} \\\\\nd_{4,1} & d_{4,3} \\\\\nd_{5,1} & d_{5,3} \\\\\nd_{6,1} & d_{6,3}  \n\\end{pmatrix}\n\\]\n\n\\[\nX = \\begin{pmatrix}\n1 & d_{1,5} \\\\\n1 & d_{2,5} \\\\\n1 & d_{3,5} \\\\\n1 & d_{4,5} \\\\\n1 & d_{5,5} \\\\\n1 & d_{6,5}  \n\\end{pmatrix}\n\\]\n\n\nThis is an example of a multivariate regression model. We encountered it before when we used the multivariate normal distribution in post 13, and when we draw from the posterior distribution of Bayesian models in post 14, but this is the first time we’ve considered multivariate modelling in the context of trying to represent something we suspect to be true about the world, rather than our uncertainty about the world. And it’s the first example of multivariate regression we’ve encountered in this series. For every previous model, no matter how apparently disparate, complicated or exotic they may appear, they’ve been univariate regression models in the sense that the response component \\(Y\\) has always only contained one column only.\nSo, with this definition of multivariate regression, let’s now look at VAR as a particular application of multivariate regression used in time series."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-24/index.html#vector-autoregression",
    "href": "posts/glms/time-series/lms-are-glms-part-24/index.html#vector-autoregression",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "Vector Autoregression",
    "text": "Vector Autoregression\nLet’s start with a semi-technical definition:\n\nIn vector autoregression (VAR) the values of two or more outcomes, \\(\\{Y_1(T), Y_2(T)\\}\\), are predicted based on previous values of those same outcomes \\(\\{Y_1(T-k), Y_2(T-k)\\}\\), for various lag periods \\(k\\).\n\nWhere \\(Y\\) has two columns, and an AR(1) specification (i.e. k is just 1), how is this different from simply running two separate AR(1) regression models, one for \\(Y_1\\), and the other for \\(Y_2\\)?\nWell, graphically, two separate AR(1) models proposes the following paths of influence:\n\n\n\n\nflowchart LR\nY1_T[\"Y1(T)\"]\nY2_T[\"Y2(T)\"]\n\nY1_T1[\"Y1(T-1)\"]\nY2_T1[\"Y2(T-1)\"]\n\nY1_T1 --&gt; Y1_T\nY2_T1 --&gt; Y2_T\n\n\n\n\n\nBy contrast, the paths implied and allowed in the corresponding VAR(1) model look more like the following:\n\n\n\n\nflowchart LR\nY1_T[\"Y1(T)\"]\nY2_T[\"Y2(T)\"]\n\nY1_T1[\"Y1(T-1)\"]\nY2_T1[\"Y2(T-1)\"]\n\nY1_T1 & Y2_T1 --&gt; Y1_T & Y2_T\n\n\n\n\n\nSo, each of the two outcomes at time T is influenced both by its own previous value, but also by the previous value of the other outcome. This other outcome influence is what is represented in the figure above by the diagonal lines: from Y2(T-1) to Y1(T), and from Y1(T-1) to Y2(T).\nExpressed verbally, if we imagine two entities - self and other - tracked through time, self is influenced both by self’s history, but also by other’s history too."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-24/index.html#example-and-application-in-r",
    "href": "posts/glms/time-series/lms-are-glms-part-24/index.html#example-and-application-in-r",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "Example and application in R",
    "text": "Example and application in R\nIn a more substantivelly focused post, I discussed how I suspect economic growth and longevity growth trends are correlated. What I proposed doesn’t exactly lend itself to the simplest kind of VAR(1) model specification, because I suggested a longer lag between the influence of economic growth on longevity growth, and a change in the fundamentals of growth in both cases. However, as an example of VAR I will ignore these complexities, and use the data I prepared for that post:\n\n\nCode\nlibrary(tidyverse)\n\ngdp_growth_pct_series &lt;- read_csv(\"../../../still-the-economy/both_series.csv\") \n\ngdp_growth_pct_series\n\n\n# A tibble: 147 × 5\n    ...1  year series            pct_change period\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt; &lt;chr&gt; \n 1     1  1949 1. Per Capita GDP     NA     Old   \n 2     2  1950 1. Per Capita GDP      2.20  Old   \n 3     3  1951 1. Per Capita GDP      2.92  Old   \n 4     4  1952 1. Per Capita GDP      1.36  Old   \n 5     5  1953 1. Per Capita GDP      5.07  Old   \n 6     6  1954 1. Per Capita GDP      3.87  Old   \n 7     7  1955 1. Per Capita GDP      3.55  Old   \n 8     8  1956 1. Per Capita GDP      1.28  Old   \n 9     9  1957 1. Per Capita GDP      1.49  Old   \n10    10  1958 1. Per Capita GDP      0.815 Old   \n# ℹ 137 more rows\n\n\nWe need to do a certain amount of reformatting to bring this into a useful format:\n\n\nCode\nwide_ts_series &lt;- \ngdp_growth_pct_series |&gt;\n    select(-c(`...1`, period)) |&gt;\n    mutate(\n        short_series = case_when(\n            series == \"1. Per Capita GDP\" ~ 'gdp',\n            series == \"2. Life Expectancy at Birth\" ~ 'e0',\n            TRUE ~ NA_character_\n        )\n    ) |&gt;\n    select(-series) |&gt;\n    pivot_wider(names_from = short_series, values_from = pct_change) |&gt;\n    arrange(year) |&gt;\n    mutate(\n        lag_gdp = lag(gdp),\n        lag_e0 = lag(e0)\n    ) %&gt;%\n    filter(complete.cases(.))\n\n\nwide_ts_series\n\n\n# A tibble: 71 × 5\n    year   gdp      e0 lag_gdp  lag_e0\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1  1951 2.92  -0.568    2.20   0.763 \n 2  1952 1.36   1.89     2.92  -0.568 \n 3  1953 5.07   0.388    1.36   1.89  \n 4  1954 3.87   0.530    5.07   0.388 \n 5  1955 3.55  -0.0570   3.87   0.530 \n 6  1956 1.28   0.385    3.55  -0.0570\n 7  1957 1.49   0.170    1.28   0.385 \n 8  1958 0.815  0.255    1.49   0.170 \n 9  1959 3.70   0.184    0.815  0.255 \n10  1960 5.72   0.268    3.70   0.184 \n# ℹ 61 more rows\n\n\nSo, we can map columns to parts of the VAR specification as follows:\n\nY1: gdp\nY2: e0 (life expectancy at birth)\nperiod T: gdp and e0\nperiod T-1: lag_gdp and lag_e0\n\nTo include two or more variables as the response part, \\(Y\\), of a linear model we can use the cbind() function to combine more than one variable to the left hand side of the linear regression formula for lm or glm:\n\n\nCode\nvar_model &lt;- lm(\n    cbind(gdp, e0) ~ lag_gdp + lag_e0,\n    data = wide_ts_series\n)\n\nvar_model\n\n\n\nCall:\nlm(formula = cbind(gdp, e0) ~ lag_gdp + lag_e0, data = wide_ts_series)\n\nCoefficients:\n             gdp       e0      \n(Intercept)   1.99440   0.28108\nlag_gdp       0.06173   0.01179\nlag_e0       -0.48525  -0.33063\n\n\nWe can see here that the model reports a small matrix of coefficients: three rows (one for each coefficient term) and two columns: one for each of the response variables. This is as we should expect.\nBack in part 12 of the series, we saw we could extract the coefficients, variance-covariance matrix, and error terms of a linear regression model using the functions coefficients, vcov, and sigma respectively.1 Let’s use those functions here too:\nFirst the coefficients\n\n\nCode\ncoefficients(var_model)\n\n\n                    gdp          e0\n(Intercept)  1.99439587  0.28108028\nlag_gdp      0.06172721  0.01178781\nlag_e0      -0.48524808 -0.33062640\n\n\nAnd now the variance-covariance matrix:\n\n\nCode\nvcov(var_model)\n\n\n                gdp:(Intercept)   gdp:lag_gdp    gdp:lag_e0 e0:(Intercept)\ngdp:(Intercept)    0.1804533467 -0.0272190933 -0.1129646084   0.0039007790\ngdp:lag_gdp       -0.0272190933  0.0164590434 -0.0180517467  -0.0005883829\ngdp:lag_e0        -0.1129646084 -0.0180517467  0.6290771233  -0.0024419053\ne0:(Intercept)     0.0039007790 -0.0005883829 -0.0024419053   0.0037904364\ne0:lag_gdp        -0.0005883829  0.0003557878 -0.0003902165  -0.0005717391\ne0:lag_e0         -0.0024419053 -0.0003902165  0.0135984779  -0.0023728303\n                   e0:lag_gdp     e0:lag_e0\ngdp:(Intercept) -0.0005883829 -0.0024419053\ngdp:lag_gdp      0.0003557878 -0.0003902165\ngdp:lag_e0      -0.0003902165  0.0135984779\ne0:(Intercept)  -0.0005717391 -0.0023728303\ne0:lag_gdp       0.0003457235 -0.0003791783\ne0:lag_e0       -0.0003791783  0.0132138133\n\n\nAnd finally the error terms\n\n\nCode\nsigma(var_model)\n\n\n      gdp        e0 \n2.6906051 0.3899529 \n\n\nThe coefficients returns the same kind of 3x2 matrix we saw previously: two models run simultaneously. The error terms is now a vector of length 2: one for each of these models. The variance-covariance matrix is a square matrix of dimension 6: i.e. 6 rows and 6 columns. This is the number of predictor coefficients in each model (the number of columns of \\(X\\), i.e. 3) times the number of models simultaneously run, i.e. 2.\n\\(6^2\\) is 36, which is the number of elements in the variance-covariance matrix of this VAR model. By contrast, if we had run two independent models - one for gdp and the other for e0 - we would have two 3x3 variance-variance matrices, producing a total of 18 2 terms. This should provide some reassurance that, when we run a multivariate regression model of two outcomes, we’re not just doing the equivalent of running separate regression models for each outcome, but in slightly fewer lines.\nNow, let’s look at the model summary:\n\n\nCode\nsummary(var_model)\n\n\nResponse gdp :\n\nCall:\nlm(formula = gdp ~ lag_gdp + lag_e0, data = wide_ts_series)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.679  -1.061   0.143   1.483   6.478 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.99440    0.42480   4.695 1.34e-05 ***\nlag_gdp      0.06173    0.12829   0.481    0.632    \nlag_e0      -0.48525    0.79314  -0.612    0.543    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.691 on 68 degrees of freedom\nMultiple R-squared:  0.007555,  Adjusted R-squared:  -0.02163 \nF-statistic: 0.2588 on 2 and 68 DF,  p-value: 0.7727\n\n\nResponse e0 :\n\nCall:\nlm(formula = e0 ~ lag_gdp + lag_e0, data = wide_ts_series)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.45680 -0.19230 -0.00385  0.24379  1.38706 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.28108    0.06157   4.565 2.15e-05 ***\nlag_gdp      0.01179    0.01859   0.634  0.52823    \nlag_e0      -0.33063    0.11495  -2.876  0.00537 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.39 on 68 degrees of freedom\nMultiple R-squared:  0.1086,    Adjusted R-squared:  0.08243 \nF-statistic: 4.144 on 2 and 68 DF,  p-value: 0.02003\n\n\nThe summary is now reported for each of the two outcomes: first gdp, then e0.\nRemember that the outcome is percentage annual change in the outcome of interest from the previous year. i.e. both series have already been ‘differenced’ to produce approximately stationary series. It also means that the intercept terms are especially important, as they indicate the long-term trends observed in each series.\nIn this case the intercepts for both series are positive and statistically significant: over the long term, GDP has grown on average around 2% each year, and life expectancy by around 0.28%. As the post this relates to makes clear, however, these long-term trends may no longer apply.\nOf the four lag (AR(1)) terms in the model(s), three are not statistically significant; not even close. The exception is the lag_e0 term for the e0 response model, which is statistically significant and negative. Its coefficient is also of similar magnitude to the intercept too.\nWhat does this mean in practice? In effect, that annual mortality improvement trends have a tendency to oscillate: a better-than-average year tends to be followed by a worse-than-average year, and a worse-than-average year to be followed by a better-than-average year, in both cases at higher-than-chance rates.\nWhat could be the cause of this oscillatory phenomenon? When it comes to longevity, the phenomenon is somewhat well understood (though perhaps not widely enough), and referred to as either ‘forward mortality displacement’ or, more chillingly, ‘harvesting’. This outcome likely comes about because, if there were an exceptionally bad year in terms of (say) influenza mortality, the most frail and vulnerable are likely to be those who die disproportionately from this additional mortality event. This means that the ‘stock’ of people remaining the following year have been selected, on average, to be slightly less frail and vulnerable than those who started the previous year. Similarly, an exceptionally ‘good’ year can mean that the average ‘stock’ of the population in the following year is slightly more frail than in an average year, so more susceptible to mortality. And so, by this means, comparatively-bad-years tend to be followed by comparatively-good-years, and comparatively-good-years by comparatively-bad-years.\nThough this general process is not pleasant to think about or reason through, statistical signals such as the negative AR(1) coefficient identified here tend to keep appearing, whether we are looking for them or not."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-24/index.html#conclusion",
    "href": "posts/glms/time-series/lms-are-glms-part-24/index.html#conclusion",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post we’ve both concluded the time series subseries, and returned to and expanded on a few posts earlier in the series. This includes the very first post, where we were first introduced to the grandmother formulae, the posts on statistical modelling using both frequentist and Bayesian methods, and a substantive post linking life expectancy with economic growth.\nAlthough we’ve now first encountered multivariate regression models in the context of time series, they are a much more general phenomenon. Pretty much any type of model we can think of and apply in a univariate fashion - where \\(Y\\) has just a single column - can conceivably be expanded to two mor more columns, leading to their more complicated multiple regression variants."
  },
  {
    "objectID": "posts/glms/time-series/lms-are-glms-part-24/index.html#footnotes",
    "href": "posts/glms/time-series/lms-are-glms-part-24/index.html#footnotes",
    "title": "Part Twenty Four: Time series - Vector Autoregression and multivariate models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA primary aim extracting these components from a linear regression in this way is to allow something approximating a Bayesian posterior distribution of coefficients to be generated, using a multivariate normal distribution (the first place we actually encountered a multivariate regression), without using a Bayesian modelling approach. This allows for the estimating and propagation of ‘honest uncertainty’ in predicted and expected outcomes. However, as we saw in part 13, it can sometimes be as or more straightforward to just use a Bayesian modelling approach.↩︎\ni.e. two times three squared.↩︎"
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "",
    "text": "The first four posts in this series formed ‘section one’, in which the focus was on framing the challenge of fitting (almost) all statistical models from the perspective of likelihood theory; and how to go beyond ‘star-gazing’ (just look at tables of coefficients), and using statistical models to make predictions and projections that answer substantively useful questions of interest. The section closed with post four, in which the challenges of describing and producing effect estimates as quantities of interest when using logistic regression models was illustrated in some detail.\nSection two, covering posts 5-10, then delved into more detail, about what likelihood theory is, and how R uses optimisation agorithms to try to solve loss functions based on likelihood to find the best combination of model parameters to represent the core relationships that exist between variables in a dataset. Within this, we identified how information and uncertainty (two sides of the same coin) about model parameters can be estimated using the Hessian, a measure of the instantaneous curvature around the position in the optimisation’s search space that maximises the log likelihood. It’s this measure of uncertainty, including joint uncertainty and correlation between predictor variables, that allows not just prediction and projection from models, but honest prediction and projection, allowing ‘what if?’ questions to be asked of a model that takes into account of parameter uncertainty."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html#recap",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html#recap",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "",
    "text": "The first four posts in this series formed ‘section one’, in which the focus was on framing the challenge of fitting (almost) all statistical models from the perspective of likelihood theory; and how to go beyond ‘star-gazing’ (just look at tables of coefficients), and using statistical models to make predictions and projections that answer substantively useful questions of interest. The section closed with post four, in which the challenges of describing and producing effect estimates as quantities of interest when using logistic regression models was illustrated in some detail.\nSection two, covering posts 5-10, then delved into more detail, about what likelihood theory is, and how R uses optimisation agorithms to try to solve loss functions based on likelihood to find the best combination of model parameters to represent the core relationships that exist between variables in a dataset. Within this, we identified how information and uncertainty (two sides of the same coin) about model parameters can be estimated using the Hessian, a measure of the instantaneous curvature around the position in the optimisation’s search space that maximises the log likelihood. It’s this measure of uncertainty, including joint uncertainty and correlation between predictor variables, that allows not just prediction and projection from models, but honest prediction and projection, allowing ‘what if?’ questions to be asked of a model that takes into account of parameter uncertainty."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html#aim",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html#aim",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "Aim",
    "text": "Aim\nThe purpose of this post is to move onto a new section, section three, in which we will look at some of the ways that quantities of interest - expected values, predicted values, and first differences - can be answered with a fitted model honestly, i.e. accounting for parameter uncertainty."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html#linear-regression-example",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html#linear-regression-example",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "Linear regression example",
    "text": "Linear regression example\nLet’s start with one of the built-in datasets, ToothGrowth, which is described as follows:\n\nThe response is the length of odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. Each animal received one of three dose levels of vitamin C (0.5, 1, and 2 mg/day) by one of two delivery methods, orange juice or ascorbic acid (a form of vitamin C and coded as VC).\n\nLet’s load the dataset and visualise\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nWhat does it look like?\n\n\nCode\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    expand_limits(x = 0, y = 0)\n\n\n\n\n\nSo, although this has just three variables, there is some complexity involved in thinking about how the two predictor variables, supp and dose, relate to the response variable len. These include:\n\nWhether the relationship between len and dose is linear in a straightforward sense, or associated in a more complicated wway\nWhether supp has the same effect on len regardless of dose, or whether there is an interaction between dose and supp.\n\n\nStage one: model fitting\nWe can address each of these questions in turn, but should probably start with a model which includes both predictors:\n\n\nCode\nmod_01 &lt;- lm(len ~ dose + supp, data = df)\n\nsummary(mod_01)\n\n\n\nCall:\nlm(formula = len ~ dose + supp, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-6.600 -3.700  0.373  2.116  8.800 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.2725     1.2824   7.231 1.31e-09 ***\ndose          9.7636     0.8768  11.135 6.31e-16 ***\nsuppVC       -3.7000     1.0936  -3.383   0.0013 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.236 on 57 degrees of freedom\nMultiple R-squared:  0.7038,    Adjusted R-squared:  0.6934 \nF-statistic: 67.72 on 2 and 57 DF,  p-value: 8.716e-16\n\n\nEach term is statistically significant at the conventional thresholds (P &lt; 0.05), with higher doses associated with greater lengths. Compared to OJ, the reference category, a vitamin C (VC) supplement is associated with lower lengths.\nTurning to the first question, about the type of relationship between len and dose, one possibility is that greater doses lead to greater lengths, but there are diminishing marginal returns: the first mg has the biggest marginal effect, then the second mg has a lower marginal effect. An easy way to model this would be to include the log of dose in the regression model, rather than the dose itself.1 We can get a sense of whether this log dose specification might be preferred by plotting the data with a log scale on the x axis, and seeing if the points look like they ‘line up’ better:\n\n\nCode\ndf |&gt;\n    ggplot(aes(y = len, x = dose, shape = supp, colour = supp)) + \n    geom_point() + \n    scale_x_log10() + \n    expand_limits(x = 0.250, y = 0)\n\n\n\n\n\nYes, with this scaling, the points associated with the three dosage regimes look like they line up better. Let’s now build this model specification:\n\n\nCode\nmod_02 &lt;- lm(len ~ log(dose) + supp, data = df)\n\nsummary(mod_02)\n\n\n\nCall:\nlm(formula = len ~ log(dose) + supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.2108 -2.9896 -0.5633  2.2842  9.1892 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  20.6633     0.7033   29.38  &lt; 2e-16 ***\nlog(dose)    11.1773     0.8788   12.72  &lt; 2e-16 ***\nsuppVC       -3.7000     0.9947   -3.72 0.000457 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.852 on 57 degrees of freedom\nMultiple R-squared:  0.755, Adjusted R-squared:  0.7464 \nF-statistic: 87.81 on 2 and 57 DF,  p-value: &lt; 2.2e-16\n\n\nAgain, the same kind of relationship between variables is observed: higher log dose: greater length; and VC rather than OJ is associated with lower growth. But is this model actually any better? The model summary for the linear dose model gives an adjusted \\(R^2\\) of 0.69, whereas for the log dose model the adjusted \\(R^2\\) is 0.75. So, as the data are fundamentally the same,2 this suggests it is. However, as we know that linear regression models are really just another kind of generalised linear models, and that model fitting tends to involve trying to maximise the log likelihood, we can also compare the log likelihoods of the two models, using the logLik() function, and so which is higher:\n\n\nCode\nlogLik(mod_01)\n\n\n'log Lik.' -170.2078 (df=4)\n\n\nCode\nlogLik(mod_02)\n\n\n'log Lik.' -164.5183 (df=4)\n\n\nBoth report the same number of degrees of freedom (‘df’), which shouldn’t be suprising as they involve the same number of parameters. But the log likelihood for mod_02 is higher, which like the Adjusted R-squared metric suggests a better fit.\nAnother approach, which generalises better to other types of model, is to compare the AICs, which are metrics that try to show the trade off between model complexity (based on number of parameters), and model fit (based on the log likelihood). By this criterion, the lower the score, the better the model:\n\n\nCode\nAIC(mod_01, mod_02)\n\n\n       df      AIC\nmod_01  4 348.4155\nmod_02  4 337.0367\n\n\nAs both models have exactly the same number of parameters, it should be of no surprise that mod_02 is still preferred.\nLet’s now address the second question: is there an interaction between dose and supp. This interaction term can be specified in one of two ways:\n\n\nCode\n# add interaction term explicitly, using the : symbol\nmod_03a &lt;- lm(len ~ log(dose) + supp + log(dose) : supp, data = df)\n\n# add interaction term implicitly, using the * symbol \nmod_03b &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(mod_03a)\n\n\n\nCall:\nlm(formula = len ~ log(dose) + supp + log(dose):supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nsummary(mod_03b)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nWe can see from the summaries that both ways of specifying the models lead to exactly the same model, with exactly the same estimates, standared errors, adjusted \\(R^2\\)s, and so on. The adjusted \\(R^2\\) is now 0.76, a slight improvement on the 0.75 value for the model without the interaction term. As before, we can also compare the trade-off between additional complexity and improved fit using AIC\n\n\nCode\nAIC(mod_02, mod_03a)\n\n\n        df      AIC\nmod_02   4 337.0367\nmod_03a  5 333.7750\n\n\nSo, the AIC of the more complex model is lower, suggesting a better model, but the additional improvement in fit is small.\nWe can also compare the fit, and answer the question of whether the two models can be compared, in a couple of other ways. Firstly, we can use BIC, AIC’s (usually) stricter cousin, which tends to penalise model complexity more harshly:\n\n\nCode\nBIC(mod_02, mod_03a)\n\n\n        df      BIC\nmod_02   4 345.4140\nmod_03a  5 344.2467\n\n\nEven using BIC, the more complex model is still preferred, though the difference in values is now much smaller.\nThe other way we can compare the models is using an F-test using the anova (analysis of variance) function:\n\n\nCode\nanova(mod_02, mod_03a)\n\n\nAnalysis of Variance Table\n\nModel 1: len ~ log(dose) + supp\nModel 2: len ~ log(dose) + supp + log(dose):supp\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     57 845.91                              \n2     56 774.89  1    71.022 5.1327 0.02737 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nHere anova compares the two models, notes that the first model can be understood as a restricted variant of the second model,3 and compares the change in model fit between the two models against the change in number of parameters used to fit the model. The key parts of the summary to look at are the F test value, 5.13, and the associated P value, which is between 0.01 and 0.05. This, again, suggests the interaction term is worth keeping.\nSo, after all that, we finally have a fitted model. Let’s look now at making some predictions from it."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html#stage-two-model-predictions",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html#stage-two-model-predictions",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "Stage Two: Model predictions",
    "text": "Stage Two: Model predictions\nThe simplest approach to getting model predictions is to use the predict function, passing it a dataframe of values for which we want predictions:\n\n\nCode\npredictor_df &lt;- expand_grid(\n    supp = c('VC', 'OJ'), \n    dose = seq(0.25, 2.25, by = 0.01)\n)\npreds_predictors_df &lt;- predictor_df |&gt;\n    mutate(pred_len = predict(mod_03a, predictor_df))\n\npreds_predictors_df\n\n\n# A tibble: 402 × 3\n   supp   dose pred_len\n   &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt;\n 1 VC     0.25   -1.20 \n 2 VC     0.26   -0.683\n 3 VC     0.27   -0.189\n 4 VC     0.28    0.288\n 5 VC     0.29    0.748\n 6 VC     0.3     1.19 \n 7 VC     0.31    1.62 \n 8 VC     0.32    2.04 \n 9 VC     0.33    2.44 \n10 VC     0.34    2.83 \n# ℹ 392 more rows\n\n\nWe can visualise these predictions as follows, with the predicted values as lines, and the observed values as points:\n\n\nCode\npreds_predictors_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, y = pred_len, colour = supp, linetype = interextrap)) + \n    geom_line() + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\"\n    )\n\n\n\n\n\nIn the above, I’ve shown the lines as solid when they represent interpolations of the data, i.e. are in the range of measured doses, and as dashed when they represent extrapolations from the data, meaning they are are predictions made outside the range of observed values. We can see an obvious issue when we extrapolate too far to the left: for low doses, and for the VC supplement, the model predicts negative tooth lengths. Extrapolation is dangerous! And gets more dangerous the further we extrapolate from available observations.\nWe can also use the predict function to produce uncertainty intervals, either of expected values, or predicted values. By default these are 95% intervals, meaning they are expected to contain 95% of the range of expected or predicted values from the model.\nLet’s first look at expected values, which include uncertainty about parameter estimates, but not observed variation in outcomes:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"confidence\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of expected values\"\n    )\n\n\n\n\n\nAnd the following shows the equivalent prediction intervals, which also incorporate known variance, as well as parameter uncertainty:\n\n\nCode\ndf_pred_intvl &lt;- predict(mod_03a, newdata = predictor_df, interval = \"prediction\")\n\npreds_predictors_intervals_df &lt;- \n    bind_cols(predictor_df, df_pred_intvl)\n\npreds_predictors_intervals_df |&gt;\n    mutate(\n        interextrap = \n        case_when(\n            dose &lt; 0.5 ~ \"extrap_below\",\n            dose &gt; 2.0 ~ \"extrap_above\",\n            TRUE ~ \"interp\"\n        )\n    ) |&gt;\n    ggplot(aes(x = dose, linetype = interextrap)) + \n    geom_line(aes(y = fit, colour = supp)) + \n    geom_ribbon(aes(ymin = lwr, ymax = upr, fill = supp), alpha = 0.2) + \n    scale_linetype_manual(values = c(`interp` = 'solid', `extrap_below` = 'dashed', `extrap_above` = 'dashed'), guide = 'none') + \n    geom_point(\n        aes(x = dose, y = len, group = supp, colour = supp, shape = supp), inherit.aes = FALSE, \n        data = df\n    ) + \n    labs(\n        x = \"Dose in mg\",\n        y = \"Tooth length in ?\",\n        title = \"Predicted and observed tooth length, dose and supplement relationships\",\n        subtitle = \"Range of predicted values\"\n    )\n\n\n\n\n\nAs should be clear from the above, and discussion of the difference between expected and predicted values in previous posts, predicted values and expected values are very different, and it is important to be aware of the difference between these two quantities of interest. Regardless, we can see once again how dangerous it is to use this particular model specification to extrapolate beyond the range of observations, expecially for lower doses."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html#summary-and-coming-up",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html#summary-and-coming-up",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "Summary and coming up",
    "text": "Summary and coming up\nIn this post, we’ve started and finished building the model, and started but not quite finished using the model to generate expected and predicted values. We’ve discussed some approaches to deciding on a model specification, by incrementally comparing a series of different specifications which test different ideas we have about how the predictor variables might be related to each other, and to the response variable.\nAs we’ve done quite a lot of work on building the model, we’ve not covered everything that I was planning to in terms of model prediction, and what we can do with a linear regression model (and generalised linear regression model), beyond (yawn) stargazing, for using models to get expected values, predicted values, and especially first differences. So, I guess that’s coming up in the next post!"
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html#footnotes",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-11/index.html#footnotes",
    "title": "Part Eleven: Honest Predictions the easier way",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe difference between 0.5mg and 1.0mg is 0.5mg, but it’s also one doubling. The difference between 1.0mg and 2.0mg is 1.0mg, but it’s also one doubling. The effect of logging dose is to space the values by the number of doublings, not the absolute difference.↩︎\nThese metrics can be used to compare different linear regression model specifications which use the same dataset, but should not be used to compare the same or different model specifications as applied to different datasets. Some data, and the data generating processes (i.e. reality) that produce them, are just inherently more variable than others, and no amount of model calibration or additional data will be able to address this.↩︎\nIn this example, our more complex model has coefficients fit from the data for the intercept, log(dose), supp and the interaction term log(dose):supp, whereas the less complex model has coefficients fit from the data for the intercept, log(dose), and supp. This means the less complex model can be specified as a restricted version of the more complex model, where the value of the coefficient on the interaction term log(dose):supp is set to be equal to zero, rather than determined from the data. An equivalent way of phrasing and thinking about this is that the two model specifications are nested, with the restricted model nested inside the unrestricted model, which includes the interaction term. It’s this requirement for models to be nested in this way which meant that mod_01 and mod_02 could not be compared using an F-test, as neither model could be described strictly as restricted variants of the other model: they’re siblings, not mothers and daughters. However, both mod_01 and mod_02 could be compared against a common ancestor model which only includes the intercept term.↩︎"
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "",
    "text": "The last post ended by showing how the predict function can be used to show point estimates and uncertainty intervals for expected values and predicted values for a model based on a toothsome dataset. In this post we will start with that model and look at other information that can be recovered from it, information that will allow the effects of joint parameter uncertainty to be propagated through to prediction."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#aim",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#aim",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "",
    "text": "The last post ended by showing how the predict function can be used to show point estimates and uncertainty intervals for expected values and predicted values for a model based on a toothsome dataset. In this post we will start with that model and look at other information that can be recovered from it, information that will allow the effects of joint parameter uncertainty to be propagated through to prediction."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#recap-of-core-concepts",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#recap-of-core-concepts",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Recap of core concepts",
    "text": "Recap of core concepts\nBack in part 8 we stated that estimates of the cloud of uncertainty in model parameters, that results from having limited numbers of observations in the data, can be represented as:\n\\[\n\\tilde{\\theta} \\sim MVN(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere MVN means multivariate normal, and needs the two quantities \\(\\dot{\\theta}\\) and \\(\\Sigma\\) as parameters.\nPreviously we showed how to extract (estimates of) these two quantities from optim(), where the first quantity, \\(\\dot{\\theta}\\), was taken from the converged parameter point estimate slot par, and the second quantity, \\(\\Sigma\\), was derived from the hessian slot.\nBut we don’t need to use optim() directly in order to recover these quantities. Instead we can get them from the standard model objects produced by either lm() or glm(). Let’s check this out…"
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#building-our-model",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#building-our-model",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Building our model",
    "text": "Building our model\nLet’s load the data and model we arrived at previously\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nCode\nbest_model &lt;- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model)\n\n\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       20.6633     0.6791  30.425  &lt; 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,    Adjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s now look at some convenience functions, other than just summary, that work with lm() and glm() objects, and recover the quantities required from MVN to represent the uncertainty cloud."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#extracting-quantities-for-modelling-uncertainty",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#extracting-quantities-for-modelling-uncertainty",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Extracting quantities for modelling uncertainty",
    "text": "Extracting quantities for modelling uncertainty\nFirstly, for the point estimates \\(\\dot{\\theta}\\), we can use the coefficients() function\n\n\nCode\ncoef &lt;- coefficients(best_model)\n\ncoef\n\n\n     (Intercept)        log(dose)           suppVC log(dose):suppVC \n       20.663333         9.254889        -3.700000         3.844782 \n\n\nAnd for the variance-covariance matrix, for representing joint uncertainty about the above estimates, we can use the vcov function\n\n\nCode\nSig &lt;- vcov(best_model)\n\nSig\n\n\n                   (Intercept)     log(dose)        suppVC log(dose):suppVC\n(Intercept)       4.612422e-01 -8.768056e-17 -4.612422e-01    -7.224251e-17\nlog(dose)        -8.768056e-17  1.440023e+00  1.753611e-16    -1.440023e+00\nsuppVC           -4.612422e-01  1.753611e-16  9.224843e-01     1.748938e-16\nlog(dose):suppVC -7.224251e-17 -1.440023e+00  1.748938e-16     2.880045e+00\n\n\nFinally, we can extract the point estimate for stochastic variation in the model, i.e. variation assumed by the model even if parameter uncertainty were minimised, using the sigma function:\n\n\nCode\nsig &lt;- sigma(best_model)\n\nsig\n\n\n[1] 3.719847\n\n\nWe now have three quantities, coef, Sig and sig (note the upper and lower case s in the above). These provide something almost but not exactly equivalent to the contents of par and that derived from hessian when using optim() previously. The section below explains this distinction in more detail.\n\nBack to the weeds (potentially skippable)\nRecall the ‘grandmother formulae’, from King, Tomz, and Wittenberg (2000), which the first few posts in this series started with:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nFor standard linear regression this becomes:\nStochastic Component\n\\[\nY_i \\sim Norm(\\theta_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\theta_i =X_i \\beta\n\\]\nOur main parameters are \\(\\theta\\), which combined our predictors \\(X_i\\) and our model parameter estimates \\(\\beta\\). Of these two components we know the data - they are what they are - but are merely estimating our model parameters \\(\\beta\\). So, any estimation uncertainty in this part of the equation results from \\(\\beta\\) alone.\nOur ancillary parameter is \\(\\sigma^2\\). This is our estimate of how much fundamental variation there is in how the data (the response variables \\(Y\\)) is drawn from the stochastic data generating process.\nWhen we used optim() directly, we estimated \\(\\sigma^2\\) along with the other \\(\\beta\\) parameters, via the \\(\\eta\\) parameter eta, defined as \\(\\sigma^2 = e^{\\eta}\\) to allow optim() to search over an unbounded real number range. If there are k \\(\\beta\\) parameters, therefore, optim()’s par vector contained k + 1 values, with this last value being the point estimate for the eta parameter. Similarly, the number of rows, columns, and length of diagonal elements in the variance-covariance matrix recoverable through optim’s hessian slot was also k + 1 rather than k, with the last row, last column, and last diagonal element being measures of covariance between \\(\\eta\\) and the \\(\\beta\\) elements, and variance in \\(\\eta\\) itself.\nBy contrast, the length of coefficients returned by coefficients(best_model) is k, the number of \\(\\beta\\) parameters being estimated, and the dimensions of vcov(best_model) returned are also k by k.\nThis means there is one fewer piece/type of information about model parameters returned by coefficients(model), vcov(model) and sigma(model) than was potentially recoverable by optim()’s par and hessian parameter slots: namely, uncertainty about the true value of the ancillary parameter \\(\\sigma^2\\). The following table summarises this difference:\n\n\n\n\n\n\n\n\nInformation type\nvia optim\nvia lm and glm\n\n\n\n\nMain parameters: point\nfirst k elements of par\ncoefficients() function\n\n\nMain parameters: uncertainty\nfirst k rows and columns of hessian\nvcov() function\n\n\nAncillary parameters: point\nk+1th through to last element of par\nsigma() function or equivalent for glm()\n\n\nAncillary parameters: uncertainty\nlast columns and rows of hessian (after rows and columns k)\n—\n\n\n\nSo long as capturing uncertainty about the fundamental variability in the stochastic part of the model isn’t critical to our predictions then omission of a measure of uncertainty in the ancillary parameters \\(\\alpha\\) is likely a price worth paying for the additional convenience of being able to use the model objects directly. However we should be aware that, whereas with optim we potentially have both \\(\\tilde{\\beta}\\) and \\(\\tilde{\\alpha}\\) to represent model uncertainty, when using the three convenience functions coefficients(), vcov() and sigma() we technically ‘only’ have \\(\\tilde{\\beta}\\) and \\(\\dot{\\alpha}\\) (i.e. point estimates alone for the ancillary parameters).\nWith the above caveat in mind, let’s now look at using the results of coefficients(), vcov() and sigma() to generate (mostly) honest representations of expected values, predicted values, and first differences"
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#model-predictions",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#model-predictions",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Model predictions",
    "text": "Model predictions\nAs covered in section two, we can use the mvrnorm function from the MASS package to create \\(\\tilde{\\beta}\\), our parameter estimates with uncertainty:\n\nParameter simulation\n\n\nCode\nbeta_tilde &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = coef, \n    Sigma = Sig\n)\n\nhead(beta_tilde)\n\n\n     (Intercept) log(dose)    suppVC log(dose):suppVC\n[1,]    21.06791  7.645689 -3.819112         6.625200\n[2,]    21.48901  7.676168 -3.516544         4.243647\n[3,]    19.14443  9.214789 -1.195671         3.862845\n[4,]    20.92187 10.205717 -3.823071         2.742209\n[5,]    20.37284  9.212964 -2.789648         3.912550\n[6,]    20.44124  9.132118 -3.280281         1.993097\n\n\nLet’s first look at each of these parameters individually:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    pivot_longer(everything(), names_to = \"coefficient\", values_to = \"value\") |&gt; \n    ggplot(aes(x = value)) + \n    facet_grid(coefficient ~ .) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nNow let’s look at a couple of coefficients jointly, to see how they’re correlated. Firstly the association between the intercept and the log dosage:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = `log(dose)`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere the covariance between the two parameters appears very low. Now let’s look at how log dosage and Vitamin C supplement factor are associated:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nAgain, the covariance appears low. Finally, the association between log dose and the interaction term\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `log(dose)`, y = `log(dose):suppVC`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nHere we have a much stronger negative covariance between the two coefficients. Let’s look at the variance-covariance extracted from the model previously to confirm this:\n\n\nCode\nknitr::kable(Sig)\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nlog(dose)\nsuppVC\nlog(dose):suppVC\n\n\n\n\n(Intercept)\n0.4612422\n0.000000\n-0.4612422\n0.000000\n\n\nlog(dose)\n0.0000000\n1.440023\n0.0000000\n-1.440023\n\n\nsuppVC\n-0.4612422\n0.000000\n0.9224843\n0.000000\n\n\nlog(dose):suppVC\n0.0000000\n-1.440023\n0.0000000\n2.880045\n\n\n\n\n\nHere we can see that the covariance between intercept and log dose is effectively zero, as is the covariance between the intercept and the interaction term, and the covariance between the log(dose) and suppVC factor. However, there is a negative covariance between log dose and the interaction term, i.e. what we have plotted above, and also between the intercept and the VC factor. For completeness, let’s look at this last assocation, which we expect to show negative association:\n\n\nCode\nbeta_tilde |&gt; \n    as_tibble() |&gt;\n    ggplot(aes(x = `(Intercept)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nYes it is! The parameter estimates follow the covariance provided by Sigma, as we would expect.\n\n\nExpected values\nLet’s stay we are initially interested in the expected values for a dosage of 1.25mg, with the OJ (rather than VC) supplement:\n\n\nCode\n# first element is 1 due to intercept\npredictor &lt;- c(1, log(1.25), 0, 0) \n\npredictions_ev &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor %*% this_beta\n)\n\nhead(predictions_ev)\n\n\n[1] 22.77400 23.20190 21.20065 23.19921 22.42865 22.47902\n\n\nLet’s now get a 95% credible interval:\n\n\nCode\nquantile(predictions_ev, probs = c(0.025, 0.500, 0.975))\n\n\n    2.5%      50%    97.5% \n21.31125 22.73087 24.18298 \n\n\nSo, the 95% interval for the expected value is between 21.31 and 24.14, with a middle (median) estimate of 22.73.1 Let’s check this against estimates from the predict() function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'confidence')\n\n\n      fit      lwr      upr\n1 22.7285 21.26607 24.19093\n\n\nThe expected values using the predict function give a 95% confidence interval of 21.27 to 24.19, with a point estimate of 22.73. These are not identical, as the methods employed are not identical,2 but they are hopefully similar enough to demonstrate they are attempts at getting at the same quantities of interest.\n\n\nPredicted values\nPredicted values also include inherent stochastic variation from the ancillary parameters \\(\\alpha\\), which for linear regression is \\(\\sigma^2\\). We can simply add these only the expected values above to produce predicted values:\n\n\nCode\nn &lt;- length(predictions_ev)\n\nshoogliness &lt;- rnorm(n=n, mean = 0, sd = sig)\n\npredictions_pv &lt;- predictions_ev + shoogliness\n\n\nhead(predictions_pv)\n\n\n[1] 19.19537 19.87588 20.12654 28.13522 26.16638 22.19669\n\n\nLet’s get the 95% interval from the above using quantile\n\n\nCode\nquantile(predictions_pv, probs = c(0.025, 0.5000, 0.975))\n\n\n    2.5%      50%    97.5% \n15.17997 22.73280 30.16406 \n\n\nAs expected, the interval is now much wider, with a 95% interval from 15.34 to 30.11. The central estimate should in theory, with an infinite number of runs, be the same, however because of random variation it will never be exactly the same to an arbitrary number of decimal places. In this case, the middle estimate is 22.75, not identical to the central estimate from the expected values distribution of 22.72. The number of simulations can always be increased to produce greater precision if needed.\nLet’s now compare this with the prediction interval produce by the predict function:\n\n\nCode\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'prediction')\n\n\n      fit      lwr     upr\n1 22.7285 15.13461 30.3224\n\n\nAgain, the interval estimates are not exactly the same, but they are very similar.\n\n\nFirst differences\nIt’s in the production of estimates of first differences - this, compared to that, holding all else constant - that the simulation approach shines for producing estimates with credible uncertainty. In our case, let’s say we are interested in asking:\n\nWhat is the expected effect of using the VC supplement, rather than the OJ supplement, where the dose is 1.25mg?\n\nSo, the first difference is from switching from OJ to VC, holding the other factor constant.\nWe can answer this question by using the same selection of \\(\\tilde{\\beta}\\) draws, but passing two different scenarios:\n\n\nCode\n#scenario 0: supplement is OJ\npredictor_x0 &lt;- c(1, log(1.25), 0, 0) \n\n#scenario 1: supplement is VC\npredictor_x1 &lt;- c(1, log(1.25), 1, 1 * log(1.25)) \n\n\npredictions_ev_x0 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x0 %*% this_beta\n)\n\npredictions_ev_x1 &lt;- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x1 %*% this_beta\n)\n\npredictions_df &lt;- \n    tibble(\n        x0 = predictions_ev_x0,\n        x1 = predictions_ev_x1\n    ) |&gt;\n    mutate(\n        fd = x1 - x0\n    )\n\npredictions_df\n\n\n# A tibble: 10,000 × 3\n      x0    x1     fd\n   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1  22.8  20.4 -2.34 \n 2  23.2  20.6 -2.57 \n 3  21.2  20.9 -0.334\n 4  23.2  20.0 -3.21 \n 5  22.4  20.5 -1.92 \n 6  22.5  19.6 -2.84 \n 7  22.1  20.0 -2.17 \n 8  23.4  20.3 -3.17 \n 9  21.7  19.9 -1.78 \n10  21.8  20.0 -1.85 \n# ℹ 9,990 more rows\n\n\nLet’s look at the distribution of both scenarios individually:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario != \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    facet_wrap(~scenario, ncol = 1) + \n    geom_histogram(bins = 100)\n\n\n\n\n\nAnd the distribution of the pairwise differences between them:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt;\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n\n\n\n\n\nIt’s this last distribution which shows our first differences, i.e. our answer, hedged with an appropriate dose of uncertainty, to the specific question shown above. We can get a 95% interval of the first difference as follows:\n\n\nCode\npredictions_df |&gt;\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |&gt;\n    filter(scenario == \"fd\") |&gt; \n    pull('estimate') |&gt;\n    quantile(probs = c(0.025, 0.500, 0.975))\n\n\n      2.5%        50%      97.5% \n-4.8919856 -2.8561481 -0.8011861 \n\n\nSo, 95% of estimates of the first difference are between -4.85 and -0.81, with the middle of this distribution (on this occasion) being -2.83.\nUnlike with the expected values and predicted values, the predict() function does not return first differences with honest uncertainty in this way. What we have above is something new."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#summary",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#summary",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Summary",
    "text": "Summary\nIn this post we’ve finally combined all the learning we’ve developed over the 11 previous posts to answer three specific ‘what if?’ questions: one on expected values, one on predicted values, and one on first differences. These are what King, Tomz, and Wittenberg (2000) refer to as quantities of interest, and I hope you agree these are more organic and reasonable types of question to ask of data and statistical models than simply looking at coefficients and p-values and reporting which ones are ‘statistically significant’.\nIf you’ve been able to follow everything in these posts, and can generalise the approach shown above to other types of statistical model, then congratulations! You’ve learned the framework for answering meaningful questions using statistical models which is at the heart of one of the toughest methods courses for social scientists offered by one of the most prestigious universities in the world.3"
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#coming-up",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#coming-up",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Coming up",
    "text": "Coming up\nThe next post uses the same dataset and model we’ve developed and applied, but shows how it can be implemented using a Bayesian rather than Frequentist modelling approach. In some ways it’s very familar, but in others it introduces a completely new paradigm to how models are fit and run."
  },
  {
    "objectID": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#footnotes",
    "href": "posts/glms/complete-simulation-example/lms-are-glms-part-12/index.html#footnotes",
    "title": "Part Twelve: Honest Predictions the slightly-less easier way",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese are the values produced the first time I ran the simulation. They are likely to be a little different each time, so may not be identical to the number of decimal places reported when I next render this document. These estimates are approximations.↩︎\nBecause the simulation approach relies on random numbers, the draws will never be the same unless the same random number seed is using using set.seed(). However with more simulations, using the n parameter from mvrnorm, the distributions of estimates should become ever closer to each other.↩︎\nI took this course via the Harvard extension school while doing my PhD in York quite a few years ago. I took it as a non-credit option - as what’s the value of a fraction of a degree when I had two already? - but was told by the tutors that I’d completed it to a ‘Grade A-’ level. So, not perfect, but good enough…↩︎"
  },
  {
    "objectID": "posts/glms/hacker-stats/bootstrapping/index.html",
    "href": "posts/glms/hacker-stats/bootstrapping/index.html",
    "title": "A brief introduction to bootstrapping",
    "section": "",
    "text": "I met with Neil Pettinger earlier today. He traded eggs benedict for some statistical advice, mainly on what bootstrapping is, and whether it could be helpful for analysing hospital length of stay data.\nHere’s a brief post on bootstrapping with some example code:"
  },
  {
    "objectID": "posts/glms/hacker-stats/bootstrapping/index.html#what-is-bootstrapping",
    "href": "posts/glms/hacker-stats/bootstrapping/index.html#what-is-bootstrapping",
    "title": "A brief introduction to bootstrapping",
    "section": "What is bootstrapping?",
    "text": "What is bootstrapping?\nAccording to Wikipedia:\n\nBootstrapping is any test or metric that uses random sampling with replacement (e.g. mimicking the sampling process), and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates.[1][2] This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.[3][4]\n\n\nBootstrapping estimates the properties of an estimand (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution function of the observed data. In the case where a set of observations can be assumed to be from an independent and identically distributed population, this can be implemented by constructing a number of resamples with replacement, of the observed data set (and of equal size to the observed data set).\n\n\nIt may also be used for constructing hypothesis tests.[5] It is often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors.\n\nn.b. The same page (In History) also states: “Other names … suggested for the ‘bootstrap’ method were: Swiss Army Knife, Meat Axe, Swan-Dive, Jack-Rabbit, and Shotgun.” So, there might not be good reasons to fear statistics, but given this list of suggestions there might be good reasons to fear some statisticians! Of these alternative names, perhaps Swiss Army Knife is the most appropriate, as it’s a very widely applicable approach!"
  },
  {
    "objectID": "posts/glms/hacker-stats/bootstrapping/index.html#a-brief-example",
    "href": "posts/glms/hacker-stats/bootstrapping/index.html#a-brief-example",
    "title": "A brief introduction to bootstrapping",
    "section": "A brief example",
    "text": "A brief example\nI’m not going to look for length-of-stay data; instead I’m going to look at length-of-teeth, and the hamster experiment dataset I used in a few previous posts.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndf &lt;- ToothGrowth |&gt; tibble()\ndf\n\n# A tibble: 60 × 3\n     len supp   dose\n   &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n\n\nLet’s say, instead of building a statistical model, I’m just interested in the following question:\n\nWhere the dose is 1mg, is using the OJ supplement instead of the VC supplement associated with a significant and detectable difference in the median tooth length?\n\nWe can do this in at least a couple of ways:\n\nCalculate the median of OJ at 1mg tooth lengths, and compare it to a bootstrapped distribution of medians from VC at 1mg.\nBootstrap both the OJ and VC (both at 1mg) populations, get the medians for each bootstrapped population, and record the difference in the medians.\n\nThese are asking slightly different questions, but both ways of using bootstrapping to address the general type of question framed above.\n\nApproach One\n\nNreps &lt;- 10000 # Number of bootstrap replicates\n\nbs_med_vc &lt;- vector(mode = 'numeric',  length = Nreps) #Vector for holding bootstrapped medians\n\ndta_vc &lt;- df |&gt;\n    filter(supp == \"VC\", dose == 1.0) # The equivalent of our 'control' population\n\ncontrol_toothlengths &lt;- dta_vc |&gt; pull(len) # Literally pulling teeth!\n\nNcontrol &lt;- length(control_toothlengths) #Length of 'control' population sample\n\nfor (i in 1:Nreps){\n    bs_c_length &lt;- sample(\n        control_toothlengths, \n        size = Ncontrol,\n        replace = TRUE\n    ) # resampling to the same length as the 'control' population\n\n    this_bs_control_median &lt;- median(bs_c_length)\n    bs_med_vc[i] &lt;- this_bs_control_median\n}\n\nWe’ve now done the bootstrapping on the ‘control’ population. Let’s look at this bootstrapped distribution of medians in comparison with the observed median from the ‘treatment’ group.\n\ntreatment_toothlengths &lt;- df |&gt;\n    filter(supp == \"OJ\", dose == 1.0) |&gt;\n    pull(len) # pulling teeth for the 'treatment' population\n\nobs_med_oj &lt;- median(treatment_toothlengths)\n\ntibble(bs_control_median = bs_med_vc) |&gt;\n    ggplot(aes(x=bs_control_median)) +\n    geom_histogram() +\n    geom_vline(xintercept = obs_med_oj, colour = \"red\", linetype = \"dashed\") + \n    geom_vline(xintercept = median(control_toothlengths), colour = \"blue\", linetype = \"dashed\") + \n    labs(\n       x = \"Median toothlength\",\n       y = \"Number of bootstraps\",\n       title = \"Bootstrapping approach One\",\n       subtitle = \"Red line: Observed median toothlength in 'treatment' arm. Blue line: Observed median in 'control' arm\"\n    )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can see here that the red line, which is the observed median in the ‘treatment’ arm, is higher than all of the bootstrapped medians from the ‘control’ arm. The blue line shows the equivalent in the observed median in the ‘control’ arm.\nSo, without even performing a calculation, we can feel more confident that the OJ supplement is associated with larger tooth length, even though both arms comprise just ten observations."
  },
  {
    "objectID": "posts/glms/hacker-stats/bootstrapping/index.html#approach-two",
    "href": "posts/glms/hacker-stats/bootstrapping/index.html#approach-two",
    "title": "A brief introduction to bootstrapping",
    "section": "Approach Two",
    "text": "Approach Two\nLet’s now use bootstrapping to produce a distributions of differences in medians between the two arms. So, this time we repeatedly resample from both the control and the treatment arm.\n\nNreps &lt;- 10000 # Number of bootstrap replicates\n\nbs_diff_meds &lt;- vector(mode = 'numeric',  length = Nreps) #Vector for holding differences in bootstrapped medians\n\ndta_vc &lt;- df |&gt;\n    filter(supp == \"VC\", dose == 1.0) # The equivalent of our 'control' population\n\ncontrol_toothlengths &lt;- dta_vc |&gt; pull(len) # Literally pulling teeth!\n\nNcontrol &lt;- length(control_toothlengths) #Length of 'control' population sample\n\ndta_oj &lt;- df |&gt;\n    filter(supp == \"OJ\", dose == 1.0) # The equivalent of our 'treamtnet' population\n\ntreatment_toothlengths &lt;- dta_oj |&gt; pull(len) # Literally pulling teeth!\n\nNtreatment &lt;- length(treatment_toothlengths) #Length of 'treatment' population sample\n\n\nfor (i in 1:Nreps){\n    bs_c_length &lt;- sample(\n        control_toothlengths, \n        size = Ncontrol,\n        replace = TRUE\n    ) # resampling to the same length as the 'control' population\n\n    this_bs_control_median &lt;- median(bs_c_length)\n\n    bs_t_length &lt;- sample(\n        treatment_toothlengths, \n        size = Ntreatment,\n        replace = TRUE\n    ) # resampling to the same length as the 'control' population\n\n    this_bs_treat_median &lt;- median(bs_t_length)\n\n    bs_diff_meds[i] &lt;- this_bs_treat_median - this_bs_control_median\n}\n\nWe now have a bootstrapped distribution of differences, each time subtracting the bootstrapped control median from the bootstrapped treat median. So, values above 0 indicate the treatment is more effective (at lengthening teeth) than the control.\nLet’s look at this distribution\n\ntibble(bs_diffs_median = bs_diff_meds) |&gt;\n    ggplot(aes(x=bs_diffs_median)) +\n    geom_histogram() +\n    geom_vline(xintercept = 0) + \n    geom_vline(\n        xintercept = median(treatment_toothlengths) - median(control_toothlengths), linetype = \"dashed\", colour = \"green\"         \n        ) + \n    labs(\n       x = \"Differences in medians\",\n       y = \"Number of bootstraps\",\n       title = \"Bootstrapping approach Two\",\n       subtitle = \"Values above 0: medians are higher in treatment group\"\n    )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nI’ve added the observed difference in medians as a vertical green line. This corresponds with the highest peak in bootstrapped differences in medians, as we might expect.\nAlmost all bootstrapped differences in medians are above 0, which again suggests we don’t even need to calculate the proportion above 0 to work out if there is likely to be a difference in medians between the two groups.\nHowever if we wanted to get this empirical p-value, we could do it as follows:\n\nsum(bs_diff_meds &lt; 0) / Nreps\n\n[1] 7e-04\n\n\nTiny!"
  },
  {
    "objectID": "posts/glms/hacker-stats/bootstrapping/index.html#going-further",
    "href": "posts/glms/hacker-stats/bootstrapping/index.html#going-further",
    "title": "A brief introduction to bootstrapping",
    "section": "Going further",
    "text": "Going further\nI suggested to Neil that writing some R code to do the bootstrapping can be a ‘good’ learning experience. This is what I’ve done in the above, using for loops as they’re easiest to reason through, even though not the most computationally efficient. Once the intuition of what bootstrapping is, how it works, and what it can do is embedded through writing out a few examples like this, there are plenty of packages that make bootstrapping even easier to do (and likely faster to run too).\nI also mentioned and can (for pedagogic purposes) recommend the infer package, which uses bootstrapping to produce estimates of distributions under the Null hypothesis, alongside parametric approaches, and produces pretty visualisations to boot!"
  },
  {
    "objectID": "posts/glms/hacker-stats/post-stratification/index.html",
    "href": "posts/glms/hacker-stats/post-stratification/index.html",
    "title": "Resampling for post-stratification",
    "section": "",
    "text": "In the introductionary post in this series on Hacker Stats, I mentioned that resampling methods can be used to perform post-stratification, meaning reweighting of observations from a sample in such a way as to make them more representative of the population of interest to us. Let’s look at this using a variation of the red coin/blue coin example from a couple of posts ago."
  },
  {
    "objectID": "posts/glms/hacker-stats/post-stratification/index.html#introduction",
    "href": "posts/glms/hacker-stats/post-stratification/index.html#introduction",
    "title": "Resampling for post-stratification",
    "section": "",
    "text": "In the introductionary post in this series on Hacker Stats, I mentioned that resampling methods can be used to perform post-stratification, meaning reweighting of observations from a sample in such a way as to make them more representative of the population of interest to us. Let’s look at this using a variation of the red coin/blue coin example from a couple of posts ago."
  },
  {
    "objectID": "posts/glms/hacker-stats/post-stratification/index.html#red-coinblue-coin",
    "href": "posts/glms/hacker-stats/post-stratification/index.html#red-coinblue-coin",
    "title": "Resampling for post-stratification",
    "section": "Red Coin/Blue Coin",
    "text": "Red Coin/Blue Coin\nImagine we have a population of two types of coin:\n\nRed Coins, which come up heads 65% of the time\nBlue Coins, which come up heads 47% of the time\n\nWithin our population, we know 75% of the coins are Blue coins, and 25 of the coins are Red Coins.\nHowever, our sample contains 20 red coins, and 20 blue coins. i.e. the distribution of coin types in our sample is different to that in our population.\nLet’s first create this sample dataset:\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(9)\n\ndraws_red &lt;- rbinom(n=20, size = 1, prob = 0.65)\ndraws_blue &lt;- rbinom(n=20, size = 1, prob = 0.47)\n\ncoin_colour &lt;- c(\n    rep(\"red\", 20),\n    rep(\"blue\", 20)\n)\n\nreal_sample_data &lt;- data.frame(\n    coin_colour = coin_colour, \n    outcome = c(draws_red, draws_blue)\n)\n\nrm(draws_red, draws_blue, coin_colour)\n\nhead(real_sample_data)\n\n\n  coin_colour outcome\n1         red       1\n2         red       1\n3         red       1\n4         red       1\n5         red       1\n6         red       1\n\n\nWhat’s the expected probability of heads in the sample?\n\n\nCode\nmean(real_sample_data$outcome)\n\n\n[1] 0.65\n\n\nCode\nreal_sample_data |&gt;\n    group_by(coin_colour) |&gt;\n    summarise(prop = mean(outcome))\n\n\n# A tibble: 2 × 2\n  coin_colour  prop\n  &lt;chr&gt;       &lt;dbl&gt;\n1 blue          0.5\n2 red           0.8\n\n\nOverall, 65% of the sample - 20 reds, 20 blues - are heads. The proportion of blues is 50%, and of reds is 80%. So, it so happens that, with this random number seed, the proportions in the sample of both reds and blues are higher than the theoretical average (the prob value arguments in the code above).\nLet’s now try to use bootstrapping to calculate a distribution around the sample mean:\n\n\nCode\nbootstrap_means &lt;- function(x, nReps = 10000){\n    out &lt;- vector(\"numeric\", nReps) \n\n    for (i in 1:nReps){\n        this_resample &lt;- sample(\n            x=x, \n            size = length(x), \n            replace = TRUE # This is what makes it bootstrapping\n        )\n        out[i] &lt;- mean(this_resample)\n    }\n    out\n}\n\nbootstrapped_means &lt;- bootstrap_means(real_sample_data$outcome)\n\nhead(bootstrapped_means)\n\n\n[1] 0.750 0.625 0.700 0.775 0.800 0.700\n\n\nWhat does this look like as a histogram?\n\n\nCode\ntibble(value = bootstrapped_means) |&gt;\n    ggplot(aes(x = value)) + \n    geom_histogram(bins = 50)\n\n\n\n\n\nWe can see the familiar bell-shaped distribution of values here. What about for blues and reds separately?\n\n\nCode\nbootstrapped_means_reds &lt;- bootstrap_means(\n    real_sample_data |&gt;\n        filter(coin_colour == \"red\") |&gt;\n        pull('outcome')  \n    )\n\nbootstrapped_means_blues &lt;- bootstrap_means(\n    real_sample_data |&gt;\n        filter(coin_colour == \"blue\") |&gt;\n        pull('outcome')  \n    )\n\n\n\n\nhead(bootstrapped_means_reds)\n\n\n[1] 0.65 0.70 0.85 0.85 1.00 0.60\n\n\nCode\nhead(bootstrapped_means_blues)\n\n\n[1] 0.45 0.60 0.50 0.45 0.70 0.55\n\n\nAnd what do these two distributions look like?\n\n\nCode\ntibble(\n    rep = 1:length(bootstrapped_means_reds),\n    red = bootstrapped_means_reds,\n    blue = bootstrapped_means_blues\n) |&gt;\n    pivot_longer(\n        cols = c(red, blue),\n        names_to = \"colour\",\n        values_to = \"value\"\n    ) |&gt;\n    ggplot(aes(x = value, fill = colour)) + \n    geom_histogram(bins = 50, position = \"dodge\")\n\n\n\n\n\nSo it’s clear the distributions for mean values of the two different coin types are different, even though there’s some overlap.\nLet’s now look at doing some post-stratification, where we sample from the two groups in proportion to the relative probabilities of encountering observations from the two groups in the population as compared with the sample. Let’s think through what this means:\n\nProportions by group in sample and population\n\n\nGroup\nSample\nPopulation\nRatio\n\n\n\n\nBlue\n0.5\n0.75\n\\(3/2\\)\n\n\nRed\n0.5\n0.25\n\\(1/2\\)\n\n\nColumn Sum\n1.00\n1.00\n\n\n\n\nIn this table, the ratio is the row-wise ratio of the population value divided by the sample value. Note that the ratios have a common denominator, 2, which we can drop in defining the probability weights, leaving us with 3 for blue and 1 for red.\nWe can adapt the standard bootstrapping approach by using the prob argument in the sample() function, using these weights:\n\n\nCode\nsample_weights &lt;- \n    tibble(\n        coin_colour = c(\"blue\", \"red\"),\n        wt = c(3, 1)\n    )\n\nreal_sample_data_wt &lt;- \n    left_join(\n        real_sample_data, sample_weights\n    )\n\nreal_sample_data_wt\n\n\n   coin_colour outcome wt\n1          red       1  1\n2          red       1  1\n3          red       1  1\n4          red       1  1\n5          red       1  1\n6          red       1  1\n7          red       1  1\n8          red       1  1\n9          red       0  1\n10         red       0  1\n11         red       1  1\n12         red       1  1\n13         red       0  1\n14         red       1  1\n15         red       1  1\n16         red       1  1\n17         red       1  1\n18         red       0  1\n19         red       1  1\n20         red       1  1\n21        blue       1  3\n22        blue       0  3\n23        blue       0  3\n24        blue       0  3\n25        blue       0  3\n26        blue       1  3\n27        blue       0  3\n28        blue       0  3\n29        blue       1  3\n30        blue       1  3\n31        blue       0  3\n32        blue       1  3\n33        blue       0  3\n34        blue       1  3\n35        blue       0  3\n36        blue       1  3\n37        blue       1  3\n38        blue       1  3\n39        blue       0  3\n40        blue       1  3\n\n\nAnd now a slightly modified version of the bootstrapping function:\n\n\nCode\nbootstrap_means_wt &lt;- function(x, wt, nReps = 10000){ #wt is the weighting\n    out &lt;- vector(\"numeric\", nReps) \n\n    for (i in 1:nReps){\n        this_resample &lt;- sample(\n            x=x, \n            size = length(x), \n            prob = wt, # This is the new argument\n            replace = TRUE # This is what makes it bootstrapping\n        )\n        out[i] &lt;- mean(this_resample)\n    }\n    out\n}\n\n\nAnd to run:\n\n\nCode\nbootstrapped_means_poststratified &lt;- bootstrap_means_wt(\n    x = real_sample_data_wt$outcome,\n    wt = real_sample_data_wt$wt\n)\n\nhead(bootstrapped_means_poststratified)\n\n\n[1] 0.750 0.550 0.625 0.525 0.575 0.600\n\n\nNow, analytically, we can calculate what the mean of the population should be given the proportion of blues and reds, and the proportion of blues that are heads, and proportion of reds that are heads:\n\n\nCode\nheads_if_blue &lt;- 0.47\nheads_if_red &lt;- 0.65\n\nexpected_pop_prop_heads &lt;- (3/4) * heads_if_blue + (1/4) * heads_if_red\n\nexpected_pop_prop_heads\n\n\n[1] 0.515\n\n\nSo within the population we would expect 51.5% of coins to come up heads.\nLet’s now look at the bootstrapped and reweighted distribution to see where 0.515 fits within this distribution:\n\n\nCode\nggplot() + \n    geom_histogram(aes(x = bootstrapped_means_poststratified), bins=50) + \n    geom_vline(aes(xintercept = expected_pop_prop_heads), linewidth = 1.2, colour = \"purple\")\n\n\n\n\n\nSo we can see that the true population mean falls within the reweighted bootstrapped distribution of the values of the mean estimated. How about if we had not performed reweighting on the sample?\n\n\nCode\ntibble(value = bootstrapped_means) |&gt;\n    ggplot() + \n    geom_histogram(aes(x = value), bins=50) + \n    geom_vline(aes(xintercept = expected_pop_prop_heads), linewidth = 1.2, colour = \"purple\")\n\n\n\n\n\nSo, although on this occasion, the true population value is also within the range of the un-reweighted bootstrapped distribution, it is further from the centre of this distribution’s mass.\nLet’s give some numbers to the above. What proportion of the bootstrapped values are below the true population value?\nFirst without reweighting:\n\n\nCode\nmean(bootstrapped_means &lt; expected_pop_prop_heads)\n\n\n[1] 0.0343\n\n\nOnly about 3.4% of the means from the unweighted bootstrapping were more extreme than the true population value.\nAnd now with reweighting:\n\n\nCode\nmean(bootstrapped_means_poststratified &lt; expected_pop_prop_heads)\n\n\n[1] 0.2102\n\n\nNow 22.4% of values of the means from the reweighted/post-stratified bootstrapped distribution are below the true value. This is the difference between the true value being in the 90% central interval or not."
  },
  {
    "objectID": "posts/glms/hacker-stats/post-stratification/index.html#summary",
    "href": "posts/glms/hacker-stats/post-stratification/index.html#summary",
    "title": "Resampling for post-stratification",
    "section": "Summary",
    "text": "Summary\nIn this post we’ve illustrated the importance of post-stratifying data were we know a sample is biased in terms of the relative weight given to the strata it contains as compared with the population. We’ve also shown, using Base R functions alone, how to perform this post-stratification using just two additional changes: a vector of weights, which was fairly straightforward to calculate; and the passing of this vector of weights to the prob argument in the sample() function.\nIn this post we’ve focused on a hypothetical example, and built the requisite functions and code from scratch. In practice, packages like survey can be used to perform post-stratification in fewer lines, svrep, and boot can make the process much more straightforward."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-02/index.html",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-02/index.html",
    "title": "Part Two: Systematic components and link functions",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-02/index.html#tldr",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-02/index.html#tldr",
    "title": "Part Two: Systematic components and link functions",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-02/index.html#part-2-systematic-components-and-link-functions",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-02/index.html#part-2-systematic-components-and-link-functions",
    "title": "Part Two: Systematic components and link functions",
    "section": "Part 2: Systematic components and link functions",
    "text": "Part 2: Systematic components and link functions\nIn part 1 of this series we introduced the following general framework for thinking about statistical models and what they contain.\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\] The terminology are as described previously.\nThese equations are too broad and abstract to be implemented directly. Instead, specific choices about the \\(f(.)\\) and \\(g(.)\\) need to be made. King, Tomz, and Wittenberg (2000) gives the following examples:\nLogistic Regression\n\\[\nY_i \\sim Bernoulli(\\pi_i)\n\\]\n\\[\n\\pi_i = \\frac{1}{1 + e^{-X_i\\beta}}\n\\]\nLinear Regression\n\\[\nY_i \\sim N(\\mu_i, \\sigma^2)\n\\] \\[\n\\mu_i = X_i\\beta\n\\]\nSo, what’s so special about linear regression, in this framework?\nIn one sense, not so much. It’s got a systematic component, and it’s got a stochastic component. But so do other models. But in another sense, quite a lot. It’s a rare case where the systematic component, \\(g(.)\\), doesn’t transform its inputs in some weird and wonderful way. We can say that \\(g(.)\\) is the identity transform, \\(I(.)\\), which in words means take what you’re given, do nothing to it, and pass it on.\nBy contrast, the systematic component for logistic regression is known as the logistic function. \\(logistic(x) := \\frac{1}{1 + e^{-x}}\\) It transforms inputs that could be anywhere on the real number line to values that lay somewhere between 0 and 1. Why 0 to 1? Because what logistic regression models produce aren’t predicted values, but predicted probabilities, and nothing can be more probable than certain (1) or less probable than impossible (0).\nWe can compare the transformations used in linear and logistic regression as follows:1\n\n# Define transformations\nident &lt;- function(x) {x}\nlgt &lt;- function(x) {1 / (1 + exp(-x))}\n\n\n# Draw the associations\ncurve(ident, -6, 6,\n      xlab = \"x (before transform)\",\n      ylab = \"z (after transform)\",\n      main = \"The Identity 'Transformation'\"\n      )\ncurve(lgt, -6, 6, \n      xlab = \"x (before transform)\", \n      ylab = \"z (after transform)\",\n      main = \"The Logistic Transformation\"\n      )\n\n\n\n\n\n\nIdentity Transformation\n\n\n\n\n\n\n\nLogistic Transformation\n\n\n\n\n\n\nThe usual input to the transformation function \\(g(.)\\) is a sum of products. For three variables, for example, this could be \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). In matrix algebra this generalises to \\(\\boldsymbol{X\\beta}\\) , where \\(\\boldsymbol{X}\\) is the predictor data whose rows are observations, columns are variables, and whose first column is a vector of 1s (for the intercept term). The \\(\\boldsymbol{\\beta}\\) term is a row-wise vector comprising each specific \\(\\beta\\) term, such as \\(\\boldsymbol{\\beta} = \\{ \\beta_0, \\beta_1, \\beta_2 \\}\\) in the three variable example above.\nWhat’s special about the identity transformation, and so linear regression, is that there is a fairly clear correspondence between a \\(\\beta_j\\) term and the estimated influence of changing a predictor variable \\(x_j\\) on the predicted outcome \\(Y\\), i.e. the ‘effect of \\(x_j\\) on \\(Y\\)’. For other transformations this tends to not be the case.\nWe’ll delve into how this is implemented in practice in part 3."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-02/index.html#footnotes",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-02/index.html#footnotes",
    "title": "Part Two: Systematic components and link functions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsing some base R graphics functions as I’m feeling masochistic↩︎"
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-04/index.html",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-04/index.html",
    "title": "Part Four: why only betas just look at betas",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-04/index.html#tldr",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-04/index.html#tldr",
    "title": "Part Four: why only betas just look at betas",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-04/index.html#part-4-why-overuse-of-linear-regression-leads-people-to-look-at-models-in-the-wrong-way",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-04/index.html#part-4-why-overuse-of-linear-regression-leads-people-to-look-at-models-in-the-wrong-way",
    "title": "Part Four: why only betas just look at betas",
    "section": "Part 4: Why overuse of linear regression leads people to look at models in the wrong way",
    "text": "Part 4: Why overuse of linear regression leads people to look at models in the wrong way\nIn the last post in this series I’ve reintroduced standard linear regression and logistic regression as both being special versions of the same generalised model formula.\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nWith standard linear regression the link function \\(g(.)\\) is \\(I(.)\\), i.e. the identity function, meaning what goes in, is what comes out. By contrast for logistic regression \\(g(.)\\) is the logistic function, which squishes and squashes any real number as an input onto a value between 0 and 1 as an output.\nThough it’s not always phrased this way, a motivating question behind the construction of most statistical models is, “What influence does a single input to the model, \\(x_j\\), have on the output, \\(Y\\)?”1 For a single variable \\(x_j\\) which is either present (1) or absent (0), this is in effect asking what is \\(E(Y | x_j = 1) - E(Y | x_j = 0)\\) ?2\nLet’s look at a linear regression case, then a logistic regression case.\n\nLinear Regression example\nUsing the iris dataset, let’s try to predict Sepal Width (a continuous variable) on Sepal Length (a continuous variable) and whether the species is setosa or not (a discrete variable). As a reminder, the data relating these three variables look as follows:\n\n\nCode\nlibrary(ggplot2)\n\niris |&gt;\n    ggplot(aes(Sepal.Length, Sepal.Width, group = Species, colour = Species, shape = Species)) + \n    geom_point()\n\n\n\n\n\nLet’s now build the model:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- iris |&gt; mutate(is_setosa = Species == 'setosa')\n\nmod_lm &lt;- lm(Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nmod_lm\n\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nCoefficients:\n  (Intercept)   Sepal.Length  is_setosaTRUE  \n       0.7307         0.3420         0.9855  \n\n\nThe coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}\\) are \\(\\{0.73, 0.34, 0.99\\}\\), and refer to the intercept, Sepal Length and is_setosa respectively.\nIf we assume a Sepel Length of 6, for example, then the expected Sepal Width (the thing we are predicting) is 0.73 + 6 * 0.34 + 0.99 or about 3.77 in the case where is_setosa is true, and 0.73 + 6 * 0.34 or about 2.78 where is_setosa is false.\nThe difference between these two values, 3.77 and 2.78, i.e. the ‘influence of setosa’ on the outcome, is 0.99, i.e. the \\(\\beta_2\\) coefficient shown before. In fact, for any conceivable (and non-conceivable, i.e. negative) value of Sepal Length, the difference is still 0.99.\nThis is the \\(\\beta_2\\) coefficient, and the reason why, for linear regression, and almost exclusively linear regression, looking at the coefficients themselves provides substantively meaningful information (something King, Tomz, and Wittenberg (2000) calls a ‘quantity of interest’) about the size of influence that a predictor has on a response.\n\n\nLogistic Regression example\nNow let’s look at an example using logistic regression. We will use another tiresomely familiar dataset, mtcars. We are interested in estimating the effect that having a straight engine (vs=1) has on the probability of the car having a manual transmission (am=1). Our model also tries to control for the miles-per-gallon (mpg). The model specification is shown, the model is run, and the coefficeints are all shown below:\n\n\nCode\nmod_logistic &lt;- glm(\n    am ~ mpg + vs,\n    data = mtcars, \n    family = binomial()\n    )\n\nmod_logistic\n\n\n\nCall:  glm(formula = am ~ mpg + vs, family = binomial(), data = mtcars)\n\nCoefficients:\n(Intercept)          mpg           vs  \n    -9.9183       0.5359      -2.7957  \n\nDegrees of Freedom: 31 Total (i.e. Null);  29 Residual\nNull Deviance:      43.23 \nResidual Deviance: 24.94    AIC: 30.94\n\n\nHere the coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}\\) are \\(\\{-9.92, 0.54, -2.80\\}\\), and refer to the intercept, mpg, and vs respectively.\nBut what does this actually mean, substantively?\n\n\n(Don’t) Stargaze\nA very common approach to trying to answer this question is to look at the statistical significance of the coefficients, which we can do with the summary() function\n\n\nCode\nsummary(mod_logistic)\n\n\n\nCall:\nglm(formula = am ~ mpg + vs, family = binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -9.9183     3.4942  -2.839  0.00453 **\nmpg           0.5359     0.1967   2.724  0.00644 **\nvs           -2.7957     1.4723  -1.899  0.05758 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 24.944  on 29  degrees of freedom\nAIC: 30.944\n\nNumber of Fisher Scoring iterations: 6\n\n\nA common practice in many social and health sciences is to offer something like a narrative summary of the above, something like:\n\nOur logistic regression model indicates that manualness is positively and significantly associated with our measure of fuel efficiency (p &lt; 0.01). There is also an indication of a negative association with straight engine, but this effect does not quite meet conventional thresholds for statistical significance (p &lt; 0.10).\n\nThis above practice is known as ‘star-gazing’, because summary tables like those above tend to have one or more * symbols in the final row, if the value of the Pr(&gt;|z|) is below 0.05, and narrative summaries like those just above tend to involve looking at the number of stars in each row, alongside whether the Estimate values have a minus sign in front of them.\nStar gazing is a very common practice. It’s also a terrible practice, which - ironically - turns the final presented output of a quantitative model into the crudest of qualitative summaries (positive, negative; significant, not significant). Star gazing is what researchers tend to default to when presented with model outputs from the above because, unlike in the linear regression example, the extent to which the \\(\\beta\\) coefficients answer substantive ‘how-much’-ness questions, like “How much does having a straight engine change the probability of manual transmission?, is not easily apparent from the coefficients themselves.\n\n\nStandardisation\nSo, how can we do better?\nOne approach is to standardise the data that goes into the model before passing them to the model. Standardisation means attempting to make the distribution and range of different variables more similar, and is especially useful when comparing between different continuous variables.\nTo give an example of this, let’s look at a specification with weight (wt) and horsepower (hp) in place of mpg, but keeping engine-type indicator (vs):\n\n\nCode\nmod_logistic_2 &lt;- glm(\n    am ~ vs + wt + hp,\n    data = mtcars, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2)\n\n\n\nCall:\nglm(formula = am ~ vs + wt + hp, family = binomial(), data = mtcars)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) 25.35510   11.24613   2.255   0.0242 *\nvs          -3.12906    2.92958  -1.068   0.2855  \nwt          -9.64982    4.05528  -2.380   0.0173 *\nhp           0.03242    0.01959   1.655   0.0979 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.5012  on 28  degrees of freedom\nAIC: 16.501\n\nNumber of Fisher Scoring iterations: 8\n\n\nHere both wt and hp are continuous variables.\nA star gazing zombie might say something like\n\nmanualness is negatively and significantly associated with weight (p &lt; 0.05); there is a positive association with horsepower but this does not meet standard thresholds of statistical significance (0.05 &lt; p &lt; 0.10).\n\nA slightly better approach would be to standardise the variables wt and hp before passing to the model. Standardising means trying to set the variables to a common scale, and giving the variables more similar statistical characteristics.\n\n\nCode\nstandardise &lt;- function(x){\n  (x - mean(x)) / sd(x)\n}\n\nmtcars_z &lt;- mtcars\nmtcars_z$wt_z = standardise(mtcars$wt)\nmtcars_z$hp_z = standardise(mtcars$hp)\n\nmod_logistic_2_z &lt;- glm(\n    am ~ vs + wt_z + hp_z,\n    data = mtcars_z, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2_z)\n\n\n\nCall:\nglm(formula = am ~ vs + wt_z + hp_z, family = binomial(), data = mtcars_z)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.9348     1.4500  -0.645   0.5191  \nvs           -3.1291     2.9296  -1.068   0.2855  \nwt_z         -9.4419     3.9679  -2.380   0.0173 *\nhp_z          2.2230     1.3431   1.655   0.0979 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.5012  on 28  degrees of freedom\nAIC: 16.501\n\nNumber of Fisher Scoring iterations: 8\n\n\nwt_z is the standardised version of wt, and hp_z is the standardised version of hp. By convention, whereas unstandardised coefficients are usually referred to as \\(\\beta\\) (‘beta’) coefficients, standardised coefficients are instead referred to as \\(b\\) coefficients. But really, it’s the same model.\nNote the p value of wt_z is the same as for wt, and the p value of hp_z is the same as that for hp. Note also the directions of effect are the same: the coefficients on wt and wt_z are both negative, and the coefficients of hp and hp_z are both positive.\nThis isn’t a coincidence. Of course standardising can’t really add any new information, can’t really change the relationship between a predictor and a response. It’s not really a new variable, it’s the same old variable, so the relationship between predictor and response that there used to be is still there now.\nSo why bother standardising?\nOne reason is it gives, subject to some assumptions and caveats, a way of gauging the relative importance of the two different continuous variables, by allowing a slightly more meaningful comparison between the two coefficients.\nIn this case, we have a standardised \\(b\\) coefficient of -9.44 for wt_z, and of 2.22 for hp_z. As with the unstandardised coefficients we can still assert that manualness is negatively associated with weight, and positively associated with horsepower. But now we can also compare the two numbers -9.44 and 2.22. The ratio of these two numbers is around 4.3. So, we might hazard to suggest something like:\n\na given increase in weight is around four times as important in negatively predicting manual transmission (i.e. in predicting an automatic transmission) as an equivalent increase in horsepower is in positively predicting manual transmission.\n\nThis isn’t a statement that’s easy to parse, but does at least allow slightly more information to be gleamed from the model. For example, it implies that, if a proposed change to a vehicle leads to similar relative (standardised) increases in both weight and horsepower then, as the weight effect is greater than the horsepower effect, the model will predict a decreased probability of manualness as a result.\nBut what about the motivating question, “What’s the effect of a straight engine (vs=1) on the probability of manual transmission (am=1)?”\nThe problem, unlike with the linear regression, is this is now a badly formulated question, based on an incorrect premise. The problem is with the word ‘the’, which implies there should be a single answer to this question, i.e. that the effect of vs on the probability of am=1 should always be the same. But, at least when it comes to absolute changes in the probability of am=1, this is no longer the case, as it depends on the values of the other variables in the model.\nInstead of assuming vs=1 has a single effect on P(am=1), we instead need to think about predictions of the marginal effects of vs on am in the context of other plausible values of the other predictors in the model, wt and hp. This involves asking the model a series of well formulated and specific questions.\n\n\nMaximum marginal effects: Divide-by-four\nBefore we do that, however, there’s a useful heuristic that can be employed when looking at discrete variables and using a logistic regression specification. The heuristic, which is based on the properties of the logistic function,3 is called divide-by-four. What this means is that, if we take the coefficient on vs of -3.13, and divide this value by four, we get a value of -0.78. Notice that the absolute value of -0.78 is between 0 and 1.4 What this value gives is the maximum possible effect that the discrete variable (the presence rather than absence of a straight engine) has on the probability of being a manual transmission. We can say, “a straight engine reduces the probability of a manual transmission by up to 78%”\nBut, as mentioned, this doesn’t quite answer the motivating question, it gives an upper bound to the answer, not the answer itself.5 We can instead start to get a sense of ‘the’ effect of the variable vs on P(am=1) by asking the model a series of questions.\n\n\nPredictions on a matrix\nWe can start by getting the range of observed values for the two continuous variables, hp and mpg:\n\n\nCode\nmin(mtcars$hp)\n\n\n[1] 52\n\n\nCode\nmax(mtcars$hp)\n\n\n[1] 335\n\n\nCode\nmin(mtcars$wt)\n\n\n[1] 1.513\n\n\nCode\nmax(mtcars$wt)\n\n\n[1] 5.424\n\n\nWe can then ask the model to make predictions of \\(P(am=1)\\) for a large number of values of hp and wt within the observed range, both in the condition in which vs=0 and in the condition in which vs=1. The expand_grid function6 can help us do this:\n\n\nCode\npredictors &lt;- expand_grid(\n  hp = seq(min(mtcars$hp), max(mtcars$hp), length.out = 100),\n  wt = seq(min(mtcars$wt), max(mtcars$wt), length.out = 100)\n)\n\npredictors_straight &lt;- predictors |&gt; \n  mutate(vs = 1)\n\npredictors_vshaped &lt;- predictors |&gt; \n  mutate(vs = 0)\n\n\nFor each of these permutations of inputs, we can use the model to get a conditional prediction. For convenience, we can also attach this as an additional column to the predictor data frame:\n\n\nCode\npredictions_predictors_straight &lt;- predictors_straight |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_straight)\n  )\n\npredictions_predictors_vshaped &lt;- predictors_vshaped |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_vshaped)\n  )\n\n\nWe can see how the predictions vary over hp and wt using a heat map or contour map:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  ggplot(aes(x = hp, y = wt, z = p_manual)) + \n  geom_contour_filled() + \n  facet_wrap(~vs) +\n  labs(\n    title = \"Predicted probability of manual transmission by wt, hp, and vs\"\n  )\n\n\n\n\n\nWe can also produce a contour map of the differences between these two contour maps, i.e. the effect of a straight (vs=1) compared with v-shaped (vs=0) engine, which gets us a bit closer to the answer:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled() + \n  labs(\n    title = \"Marginal effect of vs=1 given wt and hp on P(am=1)\"\n  )\n\n\n\n\n\nWe can see here that, for large ranges of wt and hp, the marginal effect of vs=1 is small. However, for particular combinations of hp and wt, such as where hp is around 200 and wt is slightly below 3, then the marginal effect of vs=1 becomes large, up to around a -70% reduction in the probability of manual transmission. (i.e. similar to the theoretical maximum marginal effect of around -78%).\nSo, what’s the effect of vs=1 on P(am=1)? i.e. how should we boil down all these 10,000 predicted effect sizes into a single effect size?\nI guess, if we have to try to answer this silly question, then we could take the average effect size…\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  summarise(\n    mean_diff_p_manual = mean(diff_p_manual)\n  )\n\n\n# A tibble: 1 × 1\n  mean_diff_p_manual\n               &lt;dbl&gt;\n1            -0.0821\n\n\nSo, we get an average difference of around -0.08, i.e. about an 8% reduction in probability of manual transmission.\n\n\nMarginal effects on observed data\nIs this a reasonable answer? Probably not, because although the permutations of wt and hp we looked at come from the observed range, most of these combinations are likely very ‘theoretical’. We can get a sense of this by plotting the observed values of wt and hp onto the above contour map:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled(alpha = 0.2, show.legend = FALSE) + \n  labs(\n    title = \"Observations from mtcars on the predicted probability surface\"\n  ) +\n  geom_point(\n    aes(x = hp, y = wt), inherit.aes = FALSE,\n    data = mtcars\n  )\n\n\n\n\n\nPerhaps a better option, then, would be to calculate an average marginal effect using the observed values, but switching the observations for vs to 1 in one scenario, and 0 in another scenario:\n\n\nCode\npredictions_predictors_observed_straight &lt;- mtcars |&gt; \n  select(hp, wt) |&gt; \n  mutate(vs = 1)\n\npredictions_predictors_observed_straight &lt;- predictions_predictors_observed_straight |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_straight)\n  )\n\npredictions_predictors_observed_vshaped &lt;- mtcars |&gt; \n  select(hp, wt) |&gt; \n  mutate(vs = 0) \n\npredictions_predictors_observed_vshaped &lt;- predictions_predictors_observed_vshaped |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_vshaped)\n  )\n  \n\npredictions_predictors_observed &lt;- \n  bind_rows(\n    predictions_predictors_observed_straight,\n    predictions_predictors_observed_vshaped\n  )\n\npredictions_marginal &lt;- \n  predictions_predictors_observed |&gt; \n    group_by(hp, wt) |&gt; \n    summarise(\n      diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n    )\n\npredictions_marginal |&gt; \n  ggplot(aes(x = diff_p_manual)) + \n  geom_histogram() +\n  geom_vline(aes(xintercept = mean(diff_p_manual)), colour = \"red\") + \n  geom_vline(aes(xintercept = median(diff_p_manual)), colour = \"green\")\n\n\n\n\n\nIn the above the red line indicates the mean value of these marginal differences, which is -0.12, and the green line the median value of these differences, which is around -0.02. So, even with just these two measures of central tendency, there’s around a six-fold difference in the estimate of ‘the effect’. We can also see there’s a lot of variation, from around nothing (right hand side), to around a 65% reduction (left hand side).\nIf forced to give a simple answer (to this overly simplistic question), we might plump for the mean for theoretical reasons, and say something like “The effect of a straight engine is to reduce the probability of a manual transmission by around an eighth”. But I’m sure, having seen how much variation there is in these marginal effects, we can agree this ‘around an eighth’ answer, or any single number answer, is likely to be overly reductive.\nHopefully, however, it is more informative than ‘statistically significant and negative’, (the stargazing approach) or ‘up to around 78%’ (the divide-by-four approach)."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-04/index.html#conclusion",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-04/index.html#conclusion",
    "title": "Part Four: why only betas just look at betas",
    "section": "Conclusion",
    "text": "Conclusion\nLinear regression tends to give a false impression about how straightforward it is to use a model to answer questions of the form “What is the effect of x on y?”. This is because, for linear regression, but few other model specifications, the answer to this question is in the \\(\\beta\\) coefficients themselves. For other model specifications, like the logistic regression example above, the correct-but-uninformative answer tends to be “it depends”, and potentially more informative answers tend to require a bit more work to derive and interpret."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-04/index.html#coming-up",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-04/index.html#coming-up",
    "title": "Part Four: why only betas just look at betas",
    "section": "Coming up",
    "text": "Coming up\nThis post concludes the first section of this blog series. We showed the importance of producing predictions from models, rather than just staring at tables of coefficients and producing qualitative ‘stargazing’ summaries of their statistical significance and direction of effect. Statistical significance of individual coefficients almost never answers questions of substantive significance, which instead come from model predictions.\nHowever in the predictions so far, we’ve accidentially pretended to know more than we do. For each prediction, despite imperfect knowledge, we’ve presented point estimates, a single prediction, implying our estimates are sometimes perfectly precise. To be more honest to the user, we should instead present a range of estimates that takes into account all the sources of uncertainty in our modelling which lead to uncertainty in our predictions.\nSection two of this blog series, starting with part five, takes us through the material necessary to go from presenting the kind of dishonest certainty of point estimates in predictions, to honest uncertainty in predictive intervals. This involves covering a lot of theoretical and methodological territory, and is fairly challenging. However we do this in order to make it easier for the end user of statistical models, decision makers, to get the kind of information they need and value most from models."
  },
  {
    "objectID": "posts/glms/intro-to-glms/lms-are-glms-part-04/index.html#footnotes",
    "href": "posts/glms/intro-to-glms/lms-are-glms-part-04/index.html#footnotes",
    "title": "Part Four: why only betas just look at betas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote here I’m using \\(x_j\\), not \\(x_i\\), and that \\(X\\beta\\) is shorthand for \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) and so on. In using the \\(j\\) suffix, I’m referring to just one of the specific \\(x\\) values, \\(x_1\\), \\(x_2\\), \\(x_3\\), which is equivalent to selecting one of the columns in \\(X\\). By contrast \\(i\\) should be considered shorthand for selection of one of the rows of \\(X\\), i.e. one of the series of observations that goes into the dataset \\(D\\).↩︎\n\\(E(.)\\) is the expectation operator, and \\(|\\) indicates a condition. So, the two terms mean, respectively, what is the expected value of the outcome if the variable of interest is ‘switched on’?, and what is the expected value of the outcome if the variable of interest is ‘switched off’?↩︎\nThe logistic function maps any real number z onto the value range 0 to 1. z is \\(X\\beta\\), which in non-matrix notation is equivalent to a sum of products \\(\\sum_{k=0}^{K}x_k\\beta_k\\) (where, usually, \\(x_0\\) is 1, i.e. the intercept term). Another way of expressing this would be something like \\(\\sum_{k \\in S}x_k\\beta_k\\) where by default \\(S = \\{0, 1, 2, ..., K\\}\\). We can instead imagine partitioning out \\(S = \\{S^{-J}, S^{J}\\}\\) where the superscript \\(J\\) indicates the Jth variable, and \\(-J\\) indicates everything in \\(S\\) apart from the Jth variable. Where J is a discrete variable, the effect of J on \\(P(Y=1)\\) is \\(logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k + \\beta_J}) - logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k})\\), where \\(logistic(z) = \\frac{1}{1 + e^{-z}}\\). The marginal effect of the \\(\\beta_J\\) coefficient thus depends on the other term \\(\\sum_{k \\in S^{-J}}x_k\\beta_k\\). Where this other term is set to 0 the marginal effect of \\(\\beta_J\\) becomes \\(logistic(\\beta_J) - logistic(0)\\). According to p.82 of this chapter by Gelman we can equivalently ask the question ‘what is the first derivative of the logistic regression with respect to \\(\\beta\\)?’. Asking more about this to Wolfram Alpha we get this page of information, and scrolling down to the section on the global minimum we indeed get an absolute value of \\(\\frac{1}{4}\\), so the maximum change in \\(P(Y=1)\\) given a unit change in \\(\\beta\\) is indeed one quarter of the value of \\(\\beta\\), hence why the ‘divide-by-four’ heuristic ‘works’. This isn’t quite a full derivation, but more explanation than I was planning for a footnote! In general, it’s better just to remember ‘divide-by-four’ than go down the rabbit warren of derivation each time! (As I’ve just learned, to my cost, writing this footnote!)↩︎\nWe should always expect the absolute value of a coefficient for a discrete variable to be less than four, for this reason.↩︎\nThe lower bound for the marginal effect of a discrete variable, or any variable, is zero. This is when the absolute value of the sum of the product of the other variables is infinite.↩︎\nOr the base R expand.grid function↩︎"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "",
    "text": "The last part of the series ended by showing how, except for linear regression, the kinds of answers users need from models aren’t in tables of model coefficients, but in model predictions. However, we only showed predictions as point estimates, giving the false and unreasonable impression that the model predicts with perfect accuracy.\nThis post, start of the the second section of the series, provides the theoretical and methodological background necessary to produce model predictions which are more honest, providing predictions with uncertainty intervals relating both to uncertainty about how the model has been fit, and fundamental variability in the stochastic processes that the statistical models aim to simulate. It’s likely to be the most challenging part of the blog series, but worth sticking with."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#recap",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#recap",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "",
    "text": "The last part of the series ended by showing how, except for linear regression, the kinds of answers users need from models aren’t in tables of model coefficients, but in model predictions. However, we only showed predictions as point estimates, giving the false and unreasonable impression that the model predicts with perfect accuracy.\nThis post, start of the the second section of the series, provides the theoretical and methodological background necessary to produce model predictions which are more honest, providing predictions with uncertainty intervals relating both to uncertainty about how the model has been fit, and fundamental variability in the stochastic processes that the statistical models aim to simulate. It’s likely to be the most challenging part of the blog series, but worth sticking with."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#aim",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#aim",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Aim",
    "text": "Aim\nIn the first part of this series, I stated that statistical model fitting, within the generalised model framework presented in King, Tomz, and Wittenberg (2000), involves adjusting candidate values for elements of \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_K \\}\\) such that the difference between what the model predicts given some predictor values, \\(Y_i | X_i\\), and what has been observed alongside the predictors, \\(y_i\\), is minimised on average1 in some way.\nThe aim of this post is to show how this process is typically implemented in GLMs, using likelihood theory."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#bayes-rule-and-likelihood",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#bayes-rule-and-likelihood",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Bayes’ Rule and Likelihood",
    "text": "Bayes’ Rule and Likelihood\nStatisticians and more advanced users of statistical models often divide themselves into ‘frequentists’ and ‘Bayesians’. To some extent the distinction is really between ‘improper Bayesians’ and ‘proper Bayesians’, however, as Bayes’ Rule is at the root of both approaches. Bayes’ Rule is:\n\\[\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\\]\nNote in the above the left hand side of the equation is \\(P(A|B)\\) and the right hand side of the equation includes \\(P(B|A)\\). To write it out as awkward prose, therefore, Bayes’ Rule is a way of expressing that given this in terms of this given that.\nAs with much of algebra, \\(A\\) and \\(B\\) are just placeholders. We could instead use different symbols instead, such as:\n\\[\nP(\\tilde{\\theta} | y) = \\frac{P(y | \\tilde{\\theta})P(\\tilde{\\theta})}{P(y)}\n\\]\nLikelihood theory offers a way of thinking about how good a model is in terms of its relationship to the data. According to King (1998) (p. 59), it can be expressed as:\n\\[\nL(\\tilde{\\theta}| y) = k(y) P(y | \\tilde{\\theta})\n\\]\nOr\n\\[\nL(\\tilde{\\theta} | y) \\propto P(y | \\tilde{\\theta})\n\\]\nWhere \\(\\tilde{\\theta}\\) is a proposed parameter or parameter combination for the model, and \\(y\\) is the observed outcome.2\nThe important thing to note is that both Bayes’ Rule and Likelihood Theory are ways of expressing this given that as a function of that given this. Specifically, the model given the data, as a function of the data given the model. 3"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#likelihood-for-linear-regression",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#likelihood-for-linear-regression",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Likelihood for linear regression",
    "text": "Likelihood for linear regression\nWhen, many years ago, I completed the course from this modelling framework is most associated, a hazing ritual employed near the start of the course was to require participants to derive the likelihood of different model specifications. However, I don’t feel like hazing myself right now, so instead we can use the derivation shown on slide 8 of these slides:\n\\[\nL(\\beta, \\sigma^2 | y) = \\prod{L(y_i | \\mu_i, \\sigma^2)}\n\\]\nWhere \\(\\mu = X \\beta\\), \\(i\\) indicates an observation in the data (a row of \\(X\\) when \\(X\\) is in matrix form), and \\(\\prod\\) indicates the likelihoods from each observation should be multiplied with each other to derive the overall likelihood for all observed data.\nIn practice the log Likelihood, rather than the likelihood itself, is used, because this allows calculation of a sum of terms (\\(\\sum\\)) rather than product of terms (\\(\\prod\\)), and the latter tends to be computationally easier to calculate.\nAs we are interested only in how likelihood varies as a function of those model parameters we wish to estimate, \\(\\theta = \\{\\beta, \\sigma^2\\}\\), some of the terms in the log likelihood expression can be omitted, leaving us with:\n\\[\n\\log{L(\\beta, \\sigma^2 | y)} \\doteq \\sum{-\\frac{1}{2}[\\log{\\sigma^2} + \\frac{(y_i - X_i\\beta)^2}{\\sigma^2}]}\n\\]\nFor all the complexity of the above expression, at heart it takes three inputs:\n\n\\(\\theta = \\{\\beta, \\sigma^2\\}\\) : The candidate parameters for the model.\n\\(y\\) : the observed response value from the dataset \\(D\\)\n\\(X\\) : the observed predictor values from the dataset \\(D\\)\n\nAnd returns one value, the log likelihood \\(\\log{L(.)}\\).\nTo reiterate, we can’t change the data, but we can keep changing the candidate parameters \\(\\theta\\). Each time we do so, \\(\\log{L(.)}\\) will change too.\nThe aim of model calibration, in the Likelihood framework, is to maximise the Likelihood. The parameter set that maximises the likelihood is also the parameter set that maximises the log likelihood.\nTo continue the example from the slides, we can write out a function for calculating the log likelihood of standard linear regression as follows:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\nIn the above, pars is (almost but not quite) \\(\\theta\\), the parameters to estimate. For standard linear regression \\(\\theta = \\{\\beta, \\sigma^2\\}\\), where \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_k\\}\\), i.e. a vector of beta parameters, one for each column (variable) in \\(X\\), the predictor matrix of observations; this is why \\(beta\\) is selected from the first K values in pars where K is the number of columns in \\(X\\).\nThe last value in pars is used to derive the proposed \\(\\sigma^2\\). If we call this last value eta (\\(\\eta\\)), then we can say \\(\\sigma^2 = e^{\\eta}\\). So, whereas \\(\\theta\\) is a vector that ‘packs’ \\(\\beta\\) and \\(\\sigma^2\\) into a single ordered series of values, pars packs eta in place of \\(\\sigma^2\\). This substitution of eta for \\(\\sigma^2\\) is done to make it easier for standard parameter fitting algorithms to work, as they tend to operate over the full real number range, rather than just over positive values.\nIn order to illustrate how the log likelihood function llNormal works in practice, let’s construct a simple toy dataset \\(D\\), and decompose \\(D = \\{y, X\\}\\), the two types of data input that go into the llNormal function.\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nIn the code above we have created \\(y\\), a vector of nine observed responses; and \\(X\\), a matrix of predictors with two columns (the number of variables for which \\(beta\\) terms need to be estimated) and nine rows (the number of observations).\nGraphically, the relationship between x and y looks as follows:\n\n\nCode\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()\n\n\n\n\n\nIn this toy example, but almost never in reality, we know the correct parameters for the model. These are \\({\\beta_0 = 2.5, \\beta_1 = 1.4}\\) and \\(\\sigma^2 = 0.25\\). 4 Soon, we will see how effectively we can use optimisation algorithms to recover these true model parameters. But first, let’s see how the log likelihood varies as a function jointly of different candidate values of \\(\\beta_0\\) (the intercept) and \\(\\beta_1\\) (the slope parameter), if we already set \\(\\sigma^2\\) to 0.25.\n\n\nCode\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-5, 5, by = 0.1),\n    beta_1 = seq(-5, 5, by = 0.1)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\n\n\n\nCode\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nLooking at this joint surface of values, we can see a ‘hotspot’ where \\(\\beta_0\\) is around 2.5, and \\(\\beta_1\\) is around 1.4, just as we should expect. We can check this further by filtering candidate_param_values on the highest observed values of ll.\n\n\nCode\ncandidate_param_values |&gt; \n    filter(ll == max(ll))\n\n\n# A tibble: 1 × 3\n  beta_0 beta_1    ll\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    2.4    1.4  1.41"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#summary",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#summary",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Summary",
    "text": "Summary\nWe have now introduced the concepts of Bayes Rule, Likelihood, and log likelihood, then derived the log likelihood for standard linear regression. We then built a toy dataset where we know the true parameters, and looked at how the log likelihood varies as different \\(\\beta\\) parameters are proposed. We identified a ‘hot spot’ when the \\(\\beta\\) parameters proposed are close to the ‘true values’."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#coming-up",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#coming-up",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Coming up",
    "text": "Coming up\nThe next part of this series shows how log likelihood functions tend to be used in practice, in conjunction with optimisation algorithms that (usally) arrive at good estimates of our coefficients in far fewer steps than we’ve used above."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#footnotes",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-05/index.html#footnotes",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf \\(Y_i\\) is what the model predicts given observations \\(X_i\\), and \\(y_i\\) is the outcome observed to have occurred alongside \\(X_i\\), then we can call \\(\\delta_i = h(y_i, Y_i)\\) the difference, or error, between predicted and observed value. The function \\(h(.,.)\\) is typically the squared difference between predicted and observed values, \\((Y_i - y_i)^2\\), but could also in principle be the absolute difference \\(|Y_i - y_i|\\). Term-fitting algorithms usually compare not any individual \\(\\delta_i\\), but a sum of these error terms \\(\\delta\\). The aim of the algorithm is to find the set of \\(\\beta\\) terms that is least wrong for the whole dataset \\(D\\), rather than any specific row in the dataset \\(D_i\\).↩︎\nAs King (1998) (p. 59) describes it, “\\(k(y)\\) is an unknown fuction of the data. Whereas traditional probability is a measure of absolute uncertainty … the constant \\(k(y)\\) means that likelihood is only a relative measure of uncertainty”↩︎\nFrequentist approaches can thus be considered a kind of ‘improper Bayesian’ approach by considering \\(k(y)\\) in the Likelihood formula as a stand-in for \\(\\frac{P(\\tilde{\\theta})}{P(y)}\\) in Bayes’ Rule. Roughly speaking, it’s because of the improperness of treating the two terms as equivalent, and the relativeness of \\(k(y)\\), that mean frequentist probability statements can’t be interpreted as Bayesian probability statements. But thinking of the two terms as equivalent can be helpful for spotting the similarity between the two formulae.↩︎\ni.e. the square of the sd passed to rnorm() of 0.5↩︎"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "",
    "text": "In the previous post in this series, I presented a function for calculating the log likelihood of a standard linear regression with a Normal error term. I then built a very simple dataset, ten data points linking \\(x\\) to \\(y\\), and showed how the log likelihood varied as a combination of different candidate values for the model’s intercept and slope terms (\\(\\beta_0\\) and \\(\\beta_1\\) respectively).\nThe aim of this this post is to show how the best parameter combinations tend to be estimated from a model’s log likelihood in practice, using an optimisation algorithm that iteratively tries out new parameter values, and keeps trying and trying until some kind of condition is met. This is what the last figure in the first post is trying to illustrate."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#aim",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#aim",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "",
    "text": "In the previous post in this series, I presented a function for calculating the log likelihood of a standard linear regression with a Normal error term. I then built a very simple dataset, ten data points linking \\(x\\) to \\(y\\), and showed how the log likelihood varied as a combination of different candidate values for the model’s intercept and slope terms (\\(\\beta_0\\) and \\(\\beta_1\\) respectively).\nThe aim of this this post is to show how the best parameter combinations tend to be estimated from a model’s log likelihood in practice, using an optimisation algorithm that iteratively tries out new parameter values, and keeps trying and trying until some kind of condition is met. This is what the last figure in the first post is trying to illustrate."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#optimisation-algorithms-getting-there-faster",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#optimisation-algorithms-getting-there-faster",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Optimisation algorithms: getting there faster",
    "text": "Optimisation algorithms: getting there faster\nIn the previous post, we ‘cheated’ a bit when using the log likelihood function, fixing the value for one of the parameters \\(\\sigma^2\\) to the value we used when we generated the data, so we could instead look at how the log likelihood surface varied as different combinations of \\(\\beta_0\\) and \\(\\beta_1\\) were plugged into the formula. \\(\\beta_0\\) and \\(\\beta_1\\) values ranging from -5 to 5, and at steps of 0.1, were considered: 101 values of \\(\\beta_0\\), 101 values of \\(\\beta_1\\), and so over 10,0001 unique \\(\\{\\beta_0, \\beta_1\\}\\) combinations were stepped through. This approach is known as grid search, and seldom used in practice (except for illustration purposes) because the number of calculations involved can very easily get out of hand. For example, if we were to use it to explore as many distinct values of \\(\\sigma^2\\) as we considered for \\(\\beta_0\\) and \\(\\beta_1\\), the total number of \\(\\{\\beta_0, \\beta_1, \\sigma^2 \\}\\) combinations we would crawl through would be over 100,000 2 rather than over 10,000.\nOne feature we noticed with the likelihood surface over \\(\\beta_0\\) and \\(\\beta_1\\) in the previous post is that it appears to look like a hill, with a clearly defined highest point (the region of maximum likelihood) and descent in all directions from this highest point. Where likelihood surfaces have this feature of being single-peaked in this way (known as ‘unimodal’), then a class of algorithms known as ‘hill climbing algorithms’ can be applied to find the top of such peaks in a way that tends to be both quicker (fewer steps) and more precise than the grid search approach used for illustration in the previous post."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#code-recap",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#code-recap",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Code recap",
    "text": "Code recap\nLet’s copy over the code we used in the previous post for:\n\n\nCalculating log likelihood\n\n\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\nAnd\n\n\nGenerating our tame toy dataset of 10 data points\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nTo recap, the toy dataset looks as follows:\n\n\nCode\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#optim-our-robo-chauffeur",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#optim-our-robo-chauffeur",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "optim: our Robo-Chauffeur",
    "text": "optim: our Robo-Chauffeur\nNote how the llNormal function takes a single argument, pars, which packages up all the specific candidate parameter values we want to try out. In our previous post, we also had a ‘feeder function’, feed_to_ll, which takes the various \\(\\beta\\) candidate values from the grid and packages them into pars. In our previous post, we had to specify the candidate values to try to feed to llNormal packages inside pars.\nBut we don’t have to do this. We can instead use an algorithm to take candidate parameters, try them out, then make new candidate parameters and try them out, for us. Much as a taxi driver needs to know where to meet a passenger, but doesn’t want the passenger to tell them exactly which route to take, we just need to specify a starting set of values for the parameters to optimise. R’s standard way of doing this is with the optim function. Here’s it in action:\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe optim function returns a fairly complex output structure, with the following components:\n\npar: the values for the parameters (in our case \\(\\{\\beta_0, \\beta_1, \\eta \\}\\)) which the optimisation algorithm ended up with.\nvalue: the value returned by the function fn when the optim routine was stopped.\ncounts: the number of times the function fn was repeatedly called by optim before optim decided it had had enough\nconvergence: whether the algorithm used by optim completed successfully (i.e. reached what it considers a good set of parameter estimates in par), or not.\n\nIn this case, convergence is 0, which (perhaps counterintuitively) indicates a successful completion. counts indicates that optim called the log likelihood function 216 times before stopping, and par indicates values of \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\eta = -1.34\\}\\) were arrived at. As \\(\\sigma^2 = e^\\eta\\), this means \\(\\theta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\sigma^2 = 0.26 \\}\\). As a reminder, the ‘true’ values are \\(\\{\\beta_0 = 2.50, \\beta_1 = 1.40, \\sigma^2 = 0.25\\}\\).\nSo, the optim algorithm has arrived at pretty much the correct answers for all three parameters, in 216 calls to the log likelihood function, whereas for the grid search approach in the last post we made over 10,000 calls to the log likelihood function for just two of the three parameters.\nLet’s see if we can get more information on exactly what kind of path optim took to get to this set of parameter estimates. We should be able to do this by specifying a value in the trace component in the control argument slot…"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#comparisons",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#comparisons",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Comparisons",
    "text": "Comparisons\nFor comparison let’s see what lm and glm produce.\nFirst lm:\n\n\nCode\ntoy_df &lt;- tibble(\n    x = x, \n    y = y\n)\n\n\nmod_lm &lt;- lm(y ~ x, data = toy_df)\nsummary(mod_lm)\n\n\n\nCall:\nlm(formula = y ~ x, data = toy_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6082 -0.3852 -0.1668  0.2385  1.1092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5813 on 7 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 \nF-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07\n\n\n\\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\), i.e. the same to 2 decimal places.\nAnd now with glm:\n\n\nCode\nmod_glm &lt;- glm(y ~ x, data = toy_df, family = gaussian(link = \"identity\"))\n\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nOnce again, \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\)"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#discussion",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#discussion",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Discussion",
    "text": "Discussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#coming-up",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#coming-up",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Coming up",
    "text": "Coming up\nIn the next part of this series, we’ll see how other outputs available from optim can be used to estimate uncertainty in the parameters of interest, how this information can be used to produce the kinds of estimates of standard errors around coefficients which are summarised in glm and lm summary() functions, and which many (ab)users of statistical models obsess about when star-gazing, and how information about uncertainty in parameter estimates allows for more honest model-based predictions."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#footnotes",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-06/index.html#footnotes",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(101^2 = 10201\\)↩︎\n\\(101^3 = 1030301\\)↩︎"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "",
    "text": "The last eight posts in this series have taken us into some fairly arcane territory, including concepts like the use of link functions and statistical families within GLM, the likelihood theory of inference and its relation to Bayes’ Rule, and how models are fit in practice using optimisation algorithms. In the last couple of posts we showed how optim(), R’s standard optimisation function, can be used to recover not just the maximum likelihood (point) estimates of a series of parameters to be estimated, but also estimates of how much uncertainty there is about these estimates: both singularly, which gives rise to measures like standard errors, Z scores and P-values - the place where sadly all too many statistical analyses stop at; and jointly, through the calculation of the Hessian and corresponding variance-covariance matrix of uncertainty about the parameter vector.\nIn the last post, we showed how known uncertainty about the parameter values in the statistical model can be represented by using the point estimates \\(\\dot{\\theta}\\) and variance-covariance measure of uncertainty \\(\\Sigma\\) can be used to produce a long series of plausible joint estimates of \\(\\tilde{\\theta}\\) (the parameter estimates with uncertainty) by passing the above as parameters to the multivariate normal distribution and taking repeated draws."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#recap",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#recap",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "",
    "text": "The last eight posts in this series have taken us into some fairly arcane territory, including concepts like the use of link functions and statistical families within GLM, the likelihood theory of inference and its relation to Bayes’ Rule, and how models are fit in practice using optimisation algorithms. In the last couple of posts we showed how optim(), R’s standard optimisation function, can be used to recover not just the maximum likelihood (point) estimates of a series of parameters to be estimated, but also estimates of how much uncertainty there is about these estimates: both singularly, which gives rise to measures like standard errors, Z scores and P-values - the place where sadly all too many statistical analyses stop at; and jointly, through the calculation of the Hessian and corresponding variance-covariance matrix of uncertainty about the parameter vector.\nIn the last post, we showed how known uncertainty about the parameter values in the statistical model can be represented by using the point estimates \\(\\dot{\\theta}\\) and variance-covariance measure of uncertainty \\(\\Sigma\\) can be used to produce a long series of plausible joint estimates of \\(\\tilde{\\theta}\\) (the parameter estimates with uncertainty) by passing the above as parameters to the multivariate normal distribution and taking repeated draws."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#aim",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#aim",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Aim",
    "text": "Aim\nIn this post, we’ll now, finally, show how this knowledge can be applied to do something with statistical models that ought to be done far more often: report on what King, Tomz, and Wittenberg (2000) calls quantities of interest, including predicted values, expected values, and first differences. Quantities of interest are not the direction and statistical significance (P-values) that many users of statistical models convince themselves matter, leading to the kind of mindless stargazing summaries of model outputs described in post four. Instead, they’re the kind of questions that someone, not trained to think that stargazing is satisfactory, might reasonably want answers to. These might include:\n\nWhat is the expected income of someone who completes course X in the five years after graduation? (Expected values)\nWhat is the expected range of incomes of someone who completes course X in the five years after graduation? (Predicted values)\nWhat is the expected difference in incomes between someone who completes course X, compared to course Y, in the five years after graduation? (First Differences)\n\nIn post four, we showed how to answer some of the questions of this form, for both standard linear regression and logistic regression. We showed that for linear regression such answers tend to come directly from the summary of coefficients, but that for logistic regression such answers tend to be both more ambiguous and dependent on other factors (such as gender of graduate, degree, ethnicity, age and so on), and require more processing in order to produce estimates for.\nHowever, we previously produced only point estimates for these questions, and so in a sense misled the questioner with the apparent certainty of our estimates. We now know, from post eight, that we can use information about parameter uncertainty to produce parameter estimates \\(\\tilde{\\theta}\\) that do convey parameter uncertainty, and so we can do better than the point estimates alone to answer such questions in way that takes into account such uncertainty, with a range of values rather than a single value."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#method",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#method",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Method",
    "text": "Method\nLet’s make use of our toy dataset one last time, and go through the motions to produce the \\(\\tilde{\\theta}\\) draws we ended with on the last post:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\n\n\nCode\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\n\nLet’s now look at our toy data again, and decide on some specific questions to answer:\n\n\nCode\nlibrary(tidyverse)\ntoy_df &lt;- tibble(x = x, y = y)\n\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() \n\n\n\n\n\nWithin the data itself, we have only supplied x and y values for whole numbers of x between -3 and 5. But we can use the model to produce estimates for non-integer values of x. Let’s try 2.5. For this single value of x, we can produce both predicted values and expected values, by passing the same value of x to each of the plausible estimates of \\(\\theta\\) returned by the multivariate normal function above.\n\n\nCode\ncandidate_x &lt;- 2.5"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#expected-values",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#expected-values",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Expected values",
    "text": "Expected values\nHere’s an example of estimating the expected value of y for x = 2.5 using loops and standard algebra:\n\n\nCode\n# Using standard algebra and loops\nN &lt;- nrow(param_draws)\nexpected_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    expected_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"]\n}\n\nhead(expected_y_simpler)\n\n\n[1] 6.004068 5.859547 6.121791 5.987509 5.767047 6.395820\n\n\nWe can see just from the first few values that each estimate is slightly different. Let’s order the values from lowest to highest, and find the range where 95% of values sit:\n\n\nCode\nev_range &lt;- quantile(expected_y_simpler,  probs = c(0.025, 0.500, 0.975)) \n\nev_range\n\n\n    2.5%      50%    97.5% \n5.505104 5.898148 6.291150 \n\n\nThe 95% interval is therefore between 5.51 and 6.29, with the median (similar but not quite the point estimate) being 5.90. Let’s plot this against the data:\n\n\nCode\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  median(expected_y_simpler), size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = ev_range[1], yend = ev_range[3], colour = \"blue\")\n\n\n\n\n\nThe vertical blue line therefore shows the range of estimates for \\(Y|x=2.5\\) that contain 95% of the expected values given the draws of \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which we produced from the Multivariate Normal given the point estimates and Hessian from optim(). This is our estimated range for the expected value, not predicted value. What’s the difference?"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#predicted-values",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#predicted-values",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Predicted values",
    "text": "Predicted values\nOne clue about the difference between expected value lies in the parameters from optim() we did and did not use: Whereas we have both point estimates and uncertainty estimates for the parameters \\(\\{\\beta_0, \\beta_1, \\sigma^2\\}\\),1 we only made use of the the two \\(\\beta\\) parameters when producing this estimate.\nNow let’s recall the general model formula, from the start of King, Tomz, and Wittenberg (2000), which we repeated for the first few posts in the series:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nThe manual for Zelig, the (now defunct) R package that used to support analysis using this approach, states that for Normal Linear Regression these two components are resolved as follows:\nStochastic Component\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\mu_i = x_i \\beta\n\\]\nThe page then goes onto state that the expected value, \\(E(Y)\\), is :\n\\[\nE(Y) = \\mu_i = x_i \\beta\n\\]\nSo, in this case, the expected value is the systematic component only, and does not involve the dispersion parameter in the stochastic component, which for normal linear regression is the \\(\\sigma^2\\) term. That’s why we didn’t use estimates of \\(\\sigma^2\\) when simulating the expected values.\nBut why is this? Well, it comes from the expectation operator, \\(E(.)\\). This operator means something like, return to me the value that would be expected if this experiment were performed an infinite number of times.\nThere are two types of uncertainty which give rise to variation in the predicted estimate: sampling uncertainty, and stochastic variation. In the expected value condition, this second source of variation falls to zero,2 leaving only the influence of sampling uncertainty, as in uncertainty about the true value of the \\(\\beta\\) parameters, remaining on uncertainty on the predicted outputs.\nFor predicted values, we therefore need to reintroduce stochastic variation as a source of variation in the range of estimates produced. Each \\(\\eta\\) value we have implies a different \\(\\sigma^2\\) value in the stochastic part of the equation, which we can then add onto the variation caused by parameter uncertainty alone:\n\n\nCode\nN &lt;- nrow(param_draws)\npredicted_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    predicted_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"] + \n        rnorm(\n            1, mean = 0, \n            sd = sqrt(exp(param_draws[i, \"eta\"]))\n        )\n}\n\nhead(predicted_y_simpler)\n\n\n[1] 4.802092 6.706397 7.073450 6.118750 6.757717 7.461254\n\n\nLet’s now get the 95% prediction interval for the predicted values, and compare them with the expected values predicted interval earlier\n\n\nCode\npv_range &lt;- \n    quantile(\n        predicted_y_simpler, \n        probs = c(0.025, 0.500, 0.975)\n    )\n\npv_range\n\n\n    2.5%      50%    97.5% \n4.766300 5.895763 7.055408 \n\n\nSo, whereas the median is similar to before, 5.90, the 95% interval is now from 4.77 to 7.063. This compares with the 5.51 to 6.29 range for the expected values. Let’s now plot this predicted value range just as we did with the expected values:\n\n\nCode\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  pv_range[2], size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = pv_range[1], yend = pv_range[3], colour = \"red\")\n\n\n\n\n\nClearly considerably wider."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#summary",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#summary",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Summary",
    "text": "Summary\nThis post is hopefully where our toy dataset, which we’ve been hauling with us since post five, can finally retire, happy in the knowledge that it’s taken us through some of the toughest parts of this blog series. The ideas developed over the last few posts can now finally be applied to answering some questions that are actually (or arguably) interesting!"
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#coming-up",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#coming-up",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Coming up",
    "text": "Coming up\nThe next post covers the same kind of exercise we’ve performed for standard linear regression - specifying the likelihood function, and fitting it using optim() - but for logistic regression instead. This same kind of exercise could be repeated for all kinds of other model types. But hopefully this one additional example is sufficient."
  },
  {
    "objectID": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#footnotes",
    "href": "posts/glms/likelihood-and-simulation-theory/lms-are-glms-part-09/index.html#footnotes",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhere \\(\\sigma^2\\) is from \\(\\eta\\) and we defined \\(e^{\\eta} = \\sigma^2\\), a transformation which allowed optim() to search over an unbounded rather than bounded real number line↩︎\nIt can be easier to see this by using the more conventional way of expressing Normal linear regression: \\(Y_i = x_i \\beta + \\epsilon\\), where \\(\\epsilon \\sim Normal(0, \\sigma^2)\\). The expectation is therefore \\(E(Y_i) = E( x_i \\beta + \\epsilon ) = E(x_i \\beta) + E(\\epsilon)\\). For the first part of this equation, \\(E(x_i \\beta) = x_i \\beta\\), because the systematic component is always the same value, no matter how many times a draw is taken from the model. And for the second part, \\(E(\\epsilon) = 0\\), because Normal distributions are symmetrical around their central value over the long term: on average, every large positive value drawn from this distribution will become cancelled out by an equally large negative value, meaning the expected value returned by the distribution is zero. Hence, \\(E(Y) = x_i \\beta\\).↩︎\nBecause these estimates depend on random variation, these intervals may be slightly different to two decimal places than the values I’m quoting here.↩︎"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-18/index.html",
    "href": "posts/glms/causal-inference/lms-are-glms-part-18/index.html",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "",
    "text": "Over posts 14 through to 17 I’ve discussed causal inference. However, readers who’ve been involved and interested in the topic of causal inference over the last few years might be less surprised by what I have covered than by what I’ve not, namely the causal inference framework developed by Judea Pearl, and (somewhat) popularised by his co-authored book, The Book of Why: The New Science of Cause and Effect. (Pearl and Mackenzie (2018))\nThis ‘oversight’ in posts so far has been intentional, but in this post the Pearl framework will finally be discussed. I’ll aim to: i) give an overview of the two primary ways of thinking about causal inference: either as a missing data problem; or as a ‘do-logic’ problem; ii) discuss the concept of the omitted variable vs post treatment effect bias trade-off as offering something of a bridge between the two paradigms; iii) give some brief examples of directed acyclic graphs (DAGs) and do-logic, two important ideas from the Pearl framework, as described in Pearl and Mackenzie (2018); iv) make some suggestions about the benefits and uses of the Pearl framework; and finally v) advocate for epistemic humility when it comes to trying to draw causal inferences from observational data, even where a DAG has been clearly articulated and agreed upon within a research community. 1 Without further ado, let’s begin:"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-18/index.html#introduction-correcting-an-oversight-in-discussing-causality",
    "href": "posts/glms/causal-inference/lms-are-glms-part-18/index.html#introduction-correcting-an-oversight-in-discussing-causality",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "",
    "text": "Over posts 14 through to 17 I’ve discussed causal inference. However, readers who’ve been involved and interested in the topic of causal inference over the last few years might be less surprised by what I have covered than by what I’ve not, namely the causal inference framework developed by Judea Pearl, and (somewhat) popularised by his co-authored book, The Book of Why: The New Science of Cause and Effect. (Pearl and Mackenzie (2018))\nThis ‘oversight’ in posts so far has been intentional, but in this post the Pearl framework will finally be discussed. I’ll aim to: i) give an overview of the two primary ways of thinking about causal inference: either as a missing data problem; or as a ‘do-logic’ problem; ii) discuss the concept of the omitted variable vs post treatment effect bias trade-off as offering something of a bridge between the two paradigms; iii) give some brief examples of directed acyclic graphs (DAGs) and do-logic, two important ideas from the Pearl framework, as described in Pearl and Mackenzie (2018); iv) make some suggestions about the benefits and uses of the Pearl framework; and finally v) advocate for epistemic humility when it comes to trying to draw causal inferences from observational data, even where a DAG has been clearly articulated and agreed upon within a research community. 1 Without further ado, let’s begin:"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-18/index.html#causal-inference-two-paradigms",
    "href": "posts/glms/causal-inference/lms-are-glms-part-18/index.html#causal-inference-two-paradigms",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "Causal Inference: Two paradigms",
    "text": "Causal Inference: Two paradigms\nIn the posts so far, I’ve introduced and kept returning to the idea that the fundamental problem of causal inference is that at least half of the data is always missing. i.e., for each individual observation, who has either been treated or not treated, if they had been treated then we do not observe them in the untreated state, and if they had not been treated we do not observe them in the treated state. It’s this framing of the problem which\nIn introducing causal inference from this perspective, I’ve ‘taken a side’ in an ongoing debate, or battle, or even war, between two clans of applied epistemologists. Let’s call them the Rubinites, and the Pearlites. Put crudely, the Rubinites adopt a data-centred framing of the challenge of causal inference, whereas the Pearlites adopt a model-centred framing of the challenge of causal inference. For the Rubinites, the data-centred framing leads to an intepretation of causal inference as a missing data problem, for which the solution is therefore to perform some kind of data imputation. For the Pearlites, by contrast, the solution is focused on developing, describing and drawing out causal models, which describe how we believe one thing leads to another and the paths of effect and influence that one variable has on each other variable.\nIt is likely no accident that the broader backgrounds and interests of Rubin and Pearl align with type of solution each proposes. Rubin’s other main interests are in data imputation more generally, including methods of multiple imputation which allow ‘missing values’ to be filled in stochastically, rather than deterministically, to allow some representation of uncertainty and variation in the missing values to be indicated by the range of values that are generated for a missing hole in the data. Pearl worked as a computer scientist, whose key contribution to the field was the development of Bayesian networks, which share many similarities with neural networks. For both types of network, there are nodes, and there are directed links. The nodes have values, and these values can be influenced and altered by the values of other nodes that are connected to the node in question. This influence that each node has on other nodes, through the paths indicated in the directed links, is perhaps more likely to be described as updating from the perspective of a Bayesian network, and propagation from the perspective of a neural network. But in either case, it really is correct to say that one node really does cause another node’s value to change through the causal pathway of the directed link. The main graphical tool Pearl proposes for reasoning about causality in obervational data is the directed acyclic graph (DAG), and again it should be unsurprising that DAGs look much like Bayesian networks."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-18/index.html#the-omitted-variable-bias-vs-post-treatment-bias-trade-off-as-a-potential-bridge-between-the-two-paradigms",
    "href": "posts/glms/causal-inference/lms-are-glms-part-18/index.html#the-omitted-variable-bias-vs-post-treatment-bias-trade-off-as-a-potential-bridge-between-the-two-paradigms",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "The Omitted Variable Bias vs Post Treatment Bias Trade-off as a potential bridge between the two paradigms",
    "text": "The Omitted Variable Bias vs Post Treatment Bias Trade-off as a potential bridge between the two paradigms\nThe school of inference I’m most familiar with is that of Gary King, a political scientist, methodologist and (in the hallowed halls of Harvard) populariser of statistical methods in the social sciences. In the crude paradigmatic split I’ve sketched out above, King is a Rubinite, and so I guess - mainly through historical accident but partly through conscious decision - I am too. However, I have read Pearl and Mackenzie (2018) (maybe not recently enough nor enough times to fully digest it), consider it valuable and insightful in many places, and think there’s at least one place where the epistemic gap between the two paradigms can be bridged.\nThe bridge point on the Rubinite side,2 I’d suggest, comes from thinking carefully about the sources of bias enumerated in section 3.2 of King and Zeng (2006), which posits that:\n\\[\nbias = \\Delta_o + \\Delta_p + \\Delta_i + \\Delta_e\n\\]\nThis section states:\n\nThese four terms denote exactly the four sources of bias in using observational data, with the subscripts being mnemonics for the components … . The bias components are due to, respectively, omitted variable bias (\\(\\Delta_o\\)), post-treatment bias (\\(\\Delta_p\\)), interpolation bias (\\(\\Delta_i\\)) and extrapolation bias (\\(\\Delta_e\\)). [Emphases added]\n\nOf the four sources of bias listed, it’s the first two which appear to offer a potential link between the two paradigms, and so suggest to Rubinites why some engagement with the Pearlite approach may be valuable. The section continues:\n\nBriefly, \\(\\Delta_o\\) is the bias due to omitting relevant variables such as common causes of both the treatment and the outcome variables [whereas] \\(\\Delta_p\\) is bias due to controlling for the consequences of the treatment. [Emphases added]\n\nFrom the Rubinite perspective, it seems that omitted variable bias and post-treatment bias are recognised, in combination, as constituting a wicked problem. This is because the inclusion of an specific variable can simultaneously affect both types of bias: reducing omitted variable bias, but also potentially increasing post treatment bias. You’re doomed if you do, but you’re also doomed if you don’t."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-18/index.html#with-apologies-to-economists-and-epidemiologists-alike",
    "href": "posts/glms/causal-inference/lms-are-glms-part-18/index.html#with-apologies-to-economists-and-epidemiologists-alike",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "With apologies to economists and epidemiologists alike…",
    "text": "With apologies to economists and epidemiologists alike…\nOf the two sources of bias, omitted variable bias seems to be the more discussed. And historically, it seems different social and health science disciplines have placed a different weight of addressing these two sources of bias. In particular, at least in the UK context, it’s seemed that economists tend to be more concerned about omitted variable bias, leading to the inclusion of a large number of variables in their statistical models, whereas epidemiologists (though they might not be familiar with and use the term) tend to be more concerned about post-treatment bias, leading a statistical models with fewer variables.\nThe issue of post treatment bias is especially important to consider in the context of root or fundamental causes, which again is often something more of interest to epidemiologists than economists. And the importance of the issue comes into sharp relief if considering factors like sex or race. An economist/econometrician, if asked to estimate the effect of race on (say) the probability of a successful job application to an esteemed organisation, might be very liable to try to include many additional covariates, such as previous work experience and job qualifications, as ‘control variables’ in a statistical model in addition to race. From this, they might find that the covariate associated with race is neither statistically nor substantively, and from this conclude that there is no evidence of (say) racial discrimination in employment, because any disparities in outcomes between racial groups appear to be ‘explained by’ other factors like previous experience and job qualifications.\nTo this, a methodologically minded epidemiologist might counter - very reasonably - that the econometrician’s model is over-controlling, and that the inclusion of factors like educational outcomes and previous work experience in the model risks introducing post treatment bias. If there were discrimination on the basis of race, or sex, it would be unlikely to just affect the specific outcome on the response side of the model. Instead, discrimination (or other race-based factors) would also likely affect the kind of education available to people of different races, and the kinds of educational expectations placed on people of different racial groups. This would then affect the level of educational achievement by group as well. Similarly, both because of prior differences in educational achievement, and because of concurrent effects of discrimination, race might also be expected to affect job history too. Based on this, the epidemiologist might choose to omit both qualifications and job history from the model, because both are presumed to be causallly downstream of the key factor of interest, race.\nSo which type of model is correct? The epidemiologist’s more parsimonious model, which is mindful of post-treatment bias, or the economist’s more complicated model, which is mindful of omitted variable bias? The conclusion from the four-biases position laid out above is that we don’t know, but that all biases potentially exist in observational data, and neither model specification can claim to be free from bias. Perhaps both kinds of model can be run, and perhaps looking at the estimates from both models can give something like a plausible range of possible effects. But fundamentally, we don’t know, and can’t know, and ideally we should seek better quality data, run RCTs and so on.\nPearl and Mackenzie (2018) argues that Rubinites don’t see much (or any) value in causal diagrams, stating “The Rubin causal model treats counterfactuals as abstract mathematical objects that are managed by algebraic machinery but not derived from a model.” [p. 280] Though I think this characterisation is broadly consciously correct, the recognition within the Rubinite community that such things as post-treatment bias and omitted variables exist suggests to me that, unconsciously, even Rubinites employ something like path-diagram reasoning when considering which sources of bias are likely to affect their effect estimates. Put simply: I don’t see how claims of either omitted variable or post treatment bias could be made or believed but for the kind of graphical, path-like thinking at the centre of the Pearlite paradigm.\nLet’s draw the two types of statistical model implied in the discussion above. Firstly the economist’s model:\n\n\n\n\nflowchart LR\n\nrace(race)\nqual(qualifications)\nhist(job history)\naccept(job offer)\n\nrace --&gt;|Z| accept\nqual --&gt;|X*| accept\nhist --&gt;|X*| accept \n\n\n\n\n\n\nAnd now the epidemiologist’s model:\n\n\n\n\nflowchart LR \n\nrace(race)\naccept(job offer)\n\nrace --&gt;|Z| accept\n\n\n\n\n\n\nEmploying a DAG-like causal path diagram would at the very least allow both the economist and epidemiologist to discuss whether or not they agree that the underlying causal pathways are more likely to be something like the follows:\n\n\n\n\nflowchart LR\n\n\nrace(race)\nqual(qualifications)\nhist(job history)\naccept(job offer)\n\nrace --&gt; qual\nqual --&gt; hist\nhist --&gt; accept\n\nrace --&gt; hist\nqual --&gt; accept\nrace --&gt; accept\n\n\n\n\n\n\nIf, having drawn out their presumed causal pathways like this, the economist and epidemiologist end up with the same path diagram, then the Pearlian framework offers plenty of suggestions about how, subject to various assumptions about the types of effect each node has on each downstream node, statistical models based on observational data should be specified, and how the values of various coefficients in the statistical model should be combined in order to produce an overall estimate of the left-most node on the right-most node. Even a Rubinite who does not subscribe to some of these assumptions may still find this kind of graphical, path-based reasoning helpful for thinking through what their concerns are relating to both omitted variable and post-treatment biases are, and whether there’s anything they can do about it. In the path diagram above, for example, the importance of temporal sequence appears important: first there’s education and qualification; then there’s initial labour market experience; and then there’s contemporary labour market experience. This appreciation of the sequence of events might suggest that, perhaps, data employing a longitudinal research design might be preferred to one using only cross-sectional data; and/or that what appeared intially to be only a single research question, investigated through a single statistical model, is actually a series of linked, stepped research questions, each employing a different statistical model, breaking down the cause-effect question into a series of smaller steps."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-18/index.html#summary-thoughts-on-social-complexity-and-the-need-for-epistemic-humility",
    "href": "posts/glms/causal-inference/lms-are-glms-part-18/index.html#summary-thoughts-on-social-complexity-and-the-need-for-epistemic-humility",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "Summary thoughts: on social complexity and the need for epistemic humility",
    "text": "Summary thoughts: on social complexity and the need for epistemic humility\nAs mentioned before, I probably lean somewhat more towards the Rubinite than the Pearlite framework. A lot of this is simply because this is the causal effect framework I was first introduced to, but some of it comes from more fundamental concerns I have about how some users and advocates of the Pearlite framework seem to think, or suggest, it can solve issues of causal inference from observational data that, fundamentally, I don’t think it may be possible to address.\nOne clue about what the Pearlite framework can and cannot do comes from the ‘A’ in DAG: ‘acyclic’. This means that causal pathways of the following form can be specified:\n\n\n\n\nflowchart LR\nA(A)\nB(B)\n\nA --&gt; B\n\n\n\n\n\nBut causal pathways of the following form cannot:\n\n\n\n\nflowchart LR\n\nA(A)\nB(B)\n\nA --&gt; B\nB --&gt; A\n\n\n\n\n\n\nUnfortunately, cyclic relationships between two or more factors, in which the pathways of influence go in both directions, are likely extremely common in social and economic systems, because such systems are complex rather than merely complicated. 3 One approach to trying to fit a representation of a complex coupled system into a DAG-like framework would be to use time to try to break the causal paths:\n\n\n\n\nflowchart LR\n\nc0(Chicken at T0)\ne1(Egg at T1)\nc2(Chicken at T2)\ne3(Egg at T3)\n\nc0 --&gt; e1\ne1 --&gt; c2\nc2 --&gt; e3\n\n\n\n\n\n\nBut another way of reasoning about such localised coupled complexity might be to use something like factor analysis to identify patterns of co-occurence of variables which may be consistent with this kind of localised complex coupling:\n\n\n\n\nflowchart LR\n\nce((ChickenEgg))\ne[egg]\nc[chicken]\n\nce --&gt; e\nce --&gt; c\n\n\n\n\n\n\nWithin the above diagram, based on structural equation modelling, the directed arrows have a different meaning. They’re not claims of causal effects, but instead of membership. The circle is an underlying proposed ‘latent variable’, the ChickenEgg, which is presumed to manifest through the two observed/manifest variables egg and chicken represented by the rectangles. In places with a lot of ChickenEgg, such as a hen house, we would expect to observe a lot of both chickens and eggs. The statistical model in the above case is a measurement model, rather than a causal model, but in this case is one which is informed by an implicit recognition of continual causal influence operating within members of a complex, paired, causal system.\nSo, I guess my first concern relating to DAGs is that, whereas they can be really useful in allowing researchers to express some form of causal thinking and assumptions about paths of influence between factors, their acyclic requirement can also lead researchers to disregard or underplay the role of complexity even when considering inherently complex systems. In summary, they offer the potential both to expand, but also to restrict, our ability to reason effectively about causal influence.\nMy second, related, concern about the potential over-use or over-reach of DAG-like thinking comes from conventional assumptions built into the paths of influence between nodes. We can get to the heart of this latter concern by looking at , and carefully considering the implications of, something called a double pendulum, a video of which is shown below:\n\n\nA double pendulum is not a complicated system, but it is a complex system, and also a chaotic system. The variables at play include two length variables, two mass variables, a gravity variable, and time. The chaotic complexity of the system comes from the way the length and mass of the first arm interact with the length and and mass of the second arm. This complex interaction is what leads to the position of the outer-most part of the second arm (the grey ball) at any given time.\nNow imagine trying to answer a question of the form “what is the effect of the first arm’s mass on the grey ball’s position?” This kind of question is one that it’s simply not meaningful to even ask. It’s the complex interaction between all components of the system that jointly determines the ball’s position, and attempting to decompose the causal effect of any one variable in the system is simply not a fruitful way of trying to understand the system as a whole.\nThis does not mean, however, that we cannot develop a useful understanding of the double pendulum. We know, for example, that the ball cannot be further than the sum of the length of the two arms from the centre of the system. If we were thinking about placing another object near the double pendulum, for example, this would help us work out how far apart from the pendulum we should place it. Also, if one of the arms is much longer or more massive than the other, then maybe we could approximate it with a simple pendulum too. Additionally, all double pendulums tend to behave in similar ways during their initial fall. But the nature of this kind of complex system also means some types of causal question are beyond the realm of being answerable.\nThe double pendulum, for me, is an object lesson on the importance of epistemic humility. My overall concern relating to causal inference applies nearly equally to Rubinites and Pearlites alike, and is that excessive engagement with or enthusiasm for any kind of method or framework can lead to us believing we know more than we really know more about how one thing affects another. This can potentially lead both to errors of judgement - such as not planning sufficiently for eventualities our models suggest cannot happen - and potentially to intolerance towards those who ‘join the dots’ in a different way to ourselves. 4\nIn short: stay methodologically engaged, but also stay epistemically modest."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-18/index.html#footnotes",
    "href": "posts/glms/causal-inference/lms-are-glms-part-18/index.html#footnotes",
    "title": "Part Eighteen: Causal Inference: Some closing thoughts",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI might not cover these areas in the order listed above, and thinking about this further this might be too much territory for a single post. Let’s see how this post develops…↩︎\nThe bridge point on the Pearlite side might be a recognition of the apparent bloody obviousness of the fact that, if an observational unit was treated, we don’t observe untreated, and vice versa. The kind of table with missing cells, as shown in part fifteen, would appear to follow straightforwardly from conceding this point. However, Pearl and Mackenzie (2018) includes an example of this kind of table (table 8.1; p. 273), and argues forcefully against this particular framing.↩︎\nThe economist’s model is more complicated than the epidemiologist’s model, but both are equally complex, i.e. not complex at all, because they don’t involve any pathways going from right to left.↩︎\nA majority of political disagreement, for example, seems to occur when people agree on the facts, but disagree about the primary causal pathway.↩︎"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-14/index.html",
    "href": "posts/glms/causal-inference/lms-are-glms-part-14/index.html",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "",
    "text": "Henry Dundas, as observed\n\n\n\n\n\n\n\nHenry Dundas, the unobserved good counterfactual\n\n\n\n\n\n\n\nHenry Dundas, the unobserved bad counterfactual"
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-14/index.html#high-level-notewarning",
    "href": "posts/glms/causal-inference/lms-are-glms-part-14/index.html#high-level-notewarning",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "High level note/warning",
    "text": "High level note/warning\nThere are broadly two schools of thought when it comes to thinking about the problems of causal inference. One which interprets the challenge of causal inference mainly as a missing data problem; and another which interprets it mainly in terms of a modelling problem. The posts in this series are largely drawn from the missing data interpretation. If you want an overview of the two approaches (albeit subject to my own ignorance and biases), please skip briefly to the last post in this series before continuing."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-14/index.html#henry-dundas-hero-or-villain",
    "href": "posts/glms/causal-inference/lms-are-glms-part-14/index.html#henry-dundas-hero-or-villain",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Henry Dundas: Hero or Villain?",
    "text": "Henry Dundas: Hero or Villain?\nA few minutes’ walk from where I live is St Andrew Square. And in the middle of St Andrew Square is the Melville Monument, a 40 metre tall column, on which stands a statue of Henry Dundas, 1st Viscount Melville.\nThough the Melville Monument was constructed in the 19th century to commemorate and celebrate this 18th century figure, in 2020 the City of Edimburgh Council chose to add more context to Dundas’ legacy by unveiling a plaque with the following message::\n\nAt the top of this neoclassial column stands a statue of Hentry Dundas, 1st Viscount Melville (1742-1811). He was the Scottish Lord Advocate, an MP for Edinburgh and Midlothian, and the First Lord of the Admiralty. Dundas was a contentious figure, provoking controversies that resonate to this day. While Home Secretary in 1792, and first Secretary of State for War in 1796 he was instrumental in deferring the abolition of the Atlantic slave trade. Slave trading by British ships was not abolished until 1807. As a result of this delay, more than half a million enslaved Africans crossed the Atlantic.\n\nSo, the claim of the council plaque was that Dundas caused the enslavement of hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nThe descendents of Dundas contested these claims, however, instead arguing:\n\nThe claim that Henry Dundas caused the enslavement of more than half a million Africans is patently false. The truth is: Dundas was the first MP to advocate in Parliament for the emancipation of slaves in the British territories along with the abolition of the slave trade. Dundas’s efforts resulted in the House of Commons voting in favour of ending the Atlantic slave trade for the first time in its history.\n\nSo, the claim of the descendents was that Dundas prevented the enslavement of (at least) hundreds of thousands of Africans, by promoting a gradualist policy of abolition.\nHow can the same agreed-upon historical facts lead to such diametrically opposing interpretations of the effects of Dundas and his actions?\nThe answer to this question is at the heart of causal inference, and an example of why, when trying to estimate causal effects, at least half of the data are always missing."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-14/index.html#the-unobserved-counterfactual",
    "href": "posts/glms/causal-inference/lms-are-glms-part-14/index.html#the-unobserved-counterfactual",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "The unobserved counterfactual",
    "text": "The unobserved counterfactual\nBoth parties in the Dundas debate have, as mentioned, access to the same historical facts. They agree on the same observed historical reality. And both are making bold claims about the impact of Dundas in relation to the Transatlantic slave trade. In doing this, they are both comparing this observed historical reality with something else: the unobserved counterfactual.\nThe unobserved counterfactual is the data that would have been observed if what had happened, hadn’t happened 1 However, what happened did happen, so this data isn’t observed. So, as it hasn’t been observed, it doesn’t exist in any historic facts. Instead, the unobserved counterfactual has to be imputed, or inferred… in effect, made up.\nCausal inference always involves some kind of comparison between an observed reality and an unobserved counterfactual. The issue at heart of the Dundas debate is that both parties have compared the observed reality with a different unobserved counterfactual, and from this different Dundas effects have been inferred.\nFor the council, the unobserved counterfactual appears to be something like the following:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. The more radical and rapid version of the bill passes, and slavery is abolished earlier, leading to fewer people becoming enslaved.\n\nWhereas for the descendents, the unobserved counterfactual appears to be something like this:\n\nDundas doesn’t propose a gradualist amendment to a bill in parliament. Because of this, the more radical version of the bill doesn’t have enough support in parliament (perhaps because it would be acting too much against the financial interests of some parliamentarians and powerful business interests), and so is defeated. As a result of this, the abolition of slavery is delayed, leading to more people becoming enslaved.\n\nSo, by having the same observed historical facts, the observed Dundas, but radically different counterfactuals, the two parties have used the same methodology to derive near antithetical estimates of the ‘Dundas Effect’."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-14/index.html#coming-up",
    "href": "posts/glms/causal-inference/lms-are-glms-part-14/index.html#coming-up",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Coming up",
    "text": "Coming up\nThe next post offers more of a technical treatment of the key concept introduced here: namely that causal effect estimation depends on comparing observed with counterfactual data, and as the counterfactual is unobserved, causal effect estimation is fundamentally a missing data problem."
  },
  {
    "objectID": "posts/glms/causal-inference/lms-are-glms-part-14/index.html#footnotes",
    "href": "posts/glms/causal-inference/lms-are-glms-part-14/index.html#footnotes",
    "title": "Part Fourteen: A non-technical but challenging introduction to causal inference…",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data that would have been observed if what hadn’t happened, had happened, is the other type of unobserved counterfactual.↩︎"
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html",
    "href": "posts/effective-saving-for-interest-free-credit/index.html",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "",
    "text": "I have a Monzo account, and as part of the overall Monzo package make use of Monzo Flex, an interest-free credit service which means the payment made in one month is spread over broadly equal payments over the following three months.\nHowever, I’ve always only bought something if I could afford to pay for it in full.\nThe reason for using Monzo Flex follows from an intuition: Deferring some of the payment for a good obtained in month \\(t=0\\) to months \\(\\{t=1, t=2, t=3\\}\\) should in effect offer some degree of saving on the cost of the good, as a pound in 1-3 months has a slightly lower value than a pound this month. This is because of inflation, and the higher the rate of inflation, the higher the effective interest-free credit discount should become.\nHowever, I’ve never tried to work out what this effective savings rate is expected to be. Let’s try to work that out.\nTo do this, we need to consider the following:\n\nThe relationship between annual inflation rates and monthly inflation rates.\nThe concept of net present value (NPV)."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#introduction",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#introduction",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "",
    "text": "I have a Monzo account, and as part of the overall Monzo package make use of Monzo Flex, an interest-free credit service which means the payment made in one month is spread over broadly equal payments over the following three months.\nHowever, I’ve always only bought something if I could afford to pay for it in full.\nThe reason for using Monzo Flex follows from an intuition: Deferring some of the payment for a good obtained in month \\(t=0\\) to months \\(\\{t=1, t=2, t=3\\}\\) should in effect offer some degree of saving on the cost of the good, as a pound in 1-3 months has a slightly lower value than a pound this month. This is because of inflation, and the higher the rate of inflation, the higher the effective interest-free credit discount should become.\nHowever, I’ve never tried to work out what this effective savings rate is expected to be. Let’s try to work that out.\nTo do this, we need to consider the following:\n\nThe relationship between annual inflation rates and monthly inflation rates.\nThe concept of net present value (NPV)."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#monthly-and-annual-inflation-rates",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#monthly-and-annual-inflation-rates",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "Monthly and annual inflation rates",
    "text": "Monthly and annual inflation rates\nIf prices go up 10% in 12 months, and go up the same % each month, how much do they go up each month?\nAn intuitive but wrong answer is that, as there are 12 months per year, the monthly inflation rate will be one twelfth of the annual inflation rate, which would imply a monthly inflation rate of \\(0.1/12\\) or around 0.83%. So,\n\\[\n(1 + r_m) = \\frac{1}{12}(1 + r_y)\n\\] Or equivalently\n\\[\n(1 + r_y) = 12 (1 + r_m)\n\\] Where \\(r_y\\) is the annual increase and \\(r_m\\) is the monthly increase.\nHowever this assumption, as mentioned, is wrong, because it ignores the way that each month’s increase is applied to the product of all increases that occurred in previous months. For example, for three months with different inflation rates the total increase over the the three months will be\n\\[\n(1 + r_{1,2,3}) = (1 + r_1)(1+r_2)(1+r_3)\n\\] If the monthly inflation rates for each of the three months are the same, \\(r_m\\), then this simplifies slightly to\n\\[\n(1 + r_{1,2,3}) = (1 + r_m)^3\n\\]\nBy extension, as there are twelve months in a year, where the monthly inflation rate is fixed the equation becomes:\n\\[\n(1 + r_y) = (1 + r_m)^{12}\n\\]\nThis, not \\((1 + r_y) = 12 (1 + r_m)\\), is the correct starting point. Solve for \\(r_m\\) …\n\\[\n(1 + r_y)^{\\frac{1}{12}} = 1 + r_m\n\\]\n\\[\nr_m = {(1 + r_y)}^{\\frac{1}{12}} - 1\n\\]\nPlugging in a 10% annual inflation rate, i.e. 0.1 for \\(r_y\\), we therefore get an \\(r_m\\) value of around 0.007974, so around 0.8%."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#net-present-value",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#net-present-value",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "Net Present Value",
    "text": "Net Present Value\nThe idea of Net Present Value (NPV) is to translate costs and benefits that occur at different points in time onto a single timeframe, the present. This makes it easier to compare options that take place over different timeframes.\nIn the Flex example we are comparing two options:\n\n\nPay all now\n\n\nPay interest free over three consecutive monthly installments\n\n\nLet’s say the cost of the good at month \\(t\\) is £150. Graphically, and with no interest and inflation, the two options look as follow:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- tribble(\n  ~option, ~month, ~amount,\n  \"A\", 0, 150,\n  \"A\", 1, 0,\n  \"A\", 2, 0,\n  \"A\", 3, 0,\n  \"B\", 0, 0,\n  \"B\", 1, 50,\n  \"B\", 2, 50,\n  \"B\", 3, 50\n)\n\ndf |&gt; \n  ggplot(aes(month, amount)) + \n  geom_col() + \n  facet_wrap(~ option, nrow = 2)\n\n\n\n\n\nIn the no interest / no inflation scenario, the sums for option A and option B are equal, £150.\nHowever, in scenarios with inflation, the value of money keeps decreasing. This means that a commitment to pay £50 month 3 is a commitment to pay less than in month 0. Using the 10% annual inflation rate example, we can estimate the cumulative devaluation by months 1, 2 and 3 by dividing the product of devaluations so far by the monthly inflation rate:\n\n\nCode\nannual_to_monthly &lt;- function(x) {(1 + x)^(1/12) -1}\n\nannual_inflation &lt;- 0.10\nmonthly_inflation &lt;- annual_to_monthly(annual_inflation)\n\nindex0 &lt;- 1\nindex1 &lt;- index0 / (1 + monthly_inflation)\nindex2 &lt;- index1 / (1 + monthly_inflation)\nindex3 &lt;- index2 / (1 + monthly_inflation)\n\ndf &lt;- tibble(\n  month = 0:3, \n  index = c(index0, index1, index2, index3)\n)\n\ndf\n\n\n# A tibble: 4 × 2\n  month index\n  &lt;int&gt; &lt;dbl&gt;\n1     0 1    \n2     1 0.992\n3     2 0.984\n4     3 0.976\n\n\nContinuing the example of a £150 item paid over months 1, 2 and 3, we can therefore convert to NPV by discounting each month’s costs by the index relative to month 0\n\n\nCode\ndf2 &lt;- df |&gt; \n  mutate(\n    amount = c(0, 50, 50, 50)\n  ) |&gt; \n  mutate(npv_amount = amount * index)\n\ndf2\n\n\n# A tibble: 4 × 4\n  month index amount npv_amount\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1     0 1          0        0  \n2     1 0.992     50       49.6\n3     2 0.984     50       49.2\n4     3 0.976     50       48.8\n\n\nThe sum of npv_amount is now less than the £150 in option A, pay upfront. In this example, with 10% inflation, this sum is £147.64, which represents a 1.6% discount on option A.\nLet’s now generalise to other inflation rates\n\n\nCode\ncalc_npv_discount &lt;- function(ry, total = 150) { \n  annual_to_monthly &lt;- function(x) {(1 + x)^(1/12) -1}\n  \n  rm &lt;- annual_to_monthly(ry)\n  index0 &lt;- 1\n  index1 &lt;- index0 / (1 + rm)\n  index2 &lt;- index1 / (1 + rm)\n  index3 &lt;- index2 / (1 + rm)\n\n  npv_amt1 &lt;- (total / 3) * index1\n  npv_amt2 &lt;- (total / 3) * index2\n  npv_amt3 &lt;- (total / 3) * index3\n  \n  \n  1 - sum(npv_amt1, npv_amt2, npv_amt3) / total\n}\n\ndf &lt;- \n  tibble(\n    annual_rate = seq(0, 0.15, by = 0.01)\n  ) |&gt; \n  mutate(\n    effective_discount = map_dbl(annual_rate, calc_npv_discount)\n  )\n\ngg &lt;- \n  df |&gt; \n    ggplot(aes(100 * annual_rate, 100 * effective_discount)) + \n    geom_line() + \n    labs(x = \"Annual inflation rate (%)\", \n         y = \"Effective discount on paying over 3 months (%)\",\n         title = \"Effective short-term discount rate against inflation rate\"\n         ) + \n    scale_y_continuous(breaks = seq(0, 15, by = 0.1)) +\n    annotate(\"segment\", x = 14.8, xend = 14.8, colour = \"lightblue\", y = 0, yend = 100 * calc_npv_discount(0.148)) +\n    annotate(\"segment\", x = 0, xend = 100 * 0.148, colour = \"lightblue\", y = 100 * calc_npv_discount(0.148), yend = 100 * calc_npv_discount(0.148)) +\n    annotate(\"segment\", x = 9.6, xend = 9.6, colour = \"darkblue\", y = 0, yend = 100 * calc_npv_discount(0.096)) +\n    annotate(\"segment\", x = 0, xend = 100 * 0.096, colour = \"darkblue\", y = 100 * calc_npv_discount(0.096), yend = 100 * calc_npv_discount(0.096)) +\n    annotate(\"segment\", x = 5.3, xend = 5.3, colour = \"darkgrey\", y = 0, yend = 100 * calc_npv_discount(0.053)) +\n    annotate(\"segment\", x = 0, xend = 100 * 0.053, colour = \"darkgrey\", y = 100 * calc_npv_discount(0.053), yend = 100 * calc_npv_discount(0.053)) \n    \ngg +\n    annotate(\"text\", \n             x = 2, y = 0.1 + 100 * calc_npv_discount(0.148),\n             label = \"Goods (Highest)\"\n    ) + \n    annotate(\"text\", \n             x = 2, y = 0.1 + 100 * calc_npv_discount(0.096),\n             label = \"CPIH (Highest)\"\n    ) + \n    annotate(\"text\", \n             x = 2, y = 0.1 + 100 * calc_npv_discount(0.053),\n             label = \"Services (Highest)\"\n    )  \n\n\n\n\n\nIn the above I’ve indicated the effective discount rates implied by different annual interest rates reported by the ONS in Figure 7 of this page These range from almost 2.3% for goods, to around 0.86% for services.\nHowever, fortunately, the current inflation rates are somewhat lower, with the most recent reported inflation rates being 2.9% for goods, 6.2% for services, and 2.7% for CPIH.\n\n\nCode\ngg + \n  annotate(\"segment\", x = 2.9, xend = 2.9, colour = \"lightblue\", linetype = \"dashed\", y = 0, yend = 100 * calc_npv_discount(0.029)) +\n  annotate(\"segment\", x = 0, xend = 100 * 0.029, colour = \"lightblue\", linetype = \"dashed\", y = 100 * calc_npv_discount(0.029), yend = 100 * calc_npv_discount(0.029)) +\n  annotate(\"text\", \n           x = 2, y = 0.1 + 100 * calc_npv_discount(0.029),\n           label = \"Goods (Current)\"\n  ) + \n  annotate(\"segment\", x = 4.7, xend = 4.7, colour = \"darkblue\", linetype = \"dashed\", y = 0, yend = 100 * calc_npv_discount(0.047)) +\n  annotate(\"segment\", x = 0, xend = 100 * 0.047, colour = \"darkblue\", linetype = \"dashed\", y = 100 * calc_npv_discount(0.047), yend = 100 * calc_npv_discount(0.047)) +\n  annotate(\"text\", \n           x = 2, y = 0.1 + 100 * calc_npv_discount(0.047),\n           label = \"CPIH (Current)\"\n  ) + \n  annotate(\"segment\", x = 6.2, xend = 6.2, colour = \"darkgrey\", linetype = \"dashed\", y = 0, yend = 100 * calc_npv_discount(0.062)) +\n  annotate(\"segment\", x = 0, xend = 100 * 0.062, colour = \"darkgrey\", linetype=\"dashed\", y = 100 * calc_npv_discount(0.062), yend = 100 * calc_npv_discount(0.062)) +\n  annotate(\"text\", \n           x = 2, y = 0.1 + 100 * calc_npv_discount(0.062),\n           label = \"Services (Current)\"\n  )  \n\n\n\n\n\nSo, the effective discount for deferring has fallen alongside inflation. However it’s still something."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#some-thoughts",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#some-thoughts",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "Some thoughts",
    "text": "Some thoughts\nThe immediate cost of deferring is by contrast the same. It involves clicking a couple of buttons, so a couple of seconds, in the same Monzo app.\nThere are some other consequences too: Using a higher proportion of one’s credit limit tends to lower one’s credit rating. This means the ability to acquire credit on more favourable terms can be adversely affected. Another issue is that, to avoid paying any effective interest rate on the credit, there always needs to be sufficient money in the current account to cover all upcoming payments. This requires either keeping more money in the current account than might be optimal from a savings perspective, or additional daily management of current account balances. As interest rates on savings are over 4% currently, having more money in the current account confers an opportunity cost in lost savings interest which might outweigh any benefits of using short-term interest free credit.\nHowever, for now, as a general principle, realising marginal savings by pressing a couple of buttons doesn’t seem too bad, and at some points of time, and for some items, the savings have been around 2%.\nI might return to looking at other scenarios, including how much might be lost by keeping more money in the current account, in later posts. For now I’ll leave things here, as I simply wanted to get some numbers behind the intuition that, in principle, paying later is better than paying now."
  },
  {
    "objectID": "posts/r-code/index.html",
    "href": "posts/r-code/index.html",
    "title": "Post with code",
    "section": "",
    "text": "This short post is intended to confirm that I can run and render R code within a Quarto blog post.\n\nVery simple example\nLet’s start off with some very simple base-R\n\n1 + 1\n\n[1] 2\n\n\nAnd of course let’s not forget the obligatory\n\nstatement &lt;- \"Hello World\"\n\nstatement\n\n[1] \"Hello World\"\n\n\n\n\nGraphs\nLet’s now look at a base-R graphic, again using a cliched example\n\nplot(mtcars$mpg ~ mtcars$wt)\n\n\n\n\n\n\nSome extensions\nLet’s now continue to be cliched, and load and use the tidyverse\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\nmtcars |&gt; \n  group_by(carb) |&gt; \n  summarise(\n    mean_mpg = mean(mpg)\n  ) |&gt; \n  ungroup()\n\n# A tibble: 6 × 2\n   carb mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     1     25.3\n2     2     22.4\n3     3     16.3\n4     4     15.8\n5     6     19.7\n6     8     15  \n\n\nAnd to visualise\n\nmtcars |&gt; \n  mutate(cyl = factor(cyl)) |&gt; \n  ggplot(aes(x = wt, y = mpg, colour = cyl, group= cyl)) + \n  geom_point(aes(shape = cyl)) + \n  stat_smooth(se = FALSE, method = \"lm\") + \n  labs(\n    x = \"Weight\", \n    y = \"Miles per gallon\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nConclusion\nSo far, so good…"
  },
  {
    "objectID": "posts/optimised-for-twitter/index.html",
    "href": "posts/optimised-for-twitter/index.html",
    "title": "Optimised for Twitter?",
    "section": "",
    "text": "I’ve finally got around to ‘optimising the Quarto blog for Twitter’, by following the guidance in this section of Quarto’s website and adding the following declaration to the _quarto.yml file in the project root.\nwebsite:\n  twitter-card:\n   site: \"@jonminton\"\nHowever, is Twitter optimised for Twitter, these days? Is Twitter even Twitter!?"
  },
  {
    "objectID": "posts/scientific-illustration-unit-circle/index.html",
    "href": "posts/scientific-illustration-unit-circle/index.html",
    "title": "Scientific Illustrations: Annotating the unit circle",
    "section": "",
    "text": "Here’s an example of a scientific illustration I’ve just produced to illustrate some scenarios I’m modelling for my work on economic inactivity determinants.\nI have two continuous variables (derived from the GHQ-12), mental health and physical health. Both are standardised so they have the same scale.\nBut I’m interested in the effects of improving/changing ‘health’ in general, which is obviously composed of both mental health and physical health, but not measured directly.\nAs the two variables are standardised, however, I can model an improvement in health in general as a change in both mental health and physical health concurrently.\nHowever, I want to compare like-with-like: scenarios in which the total ‘amount’ of intervention effect is kept constant, but the relative contribution of the two health components is varied.\nThis is where a little trigonometry comes in. 1 All interventions on the grey unit circle in Figure 1 represent possible scenarios in which the total amount of health change is constant, but where the relative contribution of mental and physical health is varied.\nThe aim of the scientific illustration is to make this intuition a bit clearer to understand!\nCode\nlibrary(tidyverse)\nlibrary(geomtextpath)\n\npos_y &lt;- function(x) {sqrt(1 - x^2)}\nx = seq(0, 1, by = 0.001)\ndta &lt;- tibble(\n  x = x\n) |&gt; \n  mutate(\n    y = pos_y(x)\n  )\n\ndta |&gt; \n  ggplot(aes(x = x, y = y)) + \n    geom_line(color = \"grey\") + \n    coord_equal() + \n    labs(x = \"Physical Health (Standardised)\",\n         y = \"Mental Health (Standardised)\",\n         title = \"Health improvement scenarios modelled\") + \n  theme_minimal() + \n  annotate(\"point\", x = 1, y = 0) + \n  annotate(\"point\", x = 0, y = 1) + \n  annotate(\"point\", x = 1/ sqrt(2), y = 1/ sqrt(2)) + \n  annotate(\"point\", x =  2 / sqrt(5), y = 1 / sqrt(5)) + \n  annotate(\"point\", x = 1 / sqrt(5), y = 2 / sqrt(5)) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, xend = 0, yend = 1), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S1: MH Only\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  ) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, xend = 1, yend = 0), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S2: PH Only\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  ) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, xend = 1/sqrt(2), yend = 1/sqrt(2)), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S3: Equal Gain\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  ) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, yend = 2/sqrt(5), xend = 1/sqrt(5)), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S4: MH Bias\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  )  +\n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, yend = 1/sqrt(5), xend = 2/sqrt(5)), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S5: PH Bias\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  )  \n\n\n\n\n\nFigure 1: Modelling various intervention scenarios"
  },
  {
    "objectID": "posts/scientific-illustration-unit-circle/index.html#footnotes",
    "href": "posts/scientific-illustration-unit-circle/index.html#footnotes",
    "title": "Scientific Illustrations: Annotating the unit circle",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPeople who claim you’ll never need trigonometry once you leave school are wrong. It’s valuable about once a decade!↩︎"
  },
  {
    "objectID": "posts/unpop/francis-dream-film/index.html",
    "href": "posts/unpop/francis-dream-film/index.html",
    "title": "Megalopolis: Dream Film",
    "section": "",
    "text": "Figure 1: Onion: Dream Omelet\nBack in 2008, the satirical magazine and website the Onion was in its Golden Age, successfully expanding their content from fake newspapers to fake magazine shows and then, with ClickHole, to parodying the then nascent emergence of social media. Amongst the fake TV segments they produced was one called “Chef Cooks ‘Dream Omelet’ that came to him in a dream”, a screen grab of which is shown at the head of this post. In the segment, a professional chef earnestly advises viewers wishing to replicate his dream omelette to do the following:\nThis week I decided to watch Megalopolis, Francis Ford Coppola’s much hyped (then much derided) passion project, which reportedly cost him at least one winery.1 Throughout the more than two hour runtime I kept thinking about the Onion sketch above, as I don’t think there’s any better reference point for trying to describe and understand the film.\nIn dreams, consistent rules of physics and characterisation don’t exist. Instead there are themes and motifs, which blend the humdrum with the impossible with a logic impenetrable to conscious thought. However, despite what happens and is said in dreams being linearly inscrutible, the scenes, themes and motifs themselves can potentially be informative as to the interests and preoccupations of the dreamer. In Francis Ford Coppola’s case, these themes and motifs appear to include:\nWhen in a dream, everything is what it is, and why it is is because it must be so. There is no ironic detachment; the subconscious delivers each line and shows each new symbol with weighty earnestness. This dream-like literalism and earnestness pervades Megalopolis; every shot and every word is delivered with complete seriousness, without any knowing looks or winks to the cynics and know-it-alls who may be watching 2. And because of this, for those watching this ever-so-lavish account of a dream, rather than experiencing the dream, the effect can be unintentionally hilarous. Someone hearing an account of a dream, or seeing photos of a holiday, cannot be made to themselves experience that dream, no matter how lavishly those remembered aspects of the dream are recreated for the audience. There will always be aspects of a dream that, for the dreamer, will be inherently sacred and profound - because within the dream the unconscious told the dreamer this is sacred and profound - but that for someone hearing or seeing an account of that dream will not be.\nDoes Megalopolis work as a film? The answer depends on your definition of film. It’s definitely not a movie, with that term’s connotations of easily digestable storyline, clearly identifiable genre, Campbellesque arc and third act spectacle. But at the same time, it’s stylistically - and occasionally structurally - movie-like. And this can give the impression it’s a bad movie, when perhaps it’s not trying to be a movie at all. But if film is the more permissive of the two terms - and simply means the display of a sequence of still images on a screen so as to give the illusion of motion - then of course it’s a film. Things were shown on screen, for over two hours, and they appeared to move.\nSimpler still: Does Megalopolis work? Again: depends on context and definition. I suspect the film I saw is largely the film Francis Ford Coppola intended to make. Within the series of scenes and shots, there are some that are visually engaging, looking alternately as either colour remakes of shots from impressionist silent cinema, or as prog rock album covers brought to life. Does the ever-earnest leaden dialogue work? No for a movie, or at least most movies produced in the last generation; yes for a dream film. Does the narrative arc work, as in cause us to invest emotionally in a believable protagonist facing perils and payoffs we can understand and find weighty? For me: not really - as the rules and consequences of actions in this world are as inscrutible as in any dream - but then maybe I’m being closed minded by even asking this question.\nIs there a better reference point for trying to unlock Megalopolis than the Onion dream omelette sketch mentioned above? Maybe some of Coppola’s earlier films? Perhaps the more psychedelic scenes in Apocalypse Now might work? But then even the most out-there of these scenes seemed more bound by the rules of our known physical reality than those in Megalopolis. Imagine a version of the scene in which Willard stares from his sickbed at the ceiling fan, but then in the next shot is standing upside down on the ceiling, trying to avoid the fan’s blades, which eventually hit him, causing him to shatter into a shower of butterflies. Or the scene in which Willard’s head, caked in green and brown paint, emerges from a black liquid pool, but then in the next shot his head comes to resemble a crocodile’s, but with a shimmering crest at the back of his monstrous skull that pulses with ethereal light. For better or worse, Megalopolis is Coppola’s most untethered film, in which what can and does happen appears little bound to known rules of the real world.\nAn interesting point of contrast when it comes to this concept of tethering is the work of David Lynch, whose outputs are often more easily understood as nightmare-like than dream-like. I’ve written about what I call Lynch’s shamanistic tendencies elsewhere. The comparison is instructive, I believe, because it highlights how, for all Megalopolis’ strangeness, Lynch is a fundamentally weirder artist and character. For Coppola dreamland and the real world are clearly distinct places, and Megalopolis is a film that seems to take place entirely in dreamland. For Lynch, by contrast, the distinction always appears to be tenuous. Lynch is a character for whom potentially every event and incident he encounters in the real world is pregnant with magic and meaning, a world in which electricity is a mystic force, for instance, rather than a mere instrumental application of known physical laws. Lynch understands the evils and injustices present in the real world, for example, in particular the evil of men’s violence towards women, but reads such incidents as laden with the tells of cosmic forces wielded by barely glimpsed Manichean agents. For Lynch, there is only one world - a strange and beautiful and horrifying and mystical and humdrum world that looks on its surface much like our own, but where those who care to look can see something magical behind the surface. For Coppola, by contrast, and as for most people, there are two.\nGiven everything discussed above, I’m not sure if the following observation counts as a criticism of Megalopolis or an odd kind of compliment: For parts of the film, I suspect I was asleep."
  },
  {
    "objectID": "posts/unpop/francis-dream-film/index.html#footnotes",
    "href": "posts/unpop/francis-dream-film/index.html#footnotes",
    "title": "Megalopolis: Dream Film",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe wikipedia article reports that Megalopolis cost at least $120 million to make, and has so far received around $10 million in box office receipts.↩︎\nThe experience of lucid dreaming - dreaming while being conscious of dreaming - may be the exception to this, but the vast majority of dreams are not experienced in a lucid state.↩︎"
  },
  {
    "objectID": "posts/unpop/robocop-is-wonderfully-childish/index.html",
    "href": "posts/unpop/robocop-is-wonderfully-childish/index.html",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "",
    "text": "Robocop (1987): Wonderfully Childish"
  },
  {
    "objectID": "posts/unpop/robocop-is-wonderfully-childish/index.html#preamble",
    "href": "posts/unpop/robocop-is-wonderfully-childish/index.html#preamble",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Preamble",
    "text": "Preamble\nHere’s something that’s differently nerdy. Some thoughts on the enduring and childish appeal of Robocop as a character and concept, lifted largely from some notes I made on Obsidian (which also uses Markdown)."
  },
  {
    "objectID": "posts/unpop/robocop-is-wonderfully-childish/index.html#notes-on-robocop",
    "href": "posts/unpop/robocop-is-wonderfully-childish/index.html#notes-on-robocop",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Notes on Robocop",
    "text": "Notes on Robocop\nWent through a Robocop phase. First film is good. Second film is adequate fan service (by Frank Miller), though unpleasant in terms of how it makes a boy a main antagonist. Reboot is terrible.\nI think this was inspired by seeing footage for the Robocop game Rogue City. Also remembered discussion about how in the original film Robocop’s death and resurrection is modelled on Jesus. It’s definitely true his death is a ‘passion’ (modern equivalent: torture porn?), and that his suffering at the hands of sadistic tormenters fits this pattern.\nIn the first film there’s also the sense of the protagonist reclaiming his humanity. Murphy’s memory is wiped, but through force of will he brings himself to remember who he was, and to identify as Murphy. That’s the last line. He’s thanked and addressed by the Old Man as a person, not as property That’s the arc: becoming human again.\nBy contrast the reboot started with Murphy knowing who he was, though the scientists can modify the extent to which his feelings or programming are in charge. There’s a plot about police corruption and selling weapons illegally, and plenty of exposition from scientists where they try to shoehorn in cod-philosophy on personhood and free will, but this felt like ‘filler’ between a series of action sequences, which were much faster but also much less weighty and visceral than those in the original.\nIn the original Robocop is a bullet sponge. He was slow, apparently unfeeling. Though this might be partly a function of Peter Weller not having a great deal of mobility when wearing the suit, it also feeds into the central character arc: robots don’t feel; people do.1 Robots aren’t vulnerable as people are. As Robocop becomes damaged, he becomes more vulnerable, and his face becomes visible. Vulnerability is necessary for humanity to be restored. Robocop’s suit makes him a superhero, an adolescent boy fantasy of massive strength and power, but it also makes him a prisoner, trapped and entombed.\n\n\n\nRegaining Humanity: But at what cost?\n\n\nAgain, by contrast, in the remake Murphy has lost less. He still has a human hand, still has his memories and sense of self, and still has all the speed and mobility he had as a person, only more so. He doesn’t have an arc, he has perturbations and wobbles.\nLet’s think some more about why Robocop is an adolescent, or even pre-adolescent, fantasy. Firstly, it appeals to a kind of crude creativity of combinatorials: take two things that are familiar, combine them, and make something unfamiliar. With only a limited number of schemas, even a young child can wonder what happens when they are combined, and feel excited and proud about bootstrapping from everyday experience to pure fantasy. Like the distinction between animal, vegetable and mineral, ‘man’ and ‘machine’ are different primary colours, and seeing the concept of Robocop may be for a young child like seeing red and blue make purple for the first time.\n\n\n\nTeenage Mutant Ninja Turtles: Another contemporaneous example of the ‘primary colour chimera’ attractive to children\n\n\nSecond, there’s the power fantasy. Within this, there’s the sense of seeing someone who can play an action game and be allowed to ‘cheat’. The roles of an action game, involving shooting, must include that, once shot, the player must ‘play dead’. But in Robocop is a fantasy of a character who’s still allowed to shoot others whilst being able to ignore when others shoot him. Robocop presents a fantasy for a young child of playing a game where you can do things that no-one else can, because you’re more special than everyone else.\nI suspect that’s why, even though Robocop was clearly unsuitable for children, the concept of Robocop appeared to have so much appeal to children. There’s something inherently childish about seeing such a pure chimera rendered on screen, and the possibilities and affordances of this chimera are signalled so brightly from the images alone that the (adult) source material does not need to be consulted.\nAgain, this seems to be yet another reason why the remake did not work: 2014’s Murphy was not machine enough. The suit did not make him clearly ‘other’ enough. Even though the graphic violence was toned down so that in theory children could watch, the lack of schematic purity in the ideal types being mixed meant the pleasure of seeing primary colours being mixed, in the way the original so effectively managed, was muddied and diluted."
  },
  {
    "objectID": "posts/unpop/robocop-is-wonderfully-childish/index.html#footnotes",
    "href": "posts/unpop/robocop-is-wonderfully-childish/index.html#footnotes",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlistair Gray’s Lanark played with a similar idea, in the form of Dragonhide, a magical realist representation of dermatitis, emotional distance, and lack of outlet for artistic expression. Too much Dragonhide, the book warns, causes sufferers to become unable to move, and unable to vent, their heat and energy, without expression, eventually causing them to boil to death!↩︎"
  },
  {
    "objectID": "posts/unpop/unrepentent-confessional/index.html",
    "href": "posts/unpop/unrepentent-confessional/index.html",
    "title": "David Sederis: Humourists as Unrepentent Observational Confessionals",
    "section": "",
    "text": "Whereas I devoted much of the 2022 Christmas break to (re?)learning Bayesian modelling, and over 2023 took four months off to learn software development, over the 2023 Christmas break I vowed to do nothing useful, nothing that involves thinking or effort or learning, in order to make the break as restful and uneventful as I could.\nHowever, as anyone who’s tried sitting with their own thoughts for more than a couple of minutes realises, it’s not really possible to avoid thinking about things, and attending to one’s own thoughts is often far from restful. Attempting to sit mindfully for more than a minute puts the lie to the very idea that I have thoughts. Instead the reverse is true: thoughts have us.\nAttempting to sit passively with one’s own thoughts is to attempt to recognise the eternal turbulence of what Rational Mystic Sam Harris keeps referring to as the ‘field of consciousness’. It’s like standing on a worn wet stone, surrounded by eddies and currents, and being invited to dip a toe into the waters without falling in. Failing at this task is the default state of cognition, it seems, especially (possibly uniquely?) for humans. Even if we stay mindful enough to notice that thoughts just appear to us, unbidden, without some kind of humuncular avatar of the self causing or driving them, the temptation of following a thought, of engaging with it, is something that is near impossible to resist. Drop into a thought, and it takes us to another thought, then another, then another, and before long we find ourselves thinking about cheese when we meant to think about spreadsheets, or Nazis when we meant to think about buying Christmas cards, or stuck in a ruminatory cycle where a complex of three or more thoughts cycle back onto each other in some kind of unstoppable mental vortex.\nInvariably, the thoughts we have, or rather the thoughts that have us, lead us into some dark, surreal or smelly places. We seldom report to others the mental journeys we went on to retrieve information that’s actionable or useful to others. We self-curate, and self-censor, displaying the pearl of wisdom we’ve found, without recounting the sadistic joy we felt ripping the oyster that contained it apart and ending its innocent life. We wipe off the stink and the gore accrued in the journeys we’ve been on inside our own heads, and instead try to present to others a pristine edit of these journeys, and we pretend these curated edits are our ‘true self’. And almost everyone else feels compelled to do the same, because we do, and we do, because everyone else does.\nAll this is a rambling, (mindful?) way of saying, I read David Sedaris’ latest book Happy-Go-Lucky over the Christmas break, and like his other books and stories found it humourous (as intended), engaging and refreshing as an example of how to tell stories that don’t aim to build and sell a pristine curated self to others. Though the term ‘humourist’ (or rather ‘humorist’) has been used to describe Sedaris’ outputs and the genre he dominates so effectively, a more accurate term may be “unrepentant observational confessional”.\nSedaris’ stories are finely crafted, polished, and I’m sure edited and reedited many times, often using, like most stand-up comedians, the involuntary and unvarnished feedback from audiences (such as whether people laugh) as a guide on how to improve his texts further. But they are not stories in service to the promotion of a pristine and virtuous self, a paragon to live by. No, they’re stylised records of ‘things that happen’, inside and out, and the relationship between occurrances, thoughts, feelings, and actions. And importantly, though stylised, they have a sense of verisimilitude to the inner world in which thoughts and associations are often far from linear or pristine, whether it be noting that crowds shouting “Black Lives Matter” do so with a quality and cadence of a fishmonger selling “Fresh-Caught Haddock”, the private vilification of those who move too slowly in queues, or Sedaris’ recognition that the deaths of his mentally ill sister, or homophobic father, did not evoke in him the quality of grief that a ‘good person’ ought to experience.\nHumour often comes from distance: the distance in time needed to sublimate tragedy into comedy (‘too soon?’), but also the distance between expectation and reality. The Pristine Self creates this expectation, whereas careful observation and honest accounting reveals a reality that near invariably falls short of this expectation. Sedaris’ stories work as humour because he knows how much most people edit the stories they tell themselves and others about themselves in order to maintain the Pristine Self, but he doesn’t. Instead he edits to make the blacks darker, the smells skinkier, and the circles his mind wanders in more eccentric."
  },
  {
    "objectID": "posts/unpop/soundalikes/index.html",
    "href": "posts/unpop/soundalikes/index.html",
    "title": "Mistaken (Aural) Identity: On podcasts and soundalikes",
    "section": "",
    "text": "Recently I was listening to an episode of the podcast series The Rest is Money, with Steph McGovan and Robert PESSSSS-TON. In this episode McGovan and Peston interviewed Karen Ward, which the episode blurb introduces as:\n\na Chief Market Strategist for EMEA at one of the world’s leading financial institutions, JP Morgan Asset Management, as well as a former advisor to an ex chancellor\n\nNow - as TRIM (as they probably like to call themselves) is a podcast, and I’m not much of an intrusive image search Googler - I have no idea what Karen Ward looks like. But I do have a clear idea of what she sounds like.\nKaren Ward, to my ears, sounds almost exactly like… Liz Truss.\nSo, for 47 minutes, I was hearing someone who sounded almost exactly like Liz Truss. But who was saying things that sounded reasonable, considered, and intelligent.\nThis experience created a weird kind of cognitive dissonance. Although I was consciously aware that Karen Ward was not Liz Truss, part of my subconscious was trying to simultaneously accept and reject the idea that ‘Liz Truss’ was saying sensible things about the economy. It was a much more bizarre and distracting experience than I was expecting.\nNow, back in 2001, when The Simpsons was still worth watching, there was an episode called HOMR. The episode was inspired by the short story Flowers for Algernon, and is the one and only outing of a smart version of Homer Simpson.\nFor those who’ve seen HOMR, when I was hearing the podcast, I was thinking: “Wow! Karen Ward is like Liz Truss with the crayon removed”.\nThe deeper irony of this is TRIM, a few weeks previously, featured an interview with Liz Truss of a similar length. Along with a mini-series about the Kwateng/Truss Budget, this is something for which TRIM got summarily review bombed. The Liz Truss interview, however, was much easier for my brain to cope with, because she kept saying very Liz-Truss-like things. There wasn’t a dissonance to overcome!\nSo, this got me thinking about ‘soundalikes’. Is that a word? Does it need to be, given how popular podcasts are these days? What does it mean for one person’s standing and reputation when they happen to have a very similar voice to someone with either a much better, or much worse, standing and reputation?\nThe only previous time I’ve encountered something like the Truss/Ward soundalike dissonance is in listening to podcast interviews with Daniel Schmachtenberger.\nIn brief: Schmachtenberger is a benign North American pseud. But he sounds, to my ears, almost exactly like James Lindsay, who’s - without going into details - a very malign North American pseud.\nAre there any lessons to take from this? Should people who happen to sound like annoying people try to change their way of speaking to reduce the possibility of mistaken identity? Should listeners whose brains flip when they hear these aural spitting images actually start searching for images of the two soundalikes so they should better distinguish between the two? Should we start listening to much more of both the annoying and non-annoying soundalike so we can start to tell them apart more easily?\nI don’t know. I just know that, more than a week later, I’m still reeling from the experience!"
  },
  {
    "objectID": "posts/unpop/psychomechanics-of-cultist-simulator/index.html",
    "href": "posts/unpop/psychomechanics-of-cultist-simulator/index.html",
    "title": "The Game of Life Beyond",
    "section": "",
    "text": "This post contains mild-to-moderate spoilers for the game Cultist Simulator, which is available on Steam, Switch, and likely other platforms besides. If you want to experience Cultist Simulator without these spoilers, please buy the game, play it for between ten and twenty hours, then return to this post. If you’ve already played it, have no interest in playing it, or aren’t particularly bothered by spoilers, then please read on…"
  },
  {
    "objectID": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#warningpreamble",
    "href": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#warningpreamble",
    "title": "The Game of Life Beyond",
    "section": "",
    "text": "This post contains mild-to-moderate spoilers for the game Cultist Simulator, which is available on Steam, Switch, and likely other platforms besides. If you want to experience Cultist Simulator without these spoilers, please buy the game, play it for between ten and twenty hours, then return to this post. If you’ve already played it, have no interest in playing it, or aren’t particularly bothered by spoilers, then please read on…"
  },
  {
    "objectID": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#introduction",
    "href": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#introduction",
    "title": "The Game of Life Beyond",
    "section": "Introduction",
    "text": "Introduction\nThe first time I became aware of Cultist Simulator was through a Youtube video. And when I first saw it, I thought it was either a joke, or at most highly unfinished. But I was wrong.\nCultist Simulator has no tutorial, a big virtual table (sitting atop ethereal nothingness), many cards, and some counters (that ‘accept’ cards). Superficially, there’s nothing else to it. Everything in the game happens on the table, and involves cards and counters. You can’t get up, you don’t see anyone, there’s no dialogue, no cut-scenes or grand cinematics, and in a sense nothing to do except move cards and counters around on a board.\nBut within this highly limited player canvas, Cultist Simulator is an incredible game, narratively rich, mechanically complex, hypnotic, engaging, brutal, cruel, relaxing, philosophical, psychological, literary, purposeful and intelligent. The simplicity and limitations in the ways that the player can interact with the game, and the game can feed back to the player, belie the underlying complexity and richness of the game’s storytelling and mechanics.\nSo, what is Cultist Simulator? Here’s three answers:\n\nHP Lovecraft Solitaire\nA rogue-like indie game\nA human purpose and motivation simulator\n\nIn this post, I’m going to focus on the third of these three answers. Through the medium of virtual cards, and virtual counters, on a virtual board, Cultist Simulator offers a simplified and stylised representation of real human psychological and physiological needs. So, even if you have no interest in games, this aspect of this game might still be of interest to you."
  },
  {
    "objectID": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#starting-off",
    "href": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#starting-off",
    "title": "The Game of Life Beyond",
    "section": "Starting off",
    "text": "Starting off\nWhen you first start 1 Cultist Simulator you have one card, and one counter. The card says menial employment, and the counter, which is larger than the card, says work. If you move the counter onto the card, nothing happens. But if you move the card onto the counter, a dialogue box opens showing the card - menial employment - embedded in the activity of work, the counter. You then have the option to undo this association, by moving the card out of the counter dialogue box, or to commit this action: do work with menial employment.\nSo, you learn that, broadly speaking the counters are verbs (or actions) whereas the cards are nouns (or things).\nNext, you learn that actions take time. Once you execute work with menial employment, a timer appears on the work counter, counting down to new events. You learn your job is a hospital porter, it pays poorly, brings you into contact with sickness and death, and that it’s precarious.\nVery precarious: as then you’re made redundant. As part of your redundancy package you receive some new cards, and some new counters:\n\ncards:\n\na bequest\nreason\ncoins\n\ncounters:\n\nstudy\n\n\nBefore too long, you realise some of the cards relate to resources. These are:\n\ncoin\nreason\npassion\nhealth\n\nAnd the range of counters (actions) available to you expands to:\n\nwork\nstudy\ndream\nexplore\ntalk\n\nThe human purpose and motivation simulation component of the game comes from a) realising that the four card resources above are aspects of the self that need to be effectively managed to survive; and b) realising that your own mind and body plays actions (generates counters) against you. You are, in this game, in a sense completing against yourself!"
  },
  {
    "objectID": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#early-endings",
    "href": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#early-endings",
    "title": "The Game of Life Beyond",
    "section": "Early endings",
    "text": "Early endings\nWhat does this mean in practice? Let’s consider some in-game scenarios.\n\nScenario 1: After having been made redundant, you use your reason to seek employment, and find clerical work (under the critical and watchful gaze of Mr Glover Sr., the proprietor). You keep going to work, using - but never testing or developing - your reason to work to the required standards. This gives you enough money to fend off starvation, and a little more to pursue a sideline in occult literature. However, as you repeat the same quotidian cycle - sleep, work, sleep, work - you develop a sense of restlessness, of wanting or striving to do… something that’s not what you’re doing. You try to ignore this sense of restlessness, keep with the routine and ritual criticism of Mr Glover, but then eventually this restlessness turns into dread - what if this is all I’ll ever be? Most of the time, you are able to eventually shrug off this sense of dread, wait out for this feeling to dissipate. But - as the nights get ever longer - you become more receptive to dark thoughts, and this sense of dread starts to build up, until it overwhelms you. This accumulation of dread develops into an existential black hole of despair from which you are unable to recover, and your story ends.\nScenario 2: You use your reason to find clerical work, but are also willing to listen to your heart. When you encounter restlessness, you recognise that as a signal to break from your hum-drum routine. One day, instead of going to work in the office, you use your passion to work on your restlessness, and produce passionate but poorly disciplined artworks fuelled by this restless energy. The artworks have a striking immediacy, and novel, vibrant creativity expressed within them, and manage to achieve a market value that covers your basic subsistence needs. However, in the time taken to pursue this restless passion, your tardiness at work has been noticed. You have been fired. You have the option to use your passion to beg for your old job back, but are too prideful to do this. Instead you decide to become an artist full time. However, without training and discipline in the arts, and without that burning restlessness that fuelled that initial artwork, your outputs are now lacking in both restless energy and talent, so they go unsold. Although you get some psychological succour from the process of making art, they do not pay the bills, and so do not provide succour for the body as well as the soul. So, penniless, your body slowly succumbs to starvation, and you die. Your story is at an end.\nScenario 3: You vigilantly and diligently attend your reasonably paying (but belittling and soul destroying) clerical job, but seek pharmaceutical solutions to your spiritual yearnings. You let your restlessness go unanswered, until it transforms into dread. But in the darkest months, when you are most at most susceptible to totalising melancholia, you use some of your licit earnings to buy illicit pharmacopeia, and try to find chemical contentment with which you can battle your lingering sense of dread. Unfortunately, this ‘solution’ is both expensive and harmful: You use so much of your earnings to try to anaesthetise the dark clouds of your unfulfilled yearnings that you have little or no monies left to broaden your mind or meet basic needs; and eventually your dalliances with drugs bring sickness: a sickness that, without the vitality of a healthy body or even more money to pay for treatment, leads to your death. Your story is at an end.\nScenario 4: You decide that work for you should mean labour, physical labour. You rent your body, your skeleton and muscles, to the highest bidder, and become an unskilled manual labourer. Your work doesn’t pay much, meaning you have little resource to pursue any additional hobbies and passions, but it’s honest work, that just about pays the bills. Your body gets stronger, but your mind and soul remain undeveloped. Eventually, you have some bad fortune, and become injured at work. Unfortunately, with your injury you cannot work, and without savings you cannot acquire the medicines needed to bring you back to health. And once again, without being able to work, you cannot pay the bills, and eventually die of both decrepitude and starvation. Your story is at an end.\nScenario 5: You manage to juggle your time successfully between the clerical job that pays the bills, and developing your interests and and understanding of hidden worlds and realities. You become a frequent visitor to Morland’s, a bookseller specialising in esoterica. Your shelves groan with arcane knowledge and speculation, in particular about a hidden world called the Mansus, a reality beyond our reality accessible only through dreams and incantations. Your habit in bibliography becomes as expensive to maintain as your chemically-dependent cousin’s was on that past journey. Superficially, at least, it appears less harmful. With your new-found enthusiasm for the arcane and esoteric, you can’t help but discuss your interests with others; your colleagues at work start to find you strange and odd, but at least you’re reliable, and seem oddly well connected to management, so they tolerate your eccentricities. You also meet likeminded journey-men and -women, who share many of your exotic interests. Along the way, you also attract the ‘wrong kind of attention’, from the ‘authorities’, who look at your interests with suspicion and concern. However, though they find your interests strange, they never manage to prove that your interests are illegal or dangerous. So, once again you seem to be safe, pursuing this double life of bookkeeping and occultism. Unfortunately, you start to find the contours of the Mansus, sketched piecemeal and imperfectly from your growing collection, too interesting, too seductive. Eventually, your fascination with the Mansus becomes all-consuming, this world of dreams more real, more purposive, more meaningful, than this fiction the unenlightened masses believe to be the ‘real world’. You become unable to think about anything else, and so unable to function. You are insane, driven mad by your own passions. Your story is at an end.\n\nEach of the above represents a way the game can end early, due to a psychological or physiological challenge being made to you at certain points (or seasons) in the game, which you then fail to manage to meet. Without being attended to, each of these challenges will lead to a premature demise. And the way each challenge is overcome is by finding cards that function as antidotes to the psychological or physiological poisons that are the at heart of each challenge."
  },
  {
    "objectID": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#the-challenges-of-the-seasons",
    "href": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#the-challenges-of-the-seasons",
    "title": "The Game of Life Beyond",
    "section": "The Challenges of the Seasons",
    "text": "The Challenges of the Seasons\nLet’s look, in particular, at the two seasons that relate most strongly to psychological wellbeing:\n\n\n\n\n\n\n\n\n\nSeason\nChallenge\nPoison\nAntidote\n\n\n\n\nSeason of Despair: Bleak Thoughts\nAvoid Accumulating Three Dread\nDread\nContentment\n\n\nSeason of Visions: A Trembling in the air\nAvoid Accumulating Three Fascination\nFascination\nDread or Fleeting Reminiscence 2\n\n\n\nThe first seasonal challenge is the challenge failed to reach scenario 1 above, whereas the second seasonal challenge is the challenge failed to reach scenario 5 above. Note the (anti)symmetry between the two seasons: dread, which is the poison in the contest of the Season of Despair, works as a potential antidote in the context of the Season of Visions. Put more simply, the psychological game mechanics seem to be suggesting: too much dread weights you down so heavily you can no longer move, but some dread (or at least concern and anxiety about worldly things) can be what keeps you grounded and sane. The complexity of this aspect of the game comes from this mechanic, the ways that thoughts and feelings that can be poisons in some contexts can be antidotes in other contexts, and the ways that trading off actions and rewards to balance these two central seasonal challenges leads to second order seasonal challenges that themselves need to be balanced: As mentioned, an ‘easy’ way to accumulate less dread and acquire more contentment is to become a full time artist, but in doing so you struggle to pay the bills, and so have enough resource to feed the body (Scenario 2); and an ‘easy’ way acquire contentment to combat the dread that accumulates from not pursuing your passions is through illicit pharmacopeia (dreaming on a ‘tincture of opium’), but this route is both expensive and harmful to your body (Scenario 3)."
  },
  {
    "objectID": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#cultist-simulator-as-heightened-psychological-realism",
    "href": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#cultist-simulator-as-heightened-psychological-realism",
    "title": "The Game of Life Beyond",
    "section": "Cultist Simulator as ‘heightened’ psychological realism?",
    "text": "Cultist Simulator as ‘heightened’ psychological realism?\nThe two seasonal challenges - of the Season of Despair, and the Season of Visions - appear an effective way to use game mechanics to represent - in a stylised, simplified, ideal typologised way - the dual psychological maladies of, respectively, depression and mania. For individuals diagnosed with Bipolar Disorder, there is a clear need to seek and maintain an even keel between these two psychological poles: an individual experiencing mania does not (so far as I understand it) simultaneously experience depression; and while that same individual is experiencing depression, they are not simultaneously experiencing mania. Both depression and mania can be thought of as maladies of excess, but of qualitatively different, and in some senses opposing, types of excess.\nAs human psychology exists on a continuum, however, it is worth considering and contemplating the ways that this core psychological dilemma - the need to balance ‘low’ and ‘high’ moods - is something that just about everyone experiences to some varying extent, even though for most people this extent does not meet any clinical diagnostic thresholds. We forever need to make some effort to keep ourselves either from doing the psychological equivalent both of crashing into the ground, or soaring too high and burning up in the outer atmosphere."
  },
  {
    "objectID": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#settling-as-an-anti-victory",
    "href": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#settling-as-an-anti-victory",
    "title": "The Game of Life Beyond",
    "section": "Settling as an ‘anti-Victory’",
    "text": "Settling as an ‘anti-Victory’\nBut if the psychological mechanics of Cultist Simulator evoke a quality of constantly trying to maintain an appropriate affective altitude while flying the vehicle of our own minds, the question this metaphor raises is: “What is our destination?” In Cultist Simulator, if we manage to successfully avoid the premature endings outlines in the five scenarios above, there are a number of destinations we can reach, and that lead to ‘true’ (rather than premature) game ending. And these ‘true endings’ are themselves categorised into ‘Anti-Victories’ and ‘Victories’.\nThe Anti-Victories you can reach depend on the sort of character you start with. For the basic character, known as The Aspirant, the path of this Anti-Victory is broadly as follows:\n\nYou diligently attend your office job at Glover & Glover; day after day, week after week. Your attendance remains broadly punctual, and you exhaust your Reason each day to ensure you meet the exacting standards of Glover Sr. Through your diligence, punctuality and hard work, you manage to receive promotions at work, and with this greater financial security and reduced scrutiny. The job becomes less unpleasant, as well as better paid. One day, you are given the chance to offer not just your Reason to your job, but also to give your Passion to the role. On doing so, you come to identify not just your body and your mind with the job, but also your soul. You have stopped struggling, stopped striving or believing that you are or are meant to be something other than someone who works in an office and completes paperwork. You have achieved a form of peace, and a kind of purpose, amid the forms and filing cabinets. You quietly put away your foolish, childish hopes and dreams - of being something more, of finding something grander or more meaningful in the world. Your job is now your vocation. Your story is at an end.\n\nFor other character types, there are equivalent Anti-Victories, each based around the primary occupational role you were first encouraged to fulfil: as a Detective, you can successfully lock up undesirables, rather than attempting to understand, explore, and risk becoming seduced by the ideologies that drive them; and as a Dancer, you can use your physical prowess not to seek cosmic connection with the world of dreams, but to seduce a rich (but dull) suitor whom you agree to marry. In each case, you have managed to been successful in staying on the main road you started on, but been unsuccessful in not turning onto routes less travelled."
  },
  {
    "objectID": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#conclusion",
    "href": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#conclusion",
    "title": "The Game of Life Beyond",
    "section": "Conclusion",
    "text": "Conclusion\nSo, despite its apparent simplicity, Cultist Simulator is - in addition to being beautifully designed and well written - a stylised human psychology and purpose simulator, with complex game mechanics for trying to represent the need to handle the ups-and-downs of the everyday, but also the need to strive for purpose and meaning beyond the merely functional and conventional, and that bring you closer to your dreams.\nOf course, Cultist Simulator is fantasy, and the particular courses offered in the game as to how to achieve ‘true’ endings that are not Anti-Victories are not available to us in real life. But the psychological and physiological dilemmas and challenges presented to the player in Cultist Simulator appear to represent real (albeit simplified and stylised) dilemmas and challenges that every one of us, to varying extents, will have encountered, be encountering, and forever be attempting to navigate as best we can.\nAs mentioned near the start, Cultist Simulator does not have a tutorial. And neither does life."
  },
  {
    "objectID": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#footnotes",
    "href": "posts/unpop/psychomechanics-of-cultist-simulator/index.html#footnotes",
    "title": "The Game of Life Beyond",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn Cultist Simulator, the previous game endings affect how the next game starts, in particular the role you are initially assigned. The very first role is known as The Aspirant, and this is what I’m describing here.↩︎\nFleeting Reminiscence is a card that can be obtained by using your Health to Explore. In effect it is the result of attempting to become a flaneur, mindfully observing the world as it really is, rather than as you dream it to be. However this same action can (I believe) also generate cards with opposing effects, which degenerate into Fascination.↩︎"
  },
  {
    "objectID": "posts/unpop/trading-mind-game/index.html",
    "href": "posts/unpop/trading-mind-game/index.html",
    "title": "The Trading Mind Game",
    "section": "",
    "text": "Trading Game Book Cover\nGary Stevenson is an applied economist. And by applied I mean he learned how the economy worked (or didn’t) by working as a trader for CitiBank, and using their money (and their client’s money) to make bets about currency markets, and how the values of currencies would go up or down against each other, and at different timeframes. And these bets were informed by a broader theory about the global economy. A theory that, if true, would help make him and his employer rich. But also that, if true, would be terrible.\nThe theory was that, after the 2008 Global Financial Crisis, inequalities would keep rising. Amongst other things, this implied that interest rates would remain low relative to inflation. It seems to be these low relative interest rates that he kept betting on, from around 2009 through to 2014, as this has implications for the valuation of different foreign currency exchange (FX) swaps, meaning he was willing to make some bigger bets on some kinds of swaps than his fellow traders.\nAnd from a profit perspective, the bets worked out well. According to his Wikipedia page, Stevenson become CitiBank’s most profitable trader in 2011. And this doesn’t seem to have been a fluke. In his first year, he received a bonus of almost half a million pounds, despite ‘only’ aiming for a hundred thousand. In a single year, he received more than his parents earned in decades. An amount he found stupefying, alienating, even shameful.\nThe Trading Game - from which the passage above is taken - is the name of Stevenson’s first book, about his time and experiences as a trader in Citibank. It’s also the name of a particular recruitment exercise used by Citibank at the time, a card game, a bit similar to poker, which simulates some of the essentials of financial trading more generally. Stevenson won The Trading Game, and a much sought-after internship at Citibank, then a job, soon followed."
  },
  {
    "objectID": "posts/unpop/trading-mind-game/index.html#the-book-grotesque-filmic-cartoonish-serious",
    "href": "posts/unpop/trading-mind-game/index.html#the-book-grotesque-filmic-cartoonish-serious",
    "title": "The Trading Mind Game",
    "section": "The Book: Grotesque, Filmic, Cartoonish, Serious",
    "text": "The Book: Grotesque, Filmic, Cartoonish, Serious\nThe Trading Game, the book, is grotesque. It’s also coarser, more direct, and more engaging than I was expecting, especially when read aloud by the author on Audible. It’s filmic: Imagine A Beautiful Mind meets The Big Short, as directed by Guy Ritchie, but somehow good rather than terrible. And it’s also cartoonish: Stevenson’s descriptions and pseudonyms - The Frog; The Slug; The Icicle and so on - are heightened, almost magical. One frequent observation is that most of his colleagues and bosses tend to be much larger than him, and in one memorable passages he likens them to the wagyu beef they are eating at a boozy corporate lunch: block-shaped slabs of fat marbled with muscle marbled with fat, expensive, exquisitly bred, excquisitely fed.\n\n\n\n\n\n\nA Beautiful Mind\n\n\n\n\n\n\n\nThe Big Short\n\n\n\n\n\n\n\n\nGuy Ritchie’s The Gentlemen\n\n\nAnd though it’s cartoonish, and horrible, and even fun, The Trading Game is serious, but not quite in the way I expected. I was expecting a book laden with statistics, pronouncements and denouncements about the macro stories: failing public services, insatiable corporate greed, growing structural inequaliities and the existential threat that schisms growing to chasms of privilege and opportunity have for a functioning society. And although it touches on the macro, the Trading Game is much more focused on the micro: the minutea of trading, psychology and ethnography. The Supermen of Big Finance seem, by background, a strange blend of entitled privilege and genuine meritocracy: Though having very wealthy parents makes it easier to roll the dice to have more chances to get into high earning roles, as well as to become comfortable around extreme wealth, someone who can show they have an aptitude for trading games, and so is recognised as a financial asset to the bank, will be given opportunities broadly in proportion to their abilities, even if they have humble origins.\n\n\n\nCarol and the End of the World: Similarly bleak and cartoonish\n\n\nCognitively, behaviourally, attitudinally, however, the Supermen of Big Finance seem selected into homogeneity. They live in a bubble of constant stress, constant egotism, a constant need to keep earning, to keep getting ever wealthier, to keep earning, and keep earning. Work is life and life is work. They might spend hundreds of pounds on three hour business lunches, might demand the finest wines and the best food, but this affluence appears to bring little joy. All of their colleagues are also their competitors, and everyone is always, everywhere, highly competitive."
  },
  {
    "objectID": "posts/unpop/trading-mind-game/index.html#greed-is-compulsory",
    "href": "posts/unpop/trading-mind-game/index.html#greed-is-compulsory",
    "title": "The Trading Mind Game",
    "section": "Greed is Compulsory",
    "text": "Greed is Compulsory\nStevenson’s downfall, as a trader, came about because he wanted to stop playing. Because after earning more than a couple of seven figure bonuses he thought - like many people from outside would assume more of those inside would think - that he was already wealthy enough, and that instead of continuing to work 14 hours days he should stop. For the bank, however, he was still a highly profitable employee: a single individual bringing in as much profit as perhaps dozens or hundreds of regular employees. So, the bank wouldn’t let him leave, or at least not without him having to sacrifice most of his bonuses.\nThe bank tries the Golden Carrot, and the Barrister’s Stick, and tries to beat Stevenson down until he goes back to being a top earner. But unsurprisingly, neither kind of beating works, except at making Stevenson feel beaten down. Here, culturally dislocated to Japan, he loses motivation, is no longer the first to the office, and engages in such seditious behaviour as working officially contracted hours, going for runs, and learning the local language. Eventually, the bank realises the juice is no longer worth the squeeze, and - after being made to sign what Stevenson implies is some kind of NDA - Stevenson is finally allowed to leave.\nAs the quoted passage at the top suggests, a highly competent trader is unlikely to be a highly competent human being. Such was Stevenson’s monomaniac obsession with The Trading Game that other aspects of daily life - including buying clothes, furnishings, and staying in touch with family - all took a backseat. And while he was betting on the theory that wealth inequalities will continue to increase after the 2008 GFC, there was little time or energy spent on ruminating or attempting to ameliorate on such trends. The Trading Game turns everything into a game, even when the pawns and counters are other people’s lives and livelihoods."
  },
  {
    "objectID": "posts/unpop/trading-mind-game/index.html#coda",
    "href": "posts/unpop/trading-mind-game/index.html#coda",
    "title": "The Trading Mind Game",
    "section": "Coda",
    "text": "Coda\nSince quitting at Citigroup, Stevenson completed another degree, and became a social media personality and activist arguing for urgent action to reduce wealth inequalities, including a return to the kinds of marginal (90%+) tax rates which the Beatles protested in the 1960s. He argues that the problems of growing inequalities have only increased since he left CitiGroup, and became greatly accelerated as a result of some of the UK government’s policies during the COVID-19 pandemic. He’s a compelling presence, both a gifted social systems thinker and communicator, and it’s in this capacity that I became aware of Stevenson. That’s why I wasn’t expecting something as earthy, personal or (and I hate to admit this) entertaining as the Trading Game turned out to be."
  },
  {
    "objectID": "posts/unpop/fractal-game-mechanics-and-the-illusion-of-progress/index.html",
    "href": "posts/unpop/fractal-game-mechanics-and-the-illusion-of-progress/index.html",
    "title": "Levelling up… forever. On Gaming’s Shepherd’s Tone and the Fremium Trap",
    "section": "",
    "text": "Steamworld Dig\n\n\nThe Nintendo Switch is to the games console as the smartphone is to the personal computer. Miniturisation doesn’t deliver the same experience on a smaller scale, but a qualitatively different experience. Miniturisation, along with an always-on responsiveness, brings immediate rewards to ‘giving in’ to temptation, and so makes the appeal of failing to control oneself ever more seductive. The Switch, and the smartphone, give no cooldown period between the thought - let’s scroll or let’s play - and the action - picking up and pressing - and the behaviour: becoming as much played by as playing with an attention capture device.\nAnd of course the software can add another arsenal in the battle for fingertips and eyeballs. An effectively designed game or app can hermatically seal the attentional vacuum, sucking time (and money) from the user-victim to the product owner, only letting them leave once they’re a sufficiently dessicated husk of their former self, by which point further attempts at resource extraction become more effortful than rewarding from the product owner’s perspective.\nRecently I finished playing a Switch game called Steamworld Dig You play a steam-powered robot cowboy, who in the first few seconds inherits a pickaxe, and with this the keymapped affordance to dig. And so you do. That’s the entire plot. You dig down, and you dig across, and as you do so you’re rewarded, with precious things, that you can exchange for precious money. And you use this precious money you can buy a better pickaxe, then later - for tougher areas - a steam-powered drillbit.\nAnd as you dig deeper, the rocks get tougher, and the stone gets tougher. But also the precious things get more precious, meaning they’re more valuable, meaning they can be sold for more, meaning you can now buy even better pickaxes and drills which make the tougher rocks and stones easier again.\nAnd the cycle repeats itself: tougher rocks and stone so challenge gets harder, but more valuable gems so more money, and more money means better tools, so the challenge gets easier again. But then greater depth, and the rocks and stone becomes tougher again, and so the tools aren’t good enough again, but the gems get more valuable again, so the money starts rolling in faster again, and so the tools get better again, and the challenge gets easier again. Graphically, this looks something like the following:\n\n\n\n\nflowchart LR \n\ndepth\nhardness\ngems\ntools\nmoney\n\nhardness --&gt;|++| depth\ndepth --&gt;|++| hardness\ndepth --&gt;|++| gems\ngems --&gt;|++| money\nmoney --&gt;|++| tools\ntools --&gt;|--| hardness\n\n\n\n\n\nSo, this game mechanic creates a loop whereby depth both increases hardness directly, and decreases hardness indirectly through the gems-money-tools pathway. It’s the differential the effects of these two pathways that provides the sense of both challenge and progress.\nIn particular, it’s because the relationship between depth and hardness is broadly continuous, but the acquisition of tools which reduce net hardness is discrete and discontinuous, that the apparent difficulty, i.e. the net difference between the direct path and indirect depth-hardness paths, tends to be experienced as always increasing locally.\nLet’s try to think through what this would look like.\n\n\nCode\nlibrary(tidyverse)\n\n\n\n\nCode\nalpha_0 &lt;- 1\nalpha_1 &lt;- 1\n\ntoolStep &lt;- 10\ntoolInit &lt;- 5\n\ndepthToHardness &lt;- function(depth) {alpha_0 * depth}\ndepthToMoney &lt;- function(depth) {alpha_1 * depth}\nmoneyToTools &lt;- function(money) {toolInit + toolStep * money %/% toolStep}\n\ndepth &lt;- seq(0, 30, by = 0.1)\n\ndf &lt;- \n    tibble(\n        depth = depth\n        ) |&gt; \n        mutate(hardness = depthToHardness(depth)) |&gt;\n        mutate(money = depthToMoney(depth)) |&gt;\n        mutate(tools = moneyToTools(money)) \n\ndf |&gt;\n    ggplot(aes(x = depth)) + \n    geom_line(aes(y = hardness), colour = \"red\") + \n    geom_line(aes(y = tools), colour = \"blue\") +\n    labs(\n        x = \"Depth\",\n        y = \"Ground hardness (red) and tool quality (blue)\",\n        title = \"Ground hardness (red) and tool quality (blue) against depth\"\n    )\n\n\n\n\n\nSo, while the red line is below the blue line, the game is perceived as ‘easy’, and while the red line is above the blue line, the game is perceived as ‘hard’, leading to a net perceived challenge schedule a follows:\n\n\nCode\ndf |&gt;\n    mutate(net_hardness = hardness - tools) |&gt;\n    ggplot(aes(x = depth, y = net_hardness)) + \n    geom_line() + \n    geom_hline(yintercept = 0) +\n    labs(title = \"Perceived net game hardness as a function of depth\", \n    x = \"Depth\", y= \"Perceived hardness\")\n\n\n\n\n\nNow let’s think about the locally experienced sense of challenge of this kind of game from the player’s perspective. This is in effect the first derivative of this sawtoothed line: the amount that net hardness changes per unit change in depth. Almost everywhere, except on at the moments of ‘levelling up’, this is positive, and in this example identically positive for almost the entire duration of the game.\nThis kind of game mechanic is dangerously seductive, because it’s easy for the player to feel that the constant experience of encountering and overcoming locally near-constant perceived challenges represents learning and mastery, and paradoxically, despite common stereotypes that gamers lack ambition and do not seek challenges, the reality can be quite the opposite: the seductive appeal of games is often that, in the course of learning to play a game, and play it well, they are learning, and then mastering, new skills. As with many specific skills, like those developed in ‘brain training apps’ like Lumosity, or paper-based puzzles like Sudoku or crosswords, the main potential harm comes from the amount of time invested in developing skills that may not generalise or be useful in other circumstances. However, it’s not the case that, over the course of playing and mastering a game, no skills are learned.\nFor games based around the kind of ‘levelling up’ mechanics described above, however, the actual amount of learning and skills development tends to be much less than as perceived by the player. Whenever the player ‘levels up’, the difficulty of the game resets, even if the typical experience and perception from the player’s perspective is that they keep getting better.\nThere’s an auditary illusion known as the Shepherd Scale. This is a finite sequence of notes that, when repeated, gives the illusion of either ascending or descending infinitely. A listener may want to keep listening, for a long time, to find out just how high the scale might ascend, or how low it might descend. How come, if the scale keeps ascending, it never becomes ultrasonic? Have I developed bat-like hearing? Or how come, if the scale keeps descending, it never starts to shake my internal organs? Have I become a whale? A visual analogy of the illusion would be a subtly curved lift ascending or descending inside a torus. The perception is always of going up or down, but at regular intervals the passenger ends up just where they started.\nThe Gamer’s Shepherd Tone of games based on levelling up is often weaponised by games companies. Steamworld Dig features broadly just three segments, and in each segment there are some genuinely new mechanics and challenges. However the same mechanic can be applied far more exploitatively and cynically by deliberatively varying the degree of perceived challenge over the course of a segment. In particular we could imagine a sequence in which a game starts of especially easy then rapidly becomes more ‘difficult’:\n\n\n\nGaming the player\n\n\nIn the simple sketch we have a solid line that starts off giving quite a high rate of marginal return for effort to the player. However the rate of return then becomes much shallower. The player wants to reach the horizontal dashed line, where they have the opportunity to ‘level up’. However the amount of effort at the new return rate required to reach the next level is now high. At the same time as the marginal return rate decreases, the player is offered an opportunity to return to the reward schedule they were initially used to, as indicated by the dashed diagonal line. Because the rate of return offered by the dashed line is the rate they were initially used to, this alternative proposed schedule becomes attractive to them, and so they are more likely to ‘take the offer’ to move to the dashed line schedule, rather than stay on the solid line schedule.\nAnd how is this implemented by games in practice? Well, let’s assume the game’s initial monetary cost is free. Free sounds attractive, so they start playing. And the initial reward schedule from the ‘free’ game seems good. But just as they get used to high rewards for their effort, the game starts to become a grind: they have to exert more effort to get ever diminishing rates of returns. But at the same time, when they’re just starting to become bored and frustrated with the game, they’re given the option to get a special item, or special powerup, or (say) an ‘experience pack’, that allows them to get back to the dashed line schedule, and so find the game ‘fun’ again.\nAnd the cost of moving back to the dashed line schedule: maybe just a couple of pounds, or dollars or usual. Less than the cost of a sandwich, or a coffee. So why not? The game’s still free, and it’s been fun, so why not spend less than you’d spend in a lunchbreak on food to make this lunchbreak distraction fun again?\nAnd so, now the player’s spent a couple of pounds to get more quickly to the next level. Now what happens?\nExactly the same, at the next stage. The player’s now more invested. They’ve got more to gain, and more to lose by stopping. So maybe next time they’re offered the deal, it’s no longer £2, but (say) £2.50. But because they’ve already invested £2 on the game. Why not another £2.50. Maybe for the next stage things will really get interesting. Maybe there’s a special item, a really good do-dad, that exists just around the corner. And they’d like to see what’s around the corner, but they don’t fancy the grind, so they pick the escalator option again.\nAnd so on, and so on. Like the Shepherd Scale, the player thinks they’re progressing, but all that’s really progressing is the amount they’re willing to pay, drip by drip, to keep playing. The player, without realising it, is being played.\nThis kind of game really isn’t free, it’s Fremium, and it’s dishonest and dangerous. Most people will learn that the only way to win is not to play at all. But a few people won’t. A few people will spend hundreds, or thousands, of pounds, or dollars, or Euros, to keep playing. And these people: these are the whales, and the business model for the company depends on hooking as many of these whales as possible. Draining their time, and their savings, until they’ve got nothing left. And all they’ve got to show for it is a shiny digital pickaxe and a hole in their bank balance.\nNow, to clarify, Steamworld Dig thankfully isn’t freemium. The costs are upfront, and the commitment is finite. I just recognised after playing it that I felt played by its psychological game mechanics. And the psychological games it plays are those that more unscrupulous games used for much more nefarious purposes."
  },
  {
    "objectID": "posts/unpop/david-lynch-all-american-shaman/index.html",
    "href": "posts/unpop/david-lynch-all-american-shaman/index.html",
    "title": "David Lynch: All-American Shaman",
    "section": "",
    "text": "David Lynch. Source"
  },
  {
    "objectID": "posts/unpop/david-lynch-all-american-shaman/index.html#diversity-in-neurodiversity",
    "href": "posts/unpop/david-lynch-all-american-shaman/index.html#diversity-in-neurodiversity",
    "title": "David Lynch: All-American Shaman",
    "section": "Diversity in Neurodiversity",
    "text": "Diversity in Neurodiversity\nI’ve discussed the profound contribution of the neurodiverse, to literature and broader society, at least a couple of times before: In my post on Andy Weir’s Eng-Fi; and in discussing the gifted strangeness of Barack Obama’s recent ancestry, in particular his grandfather. Arguably the dark myth of The Revenger, described in my post on The Beekeeper, suggests there’s a widespread hankering (and so ‘sanctioning’) of another kind of neurodiverse personality type: call it something like The Altruistic Psychopath. In each case, the neurodiverse personality profile is very different, as is the specific type of societal niche they are sanctioned to occupy. But what each type of strange person has in common is that, from the perspective of the neurotypical, they really are genuinely strange. And that, like the closing line in Sapolsky’s quotation, one (or a few) of them is great; but two (or many) of them would be terrible.\nWhich brings me to David Lynch."
  },
  {
    "objectID": "posts/unpop/david-lynch-all-american-shaman/index.html#modern-day-shamanism",
    "href": "posts/unpop/david-lynch-all-american-shaman/index.html#modern-day-shamanism",
    "title": "David Lynch: All-American Shaman",
    "section": "Modern Day Shamanism",
    "text": "Modern Day Shamanism\nDavid Lynch is a modern day, all-American shaman. Someone whose mind finds profound meaning, and magic, in the everyday. Someone who appears to live both here, and there, and for whom there is always here, if you care to look, to follow the connections, to join the dots. A transcendental meditator, Lynch is attuned to the symbolic associations that sounds and sights hold, and the deeper meaning they can reveal. In Lynch’s films and television, scenes often appear slow, with few or no cuts from the start to the end of an action. The sounds of nature, of traffic, the hums of air conditioners, the buzzes of electrical devices, the wheezes of purifiers, the incidental rhythms and repetitions of the world, are all presented without cuts. For Lynch appears to find the everyday, even the interstitial moments between events, enchanted and purposeful, magical. Because for Lynch there appear to be no events-between-events: everything is eventful, if one cares to understand.\nAnd Lynch’s shamanism perhaps finds no purer expression than in Gotta Light?: Episode 8 of Twin Peaks: The Return, one of the strangest and most beautiful pieces of cinematic art ever to have been broadcast on television. Gotta Light? appears to be a complete Shamanic Myth, told almost entirely through visual symbolism, about the origins of evil in the world. In particular - continuing a lifelong preoccupation Lynch seems to have - about the origins of male violence towards women. 2 Descriptively, the Myth of Gotta Light? appears something as follows:\n\nThe people had scientists. And the scientists did something terrible. They took a sacred power from the gods, from Nature, and that was the power of Nuclear Fission. And in taking this power, and bringing it to their world, they created a Primordial Evil, who did appear as a black sphere that came through a tear in reality made by the scientists’ evil magic. And this Primordial Evil is called BOB. And BOB did come down to the Earth to find its place in Man’s hearts. And with BOB emerged his demonic servants, The Woodsmen. And the Woodsmen came from the shadows, and were as shadows, and came into the world of Men, and did break men, and speak to men in their hypnotic tongue, and through this sought to find a place for BOB to crawl into the head and heart of weak men, to hide as man, to lie dormant. For BOB and The Woodsmen knew that, amongst the gods, the lady who listened to the phonograph had felt the disruption to the cosmic order the scientists had made, and the evil they had conjoured, and so told The Fireman of this evil, and of this disorder. And The Fireman, seeing how the cosmos had become unbalanced, did tear from himself an essence of pure Good, which rose as a white sphere from him, and sent it down to the world of the people. And so the gods made the world balanced again - white balancing black; good balancing evil; purity balancing dirt.\n\n\n\n\n\n\n\n???? - AKA The Fireman. Source\n\n\n\n\n\n\n\nThe Lady Listening to the Phonograph. Source\n\n\n\n\n\nThis myth is thematically manichean, focused on the ideas of there being pure evil and pure good, black and white, and the conflict between these two forces. By extension, the myth is also somewhat Christian, with BOB functioning somewhat as Satan, and the White-Sphere-that-becomes-Laura-Palmer as Jesus. However Lynch’s Myth appears his own design, his own vision, sharing a family resemblance with Christianity rather than being inspired by it. Instead, it appears Gotta Light? is an expression of something magical that, for Lynch, was somewhat concrete and ‘true’."
  },
  {
    "objectID": "posts/unpop/david-lynch-all-american-shaman/index.html#the-societal-benefits-of-shamanic-personalities",
    "href": "posts/unpop/david-lynch-all-american-shaman/index.html#the-societal-benefits-of-shamanic-personalities",
    "title": "David Lynch: All-American Shaman",
    "section": "The Societal Benefits of Shamanic Personalities",
    "text": "The Societal Benefits of Shamanic Personalities\nAs the extended quote from Sapolsky indicates, there’s probably something societally advantagous about having some people (but not too many people) who think like Lynch. How so? Well, we can get a sense of how such myths might (pre)historically have been valuable, even necessary, by considering the broader canon of Twin Peaks and its primary inciting incident: The Death of Laura Palmer. Laura, a young woman, full of potential and hope, had her life cut tragically short, brutally murdered, wrapped in plastic and dumped in the river. Later it emerged that in the months before her death she was abused and brutalised, apparently by those whom she should most have been able to trust to care for her. Why did this happen? How can such things… be allowed to happen?\n\n\n\nLaura Palmer: Cosmic Victim. Source\n\n\nIn real life, someone asking such questions won’t tend to want ‘real’ answers. But they will want some answers, that speak somehow to a higher reality. If someone neurotypical were asked to explain why such kinds of tragedy were allowed to occur, they would likely offer brief, kindly, platitudinal obfuscation: life’s a mystery; they’re in a better place now. And if someone neurodiverse but leaning towards the logical mechanistic thinking of classical autism 3 were asked such a question they would probably seek to provide a complete and coherent mechanical explanation: given the wound marks identified on the body, what is the most probable sequence in which the wounds were inflicted? What type of implement was used, and which of these wounds would likely have been fatal? Neither of these types of response is likely to console the close family and friends of an innocent victim.\nBut if a shaman were asked, they might be able to say something, like the above, that provides a much deeper sense of consolation, meaning and comfort than either the neurotypical or mechanically provided could offer. Someone inclined towards meta-magical thinking could connect the plain, tragic facts of the matter into a constellation of cosmic wonder, testify to the meaning and impact that the deceased had in life, and continues to have in death, and besides, death, over there, is still life, and everyone you have ever known and loved is still alive, just over there, not over here.\nAnd the person seeking consolation would find the shaman’s explanation to be believable, unlike the platitudes of the neurotypical, because the shaman’s half-crazy enough to believe what they’re saying too, with utter, concrete, certainty and conviction. What the shaman says isn’t just true, it’s truth."
  },
  {
    "objectID": "posts/unpop/david-lynch-all-american-shaman/index.html#stuff-of-wonder-and-nonsense",
    "href": "posts/unpop/david-lynch-all-american-shaman/index.html#stuff-of-wonder-and-nonsense",
    "title": "David Lynch: All-American Shaman",
    "section": "Stuff-of-wonder, and nonsense",
    "text": "Stuff-of-wonder, and nonsense\nIn a different time and place, people like David Lynch would have founded the myths that bound tribes and nations together. 4 Right now, many modern day shamen likely listen to, and even appear on, ponderous podcasts, talking with wide eyes about aliens, conspiracies, sunlight and nutrition, perhaps expressing vaccine skepticism, even buoying ethnonationalist fearmongering. Rather than being seen simply as geysers of misinformation, however, spouting untruth and nonsense, these half-crazy people also need to be recognised as conduits to spiritual connectedness and cosmic togetherness. And sometimes this stuff of wonder is something that keeps society functioning.\nAnd David Lynch? When he’s allowed to, he still makes television and film. But even when he’s not, he still can’t help but make art."
  },
  {
    "objectID": "posts/unpop/david-lynch-all-american-shaman/index.html#footnotes",
    "href": "posts/unpop/david-lynch-all-american-shaman/index.html#footnotes",
    "title": "David Lynch: All-American Shaman",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor followers of Twin Peaks: The Return, this reference appears especially fitting, as one of the main mystic characters encountered is called The Fireman, and literally lives in a cosmic lighthouse.↩︎\nTwin Peaks: The Return also features the apparently senseless death of a young boy in a hit-and-run↩︎\nThe tendencies towards solitary activity, and difficulty with many common activities involving others, may lead to those with Schizotypalism to be diagnosed with autism at a higher-than-chance rate. Indeed, autism was originally believed to be a type of schizophrenia. However, whereas classical autism involves difficulties with most social relationships arising from having a way of thinking that’s much more categorical, organised and ordered than most neurotypicals; schizotypalism involves difficulties with most social relationships arising from having a way of thinking that’s much more cosmic, nebulous, and disordered than most neurotypicals. At this level of abstraction, autism and schizotypalism appear almost as neurological opposites of each other. To see either as variations of the other appears, to me, as perverse as categorising both someone who’s drowned, and someone who’s died of dehydration, as both having died of ‘Water Abnormality Syndrome’.↩︎\nParadoxically, they also likely suffered under theocracies based on such myths, because theocracy tends to weigh down dreams into dogma, turn spiritualism into an institution, and so brand shamen as heretics. Shamen are creators; priests are bureacrats.↩︎"
  },
  {
    "objectID": "posts/the-other-left-right-divide/index.html",
    "href": "posts/the-other-left-right-divide/index.html",
    "title": "The Other Left-Right Divide: Iain McGilchrist and the Battle of the Hemispheres",
    "section": "",
    "text": "Iain McGilchrist on the mythos and the machine\n\n\nIn Watching The English, Fox (2005) writes that:\n\nAt the most basic level, an underlying rule in all English conversation is the proscription of ‘earnestness’. … [The] English are probably more acutely sensitive than any other nation to the distinction between ‘serious’ and ‘solemn’, between ‘sincerity’ and ‘earnestness’.\n… [The] Importance of Not Being Earnest rule is really quite simple. Seriousness is acceptable, solemnity is prohibited. Sincerity is allowed, earnestness is strictly forbidden. Pomposity and self-importance are outlawed. Serious matters can be spoken of seriously, but one must never take oneself too seriously. [p. 62]\n\nA serial violator of the Importance of Not Being Earnest Rule is Damien Walter, producer and host of the Science Fiction podcast, which states its mission as being to explore “the best in SF storytelling [and to ask] what happens when logos meets mythos, reason meets imagination and science … meets fiction”. English former Guardian journalist Damien Walter (alternately Damien G Walter) is very earnest. Which might explain why he doesn’t live in England anymore.\nIn the latest podcast, the very earnest Englishman Damien G Walter interviews the very earnest Scotsman Iain McGilchrist, talking broadly around McGilchrist’s somewhat mythic framing of the left-right divide. McGilchrist’s left-right divide isn’t a divide between the political Left and Right, but a divide between the two hemispheres of the brain.\nMcGilchrist professes that his left-right hemispheric divide isn’t mere pop science, attributing certain temperaments or qualities, like reason and creativity, to one or the other hemisphere. Instead his argument seems marginally more subtle that that, something like:\n\nThe left hemisphere’s domain is the centre. It’s the part of the brain that takes charge when you choose to focus on an object, grasp it, manipulate it, name it, take it apart and put it back together, use or abuse it as a tool.\nThe right hemisphere’s domain is the periphery. It’s what notices and contextualizes all around you, and so provides the context through which one can relate to and negotiate with the totality of the world.\n\nMcGilchrist’s broader thesis appears to be that broadly left-hemispheric thinking has become somewhat over-dominant in modern culture, leading to an overly atomistic and instrumentalist way of thinking. Everything is thought about, to some extent, in terms of how it can be used, grasped, broken down and thought about as machines and systems. Walter makes the intriguing observation that this may help explain a tendency towards literal-mindedness in much commentary and critique of modern sci-fi and fantasy, which appears blind or indifferent to underlying mythos and symbolism that stories are drawing from. I think there’s much compelling about this literal-mindedness observation, even if I’m somewhat more ambivalent about the left-right hemispheric distinction drawn by McGilchrist more generally, especially in terms of the trends or tendencies he’s proposing.\nSince starting this blog in late November, I’ve discovered most of my posts tend to focus either on statistics or stories. These dual preoccupations don’t completely map onto the left-right hemispheric distinction - for example there’s a lot of contextualisation (right-thinking) involved in finding meaning in statistical outputs; and there is value in thinking about stories in a somewhat mechanical, graspable-component-like way - but it’s not a bad first approximation. I find stories valuable to think about, especially where they bring an intense quality of emotional engagement and I want to know why. Sometimes I even risk treating the exploration and interpretation of stories with the earnestness they deserve (even when writing about Robocop).\n\n\n\n\nReferences\n\nFox, K. 2005. Watching the English: The Hidden Rules of English Behaviour. Hodder & Stoughton. https://books.google.co.uk/books?id=tNZfLeHSFvQC."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-packages/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-packages/index.html",
    "title": "Tidy Tuesday Extra - Packages and recursive searching",
    "section": "",
    "text": "The latest tidytuesday dataset contains information on R packages and how they’re related to each other. The relationship information they contain poses some interesting challenges and opportunities. I (Jon), foolishly and/or sadistically, suggested trying to build a recursive algorithm which, given a given package, traces out the other packages that either depends on it, or that it depends on.\nWe didn’t quite get where we were hoping to, but hopefully in this post I’ll be able to unpack some of the challenges and opportunities this approach could bring.\nUnlike most tidy tuesday challenges, my (horrible) suggestion brought us into the land of computer science, rather than the data science challenges that most tidy tuesday sessions tend to be focused on. Rather than cover what we did and didn’t achieve in that session, this post is largely my attempt to think through the challenge of building and developing a recursive function in R for allowing us to trace through a tree starting from a given node. The post isn’t intended to solve the challenge I initially suggested, but to lay out some of the conceptual groundwork required to do so later."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#introduction",
    "href": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#introduction",
    "title": "Tidy Tuesday Extra - Packages and recursive searching",
    "section": "",
    "text": "The latest tidytuesday dataset contains information on R packages and how they’re related to each other. The relationship information they contain poses some interesting challenges and opportunities. I (Jon), foolishly and/or sadistically, suggested trying to build a recursive algorithm which, given a given package, traces out the other packages that either depends on it, or that it depends on.\nWe didn’t quite get where we were hoping to, but hopefully in this post I’ll be able to unpack some of the challenges and opportunities this approach could bring.\nUnlike most tidy tuesday challenges, my (horrible) suggestion brought us into the land of computer science, rather than the data science challenges that most tidy tuesday sessions tend to be focused on. Rather than cover what we did and didn’t achieve in that session, this post is largely my attempt to think through the challenge of building and developing a recursive function in R for allowing us to trace through a tree starting from a given node. The post isn’t intended to solve the challenge I initially suggested, but to lay out some of the conceptual groundwork required to do so later."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#recursion-as-a-concept",
    "href": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#recursion-as-a-concept",
    "title": "Tidy Tuesday Extra - Packages and recursive searching",
    "section": "Recursion as a concept",
    "text": "Recursion as a concept\nA recursive function is a function that conditionally evokes itself. It’s a beautiful and horrifying idea - coding Inception - and as the link above suggests is often used when a more complex problem needs to be broken down into ever smaller steps. The fifth example in the above link says that it’s great for exploring and parsing tree and graph structures. And indeed that’s the kind of application I was thinking about when I saw the TidyTuesday dataset.\nI’ve only found reason to build a recursive algorithm in R once before, perhaps around a decade ago. My problem was that I had a two dimensional regular matrix of values, but some of the cells contained missing values. I wanted to build a function that, for any missing value in the matrix, would impute a value for the missing cell given the average of the values in the eight cells that surrounded it, something known as a Moore Neighbourhood. The Wikipedia example image used is as follows:\n\n\n\nMoore Neighbourhood\n\n\nIf each missing cell \\(C\\) was surrounded only by non-missing cells, then there would have been no need for recursion. However there were examples in the data where two or more contiguous/neighbouring cells cells were missing. I used recursion to solve this problem by calling the imputation function on any missing neighbours (Say \\(NE\\)) of the missing target cell \\(C\\). The missing neighbour cell would then become the new target cell \\(C\\), and if any of this target cell’s neighbours were missing, then the imputation function would be called once again, with the last stage’s neighbour cell now the new target cell. Only if the condition that a target cell has no missing neighbours would the imputation function actually impute.\nIn effect, this use of recursion meant that, for a patch of missing cells, the imputation would occur outside-to-inside, i.e. from the cell with the most non-missing neighbours to the cell with the fewest.\nAnyway, with that example in mind, let’s look at the data."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#loading-the-data",
    "href": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#loading-the-data",
    "title": "Tidy Tuesday Extra - Packages and recursive searching",
    "section": "Loading the data",
    "text": "Loading the data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidytuesdayR)\n\ndta_list &lt;- tidytuesdayR::tt_load(2024, week = 16)\n\n--- Compiling #TidyTuesday Information for 2024-04-16 ----\n--- There are 2 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `shiny_revdeps.csv`\n    Downloading file 2 of 2: `package_details.csv`\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n--- Download complete ---\n\ndta_main &lt;- dta_list$shiny_revdeps\ndta_main\n\n# A tibble: 146,135 × 3\n   child              dependency_type parent\n   &lt;chr&gt;              &lt;chr&gt;           &lt;chr&gt; \n 1 AFheritability     depends         shiny \n 2 AMPLE              depends         shiny \n 3 animalEKF          depends         shiny \n 4 bde                depends         shiny \n 5 BDP2               depends         shiny \n 6 BoneProfileR       depends         shiny \n 7 clinDR             depends         shiny \n 8 CLME               depends         shiny \n 9 cocktailApp        depends         shiny \n10 competitiontoolbox depends         shiny \n# ℹ 146,125 more rows\n\n\nOf course we didn’t notice the dataset was focused on shiny!"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#exploration",
    "href": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#exploration",
    "title": "Tidy Tuesday Extra - Packages and recursive searching",
    "section": "Exploration",
    "text": "Exploration\nWhat are the types of dependency listed?\n\nunique(dta_main$dependency_type)\n\n[1] \"depends\"   \"imports\"   \"suggests\"  \"linkingto\"\n\n\nSo, where the parent is shiny, how many types of each dependency are there?\n\ndta_main %&gt;% \n  filter(parent == \"shiny\") %&gt;% \n  count(dependency_type)\n\n# A tibble: 3 × 2\n  dependency_type     n\n  &lt;chr&gt;           &lt;int&gt;\n1 depends            78\n2 imports           793\n3 suggests          305\n\n\nIs shiny its own parent?\n\ndta_main %&gt;% \n  filter(parent == \"shiny\") |&gt;\n  filter(child == \"shiny\")\n\n# A tibble: 0 × 3\n# ℹ 3 variables: child &lt;chr&gt;, dependency_type &lt;chr&gt;, parent &lt;chr&gt;\n\n\nNo, fortunately.\nDoes the dataset contain examples where shiny is neither the parent nor the child?\n\ndta_main |&gt;\n  filter(parent != \"shiny\" & child != \"shiny\")\n\n# A tibble: 144,928 × 3\n   child                 dependency_type parent  \n   &lt;chr&gt;                 &lt;chr&gt;           &lt;chr&gt;   \n 1 FAMetA                depends         LipidMS \n 2 teal.modules.clinical depends         teal    \n 3 teal.modules.general  depends         teal    \n 4 dartR                 depends         adegenet\n 5 dartR.base            depends         adegenet\n 6 dartR.captive         depends         adegenet\n 7 dartR.data            depends         adegenet\n 8 dartR.popgen          depends         adegenet\n 9 dartR.sim             depends         adegenet\n10 dartR.spatial         depends         adegenet\n# ℹ 144,918 more rows\n\n\nYes it does."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#defining-a-problem",
    "href": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#defining-a-problem",
    "title": "Tidy Tuesday Extra - Packages and recursive searching",
    "section": "Defining a problem",
    "text": "Defining a problem\nAs we’ve finally looked enough at the dataset and documentation to know that shiny is the root, let’s work out how many packages are children where dependency type is imports and parent is shiny.\n\ndta_main |&gt;\n  filter(parent == \"shiny\" & dependency_type == \"imports\") |&gt;\n  count(child) |&gt;\n  nrow()\n\n[1] 793\n\n\nThere appear to be 793 packages that have this relationship.\nLet’s say we want to take this list of 793 packages and find all packages that have them as children.\n\nget_children &lt;- function(parent_name) {\n    dta_main |&gt; \n    filter(parent == parent_name) |&gt; \n    filter(dependency_type == \"imports\") |&gt; \n    pull(child) |&gt; \n    unique()\n}\n\nchild_shiny &lt;- get_children(\"shiny\")\n\nlength(child_shiny)\n\n[1] 793\n\n\nThere are almost 15000 packages with this relationship as children.\nWe can now start to think about the recursive search problem by running the get_children function for each child package, with the name of the child now the name of the parent.\nLet’s start with the five first packages who are direct children of shiny.\n\nsome_shiny_children &lt;- child_shiny[1:5]\n\nsome_shiny_grandchildren &lt;- some_shiny_children |&gt;\n  map(~get_children(.))\n\nsome_shiny_children\n\n[1] \"ABACUS\"        \"abstractr\"     \"activAnalyzer\" \"AdaptGauss\"   \n[5] \"adaptiveGPCA\" \n\nsome_shiny_grandchildren\n\n[[1]]\ncharacter(0)\n\n[[2]]\ncharacter(0)\n\n[[3]]\ncharacter(0)\n\n[[4]]\n[1] \"DistributionOptimization\" \"opGMMassessment\"         \n[3] \"scapGNN\"                  \"Umatrix\"                 \n\n[[5]]\ncharacter(0)\n\n\nFor packages 1, 2, 3 and 5 there are no further children. However for package four there are four packages that are children.\nLet’s see if the children of package 4 themselves have children.\n\ngreat_grandchildren &lt;- some_shiny_grandchildren[[4]] |&gt;\n  map(~get_children(.))\n\ngreat_grandchildren\n\n[[1]]\n[1] \"opGMMassessment\"\n\n[[2]]\n[1] \"EDOtrans\"\n\n[[3]]\ncharacter(0)\n\n[[4]]\ncharacter(0)\n\n\nTwo of the great grandchildren have children."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#recursive-search-with-a-toy-example",
    "href": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#recursive-search-with-a-toy-example",
    "title": "Tidy Tuesday Extra - Packages and recursive searching",
    "section": "Recursive search with a toy example",
    "text": "Recursive search with a toy example\nLet’s try to think through the fundamentals of a recursive function using a toy example.\n\ntoy_data &lt;- tribble(\n    ~parent, ~child, \n    \"A\", \"B\",\n    \"A\", \"C\",\n    \"A\", \"D\",\n    \"B\", \"E\",\n    \"C\", \"F\",\n    \"C\", \"G\",\n    \"G\", \"J\",\n    \"E\", \"H\",\n    \"E\", \"I\"\n)\n\nThis dataset shows the following set of relationships:\n\n\n\n\nflowchart TB\n\nA --&gt; B\nA --&gt; C\nA --&gt; D\nB --&gt; E\nC --&gt; F\nC --&gt; G\nG --&gt; J\nE --&gt; H\nE --&gt; I\n\n\n\n\n\n\nLet’s first see if we can identify which of these nodes are roots. i.e. nodes that are children but have no parents.\n\nis_root &lt;- function(df, node_label){\n    res &lt;- df |&gt; filter(parent == node_label) |&gt; nrow()\n\n    if(res == 0){\n        return(TRUE)\n    } else {\n        return(FALSE)\n    }\n}\n\nLet’s test this for each of the nodes in the toy dataset.\n\nall_nodes &lt;- unique(c(toy_data$parent, toy_data$child))\n\n\n# run manually for a couple of examples:\n\nis_root(toy_data, \"A\")\n\n[1] FALSE\n\nis_root(toy_data, \"B\")\n\n[1] FALSE\n\nis_root(toy_data, \"D\")\n\n[1] TRUE\n\nis_root(toy_data, \"F\")\n\n[1] TRUE\n\n# run using functional programming \n\nroots &lt;-  \n  sapply(all_nodes, function(x) is_root(toy_data, x))\n\nroots\n\n    A     B     C     G     E     D     F     J     H     I \nFALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE \n\n\nNext we want to use the is_root() function inside a find_roots() function that will return all the roots in a dataset.\n\nfind_roots &lt;- function(df){\n    all_nodes &lt;- unique(c(df$parent, df$child))\n    \n    roots &lt;-  \n      sapply(all_nodes, function(x) is_root(df, x))\n    \n    return(all_nodes[roots])\n}\n\nfind_roots(toy_data)\n\n[1] \"D\" \"F\" \"J\" \"H\" \"I\""
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#recursive-root-finding-function",
    "href": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#recursive-root-finding-function",
    "title": "Tidy Tuesday Extra - Packages and recursive searching",
    "section": "Recursive Root Finding Function",
    "text": "Recursive Root Finding Function\nLet’s now think through a trace_to_root function, that uses recursion, and how it will work.\n\nIf trace is null, then start trace with node\nIf node is root, then return trace\nIf node is not root, then add each child to a trace, and rerun trace_to_root with the current node and trace parameters.\n\nAs part of debugging and development, I’ve added an option verbose, which reports on what the function is doing at each step.\n\ntrace_to_root &lt;- function(node, trace = NULL, df, verbose = FALSE){\n    if (verbose){\n      message(\"Current node is \", node)\n      message(\"Current trace is \", trace)\n    }\n   \n    if (is.null(trace)){\n        trace &lt;- list(node)\n    }\n\n    if (is_root(df, node)){\n        if (verbose) {\n          message(\"node \", node, \" is a root, so returning trace\")\n        }\n        return(trace)\n    } else {\n        if (verbose) {\n          message(\"node \", node, \" is not a root, so continuing\")\n        }\n\n        children &lt;- df |&gt; filter(parent == node) |&gt; pull(\"child\")\n\n        if (verbose) {\n          message(\"have found \", length(children), \" children of \", node)\n        }\n\n        pass_down &lt;- function(child, trace, verbose = TRUE) {\n            if (verbose) {message(\"current child is \", child)}\n            trace &lt;- c(trace, child)\n            if (verbose) {message(\"trace is \", trace)}\n            return(trace_to_root(child, trace, df = df, verbose = verbose))\n        }\n        # This is where recursion happens\n        return(\n          map2(children, trace, pass_down)\n        )\n\n    }\n}\n\nAs with many complex functions, this was developed through a number of steps, most of which involved extensive debugging and brow-furrowing. The use of the toy example and the graph, along with the verbose mode, made it easier to see whether the function was doing what I wanted it to, even if what it returns isn’t necessarily in the nicest format.\nLet’s start with node ‘H’, which should be identified as a root, with no further children. This should mean the number of operations performed and reported should be short:\n\ntrace_to_root(\"H\", df = toy_data, verbose = TRUE)\n\nCurrent node is H\n\n\nCurrent trace is \n\n\nnode H is a root, so returning trace\n\n\n[[1]]\n[1] \"H\"\n\n\nThis seems to work as expected. Node ‘D’ should be similarly simple:\n\ntrace_to_root(\"D\", df = toy_data, verbose = TRUE)\n\nCurrent node is D\n\n\nCurrent trace is \n\n\nnode D is a root, so returning trace\n\n\n[[1]]\n[1] \"D\"\n\n\nOne step up in complexity/number of operations should be node G, which will be the first use-case that will involve some recursion.\n\ntrace_to_root(\"G\", df = toy_data, verbose = TRUE)\n\nCurrent node is G\n\n\nCurrent trace is \n\n\nnode G is not a root, so continuing\n\n\nhave found 1 children of G\n\n\ncurrent child is J\n\n\ntrace is GJ\n\n\nCurrent node is J\n\n\nCurrent trace is GJ\n\n\nnode J is a root, so returning trace\n\n\n[[1]]\n[1] \"G\" \"J\"\n\n\nThe list returned contains two elements, the first of which is G, and the second of which is J. This is the correct trace sequence.\nNow let’s look at node E\n\ntrace_to_root(\"E\", df = toy_data, verbose = TRUE)\n\nCurrent node is E\n\n\nCurrent trace is \n\n\nnode E is not a root, so continuing\n\n\nhave found 2 children of E\n\n\ncurrent child is H\n\n\ntrace is EH\n\n\nCurrent node is H\n\n\nCurrent trace is EH\n\n\nnode H is a root, so returning trace\n\n\ncurrent child is I\n\n\ntrace is EI\n\n\nCurrent node is I\n\n\nCurrent trace is EI\n\n\nnode I is a root, so returning trace\n\n\n[[1]]\n[1] \"E\" \"H\"\n\n[[2]]\n[1] \"E\" \"I\"\n\n\nThis time the outer list is of length two, each of whcih containing two elements. The first sublist denotes the path E to H, and the second the path E to I. Once again this first with what we know about the part of the tree starting at E: it splits into two paths.\nNow node C:\n\ntrace_to_root(\"C\", df = toy_data, verbose = TRUE)\n\nCurrent node is C\n\n\nCurrent trace is \n\n\nnode C is not a root, so continuing\n\n\nhave found 2 children of C\n\n\ncurrent child is F\n\n\ntrace is CF\n\n\nCurrent node is F\n\n\nCurrent trace is CF\n\n\nnode F is a root, so returning trace\n\n\ncurrent child is G\n\n\ntrace is CG\n\n\nCurrent node is G\n\n\nCurrent trace is CG\n\n\nnode G is not a root, so continuing\n\n\nhave found 1 children of G\n\n\ncurrent child is J\n\n\ntrace is CJ\n\n\nCurrent node is J\n\n\nCurrent trace is CJ\n\n\nnode J is a root, so returning trace\n\n\ncurrent child is J\n\n\ntrace is GJ\n\n\nCurrent node is J\n\n\nCurrent trace is GJ\n\n\nnode J is a root, so returning trace\n\n\n[[1]]\n[1] \"C\" \"F\"\n\n[[2]]\n[[2]][[1]]\n[1] \"C\" \"J\"\n\n[[2]][[2]]\n[1] \"G\" \"J\"\n\n\nThe list object contains two sublists. The first sublist indicates the path C to F. The second sublist itself contains two sublists: one denoting a path C to J; the second of which denotes a path G to J.\nNow let’s look at the tree as a whole, i.e. start at node A:\n\ntrace_to_root(\"A\", df = toy_data, verbose = TRUE)\n\nCurrent node is A\n\n\nCurrent trace is \n\n\nnode A is not a root, so continuing\n\n\nhave found 3 children of A\n\n\ncurrent child is B\n\n\ntrace is AB\n\n\nCurrent node is B\n\n\nCurrent trace is AB\n\n\nnode B is not a root, so continuing\n\n\nhave found 1 children of B\n\n\ncurrent child is E\n\n\ntrace is AE\n\n\nCurrent node is E\n\n\nCurrent trace is AE\n\n\nnode E is not a root, so continuing\n\n\nhave found 2 children of E\n\n\ncurrent child is H\n\n\ntrace is AH\n\n\nCurrent node is H\n\n\nCurrent trace is AH\n\n\nnode H is a root, so returning trace\n\n\ncurrent child is I\n\n\ntrace is EI\n\n\nCurrent node is I\n\n\nCurrent trace is EI\n\n\nnode I is a root, so returning trace\n\n\ncurrent child is E\n\n\ntrace is BE\n\n\nCurrent node is E\n\n\nCurrent trace is BE\n\n\nnode E is not a root, so continuing\n\n\nhave found 2 children of E\n\n\ncurrent child is H\n\n\ntrace is BH\n\n\nCurrent node is H\n\n\nCurrent trace is BH\n\n\nnode H is a root, so returning trace\n\n\ncurrent child is I\n\n\ntrace is EI\n\n\nCurrent node is I\n\n\nCurrent trace is EI\n\n\nnode I is a root, so returning trace\n\n\ncurrent child is C\n\n\ntrace is AC\n\n\nCurrent node is C\n\n\nCurrent trace is AC\n\n\nnode C is not a root, so continuing\n\n\nhave found 2 children of C\n\n\ncurrent child is F\n\n\ntrace is AF\n\n\nCurrent node is F\n\n\nCurrent trace is AF\n\n\nnode F is a root, so returning trace\n\n\ncurrent child is G\n\n\ntrace is CG\n\n\nCurrent node is G\n\n\nCurrent trace is CG\n\n\nnode G is not a root, so continuing\n\n\nhave found 1 children of G\n\n\ncurrent child is J\n\n\ntrace is CJ\n\n\nCurrent node is J\n\n\nCurrent trace is CJ\n\n\nnode J is a root, so returning trace\n\n\ncurrent child is J\n\n\ntrace is GJ\n\n\nCurrent node is J\n\n\nCurrent trace is GJ\n\n\nnode J is a root, so returning trace\n\n\ncurrent child is D\n\n\ntrace is AD\n\n\nCurrent node is D\n\n\nCurrent trace is AD\n\n\nnode D is a root, so returning trace\n\n\n[[1]]\n[[1]][[1]]\n[[1]][[1]][[1]]\n[1] \"A\" \"H\"\n\n[[1]][[1]][[2]]\n[1] \"E\" \"I\"\n\n\n[[1]][[2]]\n[[1]][[2]][[1]]\n[1] \"B\" \"H\"\n\n[[1]][[2]][[2]]\n[1] \"E\" \"I\"\n\n\n\n[[2]]\n[[2]][[1]]\n[1] \"A\" \"F\"\n\n[[2]][[2]]\n[[2]][[2]][[1]]\n[1] \"C\" \"J\"\n\n[[2]][[2]][[2]]\n[1] \"G\" \"J\"\n\n\n\n[[3]]\n[1] \"A\" \"D\"\n\n\nThis structure is more complex. At the outer level there is a list of length three."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#conclusion",
    "href": "posts/tardy-tuesday/tidy-tuesday-packages/index.html#conclusion",
    "title": "Tidy Tuesday Extra - Packages and recursive searching",
    "section": "Conclusion",
    "text": "Conclusion\nBroadly, it appears the information contained in the list structures would allow the tree structure to be recovered. However, currently no trace returned is of length greater than 2. Before applying a recursive algorithm to the real data, more work should probably be done on defining exactly what type of output should be returned, and then implementing this return type. However, the function does appear to use recursion effectively, delving into various tree structures until roots of the trees are found, rather than either just stopping at an arbitrary depth, or never stopping and evaluating."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html",
    "title": "Tidy Tuesday on Valentine’s Day",
    "section": "",
    "text": "The most recent TidyTuesday dataset is on Valentine’s Day: sales and engagement in the United States\nWe were joined by Gatz Osorio, who has a lot of experience with data science in Python, so much of the discussion at the outset was about package management in R compared with Python. We then looked at some of the trends data in Valentine’s Day sales and spend."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#introduction",
    "href": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#introduction",
    "title": "Tidy Tuesday on Valentine’s Day",
    "section": "",
    "text": "The most recent TidyTuesday dataset is on Valentine’s Day: sales and engagement in the United States\nWe were joined by Gatz Osorio, who has a lot of experience with data science in Python, so much of the discussion at the outset was about package management in R compared with Python. We then looked at some of the trends data in Valentine’s Day sales and spend."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#packages-in-r",
    "href": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#packages-in-r",
    "title": "Tidy Tuesday on Valentine’s Day",
    "section": "Packages in R",
    "text": "Packages in R\n\nWe discussed tidyverse, which is a kind of meta-packages, loading a range of specific tidyverse package.\n\nWe said using tidyverse is like going into a shed.\n\nWe then talked about some of the specific packages loaded by the tidyverse package, like dplyr and readr.\n\nWe said each of these individual tidyverse packages is like a toolbox.\n\nWe then talked about the :: (scope) operator in R. This allows us to specify a specific function in a package to use, without loading the entire package.\n\nFor example, dplyr::mutate() accesses the mutate() function in the dplyr package, without loading the entire dplyr package.\nWe said this is like getting out a single tool from a toolbox, without emptying or opening the entire toolbox.\nAnother example where this is useful is where two packages have different functions with the same name, and we need to be clear which one. For example both the MASS package and the dplyr package have a function called select(). If we are using both packages we can use the scope/namespace operator to specify exactly which function we want to use. For example, dplyr::select() if we want to use the dplyr function, and MASS::select() if we want to use the MASS function."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#package-version-management-in-r",
    "href": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#package-version-management-in-r",
    "title": "Tidy Tuesday on Valentine’s Day",
    "section": "Package version management in R",
    "text": "Package version management in R\nWe briefly discussed how R is often less specific than Python and other languages as to exactly which version of a package we want to use. For example if we did some analysis in 2021, and run the script again in 2024, the script may not work as it did previously because some of the packages and functions used may have changed in the meantime.\nWe briefly discussed the renv package for helping to address such issues. renv makes a snapshot of the versions of the packages we used when first running some code, and allows these versions (rather than the latest versions) to be restored when running the script at a later date. We saw that renv has different ways of trying to do this, which involve different tradeoffs between file size and reliability:\n\nlowest file size, most scope for problems: renv takes snapshots of package versions etc. On restore() renv tries to download the package versions used at the time. This should work most of the time, but if a package or package version is no longer available on CRAN or similar this may fail.\n\nThis is like maintaining a detailed recipe of exactly what tools etc used when the script was first run.\n\nmedium file size, less scope for problems: renv uses packrat (a precursor to renv) to save all package versions alongside the project and scripts, rather than just the recipe. This means there could be hundreds of megabytes of package content to support a few kilobytes of script.\n\nThis is like carrying around the lab in which an experiment was conducted in order to be able to repeat the experiment in almost identical conditions to when the experiment was first conducted.\n\nlargest file size, least scope for problems: renv can create a docker image to house the scripts/analysis in. A docker image is a virtual environment/machine, which will be identical on everyone’s computer. This will help avoid issues associated with one user running the script on a PC, another on a linux server, and a third running the script on a Macbook.\n\nThis is like carrying around the building and street in which the lab is based, as well as the lab itself!"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#tidytuesday-challenge-itself",
    "href": "posts/tardy-tuesday/tidy-tuesday-valentines-day/index.html#tidytuesday-challenge-itself",
    "title": "Tidy Tuesday on Valentine’s Day",
    "section": "Tidytuesday challenge itself",
    "text": "Tidytuesday challenge itself\nThere were three files as part of the Tidytuesday dataset, one with a time breakdown, a second with an age breakdown, and a third with a gender breakdown. We only looked at the time breakdown file\nWe loaded the data using the tidytuesdayR package. However in the script below we will just load it directly to avoid the tidy tuesday API denying requests.\n\n\nCode\n# load packages \nlibrary(tidyverse)\nlibrary(RColorBrewer)\n\n# Brendan introduced `pacman`, and the following line of code to ensure pacman is always loaded\n# install.packages(setdiff(\"pacman\", rownames(installed.packages())))\n\n# The following loads the datasets using the tidytuesdayR package:\n# tidytuesdayR::tt_load('2024-02-13') \n# tuesdata &lt;- tidytuesdayR::tt_load('2024-02-13')\n# historical_spending &lt;- tuesdata$historical_spending\n# gifts_age &lt;- tuesdata$gifts_age\n# gifts_gender &lt;- tuesdata$gifts_gender\n\nhistorical_spending &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-02-13/historical_spending.csv')\n\n\nWe first looked at whether the percentage of people doing something involving Valentine’s Day had changed over time\n\n\nCode\nhistorical_spending |&gt;\n  glimpse() |&gt;\n  filter(!is.na(PercentCelebrating)) |&gt;\n  glimpse() |&gt;\n  ggplot(aes(x = Year, y = PercentCelebrating)) +\n  geom_line() +\n  geom_point() +\n  geom_smooth() +\n  # ylim(0, NA) + # ylim and expand_limits seem equivalent in this case\n  expand_limits(y = 0) +\n  scale_x_continuous(breaks = 2010:2022)\n\n\nRows: 13\nColumns: 10\n$ Year               &lt;dbl&gt; 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 201…\n$ PercentCelebrating &lt;dbl&gt; 60, 58, 59, 60, 54, 55, 55, 54, 55, 51, 55, 52, 53\n$ PerPerson          &lt;dbl&gt; 103.00, 116.21, 126.03, 130.97, 133.91, 142.31, 146…\n$ Candy              &lt;dbl&gt; 8.60, 10.75, 10.85, 11.64, 10.80, 12.70, 13.11, 12.…\n$ Flowers            &lt;dbl&gt; 12.33, 12.62, 13.49, 13.48, 15.00, 15.72, 14.78, 14…\n$ Jewelry            &lt;dbl&gt; 21.52, 26.18, 29.60, 30.94, 30.58, 36.30, 33.11, 32…\n$ GreetingCards      &lt;dbl&gt; 5.91, 8.09, 6.93, 8.32, 7.97, 7.87, 8.52, 7.36, 6.5…\n$ EveningOut         &lt;dbl&gt; 23.76, 24.86, 25.66, 27.93, 27.48, 27.27, 33.46, 28…\n$ Clothing           &lt;dbl&gt; 10.93, 12.00, 10.42, 11.46, 13.37, 14.72, 15.05, 13…\n$ GiftCards          &lt;dbl&gt; 8.42, 11.21, 8.43, 10.23, 9.00, 11.05, 12.52, 10.23…\nRows: 13\nColumns: 10\n$ Year               &lt;dbl&gt; 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 201…\n$ PercentCelebrating &lt;dbl&gt; 60, 58, 59, 60, 54, 55, 55, 54, 55, 51, 55, 52, 53\n$ PerPerson          &lt;dbl&gt; 103.00, 116.21, 126.03, 130.97, 133.91, 142.31, 146…\n$ Candy              &lt;dbl&gt; 8.60, 10.75, 10.85, 11.64, 10.80, 12.70, 13.11, 12.…\n$ Flowers            &lt;dbl&gt; 12.33, 12.62, 13.49, 13.48, 15.00, 15.72, 14.78, 14…\n$ Jewelry            &lt;dbl&gt; 21.52, 26.18, 29.60, 30.94, 30.58, 36.30, 33.11, 32…\n$ GreetingCards      &lt;dbl&gt; 5.91, 8.09, 6.93, 8.32, 7.97, 7.87, 8.52, 7.36, 6.5…\n$ EveningOut         &lt;dbl&gt; 23.76, 24.86, 25.66, 27.93, 27.48, 27.27, 33.46, 28…\n$ Clothing           &lt;dbl&gt; 10.93, 12.00, 10.42, 11.46, 13.37, 14.72, 15.05, 13…\n$ GiftCards          &lt;dbl&gt; 8.42, 11.21, 8.43, 10.23, 9.00, 11.05, 12.52, 10.23…\n\n\n\n\n\nIt looks like the share has decreased over time, from around 60% to 50%.\nWe next decided to look at how the relative share of spend on different item categories had changed over time.\nWe realised this involved: - Pivoting some of the columns (individual item spend) onto wide format - Seeing whether the individual item spend categories add up to the total spend reported - Creating an additional spend category for other items which are not part of the standard categories listed\nData before:\n\n\nCode\nhistorical_spending\n\n\n# A tibble: 13 × 10\n    Year PercentCelebrating PerPerson Candy Flowers Jewelry GreetingCards\n   &lt;dbl&gt;              &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;         &lt;dbl&gt;\n 1  2010                 60      103    8.6    12.3    21.5          5.91\n 2  2011                 58      116.  10.8    12.6    26.2          8.09\n 3  2012                 59      126.  10.8    13.5    29.6          6.93\n 4  2013                 60      131.  11.6    13.5    30.9          8.32\n 5  2014                 54      134.  10.8    15      30.6          7.97\n 6  2015                 55      142.  12.7    15.7    36.3          7.87\n 7  2016                 55      147.  13.1    14.8    33.1          8.52\n 8  2017                 54      137.  12.7    14.6    32.3          7.36\n 9  2018                 55      144.  13.1    14.8    34.1          6.55\n10  2019                 51      162.  14.1    15.1    30.3          7.31\n11  2020                 55      196.  17.3    16.5    41.6          9.01\n12  2021                 52      165.  15.3    15.4    30.7          8.48\n13  2022                 53      175.  15.9    16.7    45.8          7.47\n# ℹ 3 more variables: EveningOut &lt;dbl&gt;, Clothing &lt;dbl&gt;, GiftCards &lt;dbl&gt;\n\n\nAfter pivoting and tidying:\n\n\nCode\nhistorical_spending_pivoted &lt;- historical_spending |&gt;\n  pivot_longer(!c(Year, PercentCelebrating, PerPerson)) |&gt;\n  group_by(Year) |&gt;\n  mutate(sum = sum(value), \n         other_spend = PerPerson-sum\n  ) |&gt;\n  select(-sum) |&gt;\n  pivot_wider() |&gt;\n  pivot_longer(!c(Year, PercentCelebrating, PerPerson))\n\nhistorical_spending_pivoted\n\n\n# A tibble: 104 × 5\n# Groups:   Year [13]\n    Year PercentCelebrating PerPerson name          value\n   &lt;dbl&gt;              &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1  2010                 60      103  other_spend   11.5 \n 2  2010                 60      103  Candy          8.6 \n 3  2010                 60      103  Flowers       12.3 \n 4  2010                 60      103  Jewelry       21.5 \n 5  2010                 60      103  GreetingCards  5.91\n 6  2010                 60      103  EveningOut    23.8 \n 7  2010                 60      103  Clothing      10.9 \n 8  2010                 60      103  GiftCards      8.42\n 9  2011                 58      116. other_spend   10.5 \n10  2011                 58      116. Candy         10.8 \n# ℹ 94 more rows\n\n\nWe then used the tidied and pivoted dataset to produce an area chart with a nicer and more accessible colour scheme for fill colours:\n\n\nCode\nhistorical_spending_pivoted |&gt;\n  ggplot(aes(x = Year, y = value, fill = name)) +\n  geom_area(position = \"fill\") +\n  theme_dark() +\n  scale_x_continuous(breaks = 2010:2022) +\n  scale_fill_brewer(palette = \"Paired\") +\n  scale_y_continuous(labels = scales::percent) +\n  ylab(\"Cumulative percentage of annual total spend\") +\n  ggtitle(\"Valentine's day spending by category\")\n\n\n\n\n\nJon recommended the Paired colour scheme in ColorBrewer. Brendan checked this improved the accessibility of the colours using the contrastchecker website. This showed the ColorBrewer colour scheme was much more accessible than ggplot’s default colours."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-more-life-expectancy/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-more-life-expectancy/index.html",
    "title": "Tidy Tuesday on Life Expectancy - Part Two",
    "section": "",
    "text": "In the previous week’s TidyTuesday session, we looked at Life Expectancy.\nFor this week, the Tidy Tuesday dataset of the week was of Christmas films. However, as public health folks we felt more interested in continuing to look at life expectancy, so continued with the previous week’s dataset.\nThis session was led by Andrew Saul."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-more-life-expectancy/index.html#introduction",
    "href": "posts/tardy-tuesday/tidy-tuesday-more-life-expectancy/index.html#introduction",
    "title": "Tidy Tuesday on Life Expectancy - Part Two",
    "section": "",
    "text": "In the previous week’s TidyTuesday session, we looked at Life Expectancy.\nFor this week, the Tidy Tuesday dataset of the week was of Christmas films. However, as public health folks we felt more interested in continuing to look at life expectancy, so continued with the previous week’s dataset.\nThis session was led by Andrew Saul."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-more-life-expectancy/index.html#script-and-outputs-from-session",
    "href": "posts/tardy-tuesday/tidy-tuesday-more-life-expectancy/index.html#script-and-outputs-from-session",
    "title": "Tidy Tuesday on Life Expectancy - Part Two",
    "section": "Script and outputs from session",
    "text": "Script and outputs from session\nLoading some packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\n\nUse the tidytuesdayR package to load the data (rather than a direct link):\n\n\nCode\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-05')\n\n\n\n    Downloading file 1 of 3: `life_expectancy.csv`\n    Downloading file 2 of 3: `life_expectancy_different_ages.csv`\n    Downloading file 3 of 3: `life_expectancy_female_male.csv`\n\n\nPopulate the content of the list above into three separate datasets:\n\n\nCode\nle &lt;- tuesdata[[1]]\nle_diff &lt;- tuesdata[[2]]\nle_gender &lt;- tuesdata[[3]]\n\n\nHave a quick look at the data\n\n\nCode\nglimpse(le)\n\n\nRows: 20,755\nColumns: 4\n$ Entity         &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanist…\n$ Code           &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\",…\n$ Year           &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1…\n$ LifeExpectancy &lt;dbl&gt; 27.7275, 27.9634, 28.4456, 28.9304, 29.2258, 29.9206, 3…\n\n\nCode\nglimpse(le_diff)\n\n\nRows: 20,755\nColumns: 9\n$ Entity           &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghani…\n$ Code             &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG…\n$ Year             &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958,…\n$ LifeExpectancy0  &lt;dbl&gt; 27.7275, 27.9634, 28.4456, 28.9304, 29.2258, 29.9206,…\n$ LifeExpectancy10 &lt;dbl&gt; 49.1459, 49.2941, 49.5822, 49.8634, 49.9306, 50.4315,…\n$ LifeExpectancy25 &lt;dbl&gt; 54.4422, 54.5644, 54.7998, 55.0286, 55.1165, 55.4902,…\n$ LifeExpectancy45 &lt;dbl&gt; 63.4225, 63.5006, 63.6476, 63.7889, 63.8481, 64.0732,…\n$ LifeExpectancy65 &lt;dbl&gt; 73.4901, 73.5289, 73.6018, 73.6706, 73.7041, 73.8087,…\n$ LifeExpectancy80 &lt;dbl&gt; 83.7259, 83.7448, 83.7796, 83.8118, 83.8334, 83.8760,…\n\n\nCode\nglimpse(le_gender)\n\n\nRows: 19,922\nColumns: 4\n$ Entity               &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afg…\n$ Code                 &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", …\n$ Year                 &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1…\n$ LifeExpectancyDiffFM &lt;dbl&gt; 1.261900, 1.270601, 1.288300, 1.306601, 1.276501,…\n\n\nThere are fields code and entity, where entity tends to be more verbose/descriptive. Entities include geographic regions, countries, economic groupings etc. (So fairly messy, definitely not mutally exclusive and exhaustive)\n\n\nCode\nle_diff %&gt;% \n  count(Entity) %&gt;% \n  pull(Entity)\n\n\nWe decided to look at a series of countries from across the world.\n\n\nCode\ncountries &lt;- c(\"Germany\", \"United Kingdom\", \"Saudi Arabia\", \"South Africa\",\n               \"South Korea\", \"Japan\", \"Vietnam\", \"Argentina\", \"Venezuela\", \"France\")\n\n\nToday we looked at life expectency in a selection of countries from 1900\n\n\nCode\nle1900 &lt;- le %&gt;% \n  filter(Entity %in% countries,\n         Year&gt;=1900) \n\nle1900 %&gt;% \n  ggplot(aes(x=Year, y=LifeExpectancy))+\n  geom_line()+\n  facet_wrap(vars(Entity))\n\n\n\n\n\nWe then looked at the change in life expectency per year\n\n\nCode\nle1900 %&gt;% \n  group_by(Entity) %&gt;% \n  mutate(lag_diff = LifeExpectancy - lag(LifeExpectancy, order_by = Year),\n         sign = lag_diff&gt;0) %&gt;% \n  ggplot(aes(x=Year, y=lag_diff))+\n  geom_point(aes(colour = sign))+\n  geom_hline(yintercept = 0)+\n  facet_wrap(vars(Entity))\n\n\n\n\n\nWe changed the axis magnification of each country, so that the changes were more readily observable\n\n\nCode\nle1900lag &lt;- le1900 %&gt;% \n  group_by(Entity) %&gt;% \n  mutate(lag_diff = LifeExpectancy - lag(LifeExpectancy, order_by = Year),\n         sign = lag_diff&gt;0)\n\n le1900lag %&gt;% \n  ggplot(aes(x=Year, y=lag_diff))+\n  geom_point(aes(colour = sign))+\n  geom_hline(yintercept = 0)+\n  facet_wrap(vars(Entity), scales = \"free_y\")\n\n\n\n\n\nFinally, we examined variability in the change of life expectency altered for UK, France and Germany. Here is can be seen that variability in life expectancy dramatically increased around the First and Second World Wars. Data for Germany was incomplete for this period.\nTo do this we made use of the slider package, and within this the slide_index function, to produce a rolling standard deviation of annual changes.\n\n\nCode\nlibrary(slider)\nle1900lag %&gt;% \n  arrange(Year) %&gt;% \n  filter(Entity %in% c(\"United Kingdom\", \"France\", \"Germany\")) %&gt;% \n  mutate(roll_sd = slide_index_dbl(lag_diff, Year, .before = 4, .after = 4, .f = sd, .complete = T)) %&gt;% \n  ggplot(aes(x=Year, y=roll_sd, color = Entity))+\n  geom_line()"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-dr-who/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-dr-who/index.html",
    "title": "Tidy Tuesday on Dr Who",
    "section": "",
    "text": "First we load the packages\nThe tidyverse equivalent of pacman is now pak.\nThe latest dataset is here, and the specific files to work.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndta_list &lt;- tidytuesdayR::tt_load(x = \"2023-11-28\")\n\n--- Compiling #TidyTuesday Information for 2023-11-28 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `drwho_episodes.csv`\n    Downloading file 2 of 3: `drwho_directors.csv`\n    Downloading file 3 of 3: `drwho_writers.csv`\n\n\n--- Download complete ---\n\ndta_eps &lt;- dta_list[[\"drwho_episodes\"]]\ndta_wrt &lt;- dta_list[[\"drwho_writers\"]]\n\nLet’s see how the viewship changed over time\n\ndta_eps_season &lt;- \n  dta_eps |&gt; \n  group_by(season_number) |&gt; \n  mutate(\n    mean_viewers = mean(uk_viewers),\n    mean_date = mean(first_aired)\n    ) |&gt; \n  ungroup()\n\ndta_eps_season |&gt; \n  ggplot(aes(x = first_aired, y = uk_viewers)) + \n  geom_point(colour = \"grey\") +\n  geom_point(aes(x = mean_date, y = mean_viewers), size = 2.5) + \n  scale_x_date(breaks = \"2 years\", labels = \\(x) format(x, \"%Y\")) +\n  labs(\n    x = \"First aired\",\n    y = \"UK Viewers (millions)\",\n    title = \"Viewers over time for Dr Who\",\n    subtitle = \"People don't watch TV like they used to...\"\n  ) +\n  annotate(\"text\", x = lubridate::make_date(2015), y = 10, label = \"What happened here?!\") +\n  annotate(\"text\", x = lubridate::make_date(2014), y= 8, label = \"Smartphone strangling the TV from now\", hjust = 0) + \n  stat_smooth(colour = \"blue\", se = FALSE) \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nLet’s now look at writers by season\n\ndta_eps_wrt &lt;- \n  dta_eps |&gt; \n    left_join(dta_wrt, by = \"story_number\") \n\nHow many episodes by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  summarise(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  arrange(desc(n_written))\n\n# A tibble: 40 × 2\n   writer           n_written\n   &lt;chr&gt;                &lt;int&gt;\n 1 Steven Moffat           45\n 2 Russell T Davies        31\n 3 Chris Chibnall          29\n 4 Mark Gatiss              9\n 5 Toby Whithouse           7\n 6 Gareth Roberts           5\n 7 Helen Raynor             4\n 8 Jamie Mathieson          4\n 9 Peter Harness            4\n10 Matthew Graham           3\n# ℹ 30 more rows\n\n\nSo Moffat wrote most episodes, then Davies, then Chibnall\nAnd what about popularity by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  mutate(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  filter(n_written &gt;= 5) |&gt; \n  ggplot(aes(x = fct_reorder(writer, rating), y= rating)) + \n  geom_boxplot() + \n  coord_flip() + \n  labs(\n    x = \"Distribution of ratings\",\n    y = \"Writer\", \n    title = \"Rating distribution by writer\",\n    subtitle = \"Writers who wrote at least five episodes\"\n  )\n\n\n\n\nWhen were the different writers active?\n\nmajor_writers_active &lt;- \n  dta_eps_wrt |&gt; \n    group_by(writer) |&gt;  \n    mutate(\n      n_written = n()\n    ) |&gt; \n    ungroup() |&gt; \n    filter(n_written &gt;= 5) |&gt; \n    group_by(writer) |&gt; \n    summarise(\n      started_writing = min(first_aired),\n      finished_writing = max(first_aired),\n      n_written = n_written[1]\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(\n      yr_start = year(started_writing),\n      yr_end = year(finished_writing)\n    )\n\nmajor_writers_active |&gt; \n  arrange(started_writing)\n\n# A tibble: 6 × 6\n  writer           started_writing finished_writing n_written yr_start yr_end\n  &lt;chr&gt;            &lt;date&gt;          &lt;date&gt;               &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Russell T Davies 2005-03-26      2010-01-01              31     2005   2010\n2 Mark Gatiss      2005-04-09      2017-06-10               9     2005   2017\n3 Steven Moffat    2005-05-21      2017-12-25              45     2005   2017\n4 Toby Whithouse   2006-04-29      2017-06-03               7     2006   2017\n5 Gareth Roberts   2007-04-07      2011-09-24               5     2007   2011\n6 Chris Chibnall   2007-05-19      2022-10-23              29     2007   2022\n\n\nHere we see the tenure of different major writers. Russell T Davies and Steven Moffatt are the major players."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-dr-who/index.html#tidy-tuesday-challenge",
    "href": "posts/tardy-tuesday/tidy-tuesday-dr-who/index.html#tidy-tuesday-challenge",
    "title": "Tidy Tuesday on Dr Who",
    "section": "",
    "text": "First we load the packages\nThe tidyverse equivalent of pacman is now pak.\nThe latest dataset is here, and the specific files to work.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndta_list &lt;- tidytuesdayR::tt_load(x = \"2023-11-28\")\n\n--- Compiling #TidyTuesday Information for 2023-11-28 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `drwho_episodes.csv`\n    Downloading file 2 of 3: `drwho_directors.csv`\n    Downloading file 3 of 3: `drwho_writers.csv`\n\n\n--- Download complete ---\n\ndta_eps &lt;- dta_list[[\"drwho_episodes\"]]\ndta_wrt &lt;- dta_list[[\"drwho_writers\"]]\n\nLet’s see how the viewship changed over time\n\ndta_eps_season &lt;- \n  dta_eps |&gt; \n  group_by(season_number) |&gt; \n  mutate(\n    mean_viewers = mean(uk_viewers),\n    mean_date = mean(first_aired)\n    ) |&gt; \n  ungroup()\n\ndta_eps_season |&gt; \n  ggplot(aes(x = first_aired, y = uk_viewers)) + \n  geom_point(colour = \"grey\") +\n  geom_point(aes(x = mean_date, y = mean_viewers), size = 2.5) + \n  scale_x_date(breaks = \"2 years\", labels = \\(x) format(x, \"%Y\")) +\n  labs(\n    x = \"First aired\",\n    y = \"UK Viewers (millions)\",\n    title = \"Viewers over time for Dr Who\",\n    subtitle = \"People don't watch TV like they used to...\"\n  ) +\n  annotate(\"text\", x = lubridate::make_date(2015), y = 10, label = \"What happened here?!\") +\n  annotate(\"text\", x = lubridate::make_date(2014), y= 8, label = \"Smartphone strangling the TV from now\", hjust = 0) + \n  stat_smooth(colour = \"blue\", se = FALSE) \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nLet’s now look at writers by season\n\ndta_eps_wrt &lt;- \n  dta_eps |&gt; \n    left_join(dta_wrt, by = \"story_number\") \n\nHow many episodes by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  summarise(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  arrange(desc(n_written))\n\n# A tibble: 40 × 2\n   writer           n_written\n   &lt;chr&gt;                &lt;int&gt;\n 1 Steven Moffat           45\n 2 Russell T Davies        31\n 3 Chris Chibnall          29\n 4 Mark Gatiss              9\n 5 Toby Whithouse           7\n 6 Gareth Roberts           5\n 7 Helen Raynor             4\n 8 Jamie Mathieson          4\n 9 Peter Harness            4\n10 Matthew Graham           3\n# ℹ 30 more rows\n\n\nSo Moffat wrote most episodes, then Davies, then Chibnall\nAnd what about popularity by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  mutate(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  filter(n_written &gt;= 5) |&gt; \n  ggplot(aes(x = fct_reorder(writer, rating), y= rating)) + \n  geom_boxplot() + \n  coord_flip() + \n  labs(\n    x = \"Distribution of ratings\",\n    y = \"Writer\", \n    title = \"Rating distribution by writer\",\n    subtitle = \"Writers who wrote at least five episodes\"\n  )\n\n\n\n\nWhen were the different writers active?\n\nmajor_writers_active &lt;- \n  dta_eps_wrt |&gt; \n    group_by(writer) |&gt;  \n    mutate(\n      n_written = n()\n    ) |&gt; \n    ungroup() |&gt; \n    filter(n_written &gt;= 5) |&gt; \n    group_by(writer) |&gt; \n    summarise(\n      started_writing = min(first_aired),\n      finished_writing = max(first_aired),\n      n_written = n_written[1]\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(\n      yr_start = year(started_writing),\n      yr_end = year(finished_writing)\n    )\n\nmajor_writers_active |&gt; \n  arrange(started_writing)\n\n# A tibble: 6 × 6\n  writer           started_writing finished_writing n_written yr_start yr_end\n  &lt;chr&gt;            &lt;date&gt;          &lt;date&gt;               &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Russell T Davies 2005-03-26      2010-01-01              31     2005   2010\n2 Mark Gatiss      2005-04-09      2017-06-10               9     2005   2017\n3 Steven Moffat    2005-05-21      2017-12-25              45     2005   2017\n4 Toby Whithouse   2006-04-29      2017-06-03               7     2006   2017\n5 Gareth Roberts   2007-04-07      2011-09-24               5     2007   2011\n6 Chris Chibnall   2007-05-19      2022-10-23              29     2007   2022\n\n\nHere we see the tenure of different major writers. Russell T Davies and Steven Moffatt are the major players."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-dr-who/index.html#coda",
    "href": "posts/tardy-tuesday/tidy-tuesday-dr-who/index.html#coda",
    "title": "Tidy Tuesday on Dr Who",
    "section": "Coda",
    "text": "Coda\nNeither of us know much about Dr Who!\nBut hopefully we now know a bit more!"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html",
    "title": "Tidy Tuesday 30 Jan 2024: Groundhogs",
    "section": "",
    "text": "The latest TidyTuesday dataset is on Groundhog Days, a North American tradition in which the behaviours of specific groundhogs are used to make predictions about the weather over the next six weeks, as immortalised in the eponymous sci-fi rom-com featuring Bill Murray.\nOddly, the data provided does not include meteorological information on whether the groundhogs’ predictions are accurate. (Who knows? Maybe they are!) But the data do include latitude, longitude, and other geographic information. So, we decided to see if we could plot these Groundhog Day events on an interactive map.\nFirst we load the data, using the bespoke tidytuesdayR package:\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\n# The number of queries made via tidyTuesdayR is limited. The commented code below shows how to extract the TidyTuesday data using the tidytuesdayR package. Instead I'll link directly:\n# dat &lt;- tidytuesdayR::tt_load('2024-01-30')\nCode\n# predictions &lt;- dat |&gt;\n#   pluck(1) # that's the predictions\nCode\n# dat |&gt;\n#   pluck(2) # that's the groundhogs\nCode\n# groundhogs &lt;- dat |&gt;\n#   pluck(2) \n\n# Direct approach \n\ngroundhogs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-01-30/groundhogs.csv')\n\n\nRows: 75 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (11): slug, shortname, name, city, region, country, source, current_pred...\ndbl  (4): id, latitude, longitude, predictions_count\nlgl  (2): is_groundhog, active\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\npredictions &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-01-30/predictions.csv')\n\n\nRows: 1462 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): details\ndbl (2): id, year\nlgl (1): shadow\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nWe now have the two datasets in separate objects. Let’s look at some of the information in the description field\nCode\ngroundhogs |&gt;\n  filter(!is_groundhog) |&gt;\n  select(description)\n\n\n# A tibble: 43 × 1\n   description                                                                  \n   &lt;chr&gt;                                                                        \n 1 Octoraro Orphie, from Quarryville Pennsylvania, is a taxidermied world-renow…\n 2 Concord Charlie is a presumed groundhog from Athens, West Virginia. In a tra…\n 3 Lander Lil, a bronze statue of a prairie dog, has been predicting the future…\n 4 Groundhog puppet Manitoba Merv has been predicting the arrival of spring at …\n 5 Schnogadahl Sammi is a literally unpronounceable taxidermied groundhog mount…\n 6 Poor Richard is a taxidermied groundhog who prognosticates for the Slumberin…\n 7 Balzac Billy is the “Prairie Prognosticator”, a man-sized gopher mascot who …\n 8 Every year on February 2nd, Myerstown’s favorite groundhog “Uni” is placed o…\n 9 Grover the Groundhog and Sweet Arrow Sue are a taxidermied groundhog couple …\n10 Stormy Marmot is a plush yellow-bellied marmot from Aurora, Colorado. He is …\n# ℹ 33 more rows\nCode\npredictions |&gt;\n  count(shadow, sort=T)\n\n\n# A tibble: 3 × 2\n  shadow     n\n  &lt;lgl&gt;  &lt;int&gt;\n1 TRUE     665\n2 FALSE    652\n3 NA       145\nCode\n# dat |&gt;\n#   pluck(1) |&gt;\n#   left_join(groundhogs) \npredictions |&gt; \n    left_join(groundhogs)\n\n\nJoining with `by = join_by(id)`\n\n\n# A tibble: 1,462 × 20\n      id  year shadow details         slug  shortname name  city  region country\n   &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt;  &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  \n 1     1  1886 NA     Groundhog Day … punx… Phil      Punx… Punx… Penns… USA    \n 2     1  1887 TRUE   First Official… punx… Phil      Punx… Punx… Penns… USA    \n 3     1  1888 TRUE   Saw Shadow.     punx… Phil      Punx… Punx… Penns… USA    \n 4     1  1889 NA     No Record.      punx… Phil      Punx… Punx… Penns… USA    \n 5     1  1890 FALSE  No Shadow.      punx… Phil      Punx… Punx… Penns… USA    \n 6     1  1891 NA     No Record.      punx… Phil      Punx… Punx… Penns… USA    \n 7     1  1892 NA     No Record.      punx… Phil      Punx… Punx… Penns… USA    \n 8     1  1893 NA     No Record.      punx… Phil      Punx… Punx… Penns… USA    \n 9     1  1894 NA     No Record.      punx… Phil      Punx… Punx… Penns… USA    \n10     1  1895 NA     No Record.      punx… Phil      Punx… Punx… Penns… USA    \n# ℹ 1,452 more rows\n# ℹ 10 more variables: latitude &lt;dbl&gt;, longitude &lt;dbl&gt;, source &lt;chr&gt;,\n#   current_prediction &lt;chr&gt;, is_groundhog &lt;lgl&gt;, type &lt;chr&gt;, active &lt;lgl&gt;,\n#   description &lt;chr&gt;, image &lt;chr&gt;, predictions_count &lt;dbl&gt;"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html#mapping",
    "href": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html#mapping",
    "title": "Tidy Tuesday 30 Jan 2024: Groundhogs",
    "section": "Mapping",
    "text": "Mapping\nWe decided to try using Leaflet to plot where the Groundhog day events occurred.\n\n\nCode\n#install.packages('leaflet')\nlibrary(leaflet)\n\n\nWe start by following one of the first examples in the Leaflet intro page above, adding markers to geolocate the sightings/events, and a popup with the name (assigned to) the Groundhog:\n\n\nCode\ngroundhogs |&gt;\n  select(lat = latitude, lng = longitude, popup = name) |&gt;\n  leaflet() |&gt;\n  addTiles()|&gt;\n  addMarkers(~lng, ~lat, popup = ~popup)\n\n\n\n\n\n\nWe next wanted to colour these markers based on whether the predictions are classed as ‘active’ or not. This was slightly more tricky, but the example given in the Awesome Icons section of the markers part of the Leaflet documentation gave the following code pattern we could work with:\n```{r}\n# first 20 quakes\ndf.20 &lt;- quakes[1:20,]\n\ngetColor &lt;- function(quakes) {\n  sapply(quakes$mag, function(mag) {\n  if(mag &lt;= 4) {\n    \"green\"\n  } else if(mag &lt;= 5) {\n    \"orange\"\n  } else {\n    \"red\"\n  } })\n}\n\nicons &lt;- awesomeIcons(\n  icon = 'ios-close',\n  iconColor = 'black',\n  library = 'ion',\n  markerColor = getColor(df.20)\n)\n\nleaflet(df.20) %&gt;% addTiles() %&gt;%\n  addAwesomeMarkers(~long, ~lat, icon=icons, label=~as.character(mag))\n\n```\nSo, in the above addAwesomeMarkers() is used in place of addMarkers(), and takes an argument icon. A vector of icons is created of the same length as the number of rows of the dataframe, whose colour is determined through the getColor function.\nIn our case we are interested in the active column, which has just two mutually exclusive categories: TRUE and FALSE. So we just need two colours to be specified\n\n\nCode\n# We saw if we could implement the code pattern above using list columns, but were not successful\n\n# colouring markers\n# groundhogs_icons &lt;- groundhogs |&gt;\n#   mutate(markerColor = ifelse(active, \"green\", \"red\")) |&gt;\n#   rowwise() |&gt;\n#   mutate(icon = list(awesomeIcons(\n#     icon = 'ios-close',\n#     iconColor = 'black',\n#     library = 'ion',\n#     markerColor = markerColor\n#   )))\n\ngetColor &lt;- function(groundhogs) {\n  sapply(groundhogs$active, function(active) {\n    if(active) {\n      \"green\"  } else {\n          \"red\"  } })\n}\n\n# create vector of matching vectors\nicons &lt;- awesomeIcons(\n  icon = 'ios-close',\n  iconColor = 'black',\n  library = 'ion',\n  markerColor = getColor(groundhogs)\n)\n\n# now with active/inactive icons\n\ngroundhogs |&gt;\n  select(lat = latitude, lng = longitude, popup = name) |&gt;\n  leaflet() |&gt;\n  addTiles()|&gt;\n  addAwesomeMarkers(~lng, ~lat, popup = ~popup, icon = icons)\n\n\n\n\n\n\nChallenge complete! As we would expect, most predictions are not currently active."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html#other-possibilities",
    "href": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html#other-possibilities",
    "title": "Tidy Tuesday 30 Jan 2024: Groundhogs",
    "section": "Other possibilities",
    "text": "Other possibilities\nSome other things we could have explored include:\n\nAttempting to link to appropriate meteorological data to see if the predictions came true at more than chance rates (likely a challenge)\nNatural Language Programming to identify patterns and key terms in the free text fields like description\nAdditional customisation of the leaflet maps, such as including additional popup fields, further customising the icons based on multiple variables, and adding date sliders to give a third dimension (latitude, longitude, and date) to the user display"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html#additional",
    "href": "posts/tardy-tuesday/tidy-tuesday-groundhogs/index.html#additional",
    "title": "Tidy Tuesday 30 Jan 2024: Groundhogs",
    "section": "Additional",
    "text": "Additional\nAndrew presented the following code solution for how to use plotly to produce multiple traces based on summary stats:\n\n\nCode\nlibrary(plotly)\n\ndf &lt;- \n  mpg %&gt;% \n  summarise(avg_city = mean(cty), .by = c(manufacturer, year))\n\nmanfs &lt;- df %&gt;% distinct(manufacturer) %&gt;% pull()\n\np &lt;- plot_ly()\n\nfor(manf in manfs){\n  df_manf &lt;- df %&gt;% \n    filter(manufacturer == manf)\n  p &lt;- add_trace(p,\n                 mode = \"lines+markers\",\n                 x = ~year,\n                 y = ~avg_city,\n                 data = df_manf) # must include new df as data for plolty layer\n}\n\np"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-trash/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-trash/index.html",
    "title": "Tidy Tuesday Trash",
    "section": "",
    "text": "The latest TidyTuesday dataset is on trash collected as part of the Mr Trash Wheel Baltimore Healthy Harbor Initiative.\nThis session was led by Gatz Osario, and different to previous TardyTuesday sessions in that both Gatz and Jon looked at the dataset and prepared some materials ahead of the session.\nGatz provided an expert introduction to using Python for data science and data visualisation, using the Plotly libraries for interactive visualisation. Gatz used Google Colab for the session itself, which allows jupyter notebooks to be created and run online. In this post the same python chunks are run within Quarto.\nGatz used a subset of the data containing 2 factor regression scores Jon generated in R. The R code for generating this derived dataset is shown below but was not presented at the (already packed) session."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-trash/index.html#introduction",
    "href": "posts/tardy-tuesday/tidy-tuesday-trash/index.html#introduction",
    "title": "Tidy Tuesday Trash",
    "section": "",
    "text": "The latest TidyTuesday dataset is on trash collected as part of the Mr Trash Wheel Baltimore Healthy Harbor Initiative.\nThis session was led by Gatz Osario, and different to previous TardyTuesday sessions in that both Gatz and Jon looked at the dataset and prepared some materials ahead of the session.\nGatz provided an expert introduction to using Python for data science and data visualisation, using the Plotly libraries for interactive visualisation. Gatz used Google Colab for the session itself, which allows jupyter notebooks to be created and run online. In this post the same python chunks are run within Quarto.\nGatz used a subset of the data containing 2 factor regression scores Jon generated in R. The R code for generating this derived dataset is shown below but was not presented at the (already packed) session."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-trash/index.html#factor-analysis-in-r",
    "href": "posts/tardy-tuesday/tidy-tuesday-trash/index.html#factor-analysis-in-r",
    "title": "Tidy Tuesday Trash",
    "section": "Factor Analysis in R",
    "text": "Factor Analysis in R\nJon started by loading tidyverse and the most recent dataset\nlibrary(tidyverse)\n\ndf &lt;- read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2024/2024-03-05/trashwheel.csv')\n\nglimpse(df)\nThe dataset contains the numbers of different types of item extracted each time the barge went to collect trash. These items aren’t in comparable units (i.e. they weren’t by weight or volume, which could be compared).\nJon looked at one and two factor solutions to see if there are relationships between the types of items that tend to be collected together. First one factor\nf_1 &lt;- factanal(~ PlasticBottles + Polystyrene + CigaretteButts + GlassBottles + PlasticBags + Wrappers + SportsBalls, data = df, factors = 1)\n\nf_1\nA single factor has a loading of 3.2, meaning (roughly) that it contains about three variables’ worth of informaiton.\nPolystyrene, Plastic Bags and Wrappers all had strong factor loadings. The most unique item (i.e. the one least well captured by the factor) was SportsBalls.\nNow two factor solution:\nf_2 &lt;- factanal(~ PlasticBottles + Polystyrene + CigaretteButts + GlassBottles + PlasticBags + Wrappers + SportsBalls, data = df, factors = 2, scores = \"regression\")\n\nf_2\n\nThe first factor has a strong loading on Polysytrene, Plastic bags, and Wrappers. The secton factor has a strong loading for glass bottles and cigarette butts. (So, smoking and drinking related trash?)\nThe argument scores = \"regression\" was added to allow the scores of each factor to be returned and attached to all rows in the original dataframe where it could be calculated.\ndf2 &lt;- df %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(\n        plastic_dumping_score = f_2$scores[,1],\n        drinking_smoking_score = f_2$scores[,2]\n    )\n\nThe following shows how the contents returned by the trash barge varied in terms of these two factor scores by year\ndf2 |&gt;\n    mutate(density = Weight / Volume) |&gt;\n    ggplot(aes(x = plastic_dumping_score, y = drinking_smoking_score)) + \n    geom_point(aes(alpha = density)) + \n    facet_wrap(~ Year) +\n    geom_vline(xintercept = 0) + \n    geom_hline(yintercept = 0)\n\n\nOriginally there seemed to be more variation in the types of item returned by the barge, and more glass bottles and cigarettes. Over the first few years the amount of plastic waste returned seemed to increase, but declined afer peaking in 2017.\nTo make it easier for python to read the file with factor scores we generated, I (Jon) will save it as a csv file\nwrite_csv(df2,  here::here(\"posts\", \"tardy-tuesday\", \"tidy-tuesday-trash\", \"df_with_factor_scores.csv\"))"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-trash/index.html#data-manipulation-and-visualisation-in-python",
    "href": "posts/tardy-tuesday/tidy-tuesday-trash/index.html#data-manipulation-and-visualisation-in-python",
    "title": "Tidy Tuesday Trash",
    "section": "Data manipulation and visualisation in Python",
    "text": "Data manipulation and visualisation in Python\nFirst Gatz imported the relevant libraries\n\nimport pandas as pd\nimport datetime\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\npd.options.display.max_colwidth = 300\n\nJust checking python works in the quarto document:\n\n1 + 1\n\n2\n\n\nLoad the data\n\ndf = pd.read_csv(\"df_with_factor_scores.csv\")\ndf['Date'] = pd.to_datetime(df['Date'], format = \"%m/%d/%Y\", errors = 'coerce')\nprint(df.shape)\ndf.head(3)\n\n(629, 18)\n\n\n\n\n\n\n\n\n\nID\nName\nDumpster\nMonth\nYear\nDate\nWeight\nVolume\nPlasticBottles\nPolystyrene\nCigaretteButts\nGlassBottles\nPlasticBags\nWrappers\nSportsBalls\nHomesPowered\nplastic_dumping_score\ndrinking_smoking_score\n\n\n\n\n0\nmister\nMister Trash Wheel\n1\nMay\n2014\n2014-05-16\n4.31\n18\n1450\n1820\n126000\n72\n584\n1162\n7\n0\n-1.261913\n3.589004\n\n\n1\nmister\nMister Trash Wheel\n2\nMay\n2014\n2014-05-16\n2.74\n13\n1120\n1030\n91000\n42\n496\n874\n5\n0\n-0.927719\n1.558504\n\n\n2\nmister\nMister Trash Wheel\n3\nMay\n2014\n2014-05-16\n3.45\n15\n2450\n3100\n105000\n50\n1080\n2032\n6\n0\n-0.022253\n1.875345\n\n\n\n\n\n\n\nMore information\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 629 entries, 0 to 628\nData columns (total 18 columns):\n #   Column                  Non-Null Count  Dtype         \n---  ------                  --------------  -----         \n 0   ID                      629 non-null    object        \n 1   Name                    629 non-null    object        \n 2   Dumpster                629 non-null    int64         \n 3   Month                   629 non-null    object        \n 4   Year                    629 non-null    int64         \n 5   Date                    592 non-null    datetime64[ns]\n 6   Weight                  629 non-null    float64       \n 7   Volume                  629 non-null    int64         \n 8   PlasticBottles          629 non-null    int64         \n 9   Polystyrene             629 non-null    int64         \n 10  CigaretteButts          629 non-null    int64         \n 11  GlassBottles            629 non-null    int64         \n 12  PlasticBags             629 non-null    int64         \n 13  Wrappers                629 non-null    int64         \n 14  SportsBalls             629 non-null    int64         \n 15  HomesPowered            629 non-null    int64         \n 16  plastic_dumping_score   629 non-null    float64       \n 17  drinking_smoking_score  629 non-null    float64       \ndtypes: datetime64[ns](1), float64(3), int64(11), object(3)\nmemory usage: 88.6+ KB\n\n\nConvert year to integer\n\ndf['Year'] = df['Year'].astype(int)\n\nCheck no missing observations\n\ndf.isna().sum()\n\nID                         0\nName                       0\nDumpster                   0\nMonth                      0\nYear                       0\nDate                      37\nWeight                     0\nVolume                     0\nPlasticBottles             0\nPolystyrene                0\nCigaretteButts             0\nGlassBottles               0\nPlasticBags                0\nWrappers                   0\nSportsBalls                0\nHomesPowered               0\nplastic_dumping_score      0\ndrinking_smoking_score     0\ndtype: int64\n\n\nDrop NAs (there aren’t any)\n\ndf = df.dropna()\nprint(df.shape)\ndf.isna().sum()\n\n(592, 18)\n\n\nID                        0\nName                      0\nDumpster                  0\nMonth                     0\nYear                      0\nDate                      0\nWeight                    0\nVolume                    0\nPlasticBottles            0\nPolystyrene               0\nCigaretteButts            0\nGlassBottles              0\nPlasticBags               0\nWrappers                  0\nSportsBalls               0\nHomesPowered              0\nplastic_dumping_score     0\ndrinking_smoking_score    0\ndtype: int64\n\n\nSort by date\n\ndf.sort_values(by=['Date'], inplace=True)\n\n\nVisualisation\nProduce list of theme options and select the third\n\noptions = [\"plotly\", \"plotly_white\", \"plotly_dark\", \"ggplot2\", \"seaborn\", \"simple_white\"]\ntemplate = options[2]\n\nLook at the new query syntax\n\ndf.query(\"Year &lt; 2017\")\n\n\n\n\n\n\n\n\nID\nName\nDumpster\nMonth\nYear\nDate\nWeight\nVolume\nPlasticBottles\nPolystyrene\nCigaretteButts\nGlassBottles\nPlasticBags\nWrappers\nSportsBalls\nHomesPowered\nplastic_dumping_score\ndrinking_smoking_score\n\n\n\n\n0\nmister\nMister Trash Wheel\n1\nMay\n2014\n2014-05-16\n4.31\n18\n1450\n1820\n126000\n72\n584\n1162\n7\n0\n-1.261913\n3.589004\n\n\n1\nmister\nMister Trash Wheel\n2\nMay\n2014\n2014-05-16\n2.74\n13\n1120\n1030\n91000\n42\n496\n874\n5\n0\n-0.927719\n1.558504\n\n\n2\nmister\nMister Trash Wheel\n3\nMay\n2014\n2014-05-16\n3.45\n15\n2450\n3100\n105000\n50\n1080\n2032\n6\n0\n-0.022253\n1.875345\n\n\n3\nmister\nMister Trash Wheel\n4\nMay\n2014\n2014-05-17\n3.10\n15\n2380\n2730\n100000\n52\n896\n1971\n6\n0\n-0.285576\n2.064766\n\n\n4\nmister\nMister Trash Wheel\n5\nMay\n2014\n2014-05-17\n4.06\n18\n980\n870\n120000\n72\n368\n753\n7\n0\n-1.664942\n3.678861\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n161\nmister\nMister Trash Wheel\n162\nNovember\n2016\n2016-11-30\n2.75\n18\n3460\n5840\n16000\n42\n3260\n3430\n34\n46\n2.801646\n0.685263\n\n\n162\nmister\nMister Trash Wheel\n163\nDecember\n2016\n2016-12-01\n3.41\n15\n1840\n4760\n23000\n43\n3470\n3800\n6\n57\n2.925675\n0.713372\n\n\n163\nmister\nMister Trash Wheel\n164\nDecember\n2016\n2016-12-06\n2.55\n15\n1360\n3850\n34000\n39\n2340\n4220\n24\n43\n1.972863\n0.685088\n\n\n164\nmister\nMister Trash Wheel\n165\nDecember\n2016\n2016-12-16\n1.74\n18\n1880\n2890\n26000\n59\n2100\n4040\n20\n29\n1.233737\n2.149005\n\n\n165\nmister\nMister Trash Wheel\n166\nDecember\n2016\n2017-01-02\n2.13\n15\n2460\n2740\n32000\n48\n3250\n4430\n15\n36\n2.624921\n1.110940\n\n\n\n\n162 rows × 18 columns\n\n\n\nProduce the first plot\n\ndftemp = df.query(\"Year &lt; 2017\").copy()\nfig = px.box(dftemp, x='Year',y='Volume',color = 'ID')\nfig.update_layout(\n    title = \"&lt;b&gt;Plot 1: Volume per Id box plot\",\n    xaxis = dict(title='Years available'),\n    yaxis = dict(title='Volume (m3)'),\n    template=template\n)\nfig.show()\n\n\n                                                \n\n\nSecond figure\n\ndftemp = df[['Date','ID','PlasticBottles']].copy()\ndftemp['Yearmonth'] = dftemp['Date'].apply(lambda x: x.strftime('%Y-%m'))\ndel dftemp['Date']\ndftemp=dftemp.groupby(['Yearmonth','ID']).sum()\ndftemp.reset_index(inplace=True)\nfig_line = px.line(dftemp, x = 'Yearmonth',y = 'PlasticBottles',color = 'ID',\n  labels = {'PlasticBottles': 'N of bottles', 'ID': 'Identifier', 'Yearmonth': 'Year and month'},template=template\n)\nfig_line.update_layout(\n    title = \"&lt;b&gt;Plot 2: N of bottles per year and month&lt;/b&gt;\",\n    xaxis = dict(title='Time series'),\n    yaxis = dict(title='Amount (units)')\n)\nfig_line.show()\n\n\n                                                \n\n\nCheck which unique years we have\n\ndf[\"Year\"].unique()\n\narray([2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023])\n\n\nProduce a subplot for different years\n\nfig = make_subplots(rows=1, cols=2)\nc = 1\nfor year in df[\"Year\"].unique():\n  if year &gt; 2014 and year &lt; 2017:\n    dftemp = df.query(\"Year == {}\".format(year)).copy()\n    dftemp[\"Month\"] = dftemp[\"Date\"].apply(lambda x: x.strftime('%m'))\n    dftemp = dftemp[['Year','Month','PlasticBottles']].copy()\n    dftemp = dftemp.groupby(['Year','Month']).sum()\n    dftemp.reset_index(inplace=True)\n    fig.add_trace(go.Scatter(x=dftemp.Month, y=dftemp.PlasticBottles, name=str(year)), row=1, col=c)\n    c = c + 1\nfig.update_layout(title_text=\"&lt;b&gt;Plot 3: Side By Side Subplots&lt;/b&gt;\", template=template)\nfig.show()\n\n\n                                                \n\n\nStacked subplots\n\nfig = make_subplots(rows=2, cols=1)\nr = 1\nfor year in df[\"Year\"].unique():\n  if year &gt; 2014 and year &lt; 2017:\n    dftemp = df.query(\"Year == {}\".format(year)).copy()\n    dftemp[\"Month\"] = dftemp[\"Date\"].apply(lambda x: x.strftime('%m'))\n    dftemp = dftemp[['Year','Month','PlasticBottles']].copy()\n    dftemp = dftemp.groupby(['Year','Month']).sum()\n    dftemp.reset_index(inplace=True)\n    fig.append_trace(go.Scatter(x=dftemp.Month, y=dftemp.PlasticBottles, name=str(year)), row=r, col=1)\n    r = r + 1\nfig.update_layout(title_text=\"&lt;b&gt;Plot 4: Stacked Subplots&lt;/b&gt;\", template=template)\nfig.show()\n\n\n                                                \n\n\nGridded subplots with made-up data:\n\nfig = make_subplots(rows=2, cols=2)\nfig.add_trace(go.Scatter(x=[1, 2, 3], y=[4, 5, 6]), row=1, col=1)\nfig.add_trace(go.Scatter(x=[20, 30, 40], y=[50, 60, 70]), row=1, col=2)\nfig.add_trace(go.Scatter(x=[300, 400, 500], y=[600, 700, 800]), row=2, col=1)\nfig.add_trace(go.Scatter(x=[4000, 5000, 6000], y=[7000, 8000, 9000]), row=2, col=2)\nfig.update_layout(title_text=\"Grid Subplots\", template=template)\nfig.show()\n\n\n                                                \n\n\nThere’s only one barge at the moment. I guess they’re hoping to get more?\n\ndf[\"Name\"].unique()\n\narray(['Mister Trash Wheel'], dtype=object)\n\n\n\ndftemp = df[['Date','plastic_dumping_score','Name']].copy()\ndftemp['Yearmonth'] = df['Date'].apply(lambda x: x.strftime('%Y-%m'))\ndel dftemp['Date']\ndftemp=dftemp.groupby(['Yearmonth','Name']).sum()\ndftemp.reset_index(inplace=True)\nfig_area = px.area(dftemp, x = 'Yearmonth',y = 'plastic_dumping_score',color = 'Name', template=template)\nfig_area.update_layout(\n    title = \"&lt;b&gt;Plot 4: Dumping score per year and name&lt;/b&gt;\",\n    xaxis = dict(title='Year and Month'),\n    yaxis = dict(title='Total dumping score')\n)\nfig_area.show()\n\n\n                                                \n\n\nAn interactive treemap\n\ndftemp = df[['Month','Year','PlasticBottles']].copy()\ndftemp=dftemp.groupby(['Month','Year']).sum()\ndftemp.reset_index(inplace=True)\nfig_tree_maps = px.treemap(dftemp, path= ['Year','Month'],values ='PlasticBottles',color_continuous_scale='RdBu', template=template)\nfig_tree_maps.update_layout(\n    title = \"&lt;b&gt;Plot 7: Tree map about bottles per year and month&lt;/b&gt;\"\n)\nfig_tree_maps.show()\n\n\n                                                \n\n\nAnd a 3D plot!\n\ndftemp = df[['Year','drinking_smoking_score','plastic_dumping_score','ID']].copy()\ndftemp=dftemp.groupby(['Year','ID']).mean()\ndftemp.reset_index(inplace=True)\nfig_scatter3D = px.scatter_3d(dftemp,x = 'Year',y='drinking_smoking_score', z = 'plastic_dumping_score', color = 'ID',opacity=0.7, template=template)\nfig_scatter3D.update_layout(title = \"&lt;b&gt;Plot 8: Year and plastic and drinking scores&lt;/b&gt;\")\nfig_scatter3D.show()\n\n\n                                                \n\n\nAnd a pie chart:\n\ndftemp = df[['Year','PlasticBags']].copy()\ndftemp=dftemp.groupby(['Year']).sum()\ndftemp.reset_index(inplace=True)\nfig = go.Figure(\n    data=[go.Pie(\n        labels=dftemp['Year'],\n        values=dftemp['PlasticBags'],\n        sort=False)\n    ])\nfig.update_layout(title  = \"&lt;b&gt;Plot 7: Plastic bags per year&lt;/b&gt;\", template=template)\nfig.show()"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-trash/index.html#reflections",
    "href": "posts/tardy-tuesday/tidy-tuesday-trash/index.html#reflections",
    "title": "Tidy Tuesday Trash",
    "section": "Reflections",
    "text": "Reflections\n\nGoogle colab appears a good way of getting a jupyter notebook up and running, and accessible on many devices without installing python and dependencies first.\nThere were actually more issues (related to date formatting and package versions) in running both R and python code in this quarto markdown document. Definitely a learning experience!\nKatie Pyper had questions about rules-of-thumb/conventions for defining and using outliers (as shown in the box plots) in regressions etc. An important separate topic!\nThe same colab/python training will hopefully be of interest to a broader NHS audience"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-olympics/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-olympics/index.html",
    "title": "Tardy Tuesday: Olympics",
    "section": "",
    "text": "The most recent Tidy Tuesday contained various types of data on previous Olympics. Andrew led the session, with some additional contributions from Brendan."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-olympics/index.html#andrews-script",
    "href": "posts/tardy-tuesday/tidy-tuesday-olympics/index.html#andrews-script",
    "title": "Tardy Tuesday: Olympics",
    "section": "Andrew’s script",
    "text": "Andrew’s script\nLoad packages and data\n\n\nCode\nlibrary(tidyverse)\n\ntt_list &lt;- tidytuesdayR::tt_load(\"2024-08-06\")\n\n\n\n    Downloading file 1 of 1: `olympics.csv`\n\n\nCode\nlist2env(tt_list, envir = .GlobalEnv)\n\n\n&lt;environment: R_GlobalEnv&gt;\n\n\nTake a glimpse:\n\n\nCode\nglimpse(olympics)\n\n\nRows: 271,116\nColumns: 15\n$ id     &lt;dbl&gt; 1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, …\n$ name   &lt;chr&gt; \"A Dijiang\", \"A Lamusi\", \"Gunnar Nielsen Aaby\", \"Edgar Lindenau…\n$ sex    &lt;chr&gt; \"M\", \"M\", \"M\", \"M\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"M\", \"M\", \"M\"…\n$ age    &lt;dbl&gt; 24, 23, 24, 34, 21, 21, 25, 25, 27, 27, 31, 31, 31, 31, 33, 33,…\n$ height &lt;dbl&gt; 180, 170, NA, NA, 185, 185, 185, 185, 185, 185, 188, 188, 188, …\n$ weight &lt;dbl&gt; 80, 60, NA, NA, 82, 82, 82, 82, 82, 82, 75, 75, 75, 75, 75, 75,…\n$ team   &lt;chr&gt; \"China\", \"China\", \"Denmark\", \"Denmark/Sweden\", \"Netherlands\", \"…\n$ noc    &lt;chr&gt; \"CHN\", \"CHN\", \"DEN\", \"DEN\", \"NED\", \"NED\", \"NED\", \"NED\", \"NED\", …\n$ games  &lt;chr&gt; \"1992 Summer\", \"2012 Summer\", \"1920 Summer\", \"1900 Summer\", \"19…\n$ year   &lt;dbl&gt; 1992, 2012, 1920, 1900, 1988, 1988, 1992, 1992, 1994, 1994, 199…\n$ season &lt;chr&gt; \"Summer\", \"Summer\", \"Summer\", \"Summer\", \"Winter\", \"Winter\", \"Wi…\n$ city   &lt;chr&gt; \"Barcelona\", \"London\", \"Antwerpen\", \"Paris\", \"Calgary\", \"Calgar…\n$ sport  &lt;chr&gt; \"Basketball\", \"Judo\", \"Football\", \"Tug-Of-War\", \"Speed Skating\"…\n$ event  &lt;chr&gt; \"Basketball Men's Basketball\", \"Judo Men's Extra-Lightweight\", …\n$ medal  &lt;chr&gt; NA, NA, NA, \"Gold\", NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nHow many sports of which kinds?\n\n\nCode\nolympics %&gt;% \n  count(sport, sort=T) \n\n\n# A tibble: 66 × 2\n   sport                    n\n   &lt;chr&gt;                &lt;int&gt;\n 1 Athletics            38624\n 2 Gymnastics           26707\n 3 Swimming             23195\n 4 Shooting             11448\n 5 Cycling              10859\n 6 Fencing              10735\n 7 Rowing               10595\n 8 Cross Country Skiing  9133\n 9 Alpine Skiing         8829\n10 Wrestling             7154\n# ℹ 56 more rows\n\n\nEvents within the sport of athletics\n\n\nCode\nolympics %&gt;% \n  filter(sport == \"Athletics\") %&gt;% \n  count(event, sort=T) \n\n\n# A tibble: 83 × 2\n   event                                      n\n   &lt;chr&gt;                                  &lt;int&gt;\n 1 Athletics Men's Marathon                2093\n 2 Athletics Men's 100 metres              1936\n 3 Athletics Men's 4 x 100 metres Relay    1910\n 4 Athletics Men's 4 x 400 metres Relay    1749\n 5 Athletics Men's 200 metres              1495\n 6 Athletics Men's 400 metres              1417\n 7 Athletics Men's 800 metres              1321\n 8 Athletics Women's 4 x 100 metres Relay  1174\n 9 Athletics Men's 1,500 metres            1162\n10 Athletics Women's 100 metres            1063\n# ℹ 73 more rows\n\n\nA visualisation: we wanted to show how long different sports have been running:\n\n\nCode\nplot_obj &lt;- \n  olympics %&gt;% \n  filter(sport == \"Athletics\") %&gt;% \n  group_by(event) %&gt;% \n  summarise(Earliest_year = min(year),\n            Latest_year = max(year),\n            n = n()) %&gt;%  \n  arrange(desc(n)) %&gt;% \n  mutate(year_range = Latest_year - Earliest_year+1,\n         event = fct_reorder(event, year_range)) %&gt;% \n  mutate(Sex = case_when(str_detect(str_to_lower(event), \"women\") ~ \"Female\",\n                         str_detect(str_to_lower(event), \"men\") ~ \"Male\",\n                         .default = \"Other\")) %&gt;% \n  mutate(event_status = if_else(Latest_year == 2016, \"Live\", \"Dead\"))\n\nplot_obj %&gt;%\n  slice(1:20) %&gt;% \n  ggplot(aes(y=event, color = Sex))+\n  geom_errorbarh(aes(xmin=Earliest_year, xmax=Latest_year, height=0))\n\n\n\n\n\nWhat about just Female events?\n\n\nCode\nplot_obj %&gt;%\n#  slice(1:20) %&gt;% \n  filter(Sex == \"Female\") %&gt;% \n  ggplot(aes(y=event))+\n  geom_errorbarh(aes(xmin=Earliest_year, xmax=Latest_year, height=0))+\n  scale_x_continuous(breaks = seq(1920, 2020, by=10))\n\n\n\n\n\nSlight modification to indicate whether events are still running:\n\n\nCode\nplot_obj %&gt;%\n  #  slice(1:20) %&gt;% \n  filter(Sex == \"Female\") %&gt;% \n  ggplot(aes(y=event))+\n  geom_errorbarh(aes(xmin=Earliest_year, xmax=Latest_year, height=0,\n                     linetype = event_status))+\n  scale_x_continuous(breaks = seq(1920, 2020, by=10)) + \n    scale_linetype_manual(values = c(\"dashed\", \"solid\"))"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-olympics/index.html#brendans-script",
    "href": "posts/tardy-tuesday/tidy-tuesday-olympics/index.html#brendans-script",
    "title": "Tardy Tuesday: Olympics",
    "section": "Brendan’s script",
    "text": "Brendan’s script\n\n\nCode\nolympics_dec &lt;- olympics |&gt;\n    mutate(decade = round(year, -1)) |&gt;\n    mutate(bmi = weight / (height/100)^2) |&gt;\n    filter(!is.na(bmi)) |&gt; \n    group_by(event, sex, decade) |&gt;\n    summarise(mean_bmi = mean(bmi), \n              min_bmi = min(bmi),\n              max_bmi = max(bmi)) |&gt;\n    arrange(desc(mean_bmi))\n\nolympics_dec |&gt;\n    distinct(event)\n\n\n# A tibble: 612 × 2\n# Groups:   event, sex [612]\n   sex   event                                         \n   &lt;chr&gt; &lt;chr&gt;                                         \n 1 M     Weightlifting Men's Super-Heavyweight         \n 2 M     Swimming Men's 100 metres Freestyle           \n 3 F     Weightlifting Women's Super-Heavyweight       \n 4 M     Bobsleigh Men's Two                           \n 5 M     Weightlifting Men's Heavyweight               \n 6 M     Judo Men's Heavyweight                        \n 7 M     Wrestling Men's Super-Heavyweight, Greco-Roman\n 8 F     Judo Women's Heavyweight                      \n 9 M     Wrestling Men's Super-Heavyweight, Freestyle  \n10 M     Athletics Men's Shot Put                      \n# ℹ 602 more rows\n\n\nHave shotputters got heavier or lighter over time?\n\n\nCode\nolympics_dec |&gt;\n    filter(str_detect(event, \"Shot\")) |&gt;\n    add_count(event) |&gt;\n    filter(n &gt; 3) |&gt;\n    ggplot(aes(y = mean_bmi, x = decade, color = event)) +\n    geom_smooth() +\n    geom_linerange(aes(ymin = min_bmi, ymax = max_bmi)) +\n    facet_wrap(~sex, ncol = 1) +\n    theme(legend.position = \"none\")"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "",
    "text": "This week’s Tidy Tuesday compares life expectancy across the globe and is available here:"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#loading-the-data",
    "href": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#loading-the-data",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Loading the data",
    "text": "Loading the data\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\ndata_url &lt;- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-12-05/life_expectancy_different_ages.csv\"\n \ndta &lt;- read_csv(data_url)\n\n# Alternatively\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-05')\n\n\n\n    Downloading file 1 of 3: `life_expectancy.csv`\n    Downloading file 2 of 3: `life_expectancy_different_ages.csv`\n    Downloading file 3 of 3: `life_expectancy_female_male.csv`\n\n\n\n\nCode\nnames(tuesdata)\n\n\n[1] \"life_expectancy\"                \"life_expectancy_different_ages\"\n[3] \"life_expectancy_female_male\"   \n\n\nCode\nlife_expectancy &lt;- tuesdata$life_expectancy\n\nn_distinct(life_expectancy$Entity)\n\n\n[1] 261"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#setting-a-global-plot-theme",
    "href": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#setting-a-global-plot-theme",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Setting a global plot theme",
    "text": "Setting a global plot theme\n\n\nCode\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#comparing-life-expectancy-across-regions",
    "href": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#comparing-life-expectancy-across-regions",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Comparing life expectancy across regions",
    "text": "Comparing life expectancy across regions\n\n\nCode\nregions &lt;- life_expectancy %&gt;%\n  filter(str_detect(Entity, \"region\"))\n\nregions %&gt;%\n  ggplot(aes(x = Year, y = LifeExpectancy)) +\n  geom_line(aes(col = Entity)) +\n  theme(legend.position = \"top\") +\n  annotate(geom = \"text\",\n           x = 1960, y = 50,\n           label = \"What happened here?\") +\n  geom_vline(xintercept = 2019, linetype = 2) +\n  annotate(geom = \"text\", x = 2019, y = 75,\n           label = \"Start of COVID pandemic\",\n           hjust = 1)"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#difference-in-life-expectancy-between-more-and-less-developed-regions",
    "href": "posts/tardy-tuesday/tidy-tuesday-life-expectancy/index.html#difference-in-life-expectancy-between-more-and-less-developed-regions",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Difference in life expectancy between more and less developed regions",
    "text": "Difference in life expectancy between more and less developed regions\n\n\nCode\nregions %&gt;%\n  filter(Entity %in% c(\"More developed regions\",\n                       \"Less developed regions\"))  %&gt;%\n  arrange(Year, Entity) %&gt;%\n  group_by(Year) %&gt;%\n  mutate(difference = LifeExpectancy - lag(LifeExpectancy)) %&gt;%\n  filter(!is.na(difference)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = Year, y = difference)) +\n  geom_area(alpha = 0.5) +\n  expand_limits(y = 0) +\n  labs(title = \"Difference in life expectancy between more developed and less developed regions\",\n       y = \"Difference in life expectancy (years)\")\n\n\n\n\n\nWe look at life expectancy at different ages in three specific countries.\n\n\nCode\ndata_tidy &lt;-\n  dta |&gt;\n    pivot_longer(\n      cols = LifeExpectancy0:LifeExpectancy80\n    ) |&gt;\n    mutate(\n      starting_age = str_remove(name, \"LifeExpectancy\") %&gt;%\n        as.numeric()\n    ) |&gt;\n    select(-name) |&gt;\n    rename(e_x = value)\n\n\n\n\nCode\ndata_tidy |&gt;\n  filter(\n    Entity %in% c(\n      \"Nigeria\", \"Iran\",\n      \"South Africa\"\n    )\n  ) |&gt;\n  arrange(Year)  |&gt;\n  ggplot(aes(Year, e_x, group = factor(starting_age), colour = factor(starting_age))) +\n  geom_line() +\n  facet_wrap(~Entity)\n\n\n\n\n\nSandra Nwobi, who suggested the three countries above, provides the following summary:\n\nOf the three developing countries—Iran, South Africa, and Nigeria—Nigeria has a significantly higher zero-age death rate in the late 50s and early 60s. This can be attributed to a number of factors, including socioeconomic instability, political unrest, malnutrition, and limited access to healthcare. Comparing this result to South Africa and Iran, it is comparatively higher. However, there have been noticeable improvements in Nigeria during the 1980s, with a steady increase. Nevertheless, much work needs to be done to combat this in Nigeria, as it performs significantly worse than the other two countries.\n\nThere was a noticeable decline in data in the early 2000s, particularly in South Africa. Health crises like HIV/AIDS, which may have affected people between the ages of 0 and 25, as well as a number of social and economic problems may have contributed to this decline.\n\nIran’s data indicates consistent growth across all age groups over the years, with the exception of a general decline in 2020 that was likely caused by the COVID-19 virus. Out of the three countries, South Africa is the most affected, maybe as a result of a much older demography compared to Nigeria."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-perf/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-perf/index.html",
    "title": "Tidy Tuesday R performance",
    "section": "",
    "text": "We used a 1GB reviews dataset which is available on datareviews: books review\nThere are some examples to test performance which is available on R markdown file:First_test.Rmd\nThis is a very interesting link with more examples Reference link:More examples"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-perf/index.html#introduction",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-perf/index.html#introduction",
    "title": "Tidy Tuesday R performance",
    "section": "",
    "text": "We used a 1GB reviews dataset which is available on datareviews: books review\nThere are some examples to test performance which is available on R markdown file:First_test.Rmd\nThis is a very interesting link with more examples Reference link:More examples"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-r-perf/index.html#content",
    "href": "posts/tardy-tuesday/tidy-tuesday-r-perf/index.html#content",
    "title": "Tidy Tuesday R performance",
    "section": "Content",
    "text": "Content\n\nWe discussed microbenchmark, which helps to test and compare time execution.\n\nExample to calculate number of week between 2 variables (casting as dates vs strings) using difftime\nExample to calculate mean based on a grouped column using (Base r aggregate vs dplyr group_by and summarise_at)\nExample to compare vector initialisation (x &lt;- c() vs x &lt;- vector(“integer”, n)) to calculate acumulative addition.\nExample to calculate mean in a dataframe column (mean(dt[dt\\(b &gt; .5, ]\\)a) vs mean(dt\\(a[dt\\)b &gt; .5]))\nExample to compare 1:n and seq(n)\nExample to compare old pipe and new pipe\n\nWe discussed data.table, which speeds up data manipulation.\nWe discussed different file format ‘csv’, ‘RDS’ and ‘Parquet’, their compatibilities, vulnerabilities and storage compression.\n\nWe discussed ‘arrow’ package Parquet compression types: ‘gzip’, ‘snappy’ and ‘uncompressed’."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-space-objects/index.html",
    "href": "posts/tardy-tuesday/tidy-tuesday-space-objects/index.html",
    "title": "Tidy Tuesday 23 April 2024: Space objects",
    "section": "",
    "text": "The most recent TidyTuesday dataset was taken from Our World In Data, and showed the number of objects launched into space by different entities by year. Myriam led this week’s session."
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-space-objects/index.html#data-preparation-and-exploration",
    "href": "posts/tardy-tuesday/tidy-tuesday-space-objects/index.html#data-preparation-and-exploration",
    "title": "Tidy Tuesday 23 April 2024: Space objects",
    "section": "Data preparation and exploration",
    "text": "Data preparation and exploration\n\n\nCode\n# Option 1: tidytuesdayR package \n#install.packages(\"tidytuesdayR\")\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n\n\nCode\ntuesdata &lt;- tidytuesdayR::tt_load('2024-04-23')\n\n\n\n    Downloading file 1 of 1: `outer_space_objects.csv`\n\n\nCode\nouter_space_object &lt;- tuesdata$outer_space_objects\nunique(outer_space_object$Entity)\n\n\n  [1] \"APSCO\"                 \"Algeria\"               \"Angola\"               \n  [4] \"Arabsat\"               \"Argentina\"             \"Armenia\"              \n  [7] \"Australia\"             \"Austria\"               \"Azerbaijan\"           \n [10] \"Bangladesh\"            \"Belarus\"               \"Belgium\"              \n [13] \"Bhutan\"                \"Bolivia\"               \"Brazil\"               \n [16] \"Bulgaria\"              \"Canada\"                \"Chile\"                \n [19] \"China\"                 \"Colombia\"              \"Costa Rica\"           \n [22] \"Czechia\"               \"Denmark\"               \"Djibouti\"             \n [25] \"EUMETSAT\"              \"Ecuador\"               \"Egypt\"                \n [28] \"Estonia\"               \"Ethiopia\"              \"European Space Agency\"\n [31] \"European Union\"        \"Eutelsat\"              \"Finland\"              \n [34] \"France\"                \"Germany\"               \"Ghana\"                \n [37] \"Greece\"                \"Guatemala\"             \"Hungary\"              \n [40] \"India\"                 \"Indonesia\"             \"Inmarsat\"             \n [43] \"Intelsat\"              \"Intersputnik\"          \"Iran\"                 \n [46] \"Ireland\"               \"Israel\"                \"Italy\"                \n [49] \"Japan\"                 \"Jordan\"                \"Kazakhstan\"           \n [52] \"Kenya\"                 \"Kuwait\"                \"Laos\"                 \n [55] \"Latvia\"                \"Lithuania\"             \"Luxembourg\"           \n [58] \"Malaysia\"              \"Mauritius\"             \"Mexico\"               \n [61] \"Moldova\"               \"Monaco\"                \"Mongolia\"             \n [64] \"Morocco\"               \"NATO\"                  \"Nepal\"                \n [67] \"Netherlands\"           \"New Zealand\"           \"Nigeria\"              \n [70] \"North Korea\"           \"Norway\"                \"Pakistan\"             \n [73] \"Papua New Guinea\"      \"Paraguay\"              \"Peru\"                 \n [76] \"Philippines\"           \"Poland\"                \"Portugal\"             \n [79] \"Qatar\"                 \"RASCOM\"                \"Romania\"              \n [82] \"Russia\"                \"Rwanda\"                \"Saudi Arabia\"         \n [85] \"Sea Launch\"            \"Singapore\"             \"Slovakia\"             \n [88] \"Slovenia\"              \"South Africa\"          \"South Korea\"          \n [91] \"Spain\"                 \"Sri Lanka\"             \"Starsem\"              \n [94] \"Sweden\"                \"Switzerland\"           \"Taiwan\"               \n [97] \"Thailand\"              \"Tunisia\"               \"Turkey\"               \n[100] \"Turkmenistan\"          \"Uganda\"                \"Ukraine\"              \n[103] \"United Arab Emirates\"  \"United Kingdom\"        \"United States\"        \n[106] \"Uruguay\"               \"Venezuela\"             \"Vietnam\"              \n[109] \"World\"                 \"Zimbabwe\"             \n\n\nWe wanted to know what the entities referred to:\n\n\nCode\nouter_space_object %&gt;% \n  filter(!is.na(Code)) %&gt;% \n  pull(Entity) %&gt;% \n  unique()\n\n\n [1] \"Algeria\"              \"Angola\"               \"Argentina\"           \n [4] \"Armenia\"              \"Australia\"            \"Austria\"             \n [7] \"Azerbaijan\"           \"Bangladesh\"           \"Belarus\"             \n[10] \"Belgium\"              \"Bhutan\"               \"Bolivia\"             \n[13] \"Brazil\"               \"Bulgaria\"             \"Canada\"              \n[16] \"Chile\"                \"China\"                \"Colombia\"            \n[19] \"Costa Rica\"           \"Czechia\"              \"Denmark\"             \n[22] \"Djibouti\"             \"Ecuador\"              \"Egypt\"               \n[25] \"Estonia\"              \"Ethiopia\"             \"Finland\"             \n[28] \"France\"               \"Germany\"              \"Ghana\"               \n[31] \"Greece\"               \"Guatemala\"            \"Hungary\"             \n[34] \"India\"                \"Indonesia\"            \"Iran\"                \n[37] \"Ireland\"              \"Israel\"               \"Italy\"               \n[40] \"Japan\"                \"Jordan\"               \"Kazakhstan\"          \n[43] \"Kenya\"                \"Kuwait\"               \"Laos\"                \n[46] \"Latvia\"               \"Lithuania\"            \"Luxembourg\"          \n[49] \"Malaysia\"             \"Mauritius\"            \"Mexico\"              \n[52] \"Moldova\"              \"Monaco\"               \"Mongolia\"            \n[55] \"Morocco\"              \"Nepal\"                \"Netherlands\"         \n[58] \"New Zealand\"          \"Nigeria\"              \"North Korea\"         \n[61] \"Norway\"               \"Pakistan\"             \"Papua New Guinea\"    \n[64] \"Paraguay\"             \"Peru\"                 \"Philippines\"         \n[67] \"Poland\"               \"Portugal\"             \"Qatar\"               \n[70] \"Romania\"              \"Russia\"               \"Rwanda\"              \n[73] \"Saudi Arabia\"         \"Singapore\"            \"Slovakia\"            \n[76] \"Slovenia\"             \"South Africa\"         \"South Korea\"         \n[79] \"Spain\"                \"Sri Lanka\"            \"Sweden\"              \n[82] \"Switzerland\"          \"Taiwan\"               \"Thailand\"            \n[85] \"Tunisia\"              \"Turkey\"               \"Turkmenistan\"        \n[88] \"Uganda\"               \"Ukraine\"              \"United Arab Emirates\"\n[91] \"United Kingdom\"       \"United States\"        \"Uruguay\"             \n[94] \"Venezuela\"            \"Vietnam\"              \"World\"               \n[97] \"Zimbabwe\"            \n\n\nGenerally, when an entity has a ‘code’, it is a country (though contains some other aggregations, like whole world, as well)\n\n\nCode\nouter_space_object %&gt;% \n  filter(!is.na(Code)) %&gt;% \n  distinct(Entity) %&gt;% \n  arrange(desc(Entity))\n\n\n# A tibble: 97 × 1\n   Entity              \n   &lt;chr&gt;               \n 1 Zimbabwe            \n 2 World               \n 3 Vietnam             \n 4 Venezuela           \n 5 Uruguay             \n 6 United States       \n 7 United Kingdom      \n 8 United Arab Emirates\n 9 Ukraine             \n10 Uganda              \n# ℹ 87 more rows\n\n\nWe decided to use janitor::clean_names to avoid having to remember the case of columns.\n\n\nCode\nouter_space_objects &lt;- outer_space_object %&gt;% \n  clean_names()\nouter_space_objects\n\n\n# A tibble: 1,175 × 4\n   entity  code   year num_objects\n   &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;       &lt;dbl&gt;\n 1 APSCO   &lt;NA&gt;   2023           1\n 2 Algeria DZA    2002           1\n 3 Algeria DZA    2010           1\n 4 Algeria DZA    2016           3\n 5 Algeria DZA    2017           1\n 6 Angola  AGO    2017           1\n 7 Angola  AGO    2022           1\n 8 Arabsat &lt;NA&gt;   1985           2\n 9 Arabsat &lt;NA&gt;   1992           1\n10 Arabsat &lt;NA&gt;   1996           2\n# ℹ 1,165 more rows"
  },
  {
    "objectID": "posts/tardy-tuesday/tidy-tuesday-space-objects/index.html#visualisation",
    "href": "posts/tardy-tuesday/tidy-tuesday-space-objects/index.html#visualisation",
    "title": "Tidy Tuesday 23 April 2024: Space objects",
    "section": "Visualisation",
    "text": "Visualisation\nHow many objects released into space by the world by year?\n\n\nCode\nouter_space_objects %&gt;% \n  filter(entity == \"World\") %&gt;% \n  group_by(year) %&gt;% \n  summarise(num_objects = sum(num_objects)) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(x = year, y =num_objects))+\n  geom_line()\n\n\n\n\n\nWe thought maybe there’s been, and been periods of, exponential growth, so looked at this with a log y scale too:\n\n\nCode\nouter_space_objects %&gt;% \n  filter(entity == \"World\") %&gt;% \n  group_by(year) %&gt;% \n  summarise(num_objects = sum(num_objects)) %&gt;% \n  ungroup() %&gt;% \n  ggplot(aes(x = year, y =num_objects))+\n  geom_line()+\n  scale_y_log10()\n\n\n\n\n\nHere it’s more obvious there were broadly two ‘regimens’ globally of objects released into space.\nWhich countries released most objects into space over the whole period covered by the dataset?\n\n\nCode\ntop_10 &lt;- outer_space_objects %&gt;% \n  filter(entity != \"World\", entity != \"European Space Agency\") %&gt;% \n  count(entity, wt = num_objects, name = \"amount\") %&gt;% \n  slice_max(order_by = amount, n = 10) %&gt;% \n  pull(entity)\ntop_10\n\n\n [1] \"United States\"  \"Russia\"         \"China\"          \"United Kingdom\"\n [5] \"Japan\"          \"France\"         \"India\"          \"Germany\"       \n [9] \"Canada\"         \"Luxembourg\"    \n\n\nFor these top 10 countries, we decided to look at trends over time:\n\n\nCode\nouter_space_objects %&gt;% \n  filter(entity %in% top_10) %&gt;% \n  ggplot(aes(x = year, y =num_objects, colour = entity))+\n  geom_line()+\n  scale_y_log10()+\n  facet_wrap(~entity, scales = \"free_y\")\n\n\n\n\n\nTricia suggested we ordered these facets by the total number of objects released over the whole period. For this we used the forcats package, with a bit of trial and error.\n\n\nCode\n# This use of fct_reorder looks like it's worked, but it hasn't\nouter_space_objects %&gt;%\n  filter(entity %in% top_10) %&gt;%\n  mutate(entity = fct_reorder(entity, num_objects, .desc = TRUE)) %&gt;%\n  ggplot(aes(x = year, y =num_objects, colour = entity))+\n  geom_line()+\n  scale_y_log10()+\n  facet_wrap(~entity, scales = \"free_y\")\n\n\n\n\n\nThis version looks initially that it’s worked, but it hasn’t. We can confirm this with the following:\n\n\nCode\n# This confirms the ordering is wrong in the above\nouter_space_objects %&gt;%\n  filter(entity %in% top_10) %&gt;%\n  group_by(entity) %&gt;%\n  summarise(num_objects = sum(num_objects)) %&gt;%\n  ungroup() %&gt;%\n  arrange(desc(num_objects))\n\n\n# A tibble: 10 × 2\n   entity         num_objects\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 United States         9632\n 2 Russia                3723\n 3 China                 1051\n 4 United Kingdom         765\n 5 Japan                  325\n 6 France                 151\n 7 India                  144\n 8 Germany                120\n 9 Canada                 102\n10 Luxembourg              86\n\n\nSo we use group_by but with mutate, not summarise, to create the total_num_objects column, which is then used by fct_reorder to create something that can be faceted correctly\n\n\nCode\nouter_space_objects %&gt;%\n  filter(entity %in% top_10) %&gt;%\n  group_by(entity) %&gt;%\n  mutate(total_num_objects = sum(num_objects)) %&gt;%\n  ungroup() %&gt;%\n  mutate(entity = fct_reorder(entity, total_num_objects, .desc = TRUE)) %&gt;%\n  ggplot(aes(x = year, y =num_objects, colour = entity))+\n  geom_line()+\n  scale_y_log10()+\n  facet_wrap(~entity, scales = \"free_y\") + \n  labs(\n    title = \"Number of objects in outer space by top 10 countries\",\n    subtitle = \"Arranged by total number of objects put in outer space\"\n  )\n\n\n\n\n\nWe can see, for example, that China, Japan and India’s number of objects sent into space has been growing exponentially for a long time. The USA, and UK, appear to be having more of a recent exponential growth spurt. Russia’s object release rate declined with the collapse of the USSR."
  },
  {
    "objectID": "posts/can-we-go-dutch/index.html",
    "href": "posts/can-we-go-dutch/index.html",
    "title": "Can we go Dutch? (And should we?)",
    "section": "",
    "text": "Amsterdam: Not all flowers, bikes and canals (But largely flowers, bikes, and canals!)\nI’m not good at going on holidays. Ideally I’d rather live somewhere I like living enough, and work somewhere I enjoy working enough, that planning and dreaming of living and doing something else for one month a year isn’t what sustains me through the other eleven.\nThis hopefully goes some way to explain why I’m now back from a short holiday to Amsterdam first planned for Winter 2019.\nTo the extent I like breaks, I like city breaks, because I like cities. Cities are to people as hives are to bees, dams are to beavers, and palatial mounds are to termites. They’re the ultimate extended phenotype of Homo Sapiens sapiens, a way of terraforming a piece of the planet into something that more efficiently meets human wants and needs than any other part of the Earth (and by extension the known universe). I don’t see cities as something that contrasts with nature, but something that beautifully expresses human nature. And I think Amsterdam is perhaps an especially beautiful and complete expression of this nature, or at least some of the potential that exists within this nature.\nMy suspicion is that, for various complex reasons, the British just don’t tend to do cities as well as most of our Continential European neighbours. Perhaps the key to the mystery why is locked up in that self description of England and its “green and pleasant lands”, which in Blake’s poem is contrasted with the “dark satanic mills” of industry, and in that oddly disease-like term, bucolic. England has placed its dreams in an idealised past - fields, village greens, rugged white cliffs; sketches and icons which found full expression in the 2010 UK passport design, as it happens - and the big, livable city has had no place in this iconography. The other UK nations, in this very unequal nation of nations, has then been taken along for the ride, to dreaming of ‘escaping to the country’ rather than seeking out the city.\nSo, as we don’t like living in cities, and our dreams aren’t in and of cities, we don’t dream big when it comes to designing our cities to be more livable. So, we don’t make wholesale efforts to encourage cycling and mass transit, as Amsterdam did in the 1970s. We have less active travel, more disease related to sedentary living, more suburbanisation, more traffic jams, more time spent going from, through and into places rather than being in places.\nThe British aversion to cities may help to explain why an apparently universal empirical ‘law’ of cities, known as Zipf’s Law, doesn’t seem to apply as strongly in the UK as in most other affluent nations. Zipf’s Law states that, if the log of the rank of a city (ordered from most to least populous) is plotted against the log of the size of the city, the points plotted should form a straight line. For most continental European nations, this ‘law’ seems to hold. But for the UK, it doesn’t: either the first city (London) is too large, or the other cities (Manchester, Birmingham, and so on) are too small.\nIn a very crude sense, a city reaches its population limit when the positive effects of population density get to be fully cancelled out by the negative effects of population density. Negative density effects are generally known as congestion effects (which includes but is not limited to traffic congestion) and positive density effects are referred to as agglomeration effects. Both congestion effects and agglomeration effects only go up with density, but at different rates, leading to a net density effect schedule that’s likely to be somewhat upside-down-U-shaped. The challenge of making a successful livable city - somewhere people want to go to rather than escape from - is to make the marginal agglomeration effect schedule steeper than the corresponding marginal congestion effect schedule. In practice it’s probably more important for city planners to focus on flattening out the congestion effects than on trying to capitalise further on the agglomeration effects.\nAnd how does this relate to Amsterdam? Well, a bicycle takes up less of a city and its streets than a car, and a person takes up less of a city than a bicycle. So, if more people could be encouraged to walk, or cycle, or use other forms of mass transit, then the congestion effect schedules become flatter. It seemed that, in the Netherlands, there was a recognition by the 1970s that the car-is-king philosophy that took off in the post-war decades were making Amsterdam a less pleasant experience for those living in and visiting the city. By making a large number of large scale infrastructure changes all at once, the congestion effect schedule got flattened, and so the optimal density peak got pushed back further to the right (possibly to a limit more dependent on how high buildings could be built, rather than how fast people could move within the city), and the city became livable once again.\nOf course, another thing the Netherlands, and especially Amsterdam, has become internationally renowned for is its permissive attitudes to illicit drug use and sex. Though this may at first seem unrelated to the above, perhaps it’s not. Perhaps these came about, in part, as solutions to other, less directly tangible, kinds of agglomoration/congestion effect challenge? For someone who’s crudely libertarian in outlook, of course, a city is a musterpoint for markets, and markets exist for all kind of good and service, so surely permitting rather than inhibiting markets in the wide range of goods and services denizens wish to offer and consume is just another way of increasing the steepness of the agglomeration schedule of cities and allowing people who live and work together to serve each other’s needs more effectively. From this crude libertarian perspective, the criminalisation of some goods and services then just adversely distorts and degrades these markets, making it harder for people to service each other’s wants and needs.\nThe impression I had of Amsterdam, however, was that the drug-and-sex permissiveness comes less from purely free-market libertarian ideology, focused on maximising agglomeration effects, and more from a sense of urban pragmatism, focused on minimising social and cultural congestion effects. When lots of people, from many different backgrounds, come to share the same finite physical space, there will be lots of different things they will want, not all of which we may want for them to want. 1 By simply permitting people to indulge and avail themselves of goods and services that might be risky or harmful, we at least reduce the potentially greater risks and harms that may come from attempting to prohibit or inhibit their use. Amsterdam, it appears to me, tolerates the trade in some drugs and sex, rather than celebrates it.\nThe closest UK analogy is probably vape shops: Of course, it would from first principles, from a health perspective, be better if no one sought out nicotine products, nor became nicotine dependent. But if we were to live in the real world, where tens of millions of people are nicotine addicts, it is surely less harmful they meet this need through vaping than smoking? So, beyond some level, the prohibition or restriction of vape shops is likely to be net harmful, because it is likely to reduce the level of switching from (more harmful) cigarette smoking to (less harmful) vaping. This argument does not mean vaping is great, nor that vape shops are harmless (especially when and if they encourage switching from not smoking to vaping, for example in school-aged populations), but it does mean that trying to stop this growing market risks doing more harm than good.\nThe analogy seems to work in another way too: though the growth of vape shops in the UK over the last decade or so has been substantial, only a relatively small proportion of shops are vape shops, and the rate of growth in this kind of establishment seems to be slowing. Similarly, in Amsterdam, shops selling cannabis and psilocybin products 2 are not uncommon, but at the same time are far from the only shop in Amsterdam: cafes greatly outnumber ‘coffeeshops’. Supply found equilibrium with demand, and did not overshoot. ‘Sin goods’ exist, and in Amsterdam appear to peacefully coexist with the many other markets that a large city can support. If we were to stretch the concept of ‘congestion effects’ to any kind of adverse harm that groups of people meeting can cause to each other, and include drug-related deaths in this concept, then - according to Our World In Data at least - this particular form of social congestion effect appears to be around five times worse in the UK than in the Netherlands. So it seems it’s not just our roads that are needlessly congested from failing to think pragmatically about the challenges of urban living; our morgues are too."
  },
  {
    "objectID": "posts/can-we-go-dutch/index.html#footnotes",
    "href": "posts/can-we-go-dutch/index.html#footnotes",
    "title": "Can we go Dutch? (And should we?)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAnother aspect of this is that cities tend to disproportionately draw younger adults, and single people, rather than older adults and familities; both factors likely associated with greater risk appetites/lower risk aversion.↩︎\nSales of ‘harder drugs’ appear more strongly prohibited. If pushing the vehicle congestion analogy perhaps this is like permitting the sale of unleaded petrol but prohibiting the sale of leaded petrol?↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jon Minton’s Blog",
    "section": "",
    "text": "Megalopolis: Dream Film\n\n\n\n\n\n\n\nfilms\n\n\ndreams\n\n\nutopia\n\n\nfollies\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nOn Anti-victories and Velvet Mousetraps\n\n\n\n\n\n\n\ncareers\n\n\nvocation\n\n\nsatisfaction\n\n\nsatisficing\n\n\nsettling\n\n\nmotivation\n\n\noptima\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Paradox of Tolerating Intolerance: Position A and Position B\n\n\n\n\n\n\n\nparadoxes\n\n\nethics\n\n\nculture\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nStatistics Website\n\n\n\n\n\n\n\nquarto\n\n\nstatistics\n\n\nupdates\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nBrian May: Gentleman Ecoscientist\n\n\n\n\n\n\n\nepidemiology\n\n\ntelevision\n\n\nscience\n\n\nagriculture\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: English Monarchs\n\n\n\n\n\n\n\ntidy tuesday\n\n\nmonarchs\n\n\nggplot2\n\n\ntidyverse\n\n\nhistory\n\n\nregex\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2024\n\n\nJon , Kate , Andrew , Brendan , Nic , Kennedy , Miryiam , Gatz\n\n\n\n\n\n\n  \n\n\n\n\nMistaken (Aural) Identity: On podcasts and soundalikes\n\n\n\n\n\n\n\npodcasts\n\n\nLiz Truss\n\n\nthe rest is money\n\n\nTRIM\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThrew missiles: a concealed tautology?\n\n\n\n\n\n\n\nlinguistics\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTardy Tuesday: Olympics\n\n\n\n\n\n\n\ntidy tuesday\n\n\nR\n\n\nolympics\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2024\n\n\nAndrew , Brendan , Abram , Kennedy , Imran , Gatz , Jon\n\n\n\n\n\n\n  \n\n\n\n\nInformation Warfare: Proportion of Rwandans who are Muslims\n\n\n\n\n\n\n\nwikipedia\n\n\ngoogle\n\n\nreligion\n\n\ndemography\n\n\nmisinformation\n\n\nislam\n\n\nchristianity\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPsycho-logical Arithmetic\n\n\nHow Cadbury makes subtraction look like addition\n\n\n\n\nadvertising\n\n\nchocolate\n\n\nmarketing\n\n\neconomics\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTardy Tuesday: American Idol\n\n\n\n\n\n\n\ntidy tuesday\n\n\nR\n\n\nAmerican Idol\n\n\n\n\n\n\n\n\n\n\n\nJul 24, 2024\n\n\nAbram , Kate , Andrew , Imran , Nic , Kennedy , Aoife , Jon\n\n\n\n\n\n\n  \n\n\n\n\nResampling for post-stratification\n\n\n\n\n\n\n\nstatistics\n\n\nr\n\n\nhypothesis tests\n\n\nresampling\n\n\nbootstrapping\n\n\npost-stratification\n\n\nhacker stats\n\n\n\n\n\n\n\n\n\n\n\nJul 23, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nHacker Stats: Intro and overview\n\n\n\n\n\n\n\nR\n\n\nstatistics\n\n\ninference\n\n\nhacker stats\n\n\nbootstrapping\n\n\npermutation tests\n\n\npost-stratification\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Women’s Football\n\n\n\n\n\n\n\nstatistics\n\n\ntidy tuesday\n\n\nsports\n\n\nfootball\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2024\n\n\nKate Pyper, Brendan Clarke, Kennedy , Imran , Abram , Jon\n\n\n\n\n\n\n  \n\n\n\n\nGetting started with the infer package\n\n\n\n\n\n\n\nstatistics\n\n\nr\n\n\nhypothesis tests\n\n\nresampling\n\n\nbootstrapping\n\n\nhacker stats\n\n\n\n\n\n\n\n\n\n\n\nJul 16, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPermutation Testing, and the intuition of the Null hypothesis, with Base R\n\n\n\n\n\n\n\nhypothesis testing\n\n\npermutation\n\n\nR\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nA brief introduction to bootstrapping\n\n\n(It’s only literally like pulling teeth, and only in this specific example!)\n\n\n\n\nbootstrapping\n\n\nstatistics\n\n\nnon-parametric\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nJohan Hari: Superior Inferior Superior Storyteller\n\n\n…and some thoughts about how new anti-obesity drugs could affect the UK\n\n\n\n\npopular science\n\n\nstory-telling\n\n\ndrugs\n\n\nweight loss\n\n\nNHS\n\n\nausterity\n\n\n\n\n\n\n\n\n\n\n\nJun 25, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Roaming US Holidays\n\n\n\n\n\n\n\nR\n\n\nUSA\n\n\nholidays\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2024\n\n\nBrendan Clarke, Kate Pyper, Kennedy Owusu-Afriyie, Myriam Scansetti, Andrew Saul, Jon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Game of Life Beyond\n\n\nOn the Psychological Mechanics of Cultist Simualtor\n\n\n\n\npsychology\n\n\ngames\n\n\nfantasy\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTime series: Some closing remarks\n\n\n…Including why time series isn’t not rocket science\n\n\n\n\nstatistics\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nLink-breaking Spring Clean\n\n\n\n\n\n\n\nquarto\n\n\nwebsite\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Self-generated data challenge\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\ndata wrangling\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2024\n\n\nJon Minton, Kate Pyper, Kennedy Owusu-Afriyie, Brendan Clarke, Imran Chowdhury, Andrew Saul\n\n\n\n\n\n\n  \n\n\n\n\nStatistics as circuit boards\n\n\n\n\n\n\n\nstatistics\n\n\nsimulation\n\n\nsystems\n\n\nhand written\n\n\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Twenty Four: Time series - Vector Autoregression and multivariate models\n\n\n\n\n\n\n\nlife expectancy\n\n\ntime series\n\n\nvector autoregression\n\n\nmultivariate regression\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday R performance\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nR performance\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2024\n\n\nJon Minton, Gatz Osorio, Kennedy Owuso-Afrije, Imran Chowdhury, Myriam Scansetti, Kate Pyper, Andrew Saul\n\n\n\n\n\n\n  \n\n\n\n\nA Quarto blog post on a Quarto presentation about the Quarto blog\n\n\n\n\n\n\n\nblog\n\n\nquarto\n\n\nmeta\n\n\njavascript\n\n\n\n\n\n\n\n\n\n\n\nMay 19, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nDavid Lynch: All-American Shaman\n\n\n\n\n\n\n\nneurodiversity\n\n\nfilm\n\n\ntelevision\n\n\ncreativity\n\n\nspiritualism\n\n\nreligiousity\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Twenty Three: Time series and seasonality\n\n\n\n\n\n\n\ntime series\n\n\nseasonality\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nHow factor analysis is used in testing\n\n\n\n\n\n\n\nstatistics\n\n\nfactor analysis\n\n\ntesting\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nFactor analysis with ordinal variables\n\n\nAn Analogue Explanation\n\n\n\n\nstatistics\n\n\nfactor analysis\n\n\nordinal variables\n\n\nanalogue\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Twenty Two: Time Series - ARIMA in practice\n\n\n\n\n\n\n\nstatistics\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Twenty One: Time Series: The Moving Average Model\n\n\n\n\n\n\n\nstatistics\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday 23 April 2024: Space objects\n\n\n\n\n\n\n\ntidy tuesday\n\n\nspace\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2024\n\n\nMyriam Scansetti, Brendan Clarke, Tricia Govindasamy, Jon Minton, Andrew Saul, Gatz Osorio, Kennedy Owusu-Afriyie\n\n\n\n\n\n\n  \n\n\n\n\nIt’s still the economy\n\n\nOn the lagged link between economic growth and longevity growth\n\n\n\n\neconomic growth\n\n\nmortality trends\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Twenty: Time Series: Integration\n\n\n\n\n\n\n\nstatistics\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Nineteen: Time Series: Introduction and Autoregression\n\n\n\n\n\n\n\nstatistics\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday Extra - Packages and recursive searching\n\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Solar Eclipses\n\n\n\n\n\n\n\nR\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2024\n\n\nMyriam Scansetti, Nick Christofides, Wei Fan, Kennedy Owusu-Afriyie, Jon Minton\n\n\n\n\n\n\n  \n\n\n\n\nDouble Tardy Tuesday\n\n\nMea Culpa\n\n\n\n\nTidy Tuesday\n\n\nTardy\n\n\nData Science\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nJason Statham vs the Nepobaby Cyberscammers\n\n\nThoughts on a mindless genre\n\n\n\n\nfilms\n\n\ncybercrime\n\n\naltruistic punishment\n\n\ngender\n\n\nmyth\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Trading Mind Game\n\n\n\n\n\n\n\nbooks\n\n\nmental health\n\n\ninequalities\n\n\neconomics\n\n\ndistinction\n\n\nbanking\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThoughts on Dreams from My Father\n\n\nOn Obama’s dialectical engagement with identity\n\n\n\n\nbooks\n\n\nidentity\n\n\nUSA\n\n\n\n\n\n\n\n\n\n\n\nMar 26, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nLevelling up… forever. On Gaming’s Shepherd’s Tone and the Fremium Trap\n\n\n\n\n\n\n\ngames\n\n\npsychology\n\n\ngaming\n\n\n\n\n\n\n\n\n\n\n\nMar 22, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Eighteen: Causal Inference: Some closing thoughts\n\n\nIncluding why I’ve not written about DAGs\n\n\n\n\nstatistics\n\n\ncausality\n\n\ncausal inference\n\n\nDAGs\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nCan we go Dutch? (And should we?)\n\n\nSome thoughts post Amsterdam\n\n\n\n\ncities\n\n\nEurope\n\n\nHolland\n\n\nUK\n\n\ncycling\n\n\nAmsterdam\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday Trash\n\n\nInto the pythonverse!\n\n\n\n\n\n\n\n\n\nMar 6, 2024\n\n\nGatz Osario, Jon Minton, Antony Clark, Brendan Clarke, Kennedy Owusu-Afriyie, Kate Pyper, Andrew Saul, Myriam Scansetti\n\n\n\n\n\n\n  \n\n\n\n\nMy thoughts on Dune (Part 1)\n\n\nWritten 31/10/21. Revisiting after seeing Dune (Part 2)\n\n\n\n\nsci-fi\n\n\nmyth\n\n\nfilms\n\n\nbooks\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Seventeen: Causal Inference: Controlling and Matching Approaches\n\n\n\n\n\n\n\nstatistics\n\n\ncausality\n\n\nR\n\n\nmatching\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nMar 2, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Sixteen: Causal Inference: How to try to do the impossible\n\n\n\n\n\n\n\nstatistics\n\n\ncausality\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday 27 Feb 2024: Leap Years\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nleap years\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nMyriam Scansetti, Antony Clark, Jon Minton, Nicoloas Christofidis, Brendan Clarke, Kennedy Owuso-Afriyie, Emu the cat\n\n\n\n\n\n\n  \n\n\n\n\nWhy can’t we just get on with making and fixing stuff?\n\n\nSome thoughts on Andy Weir’s Eng-Fi and neurodiversity\n\n\n\n\nstories\n\n\nfiction\n\n\nengineering\n\n\nutopia\n\n\nneurodiversity\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nEdinbr Pair Programming\n\n\n\n\n\n\n\nEdinbr\n\n\nR\n\n\npair programming\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nJim Gardner, Jon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Fifteen: Causal Inference: The platinum and gold standards\n\n\n\n\n\n\n\nstatistics\n\n\ncausality\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: 20 Feb 2024 - R Grants\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nfunding\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nKennedy Owusu-Afriyie, Antony Clark, Brendan Clarke, Jon Minton, Nick Christofides, Steph Curtis, Gats Osorio, Andrew Saul, Myrian Scansetti\n\n\n\n\n\n\n  \n\n\n\n\nPart Fourteen: A non-technical but challenging introduction to causal inference…\n\n\n…and the heroism or villainy of Henry Dundas\n\n\n\n\nhistory\n\n\ncausality\n\n\nEdinburgh\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nOn the background to my statistical inference series\n\n\n\n\n\n\n\nstatistics\n\n\ntraining\n\n\neducation\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2024\n\n\nJon Minton\n\n\n\n\n\n\n\n\n\n\n\nTidy Tuesday on Valentine's Day\n\n\n\nR\ntidy tuesday\nValentine's Day\n\n\n\n\n\n\n\n`Feb 14, 2024`{=html}\nBrendan Clarke, Jon Minton, Gatz Osorio, Kennedy Owuso-Afrije\n\n\n\n\n  \n\n\n\n\nPart Thirteen: On Marbles and Jumping Beans\n\n\n…and why Bayesians have superior posteriors\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 10, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Twelve: Honest Predictions the slightly-less easier way\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 4, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Eleven: Honest Predictions the easier way\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTardy Tuesdays: My Second Series\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday 30 Jan 2024: Groundhogs\n\n\nEven tardier than usual…\n\n\n\n\nR\n\n\nTidy Tuesday\n\n\nNorth America\n\n\n\n\n\n\n\n\n\n\n\nFeb 2, 2024\n\n\nBrendan Clarke, Jon Minton, Kennedy Owusu-Afriyie, Kate Pyper, Andrew Saul\n\n\n\n\n\n\n  \n\n\n\n\nOn Sweary Soap Operas: A Concealed Television Genre\n\n\n\n\n\n\n\ngenres\n\n\ntelevision\n\n\nstories\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Ten: Log Likelihood estimation for Logistic Regression\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidytuesday 2024-01-23\n\n\nEducational attainment and town size\n\n\n\n\nR\n\n\nEducation\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nBrendan Clarke, Andrew Saul, Nick Christofides, Kennedy Owusu-Afriyie, Jon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Other Left-Right Divide: Iain McGilchrist and the Battle of the Hemispheres\n\n\n\n\n\n\n\npodcasts\n\n\nstories\n\n\nsci-fi\n\n\nmyth\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nI got permanently banned from a politics forum for mentioning how circles work\n\n\n\n\n\n\n\npolitics\n\n\ngeometry\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Nine: Answering questions with honest uncertainty: Expected values and Predicted values\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Eight: Guessing what a landscape looks like by feeling the curves beneath our feet\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nDavid Sederis: Humourists as Unrepentent Observational Confessionals\n\n\n\n\n\n\n\nbooks\n\n\nmindfulness\n\n\ncomedy\n\n\nhumour\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Seven: Feeling Uncertain\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Six: The Robo-Chauffeur\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Five: Traversing the Likelihood Landscape\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nGLMs: My first series\n\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Four: why only betas just look at betas\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Christmas films\n\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2023\n\n\nTom Fowler, Nick Christofides, Andrew Saul, Jon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Eerie Familiarity of Frasier (2023)\n\n\n…And why it’s inverted Dr Who\n\n\n\n\nstories\n\n\nsitcoms\n\n\narchetypes\n\n\nmathematics\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nChanging tenure in Scotland\n\n\n\n\n\n\n\nhousing\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nNerdy Dialogues on Life and Death\n\n\nPart 1: Introduction; Life Expectancy\n\n\n\n\ndemography\n\n\npopulation health\n\n\nmethodology\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday on Life Expectancy - Part Two\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nLife Expectancy\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\nJon Minton, Andrew Saul, Nick Christofides, James McMahon, Kennedy Owusu-Afriyie, Sandra Nwobi\n\n\n\n\n\n\n  \n\n\n\n\nMy Economic Inactivity Modelling Package: Informative Readme File!\n\n\n\n\n\n\n\nR\n\n\nEconomic Inactivity\n\n\nNews\n\n\nPackages\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nOptimised for Twitter?\n\n\n\n\n\n\n\ntwitter\n\n\nX\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nWhy such pushback against 20 minute neighbourhoods?\n\n\n\n\n\n\n\nresearch\n\n\nwalkability\n\n\ndriving\n\n\nconspiracy theories\n\n\nhuman development\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday on Life Expectancy\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nLife Expectancy\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nNick Christofides, Jon Minton, Sandra Nwobi\n\n\n\n\n\n\n  \n\n\n\n\nBeavis and Butt-Head: When a physics graduate dons Dumbface\n\n\n\n\n\n\n\ncartoons\n\n\nstories\n\n\neugenics\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Effective Savings on Interest-free Credit\n\n\n\n\n\n\n\nstatistics\n\n\nfinances\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nHow to wrap presents\n\n\n\n\n\n\n\nChristmas\n\n\nBirthdays\n\n\nArts & Crafts\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nNew blog feature: comments\n\n\n\n\n\n\n\nquarto\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Three: glm is just fancy lm\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Two: Systematic components and link functions\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nScientific Illustrations: Annotating the unit circle\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\ngraphics\n\n\neconomic inactivity\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nInteractive Sliders with Crosstalk and Plotly\n\n\n\n\n\n\n\nR\n\n\nplotly\n\n\ncrosstalk\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nRobocop (1987) is wonderfully childish\n\n\n\n\n\n\n\nstories\n\n\nfilms\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday on Dr Who\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nDr Who\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJon Minton, Nick Christofides\n\n\n\n\n\n\n  \n\n\n\n\nPart One: Model fitting as parameter calibration\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPost with code\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nEdinbR talk on modelling economic (in)activity transitions\n\n\n\n\n\n\n\nR\n\n\nmodelling\n\n\ntalks\n\n\neconomics\n\n\nhealth\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nA Deathly Silence\n\n\n\n\n\n\n\nMortality\n\n\nEpidemiology\n\n\nPapers\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nFirst Post\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Jon. I’m interested in epidemiology, data science, cities, population structure, software development, R, javascript, python, the two cultures, cats, pop culture, storytelling and ideology, irrational rationality, taking dumb things seriously (and vice versa), cooking, and cats. Welcome to my Quarto blog, which I started in late 2023."
  }
]