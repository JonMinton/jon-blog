[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Jon. Welcome to my blog."
  },
  {
    "objectID": "posts/beavis-and-butthead-is-dumbface/index.html",
    "href": "posts/beavis-and-butthead-is-dumbface/index.html",
    "title": "Beavis and Butt-Head: When a physics graduate dons Dumbface",
    "section": "",
    "text": "Mike Judge’s Beavis and Butt-Head\n\n\nA second season of “Mike Judge’s Beavis & Butt-Head” is now available on Paramount+, continuing a series that began on MTV in the early 1990s. I’ve been watching it, generally enjoying it, but feeling a gnawing sense of discomfort while doing so. Here’s why.\nMike Judge is a physics graduate, whose other credits include: Silicon Valley, a sitcom about tech startups; King of the Hill, a surprisingly gentle and sympathetic animated sitcom about a lower middle class social conservative family; and Idiocracy, a feature length science fiction comedy whose premise is that, “People are getting dumber; society’s getting dumber; at this rate someone who’s average now will be considered a genius a few generations from now.”\nIn Idiocracy, the proposed mechanism for the world’s dumbing down is a kind of dysgenic selective breeding. Whereas smarter people, with their careful planning and fantastic career opportunities, equivocate and defer the decision to have children, dumber and more feckless people, who don’t tend to do much thinking or planning, and wouldn’t be giving up on any great opportunities, continue to breed like rabbits, or even viruses. Dumber people have a higher R number, so will outbreed smarter people until almost everyone in society’s dumb. From an evolutionary perspective, dumb is the winning strategy.\nIf this sounds like the kind of plotline a eugenicist might come up with, I think you might be right. The alternative is that Judge is a black pilled cynic, a wannabe eugenicist, who just wishes, like Marxism, it would only work in practice. Watching a Judge film or TV show is being invited to judge, to find others inferior and wanting, and so feel superior. But that short-term feeling of superiority is fleeting; what lingers is the sense of loneliness, of being ‘the only adult in the room’, the hell of other people, when the other people are idiots.\nThe intellectual elitism, and sneering at the dumb, that finds most full expression in Idiocracy, has always been present in Beavis & Butt-Head. To an extent that’s the entire plot. Beavis & Butt-Head’s lack of intellect is extrapolated to such an extent it becomes grotesquely surreal. And they combine this lack of intelligence with a lack of almost any detectable virtues or redeeming qualities, with the possible exception of Beavis’ sense of loyalty to Butt-Head, a loyalty that is often presented as misplaced, enabling the pair’s co-dependence and Butt-Head’s constant physical and emotional abuse of Beavis, his only friend in the world.\nOther targets of Beavis & Butt-Head’s humour are those characters who overestimate the two title characters, treat them with kindness, and try to help them. This includes their hippy teacher, Mr Van Driessen, who is frequently seen to permit delinquency and disruption from the titular pair, apparently to the detriment both of the pair themselves, and the rest of the class. Other recent episodes feature a kindly middle-aged couple, who happily provide the pair with provisions with which their own home will be attacked; someone who heroically rescues them from a sewer (which the pair mistakenly believe is Hell, as in their illiteracy they misread ‘Department of Sanitation’ as ‘De Apartment of Satan’); and their ever forgiving and kindly neighbour (and Hank Hill prototype?) Mr Anderson, whose property the pair damage and steal without apparent repercussion (except of the karmic variety).\nPerhaps the most depressing segments in the recent Beavis and Butt-Head are those titled Old Beavis and Butt-Head. Breaking a forth wall in long-run cartoon series, the premise of these segments is that we might expect that someone who was a teenager in the early 1990s might be middle aged (or ‘old’, from their former teenage self’s perspective) in the 2020s. And so these age-appropriate versions of the characters are presented. By now, the segments suggest, Butt-Head is jobless, obese, and living off disability payments. Beavis is wrinkly and crag-toothed, but has at least managed, after decades of (not) trying, to get a job, working as Butt-Head’s full time (taxdollar funded) carer.\nMike Judge, as well as creating the series, also voices both characters. In doing so, and in the context of his own academic achievements, just how wretchedly they are drawn, and his other outputs, I think he does the vocal equivalent of ‘donning Dumbface’. Beavis and Butt-Head aren’t just incapable and inferior along narrowly intellectual or academic lines, but in every conceivable way. Every thing they say (with the exception of some of their commentary segments), every thing they do, every scrape and escapade they put themselves in, is yet more evidence of their incorrigible worthlessness, and every attempt to help them as coming from a well-intentioned but misplaced belief that they could ever be better than they are. If we can’t get rid of people like them, the show seems to be suggesting, the best we can do is laugh at them mercilessly. (Maybe behind their backs, just to be safe.)\nAs mentioned, I’ve been watching the new series, and against my better nature enjoying it. It’s a guilty pleasure. Hopefully the above goes to illustrate just how guilty."
  },
  {
    "objectID": "posts/lms-are-glms-part-01/index.html",
    "href": "posts/lms-are-glms-part-01/index.html",
    "title": "Linear Models are General Linear Models",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/lms-are-glms-part-01/index.html#tldr",
    "href": "posts/lms-are-glms-part-01/index.html#tldr",
    "title": "Linear Models are General Linear Models",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/lms-are-glms-part-01/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "href": "posts/lms-are-glms-part-01/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "title": "Linear Models are General Linear Models",
    "section": "Part 1: What are statistical models and how are they fit?",
    "text": "Part 1: What are statistical models and how are they fit?\nIt’s common for different statistical methods to be taught as if they’re completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\nSomething I’ve found immensely helpful over the years is the following pair of equations:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nIn words, the above is saying something like:\n\nThe predicted response \\(Y_i\\) for a set of predictors \\(X_i\\) is assumed to be drawn from (the \\(\\sim\\) symbol) a stochastic distribution (\\(f(.,.)\\))\nThe stochastic distribution contains both parameters we’re interested in, and which are determined by the data \\(\\theta_i\\), and parameters we’re not interested in and might just have to assume, \\(\\alpha\\).\nThe parameters we’re interested in determining from the data \\(\\theta_i\\) are themselves determined by a systematic component \\(g(.,.)\\) which take and transform two inputs: The observed predictor data \\(X_i\\), and a set of coefficients \\(\\beta\\)\n\nAnd graphically this looks something like:\n\n\n\n\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --&gt; g\n  beta --&gt; g\n  g --&gt; theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt; Y\n\n\n\n\n\n\n\nTo understand how this fits into the ‘whole game’ of modelling, it’s worth introducing another term, \\(D\\), for the data we’re using, and to say that \\(D\\) is partitioned into observed predictors \\(X_i\\), and observed responses, \\(y_i\\).\nFor each observation, \\(i\\), we therefore have a predicted response, \\(Y_i\\), and an observed response, \\(y_i\\). We can compare \\(Y_i\\) with \\(y_i\\) to get the difference between the two, \\(\\delta_i\\).\nNow, obviously can’t change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the \\(\\beta\\) parameters that feed into the systematic component \\(g\\). Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n\n\n\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D --&gt;|partition| X\n  D --&gt;|partition| y\n  X --&gt; g\n  beta --&gt;|rerun| g\n  g --&gt;|transform| theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt;|predict| Y\n  \n  Y --&gt;|compare| diff\n  y --&gt;|compare| diff\n  \n  diff --&gt;|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n\n\n\n\n\nPretty much all statistical model fitting involves iterating along this \\(g \\to \\beta\\) and \\(\\beta \\to g\\) feedback loop until some kind of condition is met involving minimising \\(\\delta\\).\nI’ll expand on this idea further in part 2."
  },
  {
    "objectID": "posts/r-code/index.html",
    "href": "posts/r-code/index.html",
    "title": "Post with code",
    "section": "",
    "text": "This short post is intended to confirm that I can run and render R code within a Quarto blog post.\n\nVery simple example\nLet’s start off with some very simple base-R\n\n1 + 1\n\n[1] 2\n\n\nAnd of course let’s not forget the obligatory\n\nstatement &lt;- \"Hello World\"\n\nstatement\n\n[1] \"Hello World\"\n\n\n\n\nGraphs\nLet’s now look at a base-R graphic, again using a cliched example\n\nplot(mtcars$mpg ~ mtcars$wt)\n\n\n\n\n\n\nSome extensions\nLet’s now continue to be cliched, and load and use the tidyverse\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\nmtcars |&gt; \n  group_by(carb) |&gt; \n  summarise(\n    mean_mpg = mean(mpg)\n  ) |&gt; \n  ungroup()\n\n# A tibble: 6 × 2\n   carb mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     1     25.3\n2     2     22.4\n3     3     16.3\n4     4     15.8\n5     6     19.7\n6     8     15  \n\n\nAnd to visualise\n\nmtcars |&gt; \n  mutate(cyl = factor(cyl)) |&gt; \n  ggplot(aes(x = wt, y = mpg, colour = cyl, group= cyl)) + \n  geom_point(aes(shape = cyl)) + \n  stat_smooth(se = FALSE, method = \"lm\") + \n  labs(\n    x = \"Weight\", \n    y = \"Miles per gallon\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nConclusion\nSo far, so good…"
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html",
    "href": "posts/effective-saving-for-interest-free-credit/index.html",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "",
    "text": "I have a Monzo account, and as part of the overall Monzo package make use of Monzo Flex, an interest-free credit service which means the payment made in one month is spread over broadly equal payments over the following three months.\nHowever, I’ve always only bought something if I could afford to pay for it in full.\nThe reason for using Monzo Flex follows from an intuition: Deferring some of the payment for a good obtained in month \\(t=0\\) to months \\(\\{t=1, t=2, t=3\\}\\) should in effect offer some degree of saving on the cost of the good, as a pound in 1-3 months has a slightly lower value than a pound this month. This is because of inflation, and the higher the rate of inflation, the higher the effective interest-free credit discount should become.\nHowever, I’ve never tried to work out what this effective savings rate is expected to be. Let’s try to work that out.\nTo do this, we need to consider the following:\n\nThe relationship between annual inflation rates and monthly inflation rates.\nThe concept of net present value (NPV)."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#introduction",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#introduction",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "",
    "text": "I have a Monzo account, and as part of the overall Monzo package make use of Monzo Flex, an interest-free credit service which means the payment made in one month is spread over broadly equal payments over the following three months.\nHowever, I’ve always only bought something if I could afford to pay for it in full.\nThe reason for using Monzo Flex follows from an intuition: Deferring some of the payment for a good obtained in month \\(t=0\\) to months \\(\\{t=1, t=2, t=3\\}\\) should in effect offer some degree of saving on the cost of the good, as a pound in 1-3 months has a slightly lower value than a pound this month. This is because of inflation, and the higher the rate of inflation, the higher the effective interest-free credit discount should become.\nHowever, I’ve never tried to work out what this effective savings rate is expected to be. Let’s try to work that out.\nTo do this, we need to consider the following:\n\nThe relationship between annual inflation rates and monthly inflation rates.\nThe concept of net present value (NPV)."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#monthly-and-annual-inflation-rates",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#monthly-and-annual-inflation-rates",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "Monthly and annual inflation rates",
    "text": "Monthly and annual inflation rates\nIf prices go up 10% in 12 months, and go up the same % each month, how much do they go up each month?\nAn intuitive but wrong answer is that, as there are 12 months per year, the monthly inflation rate will be one twelfth of the annual inflation rate, which would imply a monthly inflation rate of \\(0.1/12\\) or around 0.83%. So,\n\\[\n(1 + r_m) = \\frac{1}{12}(1 + r_y)\n\\] Or equivalently\n\\[\n(1 + r_y) = 12 (1 + r_m)\n\\] Where \\(r_y\\) is the annual increase and \\(r_m\\) is the monthly increase.\nHowever this assumption, as mentioned, is wrong, because it ignores the way that each month’s increase is applied to the product of all increases that occurred in previous months. For example, for three months with different inflation rates the total increase over the the three months will be\n\\[\n(1 + r_{1,2,3}) = (1 + r_1)(1+r_2)(1+r_3)\n\\] If the monthly inflation rates for each of the three months are the same, \\(r_m\\), then this simplifies slightly to\n\\[\n(1 + r_{1,2,3}) = (1 + r_m)^3\n\\]\nBy extension, as there are twelve months in a year, where the monthly inflation rate is fixed the equation becomes:\n\\[\n(1 + r_y) = (1 + r_m)^{12}\n\\]\nThis, not \\((1 + r_y) = 12 (1 + r_m)\\), is the correct starting point. Solve for \\(r_m\\) …\n\\[\n(1 + r_y)^{\\frac{1}{12}} = 1 + r_m\n\\]\n\\[\nr_m = {(1 + r_y)}^{\\frac{1}{12}} - 1\n\\]\nPlugging in a 10% annual inflation rate, i.e. 0.1 for \\(r_y\\), we therefore get an \\(r_m\\) value of around 0.007974, so around 0.8%."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#net-present-value",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#net-present-value",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "Net Present Value",
    "text": "Net Present Value\nThe idea of Net Present Value (NPV) is to translate costs and benefits that occur at different points in time onto a single timeframe, the present. This makes it easier to compare options that take place over different timeframes.\nIn the Flex example we are comparing two options:\n\n\nPay all now\n\n\nPay interest free over three consecutive monthly installments\n\n\nLet’s say the cost of the good at month \\(t\\) is £150. Graphically, and with no interest and inflation, the two options look as follow:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- tribble(\n  ~option, ~month, ~amount,\n  \"A\", 0, 150,\n  \"A\", 1, 0,\n  \"A\", 2, 0,\n  \"A\", 3, 0,\n  \"B\", 0, 0,\n  \"B\", 1, 50,\n  \"B\", 2, 50,\n  \"B\", 3, 50\n)\n\ndf |&gt; \n  ggplot(aes(month, amount)) + \n  geom_col() + \n  facet_wrap(~ option, nrow = 2)\n\n\n\n\n\nIn the no interest / no inflation scenario, the sums for option A and option B are equal, £150.\nHowever, in scenarios with inflation, the value of money keeps decreasing. This means that a commitment to pay £50 month 3 is a commitment to pay less than in month 0. Using the 10% annual inflation rate example, we can estimate the cumulative devaluation by months 1, 2 and 3 by dividing the product of devaluations so far by the monthly inflation rate:\n\n\nCode\nannual_to_monthly &lt;- function(x) {(1 + x)^(1/12) -1}\n\nannual_inflation &lt;- 0.10\nmonthly_inflation &lt;- annual_to_monthly(annual_inflation)\n\nindex0 &lt;- 1\nindex1 &lt;- index0 / (1 + monthly_inflation)\nindex2 &lt;- index1 / (1 + monthly_inflation)\nindex3 &lt;- index2 / (1 + monthly_inflation)\n\ndf &lt;- tibble(\n  month = 0:3, \n  index = c(index0, index1, index2, index3)\n)\n\ndf\n\n\n# A tibble: 4 × 2\n  month index\n  &lt;int&gt; &lt;dbl&gt;\n1     0 1    \n2     1 0.992\n3     2 0.984\n4     3 0.976\n\n\nContinuing the example of a £150 item paid over months 1, 2 and 3, we can therefore convert to NPV by discounting each month’s costs by the index relative to month 0\n\n\nCode\ndf2 &lt;- df |&gt; \n  mutate(\n    amount = c(0, 50, 50, 50)\n  ) |&gt; \n  mutate(npv_amount = amount * index)\n\ndf2\n\n\n# A tibble: 4 × 4\n  month index amount npv_amount\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1     0 1          0        0  \n2     1 0.992     50       49.6\n3     2 0.984     50       49.2\n4     3 0.976     50       48.8\n\n\nThe sum of npv_amount is now less than the £150 in option A, pay upfront. In this example, with 10% inflation, this sum is £147.64, which represents a 1.6% discount on option A.\nLet’s now generalise to other inflation rates\n\n\nCode\ncalc_npv_discount &lt;- function(ry, total = 150) { \n  annual_to_monthly &lt;- function(x) {(1 + x)^(1/12) -1}\n  \n  rm &lt;- annual_to_monthly(ry)\n  index0 &lt;- 1\n  index1 &lt;- index0 / (1 + rm)\n  index2 &lt;- index1 / (1 + rm)\n  index3 &lt;- index2 / (1 + rm)\n\n  npv_amt1 &lt;- (total / 3) * index1\n  npv_amt2 &lt;- (total / 3) * index2\n  npv_amt3 &lt;- (total / 3) * index3\n  \n  \n  1 - sum(npv_amt1, npv_amt2, npv_amt3) / total\n}\n\ndf &lt;- \n  tibble(\n    annual_rate = seq(0, 0.15, by = 0.01)\n  ) |&gt; \n  mutate(\n    effective_discount = map_dbl(annual_rate, calc_npv_discount)\n  )\n\ngg &lt;- \n  df |&gt; \n    ggplot(aes(100 * annual_rate, 100 * effective_discount)) + \n    geom_line() + \n    labs(x = \"Annual inflation rate (%)\", \n         y = \"Effective discount on paying over 3 months (%)\",\n         title = \"Effective short-term discount rate against inflation rate\"\n         ) + \n    scale_y_continuous(breaks = seq(0, 15, by = 0.1)) +\n    annotate(\"segment\", x = 14.8, xend = 14.8, colour = \"lightblue\", y = 0, yend = 100 * calc_npv_discount(0.148)) +\n    annotate(\"segment\", x = 0, xend = 100 * 0.148, colour = \"lightblue\", y = 100 * calc_npv_discount(0.148), yend = 100 * calc_npv_discount(0.148)) +\n    annotate(\"segment\", x = 9.6, xend = 9.6, colour = \"darkblue\", y = 0, yend = 100 * calc_npv_discount(0.096)) +\n    annotate(\"segment\", x = 0, xend = 100 * 0.096, colour = \"darkblue\", y = 100 * calc_npv_discount(0.096), yend = 100 * calc_npv_discount(0.096)) +\n    annotate(\"segment\", x = 5.3, xend = 5.3, colour = \"darkgrey\", y = 0, yend = 100 * calc_npv_discount(0.053)) +\n    annotate(\"segment\", x = 0, xend = 100 * 0.053, colour = \"darkgrey\", y = 100 * calc_npv_discount(0.053), yend = 100 * calc_npv_discount(0.053)) \n    \ngg +\n    annotate(\"text\", \n             x = 2, y = 0.1 + 100 * calc_npv_discount(0.148),\n             label = \"Goods (Highest)\"\n    ) + \n    annotate(\"text\", \n             x = 2, y = 0.1 + 100 * calc_npv_discount(0.096),\n             label = \"CPIH (Highest)\"\n    ) + \n    annotate(\"text\", \n             x = 2, y = 0.1 + 100 * calc_npv_discount(0.053),\n             label = \"Services (Highest)\"\n    )  \n\n\n\n\n\nIn the above I’ve indicated the effective discount rates implied by different annual interest rates reported by the ONS in Figure 7 of this page These range from almost 2.3% for goods, to around 0.86% for services.\nHowever, fortunately, the current inflation rates are somewhat lower, with the most recent reported inflation rates being 2.9% for goods, 6.2% for services, and 2.7% for CPIH.\n\n\nCode\ngg + \n  annotate(\"segment\", x = 2.9, xend = 2.9, colour = \"lightblue\", linetype = \"dashed\", y = 0, yend = 100 * calc_npv_discount(0.029)) +\n  annotate(\"segment\", x = 0, xend = 100 * 0.029, colour = \"lightblue\", linetype = \"dashed\", y = 100 * calc_npv_discount(0.029), yend = 100 * calc_npv_discount(0.029)) +\n  annotate(\"text\", \n           x = 2, y = 0.1 + 100 * calc_npv_discount(0.029),\n           label = \"Goods (Current)\"\n  ) + \n  annotate(\"segment\", x = 4.7, xend = 4.7, colour = \"darkblue\", linetype = \"dashed\", y = 0, yend = 100 * calc_npv_discount(0.047)) +\n  annotate(\"segment\", x = 0, xend = 100 * 0.047, colour = \"darkblue\", linetype = \"dashed\", y = 100 * calc_npv_discount(0.047), yend = 100 * calc_npv_discount(0.047)) +\n  annotate(\"text\", \n           x = 2, y = 0.1 + 100 * calc_npv_discount(0.047),\n           label = \"CPIH (Current)\"\n  ) + \n  annotate(\"segment\", x = 6.2, xend = 6.2, colour = \"darkgrey\", linetype = \"dashed\", y = 0, yend = 100 * calc_npv_discount(0.062)) +\n  annotate(\"segment\", x = 0, xend = 100 * 0.062, colour = \"darkgrey\", linetype=\"dashed\", y = 100 * calc_npv_discount(0.062), yend = 100 * calc_npv_discount(0.062)) +\n  annotate(\"text\", \n           x = 2, y = 0.1 + 100 * calc_npv_discount(0.062),\n           label = \"Services (Current)\"\n  )  \n\n\n\n\n\nSo, the effective discount for deferring has fallen alongside inflation. However it’s still something.\nThe immediate cost of deferring is by contrast the same. It involves clicking a couple of buttons, so a couple of seconds, in the same Monzo app.\nThere are some other consequences too: Using a higher proportion of one’s credit limit tends to lower one’s credit rating. This means the ability to acquire credit on more favourable terms can be adversely affected.\nHowever, for now, as a general principle, realising marginal savings by pressing a couple of buttons doesn’t seem too bad, and at some points of time, and for some items, the savings have been around 2%."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "First Post",
    "section": "",
    "text": "Hi, this is my first blog post. I’m making this using Quarto, starting off by slavishly following the tutorial, then incrementally adapting it to suit my preferences.\n\nI’m even keeping the default image of the first blog post. It’s not dissimilar to what I’m actually looking at!"
  },
  {
    "objectID": "posts/utterences-comments/index.html",
    "href": "posts/utterences-comments/index.html",
    "title": "New blog feature: comments",
    "section": "",
    "text": "I think I’ve managed to set up a blog comment feature for each of the posts.\nAs usual, the quarto documentation is great, though information on comments is in the HTML basics section rather than the website or blog section, so takes a bit of hunting. Albert Rapp’s very comprehensive blogpost is a great resource, covering this and much else.\nThe Quarto documentation gives three options for comments:\n\nHypothes.is, which allows comments and annotations to be provided line-by-line, a bit like non-editable tracked changes.\nutterances, which is a lightweight interface based on the discussion feature in github.\ngiscus, which seems to be built on utterances, but a bit more heavy-weight/opinionated.\n\nI’ve attempted, and think I’ve managed to implement, utterances.\nIn order to make a comment on a post, you need to have a Github username, and log in.\nFrom my end, I needed to do the following:\n\nSet up a public Github repo for blog comments. I unimaginately called this BlogComments\nInstall utterances on github and associate it with this repo\nWithin the file posts/_metadata.yml, add the following declaration\n\ncomments: \n  utterances: \n    repo:  JonMinton/BlogComments\nParameters in posts/_metadata.yml are applied to all posts within the posts subdirectory. This should mean that each post will now contain a comment box at the bottom.\nWhen a comment is added by a registered Github user, metadata from the specific post being commented on should be appended to an issue/discussion post within the JonMinton/BlogComments directory. And whenever a post is rendered, all associated discussion/issue items in the BlogComments repo should be fetched and shown at the bottom of the post.\nI’ve said should because I’ve only just set this up, and there are currently no comments.\nWhy not try to add a comment and see what happens?!"
  },
  {
    "objectID": "posts/unattended-deaths/index.html",
    "href": "posts/unattended-deaths/index.html",
    "title": "A Deathly Silence",
    "section": "",
    "text": "Trends in R98/R99 deaths since 1990\n\n\nWhat does it mean when someone dies, and no one notices for days, weeks, or months on end?\nThe bodies, once found, will be decomposed to such an extent that no effective autopsy can be performed, and so no cause of death can be identified. Such deaths are then likely to be coded either as R98 (‘Unattended death’) or R99 (‘Other ill-defined and unknown causes of mortality’). Far from being ‘junk codes’, wouldn’t a sudden and sustained change in deaths coded this way (absent an obvious explanation, such as a change in coding practice) signal that something broader is afoot?\nWorking with Lu Hiam, an Oxford PhD student and former GP, and Theodore Estrin-Serlui, a histopathologist, I analysed trends in deaths with these codes, as compared with mortality trends overall in England & Wales.\nSuch codes are rarely used, but in England & Wales they sadly became many times more common over the 1990s and 2000s. Standardised mortality rates in the R98/R99 category became more than three and a half times a common between 1990 and 2010, even as general standardised mortality rates fell by around a third.\nFor every body found so decomposed that the R98/R99 category had to be used, there are usually many more that have been unattended for a few days, have started to decompose, but for which autopsy can still be successfully performed. If these deaths are the tip of the iceberg, the base of this iceberg may be a growing epidemic of loneliness and social isolation, of ever more people with connections to friends and family, with no one to turn to in times of crisis.\nOur paper, A Deathly Silence, has been published in the Journal of the Royal Society of Medicine, and received press coverage from a number of outlets."
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html",
    "href": "posts/robocop-is-wonderfully-childish/index.html",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "",
    "text": "Robocop (1987): Wonderfully Childish"
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html#preamble",
    "href": "posts/robocop-is-wonderfully-childish/index.html#preamble",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Preamble",
    "text": "Preamble\nHere’s something that’s differently nerdy. Some thoughts on the enduring and childish appeal of Robocop as a character and concept, lifted largely from some notes I made on Obsidian (which also uses Markdown)."
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html#notes-on-robocop",
    "href": "posts/robocop-is-wonderfully-childish/index.html#notes-on-robocop",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Notes on Robocop",
    "text": "Notes on Robocop\nWent through a Robocop phase. First film is good. Second film is adequate fan service (by Frank Miller), though unpleasant in terms of how it makes a boy a main antagonist. Reboot is terrible.\nI think this was inspired by seeing footage for the Robocop game Rogue City. Also remembered discussion about how in the original film Robocop’s death and resurrection is modelled on Jesus. It’s definitely true his death is a ‘passion’ (modern equivalent: torture porn?), and that his suffering at the hands of sadistic tormenters fits this pattern.\nIn the first film there’s also the sense of the protagonist reclaiming his humanity. Murphy’s memory is wiped, but through force of will he brings himself to remember who he was, and to identify as Murphy. That’s the last line. He’s thanked and addressed by the Old Man as a person, not as property That’s the arc: becoming human again.\nBy contrast the reboot started with Murphy knowing who he was, though the scientists can modify the extent to which his feelings or programming are in charge. There’s a plot about police corruption and selling weapons illegally, and plenty of exposition from scientists where they try to shoehorn in cod-philosophy on personhood and free will, but this felt like ‘filler’ between a series of action sequences, which were much faster but also much less weighty and visceral than those in the original.\nIn the original Robocop is a bullet sponge. He was slow, apparently unfeeling. Though this might be partly a function of Peter Weller not having a great deal of mobility when wearing the suit, it also feeds into the central character arc: robots don’t feel; people do.1 Robots aren’t vulnerable as people are. As Robocop becomes damaged, he becomes more vulnerable, and his face becomes visible. Vulnerability is necessary for humanity to be restored. Robocop’s suit makes him a superhero, an adolescent boy fantasy of massive strength and power, but it also makes him a prisoner, trapped and entombed.\n\n\n\nRegaining Humanity: But at what cost?\n\n\nAgain, by contrast, in the remake Murphy has lost less. He still has a human hand, still has his memories and sense of self, and still has all the speed and mobility he had as a person, only more so. He doesn’t have an arc, he has perturbations and wobbles.\nLet’s think some more about why Robocop is an adolescent, or even pre-adolescent, fantasy. Firstly, it appeals to a kind of crude creativity of combinatorials: take two things that are familiar, combine them, and make something unfamiliar. With only a limited number of schemas, even a young child can wonder what happens when they are combined, and feel excited and proud about bootstrapping from everyday experience to pure fantasy. Like the distinction between animal, vegetable and mineral, ‘man’ and ‘machine’ are different primary colours, and seeing the concept of Robocop may be for a young child like seeing red and blue make purple for the first time.\n\n\n\nTeenage Mutant Ninja Turtles: Another contemporaneous example of the ‘primary colour chimera’ attractive to children\n\n\nSecond, there’s the power fantasy. Within this, there’s the sense of seeing someone who can play an action game and be allowed to ‘cheat’. The roles of an action game, involving shooting, must include that, once shot, the player must ‘play dead’. But in Robocop is a fantasy of a character who’s still allowed to shoot others whilst being able to ignore when others shoot him. Robocop presents a fantasy for a young child of playing a game where you can do things that no-one else can, because you’re more special than everyone else.\nI suspect that’s why, even though Robocop was clearly unsuitable for children, the concept of Robocop appeared to have so much appeal to children. There’s something inherently childish about seeing such a pure chimera rendered on screen, and the possibilities and affordances of this chimera are signalled so brightly from the images alone that the (adult) source material does not need to be consulted.\nAgain, this seems to be yet another reason why the remake did not work: 2014’s Murphy was not machine enough. The suit did not make him clearly ‘other’ enough. Even though the graphic violence was toned down so that in theory children could watch, the lack of schematic purity in the ideal types being mixed meant the pleasure of seeing primary colours being mixed, in the way the original so effectively managed, was muddied and diluted."
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html#footnotes",
    "href": "posts/robocop-is-wonderfully-childish/index.html#footnotes",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlistair Gray’s Lanark played with a similar idea, in the form of Dragonhide, a magical realist representation of dermatitis, emotional distance, and lack of outlet for artistic expression. Too much Dragonhide, the book warns, causes sufferers to become unable to move, and unable to vent, their heat and energy, without expression, eventually causing them to boil to death!↩︎"
  },
  {
    "objectID": "posts/talk-at-edinbr/index.html",
    "href": "posts/talk-at-edinbr/index.html",
    "title": "EdinbR talk on modelling economic (in)activity transitions",
    "section": "",
    "text": "Yesterday I had the great privilege of being one of two speakers at the Edinburgh R Users group, called EdinbR. (Difficult to say without sounding like a pirate.)\nI spoke through some of the modelling and conceptual challenges involved in trying to model the effect that various drivers/factors/exposures have on how many people in the UK become economically inactive, especially economically inactive for reasons of long-term sickness.\nThe talk seemed to go well (though perhaps the speaker’s always the last person qualified to judge), even though some of the algebra didn’t render correctly. (Which unfortunately means I also used algebra.)\nLike this blog, the presentation also made use of Quarto, but in the presentation’s case using reveal.js.\nThe presentation is available, for those intrepid souls interested in seeing something with R code and algebra, here."
  },
  {
    "objectID": "posts/lms-are-glms-part-02/index.html",
    "href": "posts/lms-are-glms-part-02/index.html",
    "title": "Linear Models are General Linear Models",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/lms-are-glms-part-02/index.html#tldr",
    "href": "posts/lms-are-glms-part-02/index.html#tldr",
    "title": "Linear Models are General Linear Models",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/lms-are-glms-part-02/index.html#part-2-systematic-components-and-link-functions",
    "href": "posts/lms-are-glms-part-02/index.html#part-2-systematic-components-and-link-functions",
    "title": "Linear Models are General Linear Models",
    "section": "Part 2: Systematic components and link functions",
    "text": "Part 2: Systematic components and link functions\nIn part 1 of this series we introduced the following general framework for thinking about statistical models and what they contain.\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\] The terminology are as described previously.\nThese equations are too broad and abstract to be implemented directly. Instead, specific choices about the \\(f(.)\\) and \\(g(.)\\) need to be made. King, Tomz, and Wittenberg (2000) gives the following examples:\nLogistic Regression\n\\[\nY_i \\sim Bernoulli(\\pi_i)\n\\]\n\\[\n\\pi_i = \\frac{1}{1 + e^{-X_i\\beta}}\n\\]\nLinear Regression\n\\[\nY_i \\sim N(\\mu_i, \\sigma^2)\n\\] \\[\n\\mu_i = X_i\\beta\n\\]\nSo, what’s so special about linear regression, in this framework?\nIn one sense, not so much. It’s got a systematic component, and it’s got a stochastic component. But so do other models. But in another sense, quite a lot. It’s a rare case where the systematic component, \\(g(.)\\), doesn’t transform its inputs in some weird and wonderful way. We can say that \\(g(.)\\) is the identity transform, \\(I(.)\\), which in words means take what you’re given, do nothing to it, and pass it on.\nBy contrast, the systematic component for logistic regression is known as the logistic function. \\(logistic(x) := \\frac{1}{1 + e^{-x}}\\) It transforms inputs that could be anywhere on the real number line to values that lay somewhere between 0 and 1. Why 0 to 1? Because what logistic regression models produce aren’t predicted values, but predicted probabilities, and nothing can be more probable than certain (1) or less probable than impossible (0).\nWe can compare the transformations used in linear and logistic regression as follows:1\n\n# Define transformations\nident &lt;- function(x) {x}\nlgt &lt;- function(x) {1 / (1 + exp(-x))}\n\n\n# Draw the associations\ncurve(ident, -6, 6,\n      xlab = \"x (before transform)\",\n      ylab = \"z (after transform)\",\n      main = \"The Identity 'Transformation'\"\n      )\ncurve(lgt, -6, 6, \n      xlab = \"x (before transform)\", \n      ylab = \"z (after transform)\",\n      main = \"The Logistic Transformation\"\n      )\n\n\n\n\n\n\nIdentity Transformation\n\n\n\n\n\n\n\nLogistic Transformation\n\n\n\n\n\n\nThe usual input to the transformation function \\(g(.)\\) is a sum of products. For three variables, for example, this could be \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). In matrix algebra this generalises to \\(\\boldsymbol{X\\beta}\\) , where \\(\\boldsymbol{X}\\) is the predictor data whose rows are observations, columns are variables, and whose first column is a vector of 1s (for the intercept term). The \\(\\boldsymbol{\\beta}\\) term is a row-wise vector comprising each specific \\(\\beta\\) term, such as \\(\\boldsymbol{\\beta} = \\{ \\beta_0, \\beta_1, \\beta_2 \\}\\) in the three variable example above.\nWhat’s special about the identity transformation, and so linear regression, is that there is a fairly clear correspondence between a \\(\\beta_j\\) term and the estimated influence of changing a predictor variable \\(x_j\\) on the predicted outcome \\(Y\\), i.e. the ‘effect of \\(x_j\\) on \\(Y\\)’. For other transformations this tends to not be the case.\nWe’ll delve into this a bit more in a later post."
  },
  {
    "objectID": "posts/lms-are-glms-part-02/index.html#footnotes",
    "href": "posts/lms-are-glms-part-02/index.html#footnotes",
    "title": "Linear Models are General Linear Models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsing some base R graphics functions as I’m feeling masochistic↩︎"
  },
  {
    "objectID": "posts/lms-are-glms-part-03/index.html",
    "href": "posts/lms-are-glms-part-03/index.html",
    "title": "Linear Models are General Linear Models",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/lms-are-glms-part-03/index.html#tldr",
    "href": "posts/lms-are-glms-part-03/index.html#tldr",
    "title": "Linear Models are General Linear Models",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/lms-are-glms-part-03/index.html#part-3-how-to-express-a-linear-model-as-a-generalised-linear-model",
    "href": "posts/lms-are-glms-part-03/index.html#part-3-how-to-express-a-linear-model-as-a-generalised-linear-model",
    "title": "Linear Models are General Linear Models",
    "section": "Part 3: How to express a linear model as a generalised linear model",
    "text": "Part 3: How to express a linear model as a generalised linear model\nIn the last part, we introduced two types of generalised linear models, with two types of transformation for the systematic component of the model, g(.), the logit transformation, and the identity transformation. This post will show how this framework is implemented in practice in R.\nIn R, there’s the lm function for linear models, and the glm function for generalised linear models.\nI’ve argued previously that the standard linear regression is just a specific type of generalised linear model, one that makes use of an identity transformation I(.) for its systematic component g(.). Let’s now demonstrate that by producing the same model specification using both lm and glm.\nWe can start by being painfully unimaginative and picking using one of R’s standard datasets\n\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\niris |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\nIt looks like, where the petal length is over 2.5, the relationship with sepal length is fairly linear\n\niris |&gt; \n  filter(Petal.Length &gt; 2.5) |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\nSo, let’s make a linear regression just of this subset\n\niris_ss &lt;- \n  iris |&gt; \n  filter(Petal.Length &gt; 2.5) \n\nWe can produce the regression using lm as follows:\n\nmod_lm &lt;- lm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\nAnd we can use the summary function (which checks the type of mod_lm and evokes summary.lm implicitly) to get the following:\n\nsummary(mod_lm)\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09194 -0.26570  0.00761  0.21902  0.87502 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3731 on 98 degrees of freedom\nMultiple R-squared:  0.6864,    Adjusted R-squared:  0.6832 \nF-statistic: 214.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nWoohoo! Three stars next to the Petal.Length coefficient! Definitely publishable!\nTo do the same using glm.\n\nmod_glm &lt;- glm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\nAnd we can use the summary function for this data too. In this case, summary evokes summary.glm because it knows the class of mod_glm contains glm.\n\nsummary(mod_glm)\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.09194  -0.26570   0.00761   0.21902   0.87502  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nSo, the coefficients are exactly the same. But there’s also some additional information in the summary, including on the type of ‘family’ used. Why is this?\nIf we look at the help for glm we can see that, by default, the family argument is set to gaussian.\nAnd if we delve a bit further into the help file, in the details about the family argument, it links to the family help page. The usage statement of the family help file is as follows:\nfamily(object, ...)\n\nbinomial(link = \"logit\")\ngaussian(link = \"identity\")\nGamma(link = \"inverse\")\ninverse.gaussian(link = \"1/mu^2\")\npoisson(link = \"log\")\nquasi(link = \"identity\", variance = \"constant\")\nquasibinomial(link = \"logit\")\nquasipoisson(link = \"log\")\n\nEach family has a default link argument, and for this gaussian family, this link is the identity function.\nWe can also see that, for both the binomial and quasibinomial family, the default link is logit, which transforms all predictors onto a 0-1 scale, as shown in the last post.\nSo, by using the default family, the Gaussian family is selected, and by using the default Gaussian family member, the identity link is selected.\nWe can confirm this by setting the family and link explicitly, showing that we get the same results\n\nmod_glm2 &lt;- glm(Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), data = iris_ss)\nsummary(mod_glm2)\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), \n    data = iris_ss)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.09194  -0.26570   0.00761   0.21902   0.87502  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nIt’s the same!\nHow do these terms used in the glm function, family and link, relate to the general framework in King, Tomz, and Wittenberg (2000)?\n\nfamily is the stochastic component, f(.)\nlink is the systematic component, g(.)\n\nThey’re different terms, but it’s the same broad framework.\nLinear models are just one type of general linear model!"
  },
  {
    "objectID": "posts/tidy-tuesday-dr-who/index.html",
    "href": "posts/tidy-tuesday-dr-who/index.html",
    "title": "Tidy Tuesday on Dr Who",
    "section": "",
    "text": "First we load the packages\nThe tidyverse equivalent of pacman is now pak.\nThe latest dataset is here, and the specific files to work.\n\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndta_list &lt;- tidytuesdayR::tt_load(x = \"2023-11-28\")\n\n--- Compiling #TidyTuesday Information for 2023-11-28 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `drwho_episodes.csv`\n    Downloading file 2 of 3: `drwho_directors.csv`\n    Downloading file 3 of 3: `drwho_writers.csv`\n\n\n--- Download complete ---\n\ndta_eps &lt;- dta_list[[\"drwho_episodes\"]]\ndta_wrt &lt;- dta_list[[\"drwho_writers\"]]\n\nLet’s see how the viewship changed over time\n\ndta_eps_season &lt;- \n  dta_eps |&gt; \n  group_by(season_number) |&gt; \n  mutate(\n    mean_viewers = mean(uk_viewers),\n    mean_date = mean(first_aired)\n    ) |&gt; \n  ungroup()\n\ndta_eps_season |&gt; \n  ggplot(aes(x = first_aired, y = uk_viewers)) + \n  geom_point(colour = \"grey\") +\n  geom_point(aes(x = mean_date, y = mean_viewers), size = 2.5) + \n  scale_x_date(breaks = \"2 years\", labels = \\(x) format(x, \"%Y\")) +\n  labs(\n    x = \"First aired\",\n    y = \"UK Viewers (millions)\",\n    title = \"Viewers over time for Dr Who\",\n    subtitle = \"People don't watch TV like they used to...\"\n  ) +\n  annotate(\"text\", x = lubridate::make_date(2015), y = 10, label = \"What happened here?!\") +\n  annotate(\"text\", x = lubridate::make_date(2014), y= 8, label = \"Smartphone strangling the TV from now\", hjust = 0) + \n  stat_smooth(colour = \"blue\", se = FALSE) \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nLet’s now look at writers by season\n\ndta_eps_wrt &lt;- \n  dta_eps |&gt; \n    left_join(dta_wrt, by = \"story_number\") \n\nHow many episodes by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  summarise(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  arrange(desc(n_written))\n\n# A tibble: 40 × 2\n   writer           n_written\n   &lt;chr&gt;                &lt;int&gt;\n 1 Steven Moffat           45\n 2 Russell T Davies        31\n 3 Chris Chibnall          29\n 4 Mark Gatiss              9\n 5 Toby Whithouse           7\n 6 Gareth Roberts           5\n 7 Helen Raynor             4\n 8 Jamie Mathieson          4\n 9 Peter Harness            4\n10 Matthew Graham           3\n# ℹ 30 more rows\n\n\nSo Moffat wrote most episodes, then Davies, then Chibnall\nAnd what about popularity by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  mutate(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  filter(n_written &gt;= 5) |&gt; \n  ggplot(aes(x = fct_reorder(writer, rating), y= rating)) + \n  geom_boxplot() + \n  coord_flip() + \n  labs(\n    x = \"Distribution of ratings\",\n    y = \"Writer\", \n    title = \"Rating distribution by writer\",\n    subtitle = \"Writers who wrote at least five episodes\"\n  )\n\n\n\n\nWhen were the different writers active?\n\nmajor_writers_active &lt;- \n  dta_eps_wrt |&gt; \n    group_by(writer) |&gt;  \n    mutate(\n      n_written = n()\n    ) |&gt; \n    ungroup() |&gt; \n    filter(n_written &gt;= 5) |&gt; \n    group_by(writer) |&gt; \n    summarise(\n      started_writing = min(first_aired),\n      finished_writing = max(first_aired),\n      n_written = n_written[1]\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(\n      yr_start = year(started_writing),\n      yr_end = year(finished_writing)\n    )\n\nmajor_writers_active |&gt; \n  arrange(started_writing)\n\n# A tibble: 6 × 6\n  writer           started_writing finished_writing n_written yr_start yr_end\n  &lt;chr&gt;            &lt;date&gt;          &lt;date&gt;               &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Russell T Davies 2005-03-26      2010-01-01              31     2005   2010\n2 Mark Gatiss      2005-04-09      2017-06-10               9     2005   2017\n3 Steven Moffat    2005-05-21      2017-12-25              45     2005   2017\n4 Toby Whithouse   2006-04-29      2017-06-03               7     2006   2017\n5 Gareth Roberts   2007-04-07      2011-09-24               5     2007   2011\n6 Chris Chibnall   2007-05-19      2022-10-23              29     2007   2022\n\n\nHere we see the tenure of different major writers. Russell T Davies and Steven Moffatt are the major players."
  },
  {
    "objectID": "posts/tidy-tuesday-dr-who/index.html#tidy-tuesday-challenge",
    "href": "posts/tidy-tuesday-dr-who/index.html#tidy-tuesday-challenge",
    "title": "Tidy Tuesday on Dr Who",
    "section": "",
    "text": "First we load the packages\nThe tidyverse equivalent of pacman is now pak.\nThe latest dataset is here, and the specific files to work.\n\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndta_list &lt;- tidytuesdayR::tt_load(x = \"2023-11-28\")\n\n--- Compiling #TidyTuesday Information for 2023-11-28 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `drwho_episodes.csv`\n    Downloading file 2 of 3: `drwho_directors.csv`\n    Downloading file 3 of 3: `drwho_writers.csv`\n\n\n--- Download complete ---\n\ndta_eps &lt;- dta_list[[\"drwho_episodes\"]]\ndta_wrt &lt;- dta_list[[\"drwho_writers\"]]\n\nLet’s see how the viewship changed over time\n\ndta_eps_season &lt;- \n  dta_eps |&gt; \n  group_by(season_number) |&gt; \n  mutate(\n    mean_viewers = mean(uk_viewers),\n    mean_date = mean(first_aired)\n    ) |&gt; \n  ungroup()\n\ndta_eps_season |&gt; \n  ggplot(aes(x = first_aired, y = uk_viewers)) + \n  geom_point(colour = \"grey\") +\n  geom_point(aes(x = mean_date, y = mean_viewers), size = 2.5) + \n  scale_x_date(breaks = \"2 years\", labels = \\(x) format(x, \"%Y\")) +\n  labs(\n    x = \"First aired\",\n    y = \"UK Viewers (millions)\",\n    title = \"Viewers over time for Dr Who\",\n    subtitle = \"People don't watch TV like they used to...\"\n  ) +\n  annotate(\"text\", x = lubridate::make_date(2015), y = 10, label = \"What happened here?!\") +\n  annotate(\"text\", x = lubridate::make_date(2014), y= 8, label = \"Smartphone strangling the TV from now\", hjust = 0) + \n  stat_smooth(colour = \"blue\", se = FALSE) \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nLet’s now look at writers by season\n\ndta_eps_wrt &lt;- \n  dta_eps |&gt; \n    left_join(dta_wrt, by = \"story_number\") \n\nHow many episodes by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  summarise(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  arrange(desc(n_written))\n\n# A tibble: 40 × 2\n   writer           n_written\n   &lt;chr&gt;                &lt;int&gt;\n 1 Steven Moffat           45\n 2 Russell T Davies        31\n 3 Chris Chibnall          29\n 4 Mark Gatiss              9\n 5 Toby Whithouse           7\n 6 Gareth Roberts           5\n 7 Helen Raynor             4\n 8 Jamie Mathieson          4\n 9 Peter Harness            4\n10 Matthew Graham           3\n# ℹ 30 more rows\n\n\nSo Moffat wrote most episodes, then Davies, then Chibnall\nAnd what about popularity by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  mutate(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  filter(n_written &gt;= 5) |&gt; \n  ggplot(aes(x = fct_reorder(writer, rating), y= rating)) + \n  geom_boxplot() + \n  coord_flip() + \n  labs(\n    x = \"Distribution of ratings\",\n    y = \"Writer\", \n    title = \"Rating distribution by writer\",\n    subtitle = \"Writers who wrote at least five episodes\"\n  )\n\n\n\n\nWhen were the different writers active?\n\nmajor_writers_active &lt;- \n  dta_eps_wrt |&gt; \n    group_by(writer) |&gt;  \n    mutate(\n      n_written = n()\n    ) |&gt; \n    ungroup() |&gt; \n    filter(n_written &gt;= 5) |&gt; \n    group_by(writer) |&gt; \n    summarise(\n      started_writing = min(first_aired),\n      finished_writing = max(first_aired),\n      n_written = n_written[1]\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(\n      yr_start = year(started_writing),\n      yr_end = year(finished_writing)\n    )\n\nmajor_writers_active |&gt; \n  arrange(started_writing)\n\n# A tibble: 6 × 6\n  writer           started_writing finished_writing n_written yr_start yr_end\n  &lt;chr&gt;            &lt;date&gt;          &lt;date&gt;               &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Russell T Davies 2005-03-26      2010-01-01              31     2005   2010\n2 Mark Gatiss      2005-04-09      2017-06-10               9     2005   2017\n3 Steven Moffat    2005-05-21      2017-12-25              45     2005   2017\n4 Toby Whithouse   2006-04-29      2017-06-03               7     2006   2017\n5 Gareth Roberts   2007-04-07      2011-09-24               5     2007   2011\n6 Chris Chibnall   2007-05-19      2022-10-23              29     2007   2022\n\n\nHere we see the tenure of different major writers. Russell T Davies and Steven Moffatt are the major players."
  },
  {
    "objectID": "posts/tidy-tuesday-dr-who/index.html#coda",
    "href": "posts/tidy-tuesday-dr-who/index.html#coda",
    "title": "Tidy Tuesday on Dr Who",
    "section": "Coda",
    "text": "Coda\nNeither of us know much about Dr Who!\nBut hopefully we now know a bit more!"
  },
  {
    "objectID": "posts/wrapping-guide/index.html",
    "href": "posts/wrapping-guide/index.html",
    "title": "How to wrap presents",
    "section": "",
    "text": "Here’s the key information I’ve learned from watching too many Youtube videos on how to wrap presents over the years.\n\n\n\n\nflowchart TB\n\nstart[Presents to Wrap]\ndecision{Are they cuboid?}\nbox(Put them in a cuboid)\naction(Wrap them)\nfinish[Presents are wrapped]\n\nstart --&gt; decision\ndecision --&gt;|yes| action\ndecision --&gt;|no| box\nbox --&gt; action\naction --&gt; finish\n\n\n\n\n\n\nYou’re welcome!"
  },
  {
    "objectID": "posts/interactive-sliders/index.html",
    "href": "posts/interactive-sliders/index.html",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "",
    "text": "Below is an example of creating a plotly chart with an interactive slider using crosstalk.\nBy default, the plot shows the proportion of datazones in a local authority that are in the 15% most deprived datazones in Scotland. (Using the 2020 SIMD).\nThe slider allows different thresholds than the 15% default to be selected."
  },
  {
    "objectID": "posts/interactive-sliders/index.html#introduction",
    "href": "posts/interactive-sliders/index.html#introduction",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "",
    "text": "Below is an example of creating a plotly chart with an interactive slider using crosstalk.\nBy default, the plot shows the proportion of datazones in a local authority that are in the 15% most deprived datazones in Scotland. (Using the 2020 SIMD).\nThe slider allows different thresholds than the 15% default to be selected."
  },
  {
    "objectID": "posts/interactive-sliders/index.html#data-downloading-and-preparation",
    "href": "posts/interactive-sliders/index.html#data-downloading-and-preparation",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "Data Downloading and Preparation",
    "text": "Data Downloading and Preparation\nTo see the code itself, just click on the word ‘code’ to open up the block’.\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(crosstalk)\nlibrary(here)\n\n\n\n\nCode\nif(!file.exists(here(\"simd_data.xlsx\"))){\n  download.file(\n    url = \"https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2020/01/scottish-index-of-multiple-deprivation-2020-data-zone-look-up-file/documents/scottish-index-of-multiple-deprivation-data-zone-look-up/scottish-index-of-multiple-deprivation-data-zone-look-up/govscot%3Adocument/SIMD%2B2020v2%2B-%2Bdatazone%2Blookup.xlsx\",\n    destfile = here(\"simd_data.xlsx\"),\n    mode = \"wb\"\n  )\n}\n\ndta &lt;- openxlsx::readWorkbook(here(\"simd_data.xlsx\"), sheet = \"SIMD 2020v2 DZ lookup data\")\n\n\nThe code for the figure itself is below. It’s quite a convoluted process. There’s almost certaintly neater ways of doing this. The main thing to keep in mind is all the figures exist; just only one is visible at a time.\n\n\nCode\n# So let's construct a new aval containing the different x-y tuples given the threshold selected\n\ncalc_prop_deprived &lt;- function(q, dta){\n    dta %&gt;% \n      group_by(HBname) %&gt;% \n      summarise(prop_deprived = mean(pct_rank &lt; q)) %&gt;% \n      ungroup()\n}\n\ndf_rank &lt;- \n  dta %&gt;% \n    select(HBname, SIMD2020v2_Rank) %&gt;% \n    mutate(pct_rank = SIMD2020v2_Rank / max(SIMD2020v2_Rank))\n\n\nshared_df &lt;- tibble(\n  dep_quants = seq(0.05, 0.95, by = 0.05)\n) %&gt;% \n  mutate(derived_props = map(dep_quants, calc_prop_deprived, dta = df_rank)) %&gt;% \n  unnest(derived_props) %&gt;% \n  mutate(undep_quants = 1 - dep_quants) \n\n\n# Now to put it in the structure, and set active for `dep_quants = 0.15`\n\n\nunique_dep_quants &lt;- unique(shared_df$dep_quants)\nn_steps &lt;- length(unique_dep_quants)\n\ndep_vals &lt;- list()\nfor (step in 1:n_steps){\n  tmp &lt;- \n    shared_df %&gt;% \n      filter(dep_quants == unique_dep_quants[step]) %&gt;% \n      select(HBname, prop_deprived) %&gt;% \n      mutate(HBname = reorder(HBname, prop_deprived))\n  \n  dep_vals[[step]] &lt;- list(\n    visible = FALSE,\n    name = paste0('Quantile: ', unique_dep_quants[step]),\n    x=tmp$prop_deprived,\n    y=tmp$HBname\n    \n  ) \n}\n\n# 15% is the third list object \n\ndep_vals[3][[1]]$visible = TRUE\n\n# Now visualise \n\n# create steps and plot all traces\ndep_steps &lt;- list()\nfig &lt;- plot_ly() \nfor (i in c(3, 1, 2, 4:n_steps)) { # Start with 3 as this is 15% and this should determine the default HB order \n fig &lt;- add_bars(fig,x=dep_vals[i][[1]]$x,  y=dep_vals[i][[1]]$y, visible = dep_vals[i][[1]]$visible, \n                 name = dep_vals[i][[1]]$name, orientation = 'h', hoverinfo = 'x+y', color = I(\"gray\"),\n                 showlegend = FALSE) %&gt;% \n   layout(\n      title = list(\n        text = glue::glue(\"Proportion of datazones in Health Boards at least this deprived\")\n      ),\n      xaxis = list(\n        title = \"Proportion this deprived in Health Board\",\n        range = list(0, 1)\n      ),\n      yaxis = list(\n        title = \"Health Board\"\n      )\n   )\n\n  step &lt;- list(args = list('visible', rep(FALSE, length(dep_vals))),\n               method = 'restyle')\n  step$args[[2]][i] = TRUE  \n  step$label = unique_dep_quants[i]\n  dep_steps[[i]] = step \n}  \n#names(dep_steps) &lt;- unique_dep_quants\n\nfig &lt;- fig %&gt;%\n  layout(sliders = list(list(active = 2,\n                             currentvalue = list(prefix = \"Deprivation: \"),\n                             steps = dep_steps)))\n\nfig\n\n\n\n\n\n\nAs you can see, there’s still some work to do regarding formatting. But it works!"
  },
  {
    "objectID": "posts/interactive-sliders/index.html#static",
    "href": "posts/interactive-sliders/index.html#static",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "Static",
    "text": "Static\nFor comparison, here’s the same data used to produce a static plot\n\n\nCode\n# Now to put it in the structure, and set active for `dep_quants = 0.15`\n\ndf_15pc &lt;- shared_df |&gt; \n  filter(between(dep_quants, 0.149, 0.151)) |&gt; \n  select(-dep_quants, -undep_quants)\n\ndf_15pc |&gt;\n  mutate(pct_deprived = 100 * prop_deprived) |&gt; \n  ggplot(aes(y= pct_deprived, x = fct_reorder(HBname, pct_deprived))) + \n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(\n      label = ifelse(df_15pc$prop_deprived &gt; 0, sprintf(\"%.1f\", pct_deprived), \"\")\n    ), \n    color = \"white\",\n    hjust = 1, \n    nudge_y = -0.5\n  ) + \n  coord_flip() + \n  labs(\n    x = \"Health Board\",\n    y = \"Percent of datazones in 15% most deprived proportion of Scotland\",\n    title = \"Percent of datazones in Health Board in 15% most deprived areas of Scotland\",\n    subtitle = \"SIMD 2020\"\n  ) + \n  geom_hline(yintercept = 0)"
  },
  {
    "objectID": "posts/scientific-illustration-unit-circle/index.html",
    "href": "posts/scientific-illustration-unit-circle/index.html",
    "title": "Scientific Illustrations: Annotating the unit circle",
    "section": "",
    "text": "Here’s an example of a scientific illustration I’ve just produced to illustrate some scenarios I’m modelling for my work on economic inactivity determinants.\nI have two continuous variables (derived from the GHQ-12), mental health and physical health. Both are standardised so they have the same scale.\nBut I’m interested in the effects of improving/changing ‘health’ in general, which is obviously composed of both mental health and physical health, but not measured directly.\nAs the two variables are standardised, however, I can model an improvement in health in general as a change in both mental health and physical health concurrently.\nHowever, I want to compare like-with-like: scenarios in which the total ‘amount’ of intervention effect is kept constant, but the relative contribution of the two health components is varied.\nThis is where a little trigonometry comes in. 1 All interventions on the grey unit circle in Figure 1 represent possible scenarios in which the total amount of health change is constant, but where the relative contribution of mental and physical health is varied.\nThe aim of the scientific illustration is to make this intuition a bit clearer to understand!\nCode\nlibrary(tidyverse)\nlibrary(geomtextpath)\n\npos_y &lt;- function(x) {sqrt(1 - x^2)}\nx = seq(0, 1, by = 0.001)\ndta &lt;- tibble(\n  x = x\n) |&gt; \n  mutate(\n    y = pos_y(x)\n  )\n\ndta |&gt; \n  ggplot(aes(x = x, y = y)) + \n    geom_line(color = \"grey\") + \n    coord_equal() + \n    labs(x = \"Physical Health (Standardised)\",\n         y = \"Mental Health (Standardised)\",\n         title = \"Health improvement scenarios modelled\") + \n  theme_minimal() + \n  annotate(\"point\", x = 1, y = 0) + \n  annotate(\"point\", x = 0, y = 1) + \n  annotate(\"point\", x = 1/ sqrt(2), y = 1/ sqrt(2)) + \n  annotate(\"point\", x =  2 / sqrt(5), y = 1 / sqrt(5)) + \n  annotate(\"point\", x = 1 / sqrt(5), y = 2 / sqrt(5)) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, xend = 0, yend = 1), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S1: MH Only\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  ) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, xend = 1, yend = 0), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S2: PH Only\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  ) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, xend = 1/sqrt(2), yend = 1/sqrt(2)), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S3: Equal Gain\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  ) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, yend = 2/sqrt(5), xend = 1/sqrt(5)), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S4: MH Bias\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  )  +\n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, yend = 1/sqrt(5), xend = 2/sqrt(5)), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S5: PH Bias\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  )  \n\n\n\n\n\nFigure 1: Modelling various intervention scenarios"
  },
  {
    "objectID": "posts/scientific-illustration-unit-circle/index.html#footnotes",
    "href": "posts/scientific-illustration-unit-circle/index.html#footnotes",
    "title": "Scientific Illustrations: Annotating the unit circle",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPeople who claim you’ll never need trigonometry once you leave school are wrong. It’s valuable about once a decade!↩︎"
  },
  {
    "objectID": "posts/tidy-tuesday-life-expectancy/index.html",
    "href": "posts/tidy-tuesday-life-expectancy/index.html",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "",
    "text": "This week’s Tidy Tuesday compares life expectancy across the globe and is available here:"
  },
  {
    "objectID": "posts/tidy-tuesday-life-expectancy/index.html#loading-the-data",
    "href": "posts/tidy-tuesday-life-expectancy/index.html#loading-the-data",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Loading the data",
    "text": "Loading the data\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\ndata_url &lt;- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-12-05/life_expectancy_different_ages.csv\"\n \ndta &lt;- read_csv(data_url)\n\n# Alternatively\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-05')\n\n\n\n    Downloading file 1 of 3: `life_expectancy.csv`\n    Downloading file 2 of 3: `life_expectancy_different_ages.csv`\n    Downloading file 3 of 3: `life_expectancy_female_male.csv`\n\n\n\n\nCode\nnames(tuesdata)\n\n\n[1] \"life_expectancy\"                \"life_expectancy_different_ages\"\n[3] \"life_expectancy_female_male\"   \n\n\nCode\nlife_expectancy &lt;- tuesdata$life_expectancy\n\nn_distinct(life_expectancy$Entity)\n\n\n[1] 261"
  },
  {
    "objectID": "posts/tidy-tuesday-life-expectancy/index.html#setting-a-global-plot-theme",
    "href": "posts/tidy-tuesday-life-expectancy/index.html#setting-a-global-plot-theme",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Setting a global plot theme",
    "text": "Setting a global plot theme\n\n\nCode\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "posts/tidy-tuesday-life-expectancy/index.html#comparing-life-expectancy-across-regions",
    "href": "posts/tidy-tuesday-life-expectancy/index.html#comparing-life-expectancy-across-regions",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Comparing life expectancy across regions",
    "text": "Comparing life expectancy across regions\n\n\nCode\nregions &lt;- life_expectancy %&gt;%\n  filter(str_detect(Entity, \"region\"))\n\nregions %&gt;%\n  ggplot(aes(x = Year, y = LifeExpectancy)) +\n  geom_line(aes(col = Entity)) +\n  theme(legend.position = \"top\") +\n  annotate(geom = \"text\",\n           x = 1960, y = 50,\n           label = \"What happened here?\") +\n  geom_vline(xintercept = 2019, linetype = 2) +\n  annotate(geom = \"text\", x = 2019, y = 75,\n           label = \"Start of COVID pandemic\",\n           hjust = 1)"
  },
  {
    "objectID": "posts/tidy-tuesday-life-expectancy/index.html#difference-in-life-expectancy-between-more-and-less-developed-regions",
    "href": "posts/tidy-tuesday-life-expectancy/index.html#difference-in-life-expectancy-between-more-and-less-developed-regions",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Difference in life expectancy between more and less developed regions",
    "text": "Difference in life expectancy between more and less developed regions\n\n\nCode\nregions %&gt;%\n  filter(Entity %in% c(\"More developed regions\",\n                       \"Less developed regions\"))  %&gt;%\n  arrange(Year, Entity) %&gt;%\n  group_by(Year) %&gt;%\n  mutate(difference = LifeExpectancy - lag(LifeExpectancy)) %&gt;%\n  filter(!is.na(difference)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = Year, y = difference)) +\n  geom_area(alpha = 0.5) +\n  expand_limits(y = 0) +\n  labs(title = \"Difference in life expectancy between more developed and less developed regions\",\n       y = \"Difference in life expectancy (years)\")\n\n\n\n\n\nWe look at life expectancy at different ages in three specific countries.\n\n\nCode\ndata_tidy &lt;-\n  dta |&gt;\n    pivot_longer(\n      cols = LifeExpectancy0:LifeExpectancy80\n    ) |&gt;\n    mutate(\n      starting_age = str_remove(name, \"LifeExpectancy\") %&gt;%\n        as.numeric()\n    ) |&gt;\n    select(-name) |&gt;\n    rename(e_x = value)\n\n\n\n\nCode\ndata_tidy |&gt;\n  filter(\n    Entity %in% c(\n      \"Nigeria\", \"Iran\",\n      \"South Africa\"\n    )\n  ) |&gt;\n  arrange(Year)  |&gt;\n  ggplot(aes(Year, e_x, group = factor(starting_age), colour = factor(starting_age))) +\n  geom_line() +\n  facet_wrap(~Entity)\n\n\n\n\n\nSandra Nwobi, who suggested the three countries above, provides the following summary:\n\nOf the three developing countries—Iran, South Africa, and Nigeria—Nigeria has a significantly higher zero-age death rate in the late 50s and early 60s. This can be attributed to a number of factors, including socioeconomic instability, political unrest, malnutrition, and limited access to healthcare. Comparing this result to South Africa and Iran, it is comparatively higher. However, there have been noticeable improvements in Nigeria during the 1980s, with a steady increase. Nevertheless, much work needs to be done to combat this in Nigeria, as it performs significantly worse than the other two countries.\n\nThere was a noticeable decline in data in the early 2000s, particularly in South Africa. Health crises like HIV/AIDS, which may have affected people between the ages of 0 and 25, as well as a number of social and economic problems may have contributed to this decline.\n\nIran’s data indicates consistent growth across all age groups over the years, with the exception of a general decline in 2020 that was likely caused by the COVID-19 virus. Out of the three countries, South Africa is the most affected, maybe as a result of a much older demography compared to Nigeria."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jon Minton’s Blog",
    "section": "",
    "text": "Tidy Tuesday on Life Expectancy\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nLife Expectancy\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nNick Christofides, Jon Minton, Sandra Nwobi\n\n\n\n\n\n\n  \n\n\n\n\nBeavis and Butt-Head: When a physics graduate dons Dumbface\n\n\n\n\n\n\n\ncartoons\n\n\nstories\n\n\neugenics\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Effective Savings on Interest-free Credit\n\n\n\n\n\n\n\nstatistics\n\n\nfinances\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nHow to wrap presents\n\n\n\n\n\n\n\nChristmas\n\n\nBirthdays\n\n\nArts & Crafts\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nLinear Models are General Linear Models\n\n\nPart Three: glm is just fancy lm\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nNew blog feature: comments\n\n\n\n\n\n\n\nquarto\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nLinear Models are General Linear Models\n\n\nPart Two: Systematic components and link functions\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nScientific Illustrations: Annotating the unit circle\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\ngraphics\n\n\neconomic inactivity\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nInteractive Sliders with Crosstalk and Plotly\n\n\n\n\n\n\n\nR\n\n\nplotly\n\n\ncrosstalk\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nRobocop (1987) is wonderfully childish\n\n\n\n\n\n\n\nstories\n\n\nfilms\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday on Dr Who\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nDr Who\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJon Minton, Nick Christofides\n\n\n\n\n\n\n  \n\n\n\n\nPost with code\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nLinear Models are General Linear Models\n\n\nPart One: Model fitting as parameter calibration\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nEdinbR talk on modelling economic (in)activity transitions\n\n\n\n\n\n\n\nR\n\n\nmodelling\n\n\ntalks\n\n\neconomics\n\n\nhealth\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nA Deathly Silence\n\n\n\n\n\n\n\nMortality\n\n\nEpidemiology\n\n\nPapers\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nFirst Post\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\nNo matching items"
  }
]