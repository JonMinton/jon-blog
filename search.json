[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Jon. Welcome to my blog."
  },
  {
    "objectID": "posts/lms-are-glms-part-01/index.html",
    "href": "posts/lms-are-glms-part-01/index.html",
    "title": "Linear Models are General Linear Models",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/lms-are-glms-part-01/index.html#tldr",
    "href": "posts/lms-are-glms-part-01/index.html#tldr",
    "title": "Linear Models are General Linear Models",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/lms-are-glms-part-01/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "href": "posts/lms-are-glms-part-01/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "title": "Linear Models are General Linear Models",
    "section": "Part 1: What are statistical models and how are they fit?",
    "text": "Part 1: What are statistical models and how are they fit?\nIt’s common for different statistical methods to be taught as if they’re completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\nSomething I’ve found immensely helpful over the years is the following pair of equations:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nIn words, the above is saying something like:\n\nThe predicted response \\(Y_i\\) for a set of predictors \\(X_i\\) is assumed to be drawn from (the \\(\\sim\\) symbol) a stochastic distribution (\\(f(.,.)\\))\nThe stochastic distribution contains both parameters we’re interested in, and which are determined by the data \\(\\theta_i\\), and parameters we’re not interested in and might just have to assume, \\(\\alpha\\).\nThe parameters we’re interested in determining from the data \\(\\theta_i\\) are themselves determined by a systematic component \\(g(.,.)\\) which take and transform two inputs: The observed predictor data \\(X_i\\), and a set of coefficients \\(\\beta\\)\n\nAnd graphically this looks something like:\n\n\n\n\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --&gt; g\n  beta --&gt; g\n  g --&gt; theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt; Y\n\n\n\n\n\n\n\nTo understand how this fits into the ‘whole game’ of modelling, it’s worth introducing another term, \\(D\\), for the data we’re using, and to say that \\(D\\) is partitioned into observed predictors \\(X_i\\), and observed responses, \\(y_i\\).\nFor each observation, \\(i\\), we therefore have a predicted response, \\(Y_i\\), and an observed response, \\(y_i\\). We can compare \\(Y_i\\) with \\(y_i\\) to get the difference between the two, \\(\\delta_i\\).\nNow, obviously can’t change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the \\(\\beta\\) parameters that feed into the systematic component \\(g\\). Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n\n\n\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D --&gt;|partition| X\n  D --&gt;|partition| y\n  X --&gt; g\n  beta --&gt;|rerun| g\n  g --&gt;|transform| theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt;|predict| Y\n  \n  Y --&gt;|compare| diff\n  y --&gt;|compare| diff\n  \n  diff --&gt;|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n\n\n\n\n\nPretty much all statistical model fitting involves iterating along this \\(g \\to \\beta\\) and \\(\\beta \\to g\\) feedback loop until some kind of condition is met involving minimising \\(\\delta\\).\nI’ll expand on this idea further in some later posts :)"
  },
  {
    "objectID": "posts/interactive-sliders/index.html",
    "href": "posts/interactive-sliders/index.html",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "",
    "text": "Below is an example of creating a plotly chart with an interactive slider using crosstalk.\nBy default, the plot shows the proportion of datazones in a local authority that are in the 15% most deprived datazones in Scotland. (Using the 2020 SIMD).\nThe slider allows different thresholds than the 15% default to be selected."
  },
  {
    "objectID": "posts/interactive-sliders/index.html#introduction",
    "href": "posts/interactive-sliders/index.html#introduction",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "",
    "text": "Below is an example of creating a plotly chart with an interactive slider using crosstalk.\nBy default, the plot shows the proportion of datazones in a local authority that are in the 15% most deprived datazones in Scotland. (Using the 2020 SIMD).\nThe slider allows different thresholds than the 15% default to be selected."
  },
  {
    "objectID": "posts/interactive-sliders/index.html#data-downloading-and-preparation",
    "href": "posts/interactive-sliders/index.html#data-downloading-and-preparation",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "Data Downloading and Preparation",
    "text": "Data Downloading and Preparation\nTo see the code itself, just click on the word ‘code’ to open up the block’.\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(crosstalk)\nlibrary(here)\n\n\n\n\nCode\nif(!file.exists(here(\"simd_data.xlsx\"))){\n  download.file(\n    url = \"https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2020/01/scottish-index-of-multiple-deprivation-2020-data-zone-look-up-file/documents/scottish-index-of-multiple-deprivation-data-zone-look-up/scottish-index-of-multiple-deprivation-data-zone-look-up/govscot%3Adocument/SIMD%2B2020v2%2B-%2Bdatazone%2Blookup.xlsx\",\n    destfile = here(\"simd_data.xlsx\"),\n    mode = \"wb\"\n  )\n}\n\ndta &lt;- openxlsx::readWorkbook(here(\"simd_data.xlsx\"), sheet = \"SIMD 2020v2 DZ lookup data\")\n\n\nThe code for the figure itself is below. It’s quite a convoluted process. There’s almost certaintly neater ways of doing this. The main thing to keep in mind is all the figures exist; just only one is visible at a time.\n\n\nCode\n# So let's construct a new aval containing the different x-y tuples given the threshold selected\n\ncalc_prop_deprived &lt;- function(q, dta){\n    dta %&gt;% \n      group_by(HBname) %&gt;% \n      summarise(prop_deprived = mean(pct_rank &lt; q)) %&gt;% \n      ungroup()\n}\n\ndf_rank &lt;- \n  dta %&gt;% \n    select(HBname, SIMD2020v2_Rank) %&gt;% \n    mutate(pct_rank = SIMD2020v2_Rank / max(SIMD2020v2_Rank))\n\n\nshared_df &lt;- tibble(\n  dep_quants = seq(0.05, 0.95, by = 0.05)\n) %&gt;% \n  mutate(derived_props = map(dep_quants, calc_prop_deprived, dta = df_rank)) %&gt;% \n  unnest(derived_props) %&gt;% \n  mutate(undep_quants = 1 - dep_quants) \n\n\n# Now to put it in the structure, and set active for `dep_quants = 0.15`\n\n\nunique_dep_quants &lt;- unique(shared_df$dep_quants)\nn_steps &lt;- length(unique_dep_quants)\n\ndep_vals &lt;- list()\nfor (step in 1:n_steps){\n  tmp &lt;- \n    shared_df %&gt;% \n      filter(dep_quants == unique_dep_quants[step]) %&gt;% \n      select(HBname, prop_deprived) %&gt;% \n      mutate(HBname = reorder(HBname, prop_deprived))\n  \n  dep_vals[[step]] &lt;- list(\n    visible = FALSE,\n    name = paste0('Quantile: ', unique_dep_quants[step]),\n    x=tmp$prop_deprived,\n    y=tmp$HBname\n    \n  ) \n}\n\n# 15% is the third list object \n\ndep_vals[3][[1]]$visible = TRUE\n\n# Now visualise \n\n# create steps and plot all traces\ndep_steps &lt;- list()\nfig &lt;- plot_ly() \nfor (i in c(3, 1, 2, 4:n_steps)) { # Start with 3 as this is 15% and this should determine the default HB order \n fig &lt;- add_bars(fig,x=dep_vals[i][[1]]$x,  y=dep_vals[i][[1]]$y, visible = dep_vals[i][[1]]$visible, \n                 name = dep_vals[i][[1]]$name, orientation = 'h', hoverinfo = 'x+y', color = I(\"gray\"),\n                 showlegend = FALSE) %&gt;% \n   layout(\n      title = list(\n        text = glue::glue(\"Proportion of datazones in Health Boards at least this deprived\")\n      ),\n      xaxis = list(\n        title = \"Proportion this deprived in Health Board\",\n        range = list(0, 1)\n      ),\n      yaxis = list(\n        title = \"Health Board\"\n      )\n   )\n\n  step &lt;- list(args = list('visible', rep(FALSE, length(dep_vals))),\n               method = 'restyle')\n  step$args[[2]][i] = TRUE  \n  step$label = unique_dep_quants[i]\n  dep_steps[[i]] = step \n}  \n#names(dep_steps) &lt;- unique_dep_quants\n\nfig &lt;- fig %&gt;%\n  layout(sliders = list(list(active = 2,\n                             currentvalue = list(prefix = \"Deprivation: \"),\n                             steps = dep_steps)))\n\nfig\n\n\n\n\n\n\nAs you can see, there’s still some work to do regarding formatting. But it works!"
  },
  {
    "objectID": "posts/interactive-sliders/index.html#static",
    "href": "posts/interactive-sliders/index.html#static",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "Static",
    "text": "Static\nFor comparison, here’s the same data used to produce a static plot\n\n\nCode\n# Now to put it in the structure, and set active for `dep_quants = 0.15`\n\ndf_15pc &lt;- shared_df |&gt; \n  filter(between(dep_quants, 0.149, 0.151)) |&gt; \n  select(-dep_quants, -undep_quants)\n\ndf_15pc |&gt;\n  mutate(pct_deprived = 100 * prop_deprived) |&gt; \n  ggplot(aes(y= pct_deprived, x = fct_reorder(HBname, pct_deprived))) + \n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(\n      label = ifelse(df_15pc$prop_deprived &gt; 0, sprintf(\"%.1f\", pct_deprived), \"\")\n    ), \n    color = \"white\",\n    hjust = 1, \n    nudge_y = -0.5\n  ) + \n  coord_flip() + \n  labs(\n    x = \"Health Board\",\n    y = \"Percent of datazones in 15% most deprived proportion of Scotland\",\n    title = \"Percent of datazones in Health Board in 15% most deprived areas of Scotland\",\n    subtitle = \"SIMD 2020\"\n  ) + \n  geom_hline(yintercept = 0)"
  },
  {
    "objectID": "posts/tidy-tuesday-dr-who/index.html",
    "href": "posts/tidy-tuesday-dr-who/index.html",
    "title": "Tidy Tuesday on Dr Who",
    "section": "",
    "text": "First we load the packages\nThe tidyverse equivalent of pacman is now pak.\nThe latest dataset is here, and the specific files to work.\n\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndta_list &lt;- tidytuesdayR::tt_load(x = \"2023-11-28\")\n\n--- Compiling #TidyTuesday Information for 2023-11-28 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `drwho_episodes.csv`\n    Downloading file 2 of 3: `drwho_directors.csv`\n    Downloading file 3 of 3: `drwho_writers.csv`\n\n\n--- Download complete ---\n\ndta_eps &lt;- dta_list[[\"drwho_episodes\"]]\ndta_wrt &lt;- dta_list[[\"drwho_writers\"]]\n\nLet’s see how the viewship changed over time\n\ndta_eps_season &lt;- \n  dta_eps |&gt; \n  group_by(season_number) |&gt; \n  mutate(\n    mean_viewers = mean(uk_viewers),\n    mean_date = mean(first_aired)\n    ) |&gt; \n  ungroup()\n\ndta_eps_season |&gt; \n  ggplot(aes(x = first_aired, y = uk_viewers)) + \n  geom_point(colour = \"grey\") +\n  geom_point(aes(x = mean_date, y = mean_viewers), size = 2.5) + \n  scale_x_date(breaks = \"2 years\", labels = \\(x) format(x, \"%Y\")) +\n  labs(\n    x = \"First aired\",\n    y = \"UK Viewers (millions)\",\n    title = \"Viewers over time for Dr Who\",\n    subtitle = \"People don't watch TV like they used to...\"\n  ) +\n  annotate(\"text\", x = lubridate::make_date(2015), y = 10, label = \"What happened here?!\") +\n  annotate(\"text\", x = lubridate::make_date(2014), y= 8, label = \"Smartphone strangling the TV from now\", hjust = 0) + \n  stat_smooth(colour = \"blue\", se = FALSE) \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nLet’s now look at writers by season\n\ndta_eps_wrt &lt;- \n  dta_eps |&gt; \n    left_join(dta_wrt, by = \"story_number\") \n\nHow many episodes by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  summarise(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  arrange(desc(n_written))\n\n# A tibble: 40 × 2\n   writer           n_written\n   &lt;chr&gt;                &lt;int&gt;\n 1 Steven Moffat           45\n 2 Russell T Davies        31\n 3 Chris Chibnall          29\n 4 Mark Gatiss              9\n 5 Toby Whithouse           7\n 6 Gareth Roberts           5\n 7 Helen Raynor             4\n 8 Jamie Mathieson          4\n 9 Peter Harness            4\n10 Matthew Graham           3\n# ℹ 30 more rows\n\n\nSo Moffat wrote most episodes, then Davies, then Chibnall\nAnd what about popularity by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  mutate(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  filter(n_written &gt;= 5) |&gt; \n  ggplot(aes(x = fct_reorder(writer, rating), y= rating)) + \n  geom_boxplot() + \n  coord_flip() + \n  labs(\n    x = \"Distribution of ratings\",\n    y = \"Writer\", \n    title = \"Rating distribution by writer\",\n    subtitle = \"Writers who wrote at least five episodes\"\n  )\n\n\n\n\nWhen were the different writers active?\n\nmajor_writers_active &lt;- \n  dta_eps_wrt |&gt; \n    group_by(writer) |&gt;  \n    mutate(\n      n_written = n()\n    ) |&gt; \n    ungroup() |&gt; \n    filter(n_written &gt;= 5) |&gt; \n    group_by(writer) |&gt; \n    summarise(\n      started_writing = min(first_aired),\n      finished_writing = max(first_aired),\n      n_written = n_written[1]\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(\n      yr_start = year(started_writing),\n      yr_end = year(finished_writing)\n    )\n\nmajor_writers_active |&gt; \n  arrange(started_writing)\n\n# A tibble: 6 × 6\n  writer           started_writing finished_writing n_written yr_start yr_end\n  &lt;chr&gt;            &lt;date&gt;          &lt;date&gt;               &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Russell T Davies 2005-03-26      2010-01-01              31     2005   2010\n2 Mark Gatiss      2005-04-09      2017-06-10               9     2005   2017\n3 Steven Moffat    2005-05-21      2017-12-25              45     2005   2017\n4 Toby Whithouse   2006-04-29      2017-06-03               7     2006   2017\n5 Gareth Roberts   2007-04-07      2011-09-24               5     2007   2011\n6 Chris Chibnall   2007-05-19      2022-10-23              29     2007   2022\n\n\nHere we see the tenure of different major writers. Russell T Davies and Steven Moffatt are the major players."
  },
  {
    "objectID": "posts/tidy-tuesday-dr-who/index.html#tidy-tuesday-challenge",
    "href": "posts/tidy-tuesday-dr-who/index.html#tidy-tuesday-challenge",
    "title": "Tidy Tuesday on Dr Who",
    "section": "",
    "text": "First we load the packages\nThe tidyverse equivalent of pacman is now pak.\nThe latest dataset is here, and the specific files to work.\n\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndta_list &lt;- tidytuesdayR::tt_load(x = \"2023-11-28\")\n\n--- Compiling #TidyTuesday Information for 2023-11-28 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `drwho_episodes.csv`\n    Downloading file 2 of 3: `drwho_directors.csv`\n    Downloading file 3 of 3: `drwho_writers.csv`\n\n\n--- Download complete ---\n\ndta_eps &lt;- dta_list[[\"drwho_episodes\"]]\ndta_wrt &lt;- dta_list[[\"drwho_writers\"]]\n\nLet’s see how the viewship changed over time\n\ndta_eps_season &lt;- \n  dta_eps |&gt; \n  group_by(season_number) |&gt; \n  mutate(\n    mean_viewers = mean(uk_viewers),\n    mean_date = mean(first_aired)\n    ) |&gt; \n  ungroup()\n\ndta_eps_season |&gt; \n  ggplot(aes(x = first_aired, y = uk_viewers)) + \n  geom_point(colour = \"grey\") +\n  geom_point(aes(x = mean_date, y = mean_viewers), size = 2.5) + \n  scale_x_date(breaks = \"2 years\", labels = \\(x) format(x, \"%Y\")) +\n  labs(\n    x = \"First aired\",\n    y = \"UK Viewers (millions)\",\n    title = \"Viewers over time for Dr Who\",\n    subtitle = \"People don't watch TV like they used to...\"\n  ) +\n  annotate(\"text\", x = lubridate::make_date(2015), y = 10, label = \"What happened here?!\") +\n  annotate(\"text\", x = lubridate::make_date(2014), y= 8, label = \"Smartphone strangling the TV from now\", hjust = 0) + \n  stat_smooth(colour = \"blue\", se = FALSE) \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nLet’s now look at writers by season\n\ndta_eps_wrt &lt;- \n  dta_eps |&gt; \n    left_join(dta_wrt, by = \"story_number\") \n\nHow many episodes by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  summarise(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  arrange(desc(n_written))\n\n# A tibble: 40 × 2\n   writer           n_written\n   &lt;chr&gt;                &lt;int&gt;\n 1 Steven Moffat           45\n 2 Russell T Davies        31\n 3 Chris Chibnall          29\n 4 Mark Gatiss              9\n 5 Toby Whithouse           7\n 6 Gareth Roberts           5\n 7 Helen Raynor             4\n 8 Jamie Mathieson          4\n 9 Peter Harness            4\n10 Matthew Graham           3\n# ℹ 30 more rows\n\n\nSo Moffat wrote most episodes, then Davies, then Chibnall\nAnd what about popularity by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  mutate(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  filter(n_written &gt;= 5) |&gt; \n  ggplot(aes(x = fct_reorder(writer, rating), y= rating)) + \n  geom_boxplot() + \n  coord_flip() + \n  labs(\n    x = \"Distribution of ratings\",\n    y = \"Writer\", \n    title = \"Rating distribution by writer\",\n    subtitle = \"Writers who wrote at least five episodes\"\n  )\n\n\n\n\nWhen were the different writers active?\n\nmajor_writers_active &lt;- \n  dta_eps_wrt |&gt; \n    group_by(writer) |&gt;  \n    mutate(\n      n_written = n()\n    ) |&gt; \n    ungroup() |&gt; \n    filter(n_written &gt;= 5) |&gt; \n    group_by(writer) |&gt; \n    summarise(\n      started_writing = min(first_aired),\n      finished_writing = max(first_aired),\n      n_written = n_written[1]\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(\n      yr_start = year(started_writing),\n      yr_end = year(finished_writing)\n    )\n\nmajor_writers_active |&gt; \n  arrange(started_writing)\n\n# A tibble: 6 × 6\n  writer           started_writing finished_writing n_written yr_start yr_end\n  &lt;chr&gt;            &lt;date&gt;          &lt;date&gt;               &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Russell T Davies 2005-03-26      2010-01-01              31     2005   2010\n2 Mark Gatiss      2005-04-09      2017-06-10               9     2005   2017\n3 Steven Moffat    2005-05-21      2017-12-25              45     2005   2017\n4 Toby Whithouse   2006-04-29      2017-06-03               7     2006   2017\n5 Gareth Roberts   2007-04-07      2011-09-24               5     2007   2011\n6 Chris Chibnall   2007-05-19      2022-10-23              29     2007   2022\n\n\nHere we see the tenure of different major writers. Russell T Davies and Steven Moffatt are the major players."
  },
  {
    "objectID": "posts/tidy-tuesday-dr-who/index.html#coda",
    "href": "posts/tidy-tuesday-dr-who/index.html#coda",
    "title": "Tidy Tuesday on Dr Who",
    "section": "Coda",
    "text": "Coda\nNeither of us know much about Dr Who!\nBut hopefully we now know a bit more!"
  },
  {
    "objectID": "posts/talk-at-edinbr/index.html",
    "href": "posts/talk-at-edinbr/index.html",
    "title": "EdinbR talk on modelling economic (in)activity transitions",
    "section": "",
    "text": "Yesterday I had the great privilege of being one of two speakers at the Edinburgh R Users group, called EdinbR. (Difficult to say without sounding like a pirate.)\nI spoke through some of the modelling and conceptual challenges involved in trying to model the effect that various drivers/factors/exposures have on how many people in the UK become economically inactive, especially economically inactive for reasons of long-term sickness.\nThe talk seemed to go well (though perhaps the speaker’s always the last person qualified to judge), even though some of the algebra didn’t render correctly. (Which unfortunately means I also used algebra.)\nLike this blog, the presentation also made use of Quarto, but in the presentation’s case using reveal.js.\nThe presentation is available, for those intrepid souls interested in seeing something with R code and algebra, here."
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html",
    "href": "posts/robocop-is-wonderfully-childish/index.html",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "",
    "text": "Robocop (1987): Wonderfully Childish"
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html#preamble",
    "href": "posts/robocop-is-wonderfully-childish/index.html#preamble",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Preamble",
    "text": "Preamble\nHere’s something that’s differently nerdy. Some thoughts on the enduring and childish appeal of Robocop as a character and concept, lifted largely from some notes I made on Obsidian (which also uses Markdown)."
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html#notes-on-robocop",
    "href": "posts/robocop-is-wonderfully-childish/index.html#notes-on-robocop",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Notes on Robocop",
    "text": "Notes on Robocop\nWent through a Robocop phase. First film is good. Second film is adequate fan service (by Frank Miller), though unpleasant in terms of how it makes a boy a main antagonist. Reboot is terrible.\nI think this was inspired by seeing footage for the Robocop game Rogue City. Also remembered discussion about how in the original film Robocop’s death and resurrection is modelled on Jesus. It’s definitely true his death is a ‘passion’ (modern equivalent: torture porn?), and that his suffering at the hands of sadistic tormenters fits this pattern.\nIn the first film there’s also the sense of the protagonist reclaiming his humanity. Murphy’s memory is wiped, but through force of will he brings himself to remember who he was, and to identify as Murphy. That’s the last line. He’s thanked and addressed by the Old Man as a person, not as property That’s the arc: becoming human again.\nBy contrast the reboot started with Murphy knowing who he was, though the scientists can modify the extent to which his feelings or programming are in charge. There’s a plot about police corruption and selling weapons illegally, and plenty of exposition from scientists where they try to shoehorn in cod-philosophy on personhood and free will, but this felt like ‘filler’ between a series of action sequences, which were much faster but also much less weighty and visceral than those in the original.\nIn the original Robocop is a bullet sponge. He was slow, apparently unfeeling. Though this might be partly a function of Peter Weller not having a great deal of mobility when wearing the suit, it also feeds into the central character arc: robots don’t feel; people do.1 Robots aren’t vulnerable as people are. As Robocop becomes damaged, he becomes more vulnerable, and his face becomes visible. Vulnerability is necessary for humanity to be restored. Robocop’s suit makes him a superhero, an adolescent boy fantasy of massive strength and power, but it also makes him a prisoner, trapped and entombed.\n\n\n\nRegaining Humanity: But at what cost?\n\n\nAgain, by contrast, in the remake Murphy has lost less. He still has a human hand, still has his memories and sense of self, and still has all the speed and mobility he had as a person, only more so. He doesn’t have an arc, he has perturbations and wobbles.\nLet’s think some more about why Robocop is an adolescent, or even pre-adolescent, fantasy. Firstly, it appeals to a kind of crude creativity of combinatorials: take two things that are familiar, combine them, and make something unfamiliar. With only a limited number of schemas, even a young child can wonder what happens when they are combined, and feel excited and proud about bootstrapping from everyday experience to pure fantasy. Like the distinction between animal, vegetable and mineral, ‘man’ and ‘machine’ are different primary colours, and seeing the concept of Robocop may be for a young child like seeing red and blue make purple for the first time.\n\n\n\nTeenage Mutant Ninja Turtles: Another contemporaneous example of the ‘primary colour chimera’ attractive to children\n\n\nSecond, there’s the power fantasy. Within this, there’s the sense of seeing someone who can play an action game and be allowed to ‘cheat’. The roles of an action game, involving shooting, must include that, once shot, the player must ‘play dead’. But in Robocop is a fantasy of a character who’s still allowed to shoot others whilst being able to ignore when others shoot him. Robocop presents a fantasy for a young child of playing a game where you can do things that no-one else can, because you’re more special than everyone else.\nI suspect that’s why, even though Robocop was clearly unsuitable for children, the concept of Robocop appeared to have so much appeal to children. There’s something inherently childish about seeing such a pure chimera rendered on screen, and the possibilities and affordances of this chimera are signalled so brightly from the images alone that the (adult) source material does not need to be consulted.\nAgain, this seems to be yet another reason why the remake did not work: 2014’s Murphy was not machine enough. The suit did not make him clearly ‘other’ enough. Even though the graphic violence was toned down so that in theory children could watch, the lack of schematic purity in the ideal types being mixed meant the pleasure of seeing primary colours being mixed, in the way the original so effectively managed, was muddied and diluted."
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html#footnotes",
    "href": "posts/robocop-is-wonderfully-childish/index.html#footnotes",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlistair Gray’s Lanark played with a similar idea, in the form of Dragonhide, a magical realist representation of dermatitis, emotional distance, and lack of outlet for artistic expression. Too much Dragonhide, the book warns, causes sufferers to become unable to move, and unable to vent, their heat and energy, without expression, eventually causing them to boil to death!↩︎"
  },
  {
    "objectID": "posts/unattended-deaths/index.html",
    "href": "posts/unattended-deaths/index.html",
    "title": "A Deathly Silence",
    "section": "",
    "text": "Trends in R98/R99 deaths since 1990\n\n\nWhat does it mean when someone dies, and no one notices for days, weeks, or months on end?\nThe bodies, once found, will be decomposed to such an extent that no effective autopsy can be performed, and so no cause of death can be identified. Such deaths are then likely to be coded either as R98 (‘Unattended death’) or R99 (‘Other ill-defined and unknown causes of mortality’). Far from being ‘junk codes’, wouldn’t a sudden and sustained change in deaths coded this way (absent an obvious explanation, such as a change in coding practice) signal that something broader is afoot?\nWorking with Lu Hiam, an Oxford PhD student and former GP, and Theodore Estrin-Serlui, a histopathologist, I analysed trends in deaths with these codes, as compared with mortality trends overall in England & Wales.\nSuch codes are rarely used, but in England & Wales they sadly became many times more common over the 1990s and 2000s. Standardised mortality rates in the R98/R99 category became more than three and a half times a common between 1990 and 2010, even as general standardised mortality rates fell by around a third.\nFor every body found so decomposed that the R98/R99 category had to be used, there are usually many more that have been unattended for a few days, have started to decompose, but for which autopsy can still be successfully performed. If these deaths are the tip of the iceberg, the base of this iceberg may be a growing epidemic of loneliness and social isolation, of ever more people with connections to friends and family, with no one to turn to in times of crisis.\nOur paper, A Deathly Silence, has been published in the Journal of the Royal Society of Medicine, and received press coverage from a number of outlets."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "First Post",
    "section": "",
    "text": "Hi, this is my first blog post. I’m making this using Quarto, starting off by slavishly following the tutorial, then incrementally adapting it to suit my preferences.\n\nI’m even keeping the default image of the first blog post. It’s not dissimilar to what I’m actually looking at!"
  },
  {
    "objectID": "posts/r-code/index.html",
    "href": "posts/r-code/index.html",
    "title": "Post with code",
    "section": "",
    "text": "This short post is intended to confirm that I can run and render R code within a Quarto blog post.\n\nVery simple example\nLet’s start off with some very simple base-R\n\n1 + 1\n\n[1] 2\n\n\nAnd of course let’s not forget the obligatory\n\nstatement &lt;- \"Hello World\"\n\nstatement\n\n[1] \"Hello World\"\n\n\n\n\nGraphs\nLet’s now look at a base-R graphic, again using a cliched example\n\nplot(mtcars$mpg ~ mtcars$wt)\n\n\n\n\n\n\nSome extensions\nLet’s now continue to be cliched, and load and use the tidyverse\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\nmtcars |&gt; \n  group_by(carb) |&gt; \n  summarise(\n    mean_mpg = mean(mpg)\n  ) |&gt; \n  ungroup()\n\n# A tibble: 6 × 2\n   carb mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     1     25.3\n2     2     22.4\n3     3     16.3\n4     4     15.8\n5     6     19.7\n6     8     15  \n\n\nAnd to visualise\n\nmtcars |&gt; \n  mutate(cyl = factor(cyl)) |&gt; \n  ggplot(aes(x = wt, y = mpg, colour = cyl, group= cyl)) + \n  geom_point(aes(shape = cyl)) + \n  stat_smooth(se = FALSE, method = \"lm\") + \n  labs(\n    x = \"Weight\", \n    y = \"Miles per gallon\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nConclusion\nSo far, so good…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jon Minton’s Blog",
    "section": "",
    "text": "Interactive Sliders with Crosstalk and Plotly\n\n\n\n\n\n\n\nR\n\n\nplotly\n\n\ncrosstalk\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nRobocop (1987) is wonderfully childish\n\n\n\n\n\n\n\nstories\n\n\nfilms\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday on Dr Who\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nDr Who\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJon Minton, Nick Christofides\n\n\n\n\n\n\n  \n\n\n\n\nPost with code\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nLinear Models are General Linear Models\n\n\nPart One: Model fitting as parameter calibration\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nEdinbR talk on modelling economic (in)activity transitions\n\n\n\n\n\n\n\nR\n\n\nmodelling\n\n\ntalks\n\n\neconomics\n\n\nhealth\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nA Deathly Silence\n\n\n\n\n\n\n\nMortality\n\n\nEpidemiology\n\n\nPapers\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nFirst Post\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\nNo matching items"
  }
]