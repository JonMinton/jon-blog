[
  {
    "objectID": "glms.html",
    "href": "glms.html",
    "title": "Generalised Linear Models",
    "section": "",
    "text": "This page lists a series of posts I’ve written which attempt to demystify generalised linear models (GLMs), by first reintroducing linear models as a specific type of GLM."
  },
  {
    "objectID": "glms.html#introduction",
    "href": "glms.html#introduction",
    "title": "Generalised Linear Models",
    "section": "",
    "text": "This page lists a series of posts I’ve written which attempt to demystify generalised linear models (GLMs), by first reintroducing linear models as a specific type of GLM."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jon Minton’s Blog",
    "section": "",
    "text": "On Sweary Soap Operas: A Concealed Television Genre\n\n\n\n\n\n\n\ngenres\n\n\ntelevision\n\n\nstories\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Ten: Log Likelihood estimation for Logistic Regression\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidytuesday 2024-01-23\n\n\nEducational attainment and town size\n\n\n\n\nR\n\n\nEducation\n\n\nTidy Tuesday\n\n\n\n\n\n\n\n\n\n\n\nJan 25, 2024\n\n\nBrendan Clarke, Andrew Saul, Nick Christofides, Kennedy Owuso-Afrije, Jon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Other Left-Right Divide: Iain McGilchrist and the Battle of the Hemispheres\n\n\n\n\n\n\n\npodcasts\n\n\nstories\n\n\nsci-fi\n\n\nmyth\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nI got permanently banned from a politics forum for mentioning how circles work\n\n\n\n\n\n\n\npolitics\n\n\ngeometry\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Nine: Answering questions with honest uncertainty: Expected values and Predicted values\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Eight: Guessing what a landscape looks like by feeling the curves beneath our feet\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nDavid Sederis: Humourists as Unrepentent Observational Confessionals\n\n\n\n\n\n\n\nbooks\n\n\nmindfulness\n\n\ncomedy\n\n\nhumour\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Seven: Feeling Uncertain\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 4, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Six: The Robo-Chauffeur\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 3, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Five: Traversing the Likelihood Landscape\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2024\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nGLMs: My first series\n\n\n\n\n\n\n\n\n\n\n\n\nDec 31, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Four: why only betas just look at betas\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday: Christmas films\n\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2023\n\n\nTom Fowler, Nick Christofides, Andrew Saul, Jon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Eerie Familiarity of Frasier (2023)\n\n\n…And why it’s inverted Dr Who\n\n\n\n\nstories\n\n\nsitcoms\n\n\narchetypes\n\n\nmathematics\n\n\n\n\n\n\n\n\n\n\n\nDec 17, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nChanging tenure in Scotland\n\n\n\n\n\n\n\nhousing\n\n\n\n\n\n\n\n\n\n\n\nDec 15, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nNerdy Dialogues on Life and Death\n\n\nPart 1: Introduction; Life Expectancy\n\n\n\n\ndemography\n\n\npopulation health\n\n\nmethodology\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday on Life Expectancy - Part Two\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nLife Expectancy\n\n\n\n\n\n\n\n\n\n\n\nDec 13, 2023\n\n\nJon Minton, Andrew Saul, Nick Christofides, James McMahon, Kennedy Owuso-Afrije, Sandra Nwobi\n\n\n\n\n\n\n  \n\n\n\n\nMy Economic Inactivity Modelling Package: Informative Readme File!\n\n\n\n\n\n\n\nR\n\n\nEconomic Inactivity\n\n\nNews\n\n\nPackages\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nOptimised for Twitter?\n\n\n\n\n\n\n\ntwitter\n\n\nX\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nWhy such pushback against 20 minute neighbourhoods?\n\n\n\n\n\n\n\nresearch\n\n\nwalkability\n\n\ndriving\n\n\nconspiracy theories\n\n\nhuman development\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday on Life Expectancy\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nLife Expectancy\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nNick Christofides, Jon Minton, Sandra Nwobi\n\n\n\n\n\n\n  \n\n\n\n\nBeavis and Butt-Head: When a physics graduate dons Dumbface\n\n\n\n\n\n\n\ncartoons\n\n\nstories\n\n\neugenics\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nThe Effective Savings on Interest-free Credit\n\n\n\n\n\n\n\nstatistics\n\n\nfinances\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nHow to wrap presents\n\n\n\n\n\n\n\nChristmas\n\n\nBirthdays\n\n\nArts & Crafts\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nNew blog feature: comments\n\n\n\n\n\n\n\nquarto\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Three: glm is just fancy lm\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPart Two: Systematic components and link functions\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nScientific Illustrations: Annotating the unit circle\n\n\n\n\n\n\n\nstatistics\n\n\nR\n\n\ngraphics\n\n\neconomic inactivity\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nInteractive Sliders with Crosstalk and Plotly\n\n\n\n\n\n\n\nR\n\n\nplotly\n\n\ncrosstalk\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nRobocop (1987) is wonderfully childish\n\n\n\n\n\n\n\nstories\n\n\nfilms\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nTidy Tuesday on Dr Who\n\n\n\n\n\n\n\nR\n\n\ntidy tuesday\n\n\nDr Who\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nJon Minton, Nick Christofides\n\n\n\n\n\n\n  \n\n\n\n\nPart One: Model fitting as parameter calibration\n\n\n\n\n\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nPost with code\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nEdinbR talk on modelling economic (in)activity transitions\n\n\n\n\n\n\n\nR\n\n\nmodelling\n\n\ntalks\n\n\neconomics\n\n\nhealth\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nA Deathly Silence\n\n\n\n\n\n\n\nMortality\n\n\nEpidemiology\n\n\nPapers\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\n  \n\n\n\n\nFirst Post\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nNov 25, 2023\n\n\nJon Minton\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/tidy-tuesday-christmas-films/index.html",
    "href": "posts/tidy-tuesday-christmas-films/index.html",
    "title": "Tidy Tuesday: Christmas films",
    "section": "",
    "text": "A shorter and even tardier Tidy Tuesday this week, given we gave ourselves only half an hour rather than the usual hour to look over the most recent dataset.\nThe dataset was about Christmas films.\nOur first question: is Die Hard a Christmas film?\nNot according to the methods used to produce the dataset. If a film doesn’t have Christmas or equivalent in its title, it’s not coming in!\n\nloading\n\n\ntt &lt;- tidytuesdayR::tt_load('2023-12-12')\n\n--- Compiling #TidyTuesday Information for 2023-12-12 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `holiday_movies.csv`\n    Downloading file 2 of 2: `holiday_movie_genres.csv`\n\n\n--- Download complete ---\n\ndf1 &lt;- tt[[1]]\ndf2 &lt;- tt[[2]]\n\n\n\ncount of films by year\n\n\ndf1 %&gt;%\n  count(year, sort = TRUE)\n\n# A tibble: 91 × 2\n    year     n\n   &lt;dbl&gt; &lt;int&gt;\n 1  2021   183\n 2  2022   173\n 3  2020   172\n 4  2019   143\n 5  2018   129\n 6  2023   107\n 7  2017   102\n 8  2015    76\n 9  2016    75\n10  2012    68\n# ℹ 81 more rows\n\n\n\nhow many films by year -plot with log on y axis\n\n\ndf1 %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n))+\n  geom_point()+\n  #stat_smooth()+\n  scale_y_log10()\n\n\n\n\n\nhow many films by year -plot with log on y axis\nfilter by 1960 onwards\n\n\ndf1 %&gt;%\n  filter(year &gt;= 1960) %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n))+\n  geom_point()+\n  #stat_smooth()+\n  scale_y_log10()\n\n\n\n\n\nhow many films by year -plot with log on y axis\nfilter by 1960 onwards\n\n\ndf1 %&gt;%\n  filter(year &gt;= 1960) %&gt;%\n  count(year) %&gt;%\n  ggplot(aes(x = year, y = n))+\n  geom_point()+\n  stat_smooth(method = \"lm\")+\n  scale_y_log10()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nquestions\n\nhow are they published? [cinema / streaming?]\nis it on imdb?\nfull inclusion of 2023?\nare more recent films rubbish?\n\n\ndf1 %&gt;%\n  \n  group_by(year) %&gt;%\n  summarise(avg_rating = mean(average_rating)) %&gt;%\n  ggplot(aes(x = year, y = avg_rating))+\n  geom_point()\n\n\n\n\n\nnumber of films vs avg rating\nfewer films may drive extreme values\n\n\ndf1 %&gt;%\n  \n  group_by(year) %&gt;%\n  summarise(\n    avg_rating = mean(average_rating), \n    n_films = n() ) %&gt;%\n  ggplot(aes(x = n_films, y = avg_rating))+\n  geom_point()\n\n\n\n\n\nnumber of films vs avg rating\n\n\ndf1 %&gt;%\n  \n  group_by(year) %&gt;%\n  summarise(\n    avg_rating = mean(average_rating), \n    n_films = n() ) %&gt;%\n  ggplot(aes(x = n_films, y = avg_rating))+\n  geom_point()"
  },
  {
    "objectID": "posts/the-other-left-right-divide/index.html",
    "href": "posts/the-other-left-right-divide/index.html",
    "title": "The Other Left-Right Divide: Iain McGilchrist and the Battle of the Hemispheres",
    "section": "",
    "text": "Iain McGilchrist on the mythos and the machine\n\n\nIn Watching The English, Fox (2005) writes that:\n\nAt the most basic level, an underlying rule in all English conversation is the proscription of ‘earnestness’. … [The] English are probably more acutely sensitive than any other nation to the distinction between ‘serious’ and ‘solemn’, between ‘sincerity’ and ‘earnestness’.\n… [The] Importance of Not Being Earnest rule is really quite simple. Seriousness is acceptable, solemnity is prohibited. Sincerity is allowed, earnestness is strictly forbidden. Pomposity and self-importance are outlawed. Serious matters can be spoken of seriously, but one must never take oneself too seriously. [p. 62]\n\nA serial violator of the Importance of Not Being Earnest Rule is Damien Walter, producer and host of the Science Fiction podcast, which states its mission as being to explore “the best in SF storytelling [and to ask] what happens when logos meets mythos, reason meets imagination and science … meets fiction”. English former Guardian journalist Damien Walter (alternately Damien G Walter) is very earnest. Which might explain why he doesn’t live in England anymore.\nIn the latest podcast, the very earnest Englishman Damien G Walter interviews the very earnest Scotsman Iain McGilchrist, talking broadly around McGilchrist’s somewhat mythic framing of the left-right divide. McGilchrist’s left-right divide isn’t a divide between the political Left and Right, but a divide between the two hemispheres of the brain.\nMcGilchrist professes that his left-right hemispheric divide isn’t mere pop science, attributing certain temperaments or qualities, like reason and creativity, to one or the other hemisphere. Instead his argument seems marginally more subtle that that, something like:\n\nThe left hemisphere’s domain is the centre. It’s the part of the brain that takes charge when you choose to focus on an object, grasp it, manipulate it, name it, take it apart and put it back together, use or abuse it as a tool.\nThe right hemisphere’s domain is the periphery. It’s what notices and contextualizes all around you, and so provides the context through which one can relate to and negotiate with the totality of the world.\n\nMcGilchrist’s broader thesis appears to be that broadly left-hemispheric thinking has become somewhat over-dominant in modern culture, leading to an overly atomistic and instrumentalist way of thinking. Everything is thought about, to some extent, in terms of how it can be used, grasped, broken down and thought about as machines and systems. Walter makes the intriguing observation that this may help explain a tendency towards literal-mindedness in much commentary and critique of modern sci-fi and fantasy, which appears blind or indifferent to underlying mythos and symbolism that stories are drawing from. I think there’s much compelling about this literal-mindedness observation, even if I’m somewhat more ambivalent about the left-right hemispheric distinction drawn by McGilchrist more generally, especially in terms of the trends or tendencies he’s proposing.\nSince starting this blog in late November, I’ve discovered most of my posts tend to focus either on statistics or stories. These dual preoccupations don’t completely map onto the left-right hemispheric distinction - for example there’s a lot of contextualisation (right-thinking) involved in finding meaning in statistical outputs; and there is value in thinking about stories in a somewhat mechanical, graspable-component-like way - but it’s not a bad first approximation. I find stories valuable to think about, especially where they bring an intense quality of emotional engagement and I want to know why. Sometimes I even risk treating the exploration and interpretation of stories with the earnestness they deserve (even when writing about Robocop).\n\n\n\n\nReferences\n\nFox, K. 2005. Watching the English: The Hidden Rules of English Behaviour. Hodder & Stoughton. https://books.google.co.uk/books?id=tNZfLeHSFvQC."
  },
  {
    "objectID": "posts/scientific-illustration-unit-circle/index.html",
    "href": "posts/scientific-illustration-unit-circle/index.html",
    "title": "Scientific Illustrations: Annotating the unit circle",
    "section": "",
    "text": "Here’s an example of a scientific illustration I’ve just produced to illustrate some scenarios I’m modelling for my work on economic inactivity determinants.\nI have two continuous variables (derived from the GHQ-12), mental health and physical health. Both are standardised so they have the same scale.\nBut I’m interested in the effects of improving/changing ‘health’ in general, which is obviously composed of both mental health and physical health, but not measured directly.\nAs the two variables are standardised, however, I can model an improvement in health in general as a change in both mental health and physical health concurrently.\nHowever, I want to compare like-with-like: scenarios in which the total ‘amount’ of intervention effect is kept constant, but the relative contribution of the two health components is varied.\nThis is where a little trigonometry comes in. 1 All interventions on the grey unit circle in Figure 1 represent possible scenarios in which the total amount of health change is constant, but where the relative contribution of mental and physical health is varied.\nThe aim of the scientific illustration is to make this intuition a bit clearer to understand!\nCode\nlibrary(tidyverse)\nlibrary(geomtextpath)\n\npos_y &lt;- function(x) {sqrt(1 - x^2)}\nx = seq(0, 1, by = 0.001)\ndta &lt;- tibble(\n  x = x\n) |&gt; \n  mutate(\n    y = pos_y(x)\n  )\n\ndta |&gt; \n  ggplot(aes(x = x, y = y)) + \n    geom_line(color = \"grey\") + \n    coord_equal() + \n    labs(x = \"Physical Health (Standardised)\",\n         y = \"Mental Health (Standardised)\",\n         title = \"Health improvement scenarios modelled\") + \n  theme_minimal() + \n  annotate(\"point\", x = 1, y = 0) + \n  annotate(\"point\", x = 0, y = 1) + \n  annotate(\"point\", x = 1/ sqrt(2), y = 1/ sqrt(2)) + \n  annotate(\"point\", x =  2 / sqrt(5), y = 1 / sqrt(5)) + \n  annotate(\"point\", x = 1 / sqrt(5), y = 2 / sqrt(5)) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, xend = 0, yend = 1), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S1: MH Only\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  ) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, xend = 1, yend = 0), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S2: PH Only\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  ) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, xend = 1/sqrt(2), yend = 1/sqrt(2)), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S3: Equal Gain\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  ) + \n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, yend = 2/sqrt(5), xend = 1/sqrt(5)), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S4: MH Bias\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  )  +\n  geom_textcurve(\n    data = data.frame(x = 0, y = 0, yend = 1/sqrt(5), xend = 2/sqrt(5)), \n                      mapping = aes(x, y, xend = xend,  yend = yend), \n                      label = \"S5: PH Bias\", \n    curvature = 0, hjust = 0.5, arrow = arrow(),\n    vjust = 0.5\n  )  \n\n\n\n\n\nFigure 1: Modelling various intervention scenarios"
  },
  {
    "objectID": "posts/scientific-illustration-unit-circle/index.html#footnotes",
    "href": "posts/scientific-illustration-unit-circle/index.html#footnotes",
    "title": "Scientific Illustrations: Annotating the unit circle",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPeople who claim you’ll never need trigonometry once you leave school are wrong. It’s valuable about once a decade!↩︎"
  },
  {
    "objectID": "posts/r-code/index.html",
    "href": "posts/r-code/index.html",
    "title": "Post with code",
    "section": "",
    "text": "This short post is intended to confirm that I can run and render R code within a Quarto blog post.\n\nVery simple example\nLet’s start off with some very simple base-R\n\n1 + 1\n\n[1] 2\n\n\nAnd of course let’s not forget the obligatory\n\nstatement &lt;- \"Hello World\"\n\nstatement\n\n[1] \"Hello World\"\n\n\n\n\nGraphs\nLet’s now look at a base-R graphic, again using a cliched example\n\nplot(mtcars$mpg ~ mtcars$wt)\n\n\n\n\n\n\nSome extensions\nLet’s now continue to be cliched, and load and use the tidyverse\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\nmtcars |&gt; \n  group_by(carb) |&gt; \n  summarise(\n    mean_mpg = mean(mpg)\n  ) |&gt; \n  ungroup()\n\n# A tibble: 6 × 2\n   carb mean_mpg\n  &lt;dbl&gt;    &lt;dbl&gt;\n1     1     25.3\n2     2     22.4\n3     3     16.3\n4     4     15.8\n5     6     19.7\n6     8     15  \n\n\nAnd to visualise\n\nmtcars |&gt; \n  mutate(cyl = factor(cyl)) |&gt; \n  ggplot(aes(x = wt, y = mpg, colour = cyl, group= cyl)) + \n  geom_point(aes(shape = cyl)) + \n  stat_smooth(se = FALSE, method = \"lm\") + \n  labs(\n    x = \"Weight\", \n    y = \"Miles per gallon\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nConclusion\nSo far, so good…"
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html",
    "href": "posts/effective-saving-for-interest-free-credit/index.html",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "",
    "text": "I have a Monzo account, and as part of the overall Monzo package make use of Monzo Flex, an interest-free credit service which means the payment made in one month is spread over broadly equal payments over the following three months.\nHowever, I’ve always only bought something if I could afford to pay for it in full.\nThe reason for using Monzo Flex follows from an intuition: Deferring some of the payment for a good obtained in month \\(t=0\\) to months \\(\\{t=1, t=2, t=3\\}\\) should in effect offer some degree of saving on the cost of the good, as a pound in 1-3 months has a slightly lower value than a pound this month. This is because of inflation, and the higher the rate of inflation, the higher the effective interest-free credit discount should become.\nHowever, I’ve never tried to work out what this effective savings rate is expected to be. Let’s try to work that out.\nTo do this, we need to consider the following:\n\nThe relationship between annual inflation rates and monthly inflation rates.\nThe concept of net present value (NPV)."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#introduction",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#introduction",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "",
    "text": "I have a Monzo account, and as part of the overall Monzo package make use of Monzo Flex, an interest-free credit service which means the payment made in one month is spread over broadly equal payments over the following three months.\nHowever, I’ve always only bought something if I could afford to pay for it in full.\nThe reason for using Monzo Flex follows from an intuition: Deferring some of the payment for a good obtained in month \\(t=0\\) to months \\(\\{t=1, t=2, t=3\\}\\) should in effect offer some degree of saving on the cost of the good, as a pound in 1-3 months has a slightly lower value than a pound this month. This is because of inflation, and the higher the rate of inflation, the higher the effective interest-free credit discount should become.\nHowever, I’ve never tried to work out what this effective savings rate is expected to be. Let’s try to work that out.\nTo do this, we need to consider the following:\n\nThe relationship between annual inflation rates and monthly inflation rates.\nThe concept of net present value (NPV)."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#monthly-and-annual-inflation-rates",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#monthly-and-annual-inflation-rates",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "Monthly and annual inflation rates",
    "text": "Monthly and annual inflation rates\nIf prices go up 10% in 12 months, and go up the same % each month, how much do they go up each month?\nAn intuitive but wrong answer is that, as there are 12 months per year, the monthly inflation rate will be one twelfth of the annual inflation rate, which would imply a monthly inflation rate of \\(0.1/12\\) or around 0.83%. So,\n\\[\n(1 + r_m) = \\frac{1}{12}(1 + r_y)\n\\] Or equivalently\n\\[\n(1 + r_y) = 12 (1 + r_m)\n\\] Where \\(r_y\\) is the annual increase and \\(r_m\\) is the monthly increase.\nHowever this assumption, as mentioned, is wrong, because it ignores the way that each month’s increase is applied to the product of all increases that occurred in previous months. For example, for three months with different inflation rates the total increase over the the three months will be\n\\[\n(1 + r_{1,2,3}) = (1 + r_1)(1+r_2)(1+r_3)\n\\] If the monthly inflation rates for each of the three months are the same, \\(r_m\\), then this simplifies slightly to\n\\[\n(1 + r_{1,2,3}) = (1 + r_m)^3\n\\]\nBy extension, as there are twelve months in a year, where the monthly inflation rate is fixed the equation becomes:\n\\[\n(1 + r_y) = (1 + r_m)^{12}\n\\]\nThis, not \\((1 + r_y) = 12 (1 + r_m)\\), is the correct starting point. Solve for \\(r_m\\) …\n\\[\n(1 + r_y)^{\\frac{1}{12}} = 1 + r_m\n\\]\n\\[\nr_m = {(1 + r_y)}^{\\frac{1}{12}} - 1\n\\]\nPlugging in a 10% annual inflation rate, i.e. 0.1 for \\(r_y\\), we therefore get an \\(r_m\\) value of around 0.007974, so around 0.8%."
  },
  {
    "objectID": "posts/effective-saving-for-interest-free-credit/index.html#net-present-value",
    "href": "posts/effective-saving-for-interest-free-credit/index.html#net-present-value",
    "title": "The Effective Savings on Interest-free Credit",
    "section": "Net Present Value",
    "text": "Net Present Value\nThe idea of Net Present Value (NPV) is to translate costs and benefits that occur at different points in time onto a single timeframe, the present. This makes it easier to compare options that take place over different timeframes.\nIn the Flex example we are comparing two options:\n\n\nPay all now\n\n\nPay interest free over three consecutive monthly installments\n\n\nLet’s say the cost of the good at month \\(t\\) is £150. Graphically, and with no interest and inflation, the two options look as follow:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- tribble(\n  ~option, ~month, ~amount,\n  \"A\", 0, 150,\n  \"A\", 1, 0,\n  \"A\", 2, 0,\n  \"A\", 3, 0,\n  \"B\", 0, 0,\n  \"B\", 1, 50,\n  \"B\", 2, 50,\n  \"B\", 3, 50\n)\n\ndf |&gt; \n  ggplot(aes(month, amount)) + \n  geom_col() + \n  facet_wrap(~ option, nrow = 2)\n\n\n\n\n\nIn the no interest / no inflation scenario, the sums for option A and option B are equal, £150.\nHowever, in scenarios with inflation, the value of money keeps decreasing. This means that a commitment to pay £50 month 3 is a commitment to pay less than in month 0. Using the 10% annual inflation rate example, we can estimate the cumulative devaluation by months 1, 2 and 3 by dividing the product of devaluations so far by the monthly inflation rate:\n\n\nCode\nannual_to_monthly &lt;- function(x) {(1 + x)^(1/12) -1}\n\nannual_inflation &lt;- 0.10\nmonthly_inflation &lt;- annual_to_monthly(annual_inflation)\n\nindex0 &lt;- 1\nindex1 &lt;- index0 / (1 + monthly_inflation)\nindex2 &lt;- index1 / (1 + monthly_inflation)\nindex3 &lt;- index2 / (1 + monthly_inflation)\n\ndf &lt;- tibble(\n  month = 0:3, \n  index = c(index0, index1, index2, index3)\n)\n\ndf\n\n\n# A tibble: 4 × 2\n  month index\n  &lt;int&gt; &lt;dbl&gt;\n1     0 1    \n2     1 0.992\n3     2 0.984\n4     3 0.976\n\n\nContinuing the example of a £150 item paid over months 1, 2 and 3, we can therefore convert to NPV by discounting each month’s costs by the index relative to month 0\n\n\nCode\ndf2 &lt;- df |&gt; \n  mutate(\n    amount = c(0, 50, 50, 50)\n  ) |&gt; \n  mutate(npv_amount = amount * index)\n\ndf2\n\n\n# A tibble: 4 × 4\n  month index amount npv_amount\n  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1     0 1          0        0  \n2     1 0.992     50       49.6\n3     2 0.984     50       49.2\n4     3 0.976     50       48.8\n\n\nThe sum of npv_amount is now less than the £150 in option A, pay upfront. In this example, with 10% inflation, this sum is £147.64, which represents a 1.6% discount on option A.\nLet’s now generalise to other inflation rates\n\n\nCode\ncalc_npv_discount &lt;- function(ry, total = 150) { \n  annual_to_monthly &lt;- function(x) {(1 + x)^(1/12) -1}\n  \n  rm &lt;- annual_to_monthly(ry)\n  index0 &lt;- 1\n  index1 &lt;- index0 / (1 + rm)\n  index2 &lt;- index1 / (1 + rm)\n  index3 &lt;- index2 / (1 + rm)\n\n  npv_amt1 &lt;- (total / 3) * index1\n  npv_amt2 &lt;- (total / 3) * index2\n  npv_amt3 &lt;- (total / 3) * index3\n  \n  \n  1 - sum(npv_amt1, npv_amt2, npv_amt3) / total\n}\n\ndf &lt;- \n  tibble(\n    annual_rate = seq(0, 0.15, by = 0.01)\n  ) |&gt; \n  mutate(\n    effective_discount = map_dbl(annual_rate, calc_npv_discount)\n  )\n\ngg &lt;- \n  df |&gt; \n    ggplot(aes(100 * annual_rate, 100 * effective_discount)) + \n    geom_line() + \n    labs(x = \"Annual inflation rate (%)\", \n         y = \"Effective discount on paying over 3 months (%)\",\n         title = \"Effective short-term discount rate against inflation rate\"\n         ) + \n    scale_y_continuous(breaks = seq(0, 15, by = 0.1)) +\n    annotate(\"segment\", x = 14.8, xend = 14.8, colour = \"lightblue\", y = 0, yend = 100 * calc_npv_discount(0.148)) +\n    annotate(\"segment\", x = 0, xend = 100 * 0.148, colour = \"lightblue\", y = 100 * calc_npv_discount(0.148), yend = 100 * calc_npv_discount(0.148)) +\n    annotate(\"segment\", x = 9.6, xend = 9.6, colour = \"darkblue\", y = 0, yend = 100 * calc_npv_discount(0.096)) +\n    annotate(\"segment\", x = 0, xend = 100 * 0.096, colour = \"darkblue\", y = 100 * calc_npv_discount(0.096), yend = 100 * calc_npv_discount(0.096)) +\n    annotate(\"segment\", x = 5.3, xend = 5.3, colour = \"darkgrey\", y = 0, yend = 100 * calc_npv_discount(0.053)) +\n    annotate(\"segment\", x = 0, xend = 100 * 0.053, colour = \"darkgrey\", y = 100 * calc_npv_discount(0.053), yend = 100 * calc_npv_discount(0.053)) \n    \ngg +\n    annotate(\"text\", \n             x = 2, y = 0.1 + 100 * calc_npv_discount(0.148),\n             label = \"Goods (Highest)\"\n    ) + \n    annotate(\"text\", \n             x = 2, y = 0.1 + 100 * calc_npv_discount(0.096),\n             label = \"CPIH (Highest)\"\n    ) + \n    annotate(\"text\", \n             x = 2, y = 0.1 + 100 * calc_npv_discount(0.053),\n             label = \"Services (Highest)\"\n    )  \n\n\n\n\n\nIn the above I’ve indicated the effective discount rates implied by different annual interest rates reported by the ONS in Figure 7 of this page These range from almost 2.3% for goods, to around 0.86% for services.\nHowever, fortunately, the current inflation rates are somewhat lower, with the most recent reported inflation rates being 2.9% for goods, 6.2% for services, and 2.7% for CPIH.\n\n\nCode\ngg + \n  annotate(\"segment\", x = 2.9, xend = 2.9, colour = \"lightblue\", linetype = \"dashed\", y = 0, yend = 100 * calc_npv_discount(0.029)) +\n  annotate(\"segment\", x = 0, xend = 100 * 0.029, colour = \"lightblue\", linetype = \"dashed\", y = 100 * calc_npv_discount(0.029), yend = 100 * calc_npv_discount(0.029)) +\n  annotate(\"text\", \n           x = 2, y = 0.1 + 100 * calc_npv_discount(0.029),\n           label = \"Goods (Current)\"\n  ) + \n  annotate(\"segment\", x = 4.7, xend = 4.7, colour = \"darkblue\", linetype = \"dashed\", y = 0, yend = 100 * calc_npv_discount(0.047)) +\n  annotate(\"segment\", x = 0, xend = 100 * 0.047, colour = \"darkblue\", linetype = \"dashed\", y = 100 * calc_npv_discount(0.047), yend = 100 * calc_npv_discount(0.047)) +\n  annotate(\"text\", \n           x = 2, y = 0.1 + 100 * calc_npv_discount(0.047),\n           label = \"CPIH (Current)\"\n  ) + \n  annotate(\"segment\", x = 6.2, xend = 6.2, colour = \"darkgrey\", linetype = \"dashed\", y = 0, yend = 100 * calc_npv_discount(0.062)) +\n  annotate(\"segment\", x = 0, xend = 100 * 0.062, colour = \"darkgrey\", linetype=\"dashed\", y = 100 * calc_npv_discount(0.062), yend = 100 * calc_npv_discount(0.062)) +\n  annotate(\"text\", \n           x = 2, y = 0.1 + 100 * calc_npv_discount(0.062),\n           label = \"Services (Current)\"\n  )  \n\n\n\n\n\nSo, the effective discount for deferring has fallen alongside inflation. However it’s still something.\nThe immediate cost of deferring is by contrast the same. It involves clicking a couple of buttons, so a couple of seconds, in the same Monzo app.\nThere are some other consequences too: Using a higher proportion of one’s credit limit tends to lower one’s credit rating. This means the ability to acquire credit on more favourable terms can be adversely affected.\nHowever, for now, as a general principle, realising marginal savings by pressing a couple of buttons doesn’t seem too bad, and at some points of time, and for some items, the savings have been around 2%."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html",
    "href": "posts/glms/lms-are-glms-part-07/index.html",
    "title": "Part Seven: Feeling Uncertain",
    "section": "",
    "text": "In the previous post we managed to use numerical optimisation, with the optim() function, to good \\(\\beta\\) estimates for linear regression model fit to some toy data. In this post, we will explore how the optim() function can be used to produce estimates of uncertainty about these \\(\\beta\\) coefficients, and how these relates to measures of uncertainty presented in the standard lm and glm summary functions."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-07/index.html#aim",
    "title": "Part Seven: Feeling Uncertain",
    "section": "",
    "text": "In the previous post we managed to use numerical optimisation, with the optim() function, to good \\(\\beta\\) estimates for linear regression model fit to some toy data. In this post, we will explore how the optim() function can be used to produce estimates of uncertainty about these \\(\\beta\\) coefficients, and how these relates to measures of uncertainty presented in the standard lm and glm summary functions."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#prereqs",
    "href": "posts/glms/lms-are-glms-part-07/index.html#prereqs",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Prereqs",
    "text": "Prereqs\nAs before, we’ll be using the same toy dataset, and same log likelihood function, as in the last two posts in this series. Let’s create these again:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nLet’s also run and save our parameter estimates produced both ‘the hard way’ (using optim), and ‘the easier way’ (using ‘glm’)\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nCode\npars_optim &lt;- optim_results$par\n\nnames(pars_optim) &lt;- c(\"beta0\", \"beta1\", \"eta\")\n\npars_optim\n\n\n    beta0     beta1       eta \n 2.460571  1.375421 -1.336209 \n\n\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- tibble(x = x, y = y)\nmod_glm &lt;- glm(y ~ x, data = df, family = gaussian(link=\"identity\"))\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.6082  -0.3852  -0.1668   0.2385   1.1092  \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nSo, both optim and the summary to mod_glm report \\(\\{\\beta_0 = 2.36, \\beta_1 = 1.38\\}\\), so both approaches appear to arrive at the same point on the log likelihood surface.\nHowever, note that the glm summary reports not just the estimates themselves (in the Estimate column of coefficients), but also standard errors (the Std. Error column) and derived quantities (t value, Pr(&gt;|t|), and the damnable stars at the very right of the table). How can these measures of uncertainty about the true value of the \\(\\beta\\) coefficients be derived from optim?"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#barefoot-and-blind-a-weird-analogy-for-a-complicated-idea",
    "href": "posts/glms/lms-are-glms-part-07/index.html#barefoot-and-blind-a-weird-analogy-for-a-complicated-idea",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Barefoot and Blind: A weird analogy for a complicated idea",
    "text": "Barefoot and Blind: A weird analogy for a complicated idea\nImagine optim, your hill-finding robo-chauffeur, has taken you to the top of a likelihood surface. Then it leaves you there…\n… and you’re blind, and have no shoes. (You also have an uncanny sense of your orientation, whether north-south, east-west, or some other angle.)\nSo, you know you’re at the top of the hill, but you can’t see what the landscape around you looks like. However, you still want to get a sense of this landscape, and how it varies around the spot you’re standing on.\nWhat do you do?\nIf you’re playing along with this weird thought experiment, one approach would be to use your feet as depth sensors. You make sure you never stray from where you started, and to always keep one foot planted on this initial spot (which you understand to be the highest point on the landscape). Then you use your other foot to work out how much further down the surface is from the highest point as you venture away from the highest point in different directions.\nSay you keep your left foot planted on the highest point, and make sure your right foot is always positioned (say) 10 cm horizontally from your left foot. Initially your two feet are arranged east-west; let’s call this 0 degrees. When you put your right foot down, you notice it needs to travel 2 cm further down to reach terra ferma relative to your left foot.\n2cm at 0 degrees. You’ll remember that.\nNow you rotate yourself 45 degrees, and repeat the same right foot drop. This time it needs to travel 3cm down relative to your left foot.\n3cm at 45 degrees. You remember that too.\nNow you rotate another 45 degrees, north-south orientation, place your right foot down; now it falls 5cm down relative to your left foot.\n2cm at 0 degrees; 3cm at 45 degrees; 5cm at 90 degrees.\nNow with this information, you try to construct the landscape you’re on top of with your mind’s eye, making the assumption that the way it has to have curved from the peak you’re on to lead to the drops you’ve observed is consistent all around you; i.e. that there’s only one hill, you’re on top of it, and it’s smoothly curved in all directions.\nIf you could further entertain the idea that your feet are infinitely small, and the gap between feet is also infinitely small (rather than the 10cm above), then you have the intuition behind this scary-looking but very important formula from King (1998) (p. 89):\n\\[\n\\widehat{V(\\hat{\\theta})} = - \\frac{1}{n}[\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]^{-1}_{\\tilde{\\theta} = \\hat{\\theta}}\n\\]\nWhat this is saying, in something closer to humanese, is something like:\n\nOur best estimate of the amount of uncertainty we have in our estimates is a function of how much the likelihood surface curves at the highest point on the surface. (It also gets less uncertain, the more observations we have)."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#information-and-uncertainty",
    "href": "posts/glms/lms-are-glms-part-07/index.html#information-and-uncertainty",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Information and uncertainty",
    "text": "Information and uncertainty\nAmongst the various bells, whistles and decals in the previous formula is the superscript \\((.)^{-1}\\). This means invert, which for a single value means \\(\\frac{1}{.}\\) but for a matrix means something conceptually the same but technically not.\nAnd what’s being inverted in the last formula? A horrible-looking expression, \\([\\frac{\\delta^2lnL(\\tilde{\\theta}|y)}{\\delta \\tilde{\\theta} \\delta \\tilde{\\theta}^{'}}]_{\\tilde{\\theta} = \\hat{\\theta}}\\), that’s basically an answer to the question of how curvy is the log likelihood surface at its peak position?\nWithin King (1998) (p.89, eq. 4.18), this expression (or rather the negative of the term) is defined as \\(I(\\hat{\\theta} | y)\\), where \\(I(.)\\) stands for information.\nSo, the algebra are saying\n\nUncertainty is inversely related to information\n\nOr perhaps even more intuitively\n\nThe more information we have, the less uncertain we are\n\nOf course this makes sense. If you ask someone “How long will this task take?”, and they say “Between one hour and one month”, they likely have less information about how long the task will actually than if they had said “Between two and a half and three hours”. More generally:\n\nShallow gradients mean wide uncertainty intervals mean low information\nSharp gradients mean narrow uncertaintly intervals mean high information\n\nThis is, fundamentally, what the blind and barefoot person in the previous analogy is trying to achieve: by feeling out the local curvature around the highest point, they are trying to work out how much information they have about different pieces of the model. The curvature along any one dimension of the surface (equivalent to the 0 and 90 degree explorations) indicates how much information there is about any single coefficient, and the curvature along the equivalent of a 45 degree plane gives a measure of how associated any two coefficients tend to be.\nWith these many analogies and equations spinning in our heads, let’s now see how these concepts can be applied in practice."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#optimal-uncertainty",
    "href": "posts/glms/lms-are-glms-part-07/index.html#optimal-uncertainty",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Optimal uncertainty",
    "text": "Optimal uncertainty\nHaving reminded myself of the particular options for optim that are typically used to report parameter uncertainty, let’s run the follows:\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nWe have used a slightly different algorithm (‘BFGS’), and a different way of specifying the function to search over (using fnscale = -1 to invert the likelihood), but we have the same par estimates as before: \\(\\beta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\). So the changes we’ve made to the optim arguments haven’t changed what it estimates.\nOne new argument we’ve set in optim is hessian = TRUE. Hessian is a kind of coarse fabric made from vegetable waste, typically woven in a criss-crossing, grid-like pattern. Hessian matrices are matrices of second derivatives, as described in the wikipedia article. 1 If you can bear to recall the really complex expression above, for calculating the curvature around a point on a surface, you’ll recall it’s also about second derivatives.\nNone of this is a coincidence. The hessian component of the optim output above contains what we need.\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\nhess\n\n\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nYou might notice that the Hessian matrix is square, with as many columns as rows. And, that the number of columns (or rows) is equal to the number of parameters we have estimated, i.e. three in this case.\nYou might also notice that the values are symmetrical about the diagonal running from the top left to the bottom right.\nAgain, this is no accident.\nRemember that variation is inversely related to information, and that \\((.)^{-1}\\) is the inversion operator on \\(I(.)\\), the Information Matrix. Well, this Hessian is (pretty much) \\(I(.)\\). So let’s see what happens when we invert it (using the solve operator):\n\n\nCode\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\nAs with hess, inv_hess is symmetric around the top-left to bottom-right diagonal. For example, the value on row 2 and column 1 is the same as on row 1, column 2.\nWe’re mainly interested in the first two columns and rows, as these contain the values most comparable with the glm summary reports\n\n\nCode\ninv_hess_betas &lt;- inv_hess[1:2, 1:2]\n\ninv_hess_betas\n\n\n             [,1]         [,2]\n[1,]  0.033577455 -0.004379668\n[2,] -0.004379668  0.004379668\n\n\nWhat the elements of the above matrix provide are estimates of the variances of a single parameter \\(\\beta_j\\), and/or the covariances between any two parameters \\(\\{\\beta_0, \\beta_1\\}\\). In this example:\n\\[\n\\begin{bmatrix}\nvar(\\beta_0) & cov(\\beta_0, \\beta_1) \\\\\ncov(\\beta_1, \\beta_0) & var(\\beta_1)\n\\end{bmatrix}\n\\]\nIt’s because the on-diagonal terms are variances of uncertaintly for a single term, that it can be useful to take the square root of these terms to get estimates of the standard errors:\n\n\nCode\nsqrt(diag(inv_hess_betas))\n\n\n[1] 0.18324152 0.06617906\n\n\nCompare with the Std Err term in the following:\n\n\nCode\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.6082  -0.3852  -0.1668   0.2385   1.1092  \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe estimates from the Hessian in optim, of \\(\\{0.18, 0.07\\}\\), are not exactly the same as the \\(\\{0.21, 0.08\\}\\) reported for mod_glm; the methods employed are not identical. But they are hopefully similar enough to demonstrate they provide similar information about similar quantities of uncertainty."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#summary",
    "href": "posts/glms/lms-are-glms-part-07/index.html#summary",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Summary",
    "text": "Summary\nThis is probably the most difficult single section so far. Don’t worry: it’s likely to get easier from here on in."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-07/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-07/index.html#footnotes",
    "title": "Part Seven: Feeling Uncertain",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough I had assumed Hessian matrices are called Hessian matrices because they sort-of resemble the criss-crossing grids of Hessian bags, they’re actually named after Otto Hesse, who proposed them.↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-01/index.html",
    "href": "posts/glms/lms-are-glms-part-01/index.html",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-01/index.html#tldr",
    "href": "posts/glms/lms-are-glms-part-01/index.html#tldr",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-01/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "href": "posts/glms/lms-are-glms-part-01/index.html#part-1-what-are-statistical-models-and-how-are-they-fit",
    "title": "Part One: Model fitting as parameter calibration",
    "section": "Part 1: What are statistical models and how are they fit?",
    "text": "Part 1: What are statistical models and how are they fit?\nIt’s common for different statistical methods to be taught as if they’re completely different species or families. In particular, for standard linear regression to be taught first, then additional, more exotic models, like logistic or Poisson regression, to be introduced at a later stage, in an advanced course.\nThe disadvantage with this standard approach to teaching statistics is that it obscures the way that almost all statistical models are, fundamentally, trying to do something very similar, and work in very similar ways.\nSomething I’ve found immensely helpful over the years is the following pair of equations:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nIn words, the above is saying something like:\n\nThe predicted response \\(Y_i\\) for a set of predictors \\(X_i\\) is assumed to be drawn from (the \\(\\sim\\) symbol) a stochastic distribution (\\(f(.,.)\\))\nThe stochastic distribution contains both parameters we’re interested in, and which are determined by the data \\(\\theta_i\\), and parameters we’re not interested in and might just have to assume, \\(\\alpha\\).\nThe parameters we’re interested in determining from the data \\(\\theta_i\\) are themselves determined by a systematic component \\(g(.,.)\\) which take and transform two inputs: The observed predictor data \\(X_i\\), and a set of coefficients \\(\\beta\\)\n\nAnd graphically this looks something like:\n\n\n\n\nflowchart LR\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  \n  X --&gt; g\n  beta --&gt; g\n  g --&gt; theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt; Y\n\n\n\n\n\n\n\nTo understand how this fits into the ‘whole game’ of modelling, it’s worth introducing another term, \\(D\\), for the data we’re using, and to say that \\(D\\) is partitioned into observed predictors \\(X_i\\), and observed responses, \\(y_i\\).\nFor each observation, \\(i\\), we therefore have a predicted response, \\(Y_i\\), and an observed response, \\(y_i\\). We can compare \\(Y_i\\) with \\(y_i\\) to get the difference between the two, \\(\\delta_i\\).\nNow, obviously can’t change the data to make it fit our model better. But what we can do is calibrate the model a little better. How do we do this? Through adjusting the \\(\\beta\\) parameters that feed into the systematic component \\(g\\). Graphically, this process of comparison, adjustment, and calibration looks as follows:\n\n\n\n\nflowchart LR\n  D\n  y\n  X\n  beta\n  g\n  f\n  alpha\n  theta\n  Y\n  diff\n  \n  D --&gt;|partition| X\n  D --&gt;|partition| y\n  X --&gt; g\n  beta --&gt;|rerun| g\n  g --&gt;|transform| theta\n  theta --&gt; f\n  alpha --&gt; f\n  \n  f --&gt;|predict| Y\n  \n  Y --&gt;|compare| diff\n  y --&gt;|compare| diff\n  \n  diff --&gt;|adjust| beta\n  \n  \n  \n  linkStyle default stroke:blue, stroke-width:1px\n\n\n\n\n\n\nPretty much all statistical model fitting involves iterating along this \\(g \\to \\beta\\) and \\(\\beta \\to g\\) feedback loop until some kind of condition is met involving minimising \\(\\delta\\).\nI’ll expand on this idea further in part 2."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-04/index.html",
    "href": "posts/glms/lms-are-glms-part-04/index.html",
    "title": "Part Four: why only betas just look at betas",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-04/index.html#tldr",
    "href": "posts/glms/lms-are-glms-part-04/index.html#tldr",
    "title": "Part Four: why only betas just look at betas",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-04/index.html#part-4-why-overuse-of-linear-regression-leads-people-to-look-at-models-in-the-wrong-way",
    "href": "posts/glms/lms-are-glms-part-04/index.html#part-4-why-overuse-of-linear-regression-leads-people-to-look-at-models-in-the-wrong-way",
    "title": "Part Four: why only betas just look at betas",
    "section": "Part 4: Why overuse of linear regression leads people to look at models in the wrong way",
    "text": "Part 4: Why overuse of linear regression leads people to look at models in the wrong way\nIn the last post in this series I’ve reintroduced standard linear regression and logistic regression as both being special versions of the same generalised model formula.\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nWith standard linear regression the link function \\(g(.)\\) is \\(I(.)\\), i.e. the identity function, meaning what goes in, is what comes out. By contrast for logistic regression \\(g(.)\\) is the logistic function, which squishes and squashes any real number as an input onto a value between 0 and 1 as an output.\nThough it’s not always phrased this way, a motivating question behind the construction of most statistical models is, “What influence does a single input to the model, \\(x_j\\), have on the output, \\(Y\\)?”1 For a single variable \\(x_j\\) which is either present (1) or absent (0), this is in effect asking what is \\(E(Y | x_j = 1) - E(Y | x_j = 0)\\) ?2\nLet’s look at a linear regression case, then a logistic regression case.\n\nLinear Regression example\nUsing the iris dataset, let’s try to predict Sepal Width (a continuous variable) on Sepal Length (a continuous variable) and whether the species is setosa or not (a discrete variable). As a reminder, the data relating these three variables look as follows:\n\n\nCode\nlibrary(ggplot2)\n\niris |&gt;\n    ggplot(aes(Sepal.Length, Sepal.Width, group = Species, colour = Species, shape = Species)) + \n    geom_point()\n\n\n\n\n\nLet’s now build the model:\n\n\nCode\nlibrary(tidyverse)\ndf &lt;- iris |&gt; mutate(is_setosa = Species == 'setosa')\n\nmod_lm &lt;- lm(Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nmod_lm\n\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nCoefficients:\n  (Intercept)   Sepal.Length  is_setosaTRUE  \n       0.7307         0.3420         0.9855  \n\n\nThe coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}\\) are \\(\\{0.73, 0.34, 0.99\\}\\), and refer to the intercept, Sepal Length and is_setosa respectively.\nIf we assume a Sepel Length of 6, for example, then the expected Sepal Width (the thing we are predicting) is 0.73 + 6 * 0.34 + 0.99 or about 3.77 in the case where is_setosa is true, and 0.73 + 6 * 0.34 or about 2.78 where is_setosa is false.\nThe difference between these two values, 3.77 and 2.78, i.e. the ‘influence of setosa’ on the outcome, is 0.99, i.e. the \\(\\beta_2\\) coefficient shown before. In fact, for any conceivable (and non-conceivable, i.e. negative) value of Sepal Length, the difference is still 0.99.\nThis is the \\(\\beta_2\\) coefficient, and the reason why, for linear regression, and almost exclusively linear regression, looking at the coefficients themselves provides substantively meaningful information (something King, Tomz, and Wittenberg (2000) calls a ‘quantity of interest’) about the size of influence that a predictor has on a response.\n\n\nLogistic Regression example\nNow let’s look at an example using logistic regression. We will use another tiresomely familiar dataset, mtcars. We are interested in estimating the effect that having a straight engine (vs=1) has on the probability of the car having a manual transmission (am=1). Our model also tries to control for the miles-per-gallon (mpg). The model specification is shown, the model is run, and the coefficeints are all shown below:\n\n\nCode\nmod_logistic &lt;- glm(\n    am ~ mpg + vs,\n    data = mtcars, \n    family = binomial()\n    )\n\nmod_logistic\n\n\n\nCall:  glm(formula = am ~ mpg + vs, family = binomial(), data = mtcars)\n\nCoefficients:\n(Intercept)          mpg           vs  \n    -9.9183       0.5359      -2.7957  \n\nDegrees of Freedom: 31 Total (i.e. Null);  29 Residual\nNull Deviance:      43.23 \nResidual Deviance: 24.94    AIC: 30.94\n\n\nHere the coefficients \\(\\boldsymbol{\\beta} = \\{\\beta_0, \\beta_1, \\beta_2\\}\\) are \\(\\{-9.92, 0.54, -2.80\\}\\), and refer to the intercept, mpg, and vs respectively.\nBut what does this actually mean, substantively?\n\n\n(Don’t) Stargaze\nA very common approach to trying to answer this question is to look at the statistical significance of the coefficients, which we can do with the summary() function\n\n\nCode\nsummary(mod_logistic)\n\n\n\nCall:\nglm(formula = am ~ mpg + vs, family = binomial(), data = mtcars)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.3389  -0.6304  -0.2980   0.3069   2.0106  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -9.9183     3.4942  -2.839  0.00453 **\nmpg           0.5359     0.1967   2.724  0.00644 **\nvs           -2.7957     1.4723  -1.899  0.05758 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.230  on 31  degrees of freedom\nResidual deviance: 24.944  on 29  degrees of freedom\nAIC: 30.944\n\nNumber of Fisher Scoring iterations: 6\n\n\nA common practice in many social and health sciences is to offer something like a narrative summary of the above, something like:\n\nOur logistic regression model indicates that manualness is positively and significantly associated with our measure of fuel efficiency (p &lt; 0.01). There is also an indication of a negative association with straight engine, but this effect does not quite meet conventional thresholds for statistical significance (p &lt; 0.10).\n\nThis above practice is known as ‘star-gazing’, because summary tables like those above tend to have one or more * symbols in the final row, if the value of the Pr(&gt;|z|) is below 0.05, and narrative summaries like those just above tend to involve looking at the number of stars in each row, alongside whether the Estimate values have a minus sign in front of them.\nStar gazing is a very common practice. It’s also a terrible practice, which - ironically - turns the final presented output of a quantitative model into the crudest of qualitative summaries (positive, negative; significant, not significant). Star gazing is what researchers tend to default to when presented with model outputs from the above because, unlike in the linear regression example, the extent to which the \\(\\beta\\) coefficients answer substantive ‘how-much’-ness questions, like “How much does having a straight engine change the probability of manual transmission?, is not easily apparent from the coefficients themselves.\n\n\nStandardisation\nSo, how can we do better?\nOne approach is to standardise the data that goes into the model before passing them to the model. Standardisation means attempting to make the distribution and range of different variables more similar, and is especially useful when comparing between different continuous variables.\nTo give an example of this, let’s look at a specification with weight (wt) and horsepower (hp) in place of mpg, but keeping engine-type indicator (vs):\n\n\nCode\nmod_logistic_2 &lt;- glm(\n    am ~ vs + wt + hp,\n    data = mtcars, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2)\n\n\n\nCall:\nglm(formula = am ~ vs + wt + hp, family = binomial(), data = mtcars)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.88193  -0.10513  -0.01248   0.10256   1.65058  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) 25.35510   11.24613   2.255   0.0242 *\nvs          -3.12906    2.92958  -1.068   0.2855  \nwt          -9.64982    4.05528  -2.380   0.0173 *\nhp           0.03242    0.01959   1.655   0.0979 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.5012  on 28  degrees of freedom\nAIC: 16.501\n\nNumber of Fisher Scoring iterations: 8\n\n\nHere both wt and hp are continuous variables.\nA star gazing zombie might say something like\n\nmanualness is negatively and significantly associated with weight (p &lt; 0.05); there is a positive association with horsepower but this does not meet standard thresholds of statistical significance (0.05 &lt; p &lt; 0.10).\n\nA slightly better approach would be to standardise the variables wt and hp before passing to the model. Standardising means trying to set the variables to a common scale, and giving the variables more similar statistical characteristics.\n\n\nCode\nstandardise &lt;- function(x){\n  (x - mean(x)) / sd(x)\n}\n\nmtcars_z &lt;- mtcars\nmtcars_z$wt_z = standardise(mtcars$wt)\nmtcars_z$hp_z = standardise(mtcars$hp)\n\nmod_logistic_2_z &lt;- glm(\n    am ~ vs + wt_z + hp_z,\n    data = mtcars_z, \n    family = binomial()\n    )\n\nsummary(mod_logistic_2_z)\n\n\n\nCall:\nglm(formula = am ~ vs + wt_z + hp_z, family = binomial(), data = mtcars_z)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.88193  -0.10513  -0.01248   0.10256   1.65058  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)  -0.9348     1.4500  -0.645   0.5191  \nvs           -3.1291     2.9296  -1.068   0.2855  \nwt_z         -9.4419     3.9679  -2.380   0.0173 *\nhp_z          2.2230     1.3431   1.655   0.0979 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.2297  on 31  degrees of freedom\nResidual deviance:  8.5012  on 28  degrees of freedom\nAIC: 16.501\n\nNumber of Fisher Scoring iterations: 8\n\n\nwt_z is the standardised version of wt, and hp_z is the standardised version of hp. By convention, whereas unstandardised coefficients are usually referred to as \\(\\beta\\) (‘beta’) coefficients, standardised coefficients are instead referred to as \\(b\\) coefficients. But really, it’s the same model.\nNote the p value of wt_z is the same as for wt, and the p value of hp_z is the same as that for hp. Note also the directions of effect are the same: the coefficients on wt and wt_z are both negative, and the coefficients of hp and hp_z are both positive.\nThis isn’t a coincidence. Of course standardising can’t really add any new information, can’t really change the relationship between a predictor and a response. It’s not really a new variable, it’s the same old variable, so the relationship between predictor and response that there used to be is still there now.\nSo why bother standardising?\nOne reason is it gives, subject to some assumptions and caveats, a way of gauging the relative importance of the two different continuous variables, by allowing a slightly more meaningful comparison between the two coefficients.\nIn this case, we have a standardised \\(b\\) coefficient of -9.44 for wt_z, and of 2.22 for hp_z. As with the unstandardised coefficients we can still assert that manualness is negatively associated with weight, and positively associated with horsepower. But now we can also compare the two numbers -9.44 and 2.22. The ratio of these two numbers is around 4.3. So, we might hazard to suggest something like:\n\na given increase in weight is around four times as important in negatively predicting manual transmission (i.e. in predicting an automatic transmission) as an equivalent increase in horsepower is in positively predicting manual transmission.\n\nThis isn’t a statement that’s easy to parse, but does at least allow slightly more information to be gleamed from the model. For example, it implies that, if a proposed change to a vehicle leads to similar relative (standardised) increases in both weight and horsepower then, as the weight effect is greater than the horsepower effect, the model will predict a decreased probability of manualness as a result.\nBut what about the motivating question, “What’s the effect of a straight engine (vs=1) on the probability of manual transmission (am=1)?”\nThe problem, unlike with the linear regression, is this is now a badly formulated question, based on an incorrect premise. The problem is with the word ‘the’, which implies there should be a single answer to this question, i.e. that the effect of vs on the probability of am=1 should always be the same. But, at least when it comes to absolute changes in the probability of am=1, this is no longer the case, as it depends on the values of the other variables in the model.\nInstead of assuming vs=1 has a single effect on P(am=1), we instead need to think about predictions of the marginal effects of vs on am in the context of other plausible values of the other predictors in the model, wt and hp. This involves asking the model a series of well formulated and specific questions.\n\n\nMaximum marginal effects: Divide-by-four\nBefore we do that, however, there’s a useful heuristic that can be employed when looking at discrete variables and using a logistic regression specification. The heuristic, which is based on the properties of the logistic function,3 is called divide-by-four. What this means is that, if we take the coefficient on vs of -3.13, and divide this value by four, we get a value of -0.78. Notice that the absolute value of -0.78 is between 0 and 1.4 What this value gives is the maximum possible effect that the discrete variable (the presence rather than absence of a straight engine) has on the probability of being a manual transmission. We can say, “a straight engine reduces the probability of a manual transmission by up to 78%”\nBut, as mentioned, this doesn’t quite answer the motivating question, it gives an upper bound to the answer, not the answer itself.5 We can instead start to get a sense of ‘the’ effect of the variable vs on P(am=1) by asking the model a series of questions.\n\n\nPredictions on a matrix\nWe can start by getting the range of observed values for the two continuous variables, hp and mpg:\n\n\nCode\nmin(mtcars$hp)\n\n\n[1] 52\n\n\nCode\nmax(mtcars$hp)\n\n\n[1] 335\n\n\nCode\nmin(mtcars$wt)\n\n\n[1] 1.513\n\n\nCode\nmax(mtcars$wt)\n\n\n[1] 5.424\n\n\nWe can then ask the model to make predictions of \\(P(am=1)\\) for a large number of values of hp and wt within the observed range, both in the condition in which vs=0 and in the condition in which vs=1. The expand_grid function6 can help us do this:\n\n\nCode\npredictors &lt;- expand_grid(\n  hp = seq(min(mtcars$hp), max(mtcars$hp), length.out = 100),\n  wt = seq(min(mtcars$wt), max(mtcars$wt), length.out = 100)\n)\n\npredictors_straight &lt;- predictors |&gt; \n  mutate(vs = 1)\n\npredictors_vshaped &lt;- predictors |&gt; \n  mutate(vs = 0)\n\n\nFor each of these permutations of inputs, we can use the model to get a conditional prediction. For convenience, we can also attach this as an additional column to the predictor data frame:\n\n\nCode\npredictions_predictors_straight &lt;- predictors_straight |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_straight)\n  )\n\npredictions_predictors_vshaped &lt;- predictors_vshaped |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictors_vshaped)\n  )\n\n\nWe can see how the predictions vary over hp and wt using a heat map or contour map:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  ggplot(aes(x = hp, y = wt, z = p_manual)) + \n  geom_contour_filled() + \n  facet_wrap(~vs) +\n  labs(\n    title = \"Predicted probability of manual transmission by wt, hp, and vs\"\n  )\n\n\n\n\n\nWe can also produce a contour map of the differences between these two contour maps, i.e. the effect of a straight (vs=1) compared with v-shaped (vs=0) engine, which gets us a bit closer to the answer:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled() + \n  labs(\n    title = \"Marginal effect of vs=1 given wt and hp on P(am=1)\"\n  )\n\n\n\n\n\nWe can see here that, for large ranges of wt and hp, the marginal effect of vs=1 is small. However, for particular combinations of hp and wt, such as where hp is around 200 and wt is slightly below 3, then the marginal effect of vs=1 becomes large, up to around a -70% reduction in the probability of manual transmission. (i.e. similar to the theoretical maximum marginal effect of around -78%).\nSo, what’s the effect of vs=1 on P(am=1)? i.e. how should we boil down all these 10,000 predicted effect sizes into a single effect size?\nI guess, if we have to try to answer this silly question, then we could take the average effect size…\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  summarise(\n    mean_diff_p_manual = mean(diff_p_manual)\n  )\n\n\n# A tibble: 1 × 1\n  mean_diff_p_manual\n               &lt;dbl&gt;\n1            -0.0821\n\n\nSo, we get an average difference of around -0.08, i.e. about an 8% reduction in probability of manual transmission.\n\n\nMarginal effects on observed data\nIs this a reasonable answer? Probably not, because although the permutations of wt and hp we looked at come from the observed range, most of these combinations are likely very ‘theoretical’. We can get a sense of this by plotting the observed values of wt and hp onto the above contour map:\n\n\nCode\npredictions_predictors_straight |&gt; \n  bind_rows(\n    predictions_predictors_vshaped\n  ) |&gt; \n  group_by(hp, wt) |&gt; \n  summarise(\n    diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n  ) |&gt; \n  ungroup() |&gt; \n  ggplot(\n    aes(x = hp, y = wt, z = diff_p_manual)\n  ) + \n  geom_contour_filled(alpha = 0.2, show.legend = FALSE) + \n  labs(\n    title = \"Observations from mtcars on the predicted probability surface\"\n  ) +\n  geom_point(\n    aes(x = hp, y = wt), inherit.aes = FALSE,\n    data = mtcars\n  )\n\n\n\n\n\nPerhaps a better option, then, would be to calculate an average marginal effect using the observed values, but switching the observations for vs to 1 in one scenario, and 0 in another scenario:\n\n\nCode\npredictions_predictors_observed_straight &lt;- mtcars |&gt; \n  select(hp, wt) |&gt; \n  mutate(vs = 1)\n\npredictions_predictors_observed_straight &lt;- predictions_predictors_observed_straight |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_straight)\n  )\n\npredictions_predictors_observed_vshaped &lt;- mtcars |&gt; \n  select(hp, wt) |&gt; \n  mutate(vs = 0) \n\npredictions_predictors_observed_vshaped &lt;- predictions_predictors_observed_vshaped |&gt; \n  mutate(\n    p_manual = predict(mod_logistic_2, type = \"response\", newdata = predictions_predictors_observed_vshaped)\n  )\n  \n\npredictions_predictors_observed &lt;- \n  bind_rows(\n    predictions_predictors_observed_straight,\n    predictions_predictors_observed_vshaped\n  )\n\npredictions_marginal &lt;- \n  predictions_predictors_observed |&gt; \n    group_by(hp, wt) |&gt; \n    summarise(\n      diff_p_manual = p_manual[vs==1] - p_manual[vs==0]\n    )\n\npredictions_marginal |&gt; \n  ggplot(aes(x = diff_p_manual)) + \n  geom_histogram() +\n  geom_vline(aes(xintercept = mean(diff_p_manual)), colour = \"red\") + \n  geom_vline(aes(xintercept = median(diff_p_manual)), colour = \"green\")\n\n\n\n\n\nIn the above the red line indicates the mean value of these marginal differences, which is -0.12, and the green line the median value of these differences, which is around -0.02. So, even with just these two measures of central tendency, there’s around a six-fold difference in the estimate of ‘the effect’. We can also see there’s a lot of variation, from around nothing (right hand side), to around a 65% reduction (left hand side).\nIf forced to give a simple answer (to this overly simplistic question), we might plump for the mean for theoretical reasons, and say something like “The effect of a straight engine is to reduce the probability of a manual transmission by around an eighth”. But I’m sure, having seen how much variation there is in these marginal effects, we can agree this ‘around an eighth’ answer, or any single number answer, is likely to be overly reductive.\nHopefully, however, it is more informative than ‘statistically significant and negative’, (the stargazing approach) or ‘up to around 78%’ (the divide-by-four approach)."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-04/index.html#conclusion",
    "href": "posts/glms/lms-are-glms-part-04/index.html#conclusion",
    "title": "Part Four: why only betas just look at betas",
    "section": "Conclusion",
    "text": "Conclusion\nLinear regression tends to give a false impression about how straightforward it is to use a model to answer questions of the form “What is the effect of x on y?”. This is because, for linear regression, but few other model specifications, the answer to this question is in the \\(\\beta\\) coefficients themselves. For other model specifications, like the logistic regression example above, the correct-but-uninformative answer tends to be “it depends”, and potentially more informative answers tend to require a bit more work to derive and interpret."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-04/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-04/index.html#footnotes",
    "title": "Part Four: why only betas just look at betas",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote here I’m using \\(x_j\\), not \\(x_i\\), and that \\(X\\beta\\) is shorthand for \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\\) and so on. In using the \\(j\\) suffix, I’m referring to just one of the specific \\(x\\) values, \\(x_1\\), \\(x_2\\), \\(x_3\\), which is equivalent to selecting one of the columns in \\(X\\). By contrast \\(i\\) should be considered shorthand for selection of one of the rows of \\(X\\), i.e. one of the series of observations that goes into the dataset \\(D\\).↩︎\n\\(E(.)\\) is the expectation operator, and \\(|\\) indicates a condition. So, the two terms mean, respectively, what is the expected value of the outcome if the variable of interest is ‘switched on’?, and what is the expected value of the outcome if the variable of interest is ‘switched off’?↩︎\nThe logistic function maps any real number z onto the value range 0 to 1. z is \\(X\\beta\\), which in non-matrix notation is equivalent to a sum of products \\(\\sum_{k=0}^{K}x_k\\beta_k\\) (where, usually, \\(x_0\\) is 1, i.e. the intercept term). Another way of expressing this would be something like \\(\\sum_{k \\in S}x_k\\beta_k\\) where by default \\(S = \\{0, 1, 2, ..., K\\}\\). We can instead imagine partitioning out \\(S = \\{S^{-J}, S^{J}\\}\\) where the superscript \\(J\\) indicates the Jth variable, and \\(-J\\) indicates everything in \\(S\\) apart from the Jth variable. Where J is a discrete variable, the effect of J on \\(P(Y=1)\\) is \\(logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k + \\beta_J}) - logistic({\\sum_{k \\in S^{-J}}x_k\\beta_k})\\), where \\(logistic(z) = \\frac{1}{1 + e^{-z}}\\). The marginal effect of the \\(\\beta_J\\) coefficient thus depends on the other term \\(\\sum_{k \\in S^{-J}}x_k\\beta_k\\). Where this other term is set to 0 the marginal effect of \\(\\beta_J\\) becomes \\(logistic(\\beta_J) - logistic(0)\\). According to p.82 of this chapter by Gelman we can equivalently ask the question ‘what is the first derivative of the logistic regression with respect to \\(\\beta\\)?’. Asking more about this to Wolfram Alpha we get this page of information, and scrolling down to the section on the global minimum we indeed get an absolute value of \\(\\frac{1}{4}\\), so the maximum change in \\(P(Y=1)\\) given a unit change in \\(\\beta\\) is indeed one quarter of the value of \\(\\beta\\), hence why the ‘divide-by-four’ heuristic ‘works’. This isn’t quite a full derivation, but more explanation than I was planning for a footnote! In general, it’s better just to remember ‘divide-by-four’ than go down the rabbit warren of derivation each time! (As I’ve just learned, to my cost, writing this footnote!)↩︎\nWe should always expect the absolute value of a coefficient for a discrete variable to be less than four, for this reason.↩︎\nThe lower bound for the marginal effect of a discrete variable, or any variable, is zero. This is when the absolute value of the sum of the product of the other variables is infinite.↩︎\nOr the base R expand.grid function↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-02/index.html",
    "href": "posts/glms/lms-are-glms-part-02/index.html",
    "title": "Part Two: Systematic components and link functions",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-02/index.html#tldr",
    "href": "posts/glms/lms-are-glms-part-02/index.html#tldr",
    "title": "Part Two: Systematic components and link functions",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000) ."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-02/index.html#part-2-systematic-components-and-link-functions",
    "href": "posts/glms/lms-are-glms-part-02/index.html#part-2-systematic-components-and-link-functions",
    "title": "Part Two: Systematic components and link functions",
    "section": "Part 2: Systematic components and link functions",
    "text": "Part 2: Systematic components and link functions\nIn part 1 of this series we introduced the following general framework for thinking about statistical models and what they contain.\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\] The terminology are as described previously.\nThese equations are too broad and abstract to be implemented directly. Instead, specific choices about the \\(f(.)\\) and \\(g(.)\\) need to be made. King, Tomz, and Wittenberg (2000) gives the following examples:\nLogistic Regression\n\\[\nY_i \\sim Bernoulli(\\pi_i)\n\\]\n\\[\n\\pi_i = \\frac{1}{1 + e^{-X_i\\beta}}\n\\]\nLinear Regression\n\\[\nY_i \\sim N(\\mu_i, \\sigma^2)\n\\] \\[\n\\mu_i = X_i\\beta\n\\]\nSo, what’s so special about linear regression, in this framework?\nIn one sense, not so much. It’s got a systematic component, and it’s got a stochastic component. But so do other models. But in another sense, quite a lot. It’s a rare case where the systematic component, \\(g(.)\\), doesn’t transform its inputs in some weird and wonderful way. We can say that \\(g(.)\\) is the identity transform, \\(I(.)\\), which in words means take what you’re given, do nothing to it, and pass it on.\nBy contrast, the systematic component for logistic regression is known as the logistic function. \\(logistic(x) := \\frac{1}{1 + e^{-x}}\\) It transforms inputs that could be anywhere on the real number line to values that lay somewhere between 0 and 1. Why 0 to 1? Because what logistic regression models produce aren’t predicted values, but predicted probabilities, and nothing can be more probable than certain (1) or less probable than impossible (0).\nWe can compare the transformations used in linear and logistic regression as follows:1\n\n# Define transformations\nident &lt;- function(x) {x}\nlgt &lt;- function(x) {1 / (1 + exp(-x))}\n\n\n# Draw the associations\ncurve(ident, -6, 6,\n      xlab = \"x (before transform)\",\n      ylab = \"z (after transform)\",\n      main = \"The Identity 'Transformation'\"\n      )\ncurve(lgt, -6, 6, \n      xlab = \"x (before transform)\", \n      ylab = \"z (after transform)\",\n      main = \"The Logistic Transformation\"\n      )\n\n\n\n\n\n\nIdentity Transformation\n\n\n\n\n\n\n\nLogistic Transformation\n\n\n\n\n\n\nThe usual input to the transformation function \\(g(.)\\) is a sum of products. For three variables, for example, this could be \\(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2\\). In matrix algebra this generalises to \\(\\boldsymbol{X\\beta}\\) , where \\(\\boldsymbol{X}\\) is the predictor data whose rows are observations, columns are variables, and whose first column is a vector of 1s (for the intercept term). The \\(\\boldsymbol{\\beta}\\) term is a row-wise vector comprising each specific \\(\\beta\\) term, such as \\(\\boldsymbol{\\beta} = \\{ \\beta_0, \\beta_1, \\beta_2 \\}\\) in the three variable example above.\nWhat’s special about the identity transformation, and so linear regression, is that there is a fairly clear correspondence between a \\(\\beta_j\\) term and the estimated influence of changing a predictor variable \\(x_j\\) on the predicted outcome \\(Y\\), i.e. the ‘effect of \\(x_j\\) on \\(Y\\)’. For other transformations this tends to not be the case.\nWe’ll delve into how this is implemented in practice in part 3."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-02/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-02/index.html#footnotes",
    "title": "Part Two: Systematic components and link functions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsing some base R graphics functions as I’m feeling masochistic↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-10/index.html",
    "href": "posts/glms/lms-are-glms-part-10/index.html",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "",
    "text": "Within this series, parts 1-4 formed what we might call ‘section one’, and part 5-9 ‘section two’.\nSection one (re)introduced statistical models as siblings, children of a mother model which combines a systematic component (an equation with a \\(=\\) symbol in it) and a stochastic component (an equation with a \\(\\sim\\) in it, which can largely be read as ‘drawn from’). Part one provided a graphical representation of the challenge of model fitting from an algorithmic perspective, in which the parameters that go into the two component are tweaked and tweaked until some condition is met: usually that the discrepency between model predictions and observed outcomes are minimised some way. The two component mother model is largely equivalent to the concept of the generalised linear model: parts two and three explored this association a bit more. Part four demonstrated how, for statistical models other than standard linear regression, the kinds of answer one usually wants from a model are not readily apparent from the model coefficients themselves, and so careful use of model predictions, and calibration of the questions, are required to use models to answer substantivelly meaningful questions.\nSection two aimed to show how likelihood theory is used in practice in order to justify a loss function that algorithms can be used to try to ‘solve’.1 These loss functions and optimisation algorithms are usually called implicitly by statistical model functions, but we did things the hard way by building the loss function from scratch, and evoking the algorithms more directly, using R’s optim() function. As well as the pedagogical value (and bragging rights) of being able to create and fit statistical models directly, an additional benefit of using optim() (with some of its algorithms) is that it returns something called the Hessian. The Hessian is what allows us to be honest when making model predictions and projections, showing how our uncertainty about the true value of the model parameters (the multiple inputs that optim() algorithms try to tweak until they’re good enough) leads to uncertainty in what we’re predicting and projecting.\nUnfortunately, we’re still in section two. The material below aims to repeat the same kind of exercise performed for standard linear regression, but using logistic regression instead."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-10/index.html#recap",
    "href": "posts/glms/lms-are-glms-part-10/index.html#recap",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "",
    "text": "Within this series, parts 1-4 formed what we might call ‘section one’, and part 5-9 ‘section two’.\nSection one (re)introduced statistical models as siblings, children of a mother model which combines a systematic component (an equation with a \\(=\\) symbol in it) and a stochastic component (an equation with a \\(\\sim\\) in it, which can largely be read as ‘drawn from’). Part one provided a graphical representation of the challenge of model fitting from an algorithmic perspective, in which the parameters that go into the two component are tweaked and tweaked until some condition is met: usually that the discrepency between model predictions and observed outcomes are minimised some way. The two component mother model is largely equivalent to the concept of the generalised linear model: parts two and three explored this association a bit more. Part four demonstrated how, for statistical models other than standard linear regression, the kinds of answer one usually wants from a model are not readily apparent from the model coefficients themselves, and so careful use of model predictions, and calibration of the questions, are required to use models to answer substantivelly meaningful questions.\nSection two aimed to show how likelihood theory is used in practice in order to justify a loss function that algorithms can be used to try to ‘solve’.1 These loss functions and optimisation algorithms are usually called implicitly by statistical model functions, but we did things the hard way by building the loss function from scratch, and evoking the algorithms more directly, using R’s optim() function. As well as the pedagogical value (and bragging rights) of being able to create and fit statistical models directly, an additional benefit of using optim() (with some of its algorithms) is that it returns something called the Hessian. The Hessian is what allows us to be honest when making model predictions and projections, showing how our uncertainty about the true value of the model parameters (the multiple inputs that optim() algorithms try to tweak until they’re good enough) leads to uncertainty in what we’re predicting and projecting.\nUnfortunately, we’re still in section two. The material below aims to repeat the same kind of exercise performed for standard linear regression, but using logistic regression instead."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-10/index.html#log-likelihood-for-logistic-regression",
    "href": "posts/glms/lms-are-glms-part-10/index.html#log-likelihood-for-logistic-regression",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "Log likelihood for logistic regression",
    "text": "Log likelihood for logistic regression\nPreviously we focused on the log likelihood for standard linear regression. Let’s now do the same for logistic regression. According to the relevant section of the Zelig website:\nStochastic component \\[\nY_i \\sim Bernoulli(y_i | \\pi_i )\n\\]\n\\[\nY_i = \\pi_i^{y_i}(1 - \\pi_i)^{1-y_i}\n\\]\nwhere \\(\\pi_i = P(Y_i = 1)\\)\nAnd\nSystematic Component\n\\[\n\\pi_i = \\frac{1}{1 + \\exp{(-x_i \\beta)}}\n\\]\nThe likelihood is the product of the above for all observations in the dataset \\(i \\in N\\)\n\\[\nL(.) = \\prod{\\pi_i^{y_i}(1 - \\pi_i)^{1-y_i}}\n\\]\nThe effect of logging the above2:\n\\[\n\\log{L(.)} = \\sum{[y_i \\log{\\pi_i} + (1-y_i)\\log{(1-y_i)}]}\n\\]\nThis can now be implemented as a function:\n\n\nCode\nllogit &lt;- function(par, y, X){\n    xform &lt;- function(z) {1 / (1 + exp(-z))}\n    p &lt;- xform(X%*%par)\n    sum(y * log(p) + (1-y) * log(1 - p))\n}\n\n\nLet’s pick an appropriate dataset. How about… picking a Palmer Penguin!?\n\n\nCode\nlibrary(tidyverse)\npalmerpenguins::penguins\n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nLet’s say we want to predict whether a penguin is of the Chinstrap species\n\n\nCode\npalmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    ggplot(aes(x = bill_length_mm, y = bill_depth_mm, colour = is_chinstrap, shape = sex)) + \n    geom_point()\n\n\n\n\n\nNeither bill length nor bill depth alone appears to distinguish between chinstrap and other species. But perhaps the interaction (product) of the two terms would do:\n\n\nCode\npalmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    mutate(bill_size = bill_length_mm * bill_depth_mm) |&gt;\n    ggplot(aes(x = bill_size, fill = is_chinstrap)) + \n    facet_wrap(~sex) + \n    geom_histogram()\n\n\n\n\n\nThe interaction term isn’t great at separating the two classes, but seems to be better than either length or size alone. So I’ll include it in the model.\n\n\nCode\ndf &lt;- palmerpenguins::penguins %&gt;%\n    filter(complete.cases(.)) |&gt;\n    mutate(is_chinstrap = species == \"Chinstrap\") |&gt;\n    mutate(bill_size = bill_length_mm * bill_depth_mm) |&gt;\n    mutate(is_male = as.numeric(sex == \"male\"))\n\ny &lt;- df$is_chinstrap\n\nX &lt;- cbind(1, df[,c(\"bill_length_mm\", \"bill_depth_mm\", \"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\n\nSo, including the intercept term, our predictor matrix \\(X\\) contains 5 columns, including the interaction term bill_size. 3\nLet’s try now to use the above in optim()\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = rep(0, 5), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] 82.9075239 -2.4368673 -6.4311531  0.1787047 -6.4900678\n\n$value\n[1] -33.31473\n\n$counts\nfunction gradient \n     137       45 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n             [,1]         [,2]          [,3]         [,4]         [,5]\n[1,]   -12.103063    -550.0621    -209.30944    -9674.925    -3.700623\n[2,]  -550.062097  -25256.3082   -9500.55848  -443670.225  -184.360139\n[3,]  -209.309443   -9500.5585   -3650.65107  -168517.417   -68.158844\n[4,] -9674.924703 -443670.2251 -168517.41718 -7846293.352 -3464.964868\n[5,]    -3.700623    -184.3601     -68.15884    -3464.965    -3.700623\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n            [,1]         [,2]         [,3]          [,4]         [,5]\n[1,] 41.95816335 -0.156192235 -0.309892876 -4.036895e-02  9.329019450\n[2,] -0.15619224 -0.005017392 -0.024806420  1.070652e-03 -0.139430425\n[3,] -0.30989288 -0.024806420 -0.042869947  2.854565e-03 -0.337480429\n[4,] -0.04036895  0.001070652  0.002854565 -7.331214e-05  0.003098092\n[5,]  9.32901945 -0.139430425 -0.337480429  3.098092e-03  1.202424836\n\n\nNow let’s compare with glm()\n\n\nCode\nmod_glm &lt;- glm(is_chinstrap ~ bill_length_mm * bill_depth_mm +is_male, data = df, \nfamily = binomial())\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = is_chinstrap ~ bill_length_mm * bill_depth_mm + \n    is_male, family = binomial(), data = df)\n\nCoefficients:\n                             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                  365.2924    88.3341   4.135 3.54e-05 ***\nbill_length_mm                -8.9312     2.0713  -4.312 1.62e-05 ***\nbill_depth_mm                -23.6184     5.5003  -4.294 1.75e-05 ***\nis_male                      -11.8725     2.6121  -4.545 5.49e-06 ***\nbill_length_mm:bill_depth_mm   0.5752     0.1292   4.452 8.53e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 337.113  on 332  degrees of freedom\nResidual deviance:  49.746  on 328  degrees of freedom\nAIC: 59.746\n\nNumber of Fisher Scoring iterations: 9\n\n\nUh oh! On this occasion it appears one or both approaches have become confused. A five dimensional search space might be too much for the algorithms to cope with, especially with collinearity 4 between some of the terms. Let’s simplify the task a bit, and just use intercept, bill size, and is_male as covariates. First with the standard package:\n\n\nCode\nmod_glm_simpler &lt;- glm(is_chinstrap ~ bill_size +is_male,   data = df, \nfamily = binomial())\nsummary(mod_glm_simpler)\n\n\n\nCall:\nglm(formula = is_chinstrap ~ bill_size + is_male, family = binomial(), \n    data = df)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -32.815339   4.325143  -7.587 3.27e-14 ***\nbill_size     0.043433   0.005869   7.400 1.36e-13 ***\nis_male      -7.038215   1.207740  -5.828 5.62e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 337.11  on 332  degrees of freedom\nResidual deviance:  90.60  on 330  degrees of freedom\nAIC: 96.6\n\nNumber of Fisher Scoring iterations: 7\n\n\nAnd now with the bespoke function and optim\n\n\nCode\nX &lt;- cbind(1, df[,c(\"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\nfuller_optim_output &lt;- optim(\n    par = rep(0, 3), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] -32.60343219   0.04314546  -6.98585077\n\n$value\n[1] -45.30114\n\n$counts\nfunction gradient \n      73       18 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]         [,2]         [,3]\n[1,]    -13.008605   -10662.078    -5.201308\n[2,] -10662.078251 -8846787.584 -4846.390833\n[3,]     -5.201308    -4846.391    -5.201308\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n             [,1]          [,2]         [,3]\n[1,] -536.7022079  0.7206703142 -134.7923170\n[2,]    0.7206703 -0.0009674672    0.1807806\n[3,] -134.7923170  0.1807806218  -33.4602664\n\n\nThe estimates from the two approaches are now much closer, even if they aren’t as close to each other as in the earlier examples. Using optim(), we have parameter estimates \\(\\beta = \\{\\beta_0 = -32.60, \\beta_1 = 0.04, \\beta_2 = -6.99\\}\\), and using glm(), we have estimates \\(\\beta = \\{\\beta_0 = -32.82, \\beta_1 = 0.04, \\beta_2 = -7.04 \\}\\)\nIf we cheat a bit, and give the five dimensional version starting values closer to the estimates from glm(), we can probably get similar estimates too.\n\n\nCode\nX &lt;- cbind(1, df[,c(\"bill_length_mm\", \"bill_depth_mm\", \"bill_size\", \"is_male\")]) |&gt;\nas.matrix()\n\nfuller_optim_output &lt;- optim(\n    par = c(300, -10, -29, 0.5, -10), \n    fn = llogit,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1] 299.5512512  -7.3684567 -19.3951742   0.4747209  -9.7521255\n\n$value\n[1] -25.33208\n\n$counts\nfunction gradient \n     153       22 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n             [,1]          [,2]          [,3]         [,4]         [,5]\n[1,]    -8.378918    -370.41592    -140.86865    -6342.301    -1.800406\n[2,]  -370.415921  -16580.87909   -6238.75358  -284403.350   -91.239716\n[3,]  -140.868648   -6238.75358   -2387.19776  -107598.410   -33.018551\n[4,] -6342.300809 -284403.34960 -107598.40987 -4906697.476 -1685.235507\n[5,]    -1.800406     -91.23972     -33.01855    -1685.236    -1.800406\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n            [,1]         [,2]        [,3]          [,4]         [,5]\n[1,] -59.5448267  2.316365876  5.14842594 -0.1737609491 10.383684649\n[2,]   2.3163659 -0.064512887 -0.16844980  0.0044962968 -0.166413655\n[3,]   5.1484259 -0.168449797 -0.33888931  0.0106735535 -0.387558164\n[4,]  -0.1737609  0.004496297  0.01067355 -0.0002712683  0.004068597\n[5,]  10.3836846 -0.166413655 -0.38755816  0.0040685965  1.904433768\n\n\nWell, they are closer, but they aren’t very close. As mentioned, the glm() model produced warnings, and some of the variables are likely to be collinear, so this initial specification may have been especially difficult to fit. Both approaches found an answer, but neither seem happy about it!"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-10/index.html#summary",
    "href": "posts/glms/lms-are-glms-part-10/index.html#summary",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "Summary",
    "text": "Summary\nIn the exercise above we did for logistic regression what the previous few posts in section two did for standard regression: i.e. we derived the log likelihood, applied it using optim, and compared with results from the glm() package. We saw in this case that fitting models isn’t always straightforward. We were - well, I was - overly ambitious in building and applying an overly parameterised model specification. But we eventually got to similar parameter values using both approaches.\nThough this wasn’t as straightforward as I was hoping for, I’m presenting it warts-and-all. In principle, the log-likelihood maximisation approach generalises to a great many model specifications, even if in practice some model structures aren’t as straightforward to fit as others."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-10/index.html#coming-up",
    "href": "posts/glms/lms-are-glms-part-10/index.html#coming-up",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "Coming up",
    "text": "Coming up\nIn the next few posts, I’ll finally be moving off ‘section two’, with its algebra and algorithms, and showing some tools that can be used to make honest prediction and projections with models, but without all the efforts undertaken here and in the last few posts."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-10/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-10/index.html#footnotes",
    "title": "Part Ten: Log Likelihood estimation for Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBy ‘loss function’ I mean a function that takes one or more numeric inputs and returns a single numeric output. The aim of the algorithm is to find the combination of inputs that minimises (or maximises) the function’s output.↩︎\nThanks to this post. My calculus is a bit rusty these days.↩︎\nAn important point to note is that, though bill_size is derived from other variables, it’s its own variable, and so has another distinct ‘slot’ in the vector of \\(\\beta\\) parameters. It’s just another dimension in the search space for optim to search through.↩︎\nThis is fancy-speak for when two terms aren’t independent, or both adding unique information. For example, length in mm, length in cm, and length in inches would all be perfectly collinear, so shouldn’t all be included in the model.↩︎"
  },
  {
    "objectID": "posts/changing-tenure-in-scotland/index.html",
    "href": "posts/changing-tenure-in-scotland/index.html",
    "title": "Changing tenure in Scotland",
    "section": "",
    "text": "As it’s something I’ve done already for work, but it’s all using public domain data, here’s a link to some charts showing how housing tenure in Scotland has changed over time. As Social Housing stock went down, private rental stock went up."
  },
  {
    "objectID": "posts/x-minute-neighbourhoods/index.html",
    "href": "posts/x-minute-neighbourhoods/index.html",
    "title": "Why such pushback against 20 minute neighbourhoods?",
    "section": "",
    "text": "I had the privilege yesterday of hearing a series of talks by researchers at the University of Glasgow on 20 minute neighbourhoods.1 The talks covered areas like evidence surveys of health associations, GIS methods,2 and engagement with historical amenities.\nI was vaguely aware that this kind of initiative is sometimes conflated with (ultra) low emission zones, and in recent years sometimes receives a hostile response from some audiences. So, in the Q&A, I asked if this had been their experience, what they think the causes of the hositility were, and what (if anything) is best to do about it.\nThe researchers had encountered such responses, and the coordinator sent a link to a youtube video introducing the research they were involved in. Only around 2% of those who viewed the video decided to comment on it, but it surprised me that the vast majority did express the kind of hostility I was thinking about. The top few comments are indicative:\nOnly by the fifth ranked comment is there a response broadly supportive of the initiative, though is more lamenting than hopeful:\nOn the possible reasons for such responses, the researchers suggests that COVID may be a factor. On what to do about it, there was less clarity, except to be mindful that many people may not change their mind on such issues, so engaging with them might not be worth the time involved.\nThe COVID explanation definitely seems part of it, and is evident in some of the examples above. 2020 and 2021 was a confusing time, and the popularity of conspiracy theories which offered ‘answers’ seems to have grown as a result. Within the conspiracy theory linking Lockdown to ULEZ and walkable neighbourhoods, Lockdown was a dress rehearsal, an attempt to understand just how pliant and willing to give up on hard earned freedoms the populace at large would be when told such restrictions were necessary and temporary. Initiatives like Walkable Neighbourhoods are then framed as something like ‘the next phase’, initiatives which curtail freedom on a permanent rather than temporary basis, with the ultimate endpoint being something like ‘prison cities’, where everyone is controlled and monitored at all times in some kind of Orwellian nightmare.\nClearly, there seems to be a lot of imputation and extrapolation involved in getting from ‘being able to walk to school while passing some nice buildings’ to 1984. But perhaps having a preexisting set of assumptions, which link driving to freedom and so walking to tyranny, is something that makes people more susceptible to the conspiratorial way of thinking outlined above. Let’s consider this some more.\nIn surveys of household affluence from decades gone by, my understanding3 is that some surveys used to ask UK adults questions along the lines of: “How many cooked meals with meat did you eat in the last week?” The idea of such questions was that, if people could afford to eat more meat, they would do. Such questions were considered unobtrusive measures of individual and household means, because the individual wants, to eat as much meat as one could afford to do so, was simply taken as given.\nA few years ago Gapminder generalised something like this principle to international development, providing simple but graphic illustrations of how what people eat, drink, and use as transport varies across four very broad income levels. I’ve made this illustration the main image for this blog post.\nIf we look at income level 1, under $2 a day, people are obligate walkers, and they’re likely to be obligate vegans, relying on a simple grain to survive. As they reach higher levels, they start to be able to afford to augment their simple stable dish with vegetables, spices and meat. And they start to move from walking, to being able to afford a bicycle, then a motorcycle, then finally a car. The changing transport mode is presented as what people move onto when they can afford to do so, with each form presenting new found physical freedoms to go along with the new found financial freedoms their higher income level now affords them.\nMy suspicion is that many people who adopted the kind of conspiracy theory sketched above, which leads to the kind of hostile comments to the kind of walkability initiatives being discussed, did so because they internalised something like the Gapminder model of development both too deeply and too crudely. In particular, they conflate driving with money and freedom, and so not driving with poverty and restriction. I suspect it’s easier to subscribe to the car=freedom equation when you have direct experience of not being able to afford to own or run a car, of driving being a genuine hard-won freedom. In social epi parlance, I suspect there’s likely to be a socioeconomic gradient in hostility to walkability initiatives, as for many poorer people the idea of not being allowed to do something you can only just afford to do (and want to do in large part because you can only just afford to do), would seem inherently perverse.\nPersonally, as a city-dwelling vegetarian, my intuitions are all in support of Walkability initiatives. I’m just trying to be mindful of how those with different circumstances may look at the same things, but see something very different!"
  },
  {
    "objectID": "posts/x-minute-neighbourhoods/index.html#footnotes",
    "href": "posts/x-minute-neighbourhoods/index.html#footnotes",
    "title": "Why such pushback against 20 minute neighbourhoods?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis generalises to ‘x minute neighbourhoods’, e.g. 15 minute neighbourhoods, 30 minute neighbourhoods, 10 minute neighbourhoods.↩︎\nIn practice this seemed to be the identification of whether and how many common amenities are within 800 metres of someone’s home.↩︎\nPlease correct me if I’m wrong, or provide an example or two if I’m not. I wasn’t able to find an example dataset before, so might be confabulating this!↩︎"
  },
  {
    "objectID": "posts/socatic-dialogue-part-01/index.html",
    "href": "posts/socatic-dialogue-part-01/index.html",
    "title": "Nerdy Dialogues on Life and Death",
    "section": "",
    "text": "Two cats, Emu and Goose. Goose is invading Emu’s personal space slightly\nHere’s an attempt to think some more about how and why standardised rates are used to compare populations. I’m doing so via the medium of a Socratic Dialogue1."
  },
  {
    "objectID": "posts/socatic-dialogue-part-01/index.html#why-age-standardise-a-dialogue",
    "href": "posts/socatic-dialogue-part-01/index.html#why-age-standardise-a-dialogue",
    "title": "Nerdy Dialogues on Life and Death",
    "section": "Why age standardise? A dialogue",
    "text": "Why age standardise? A dialogue\nMore than twice as many deaths are reported in population A than population B\nFirst question: Does population A have about twice the population as population B?\nOkay. I’ve got the number of deaths in population A, \\(n_A\\), and the population size in population A, \\(N_A\\). So, I’ve calculated the rate \\(r_A = \\frac{n_A}{N_A}\\) for population A, and done the same for population B, \\(r_B = \\frac{n_B}{N_B}\\).\nOkay… so what’s the ratio of \\(r_A\\) to \\(r_B\\)?\nIt’s 1.4. So, the mortality rate in population A is 40% higher than in population B\nAnd what does that mean?\nPopulation A are exposed to more of something bad, or maybe less to something good, than those in population B.\nPossibly. What if I told you that the mortality rate in a care home was 40% higher than the mortality rate in a combat unit fighting on the front line? If the differences in rates is just due to differences in exposures, surely if we were to move the people in the care home into the combat unit, the differences in mortality rates between the two populations should disappear?\nThat doesn’t sound right. I think the mortality rates of the care home population would be even higher if they were in the combat unit.\nAnd what does that imply?\nDifferences in health outcomes between populations can be due to differences in the characteristics of the populations being compared, as well as differences in the exposures the two populations encounter\nExactly. And what are the main differences in characteristics between the care home population and the combat unit population likely to be?\nI’d expect the combat unit population to be much younger than the care home population. I’d also expect the combat unit population to be overwhelmingly male, whereas the care home population might be more mixed, but perhaps skewed more towards females than males.\nGood. So what does this mean for methodology?\nWe need to look to compare like-with-like when trying to work out how much of a difference in health outcome is due to differences in exposures. At the very least, we should try to compare like-with-like on age and sex, as these are very important determinants of mortality risk.\nGreat. So, instead of just a single ratio to compare between populations, we can compare a load of ratios, one for each combination of age and sex we’ve got common data for. So, the ratio of mortality rates in 25 year old females, 37 year old males, 60 year old females, 82 year old males, and so on…\nMaybe…\nIf we’ve got males and females, each by age in single year up to age 90, that means we have almost 200 such ratios to compare. Any difficulties with that?\nI guess that makes it hard to see the wood for the trees, one or two numbers is easier to convey than one hundred or two hundred.\n…\nSo I guess we need some way of summarising this further, making sure the summary measure presented is a reasonable summary of all of the like-with-like comparisons we’ve got?\nYes. What might be some ways of doing this?\nI guess we could do something like the mean or median value of these age-sex specific ratios??\nFrom first principles, that doesn’t seem like a terrible idea. However it would have some problems.\nSuch as?\nFor example, if a few ratios are based on small numbers of deaths and population counts, they could be very big or very small due to sample estimation issues alone. This would be more of an issue if using the mean than the median.\nAlso all subpopulations’ estimates would contribute equally to such a summary measure, even if some subpopulations contribute much more to the overall health outcome in the population than others.\nSo, what are some better alternatives?\nWell, I guess, for overall mortality, you could use life expectancy.\nAh, we’re all familiar with that.\nMaybe not as familiar as you think you are. It’s less straightforward to calculate and interpret than you might think. For example, imagine it’s 1890, and life expectancy is 51 years of age. You’re 31 years old. How long can you expect to live?\nTwenty more years? More if I’m careful\nProbably quite a bit longer. ‘Life expectancy’ \\(e\\) is usually used as shorthand for ‘period life expectancy from birth’, \\(e_0\\), or ‘unconditional period life expectancy’. Historically, the first year of life was one of the most dangerous ages to be alive.2 It’s a tall and weighty hurdle to cross. But as a 31 year old you’ve already crossed it. Your life expectancy isn’t ‘unconditional’ life expectancy from birth, \\(e_0\\), but ‘conditional’ life expectancy from age 31, \\(e_{31}\\).\nWhat does this mean?\nSay there were 10,000 people running the obstacle course of life, and they’re starting at the start of the course. The further you follow the course along, the fewer people reach each stage. Life expectancy at birth \\(e_0\\), heuristically, answers the question “at what stage in the course should you expect there’ll only be 5,000 of the original contestants remaining, on average?”\nAnd \\(e_{31}\\)?\nAlmost the same, except instead of the 10,000 people starting at the start of the course, they’re all allowed to start at stage 31 instead.\nAh! I think I see now why \\(e_{31}\\) should be greater than \\(e_0\\)!\nYes, and as mentioned it used to be a lot higher, because the first stage used to be one of the toughest.\nOkay. I think I understand why life expectancy isn’t a completely straightforward concept. Let’s go into the weeds even further. Why did you refer to life expectancy as period life expectancy? What’s the alternative?\nWell, let’s keep with the obstacle course analogy. Imagine two more things…\nOkay, but you’re hurting my brain.\n… Firstly, that the 10,000 contestants aren’t all the contestants. Instead, they’re just one of a series of cohorts of contestants. Every fifteen minutes, say, another 10,000 contestants are lined up at the starting block and, when their starting pistol goes, they start the course.\nSounds pretty crowded…\nYes. None of this is practically possible; we also have to assume everyone runs at the same rate for the analogy to work.\nAnyway, the second big thing to imagine is that the designers of the obstacle course are constantly redesigning it. They’re making some of the hurdles higher, and other hurdles lower, and they’re doing this all the time.\nAh, so the obstacle course is never the same for any two cohorts who traverse it?\nExactly! You’ve got it!3\nSo, it’s cohorts who traverse the course, but you said life expectancy is usually period life expectancy. What does this mean?\nA period life expectancy is like taking a snapshot, or just a few seconds-long clip, of people who are currently on the obstacle course, at all stages, and using this information to try to work out how far a cohort, starting at the start of the course, would likely get along the course, if the obstacle course never changed while the cohort is traversing the course.\nThat sounds like an important caveat, given you just said the course is always changing.\nIt certainly is. It’s for this reason a period life expectancy is sometimes also called a synthetic cohort life expectancy, because the cohort imagined to traverse the obstacle course doesn’t actually exist, but is made up of different pieces of different cohorts at different stages of the course.\nSo, why not use a real cohort?\nTwo reasons: Relevance, and data.\nGo on..\nEither you have a completed cohort, where there’s the data, but not the relevance. Or you have an incomplete cohort, where there’s the relevance, but not the data.\nI think I understand: The completed cohort completed the obstacle course, especially the start of it, when it was very different to how it is now, so although the data’s complete, much of it doesn’t speak to the current challenges on the course. And for the incomplete cohort, as they’ve not yet reached all the hurdles, we can’t yet know how many of them will reach each stage.\nCorrect, and correct. Two reasons why period life expectancies are usually used, even though they’re based on some pretty weird fictions.\nThis whole dialogue is a weird fiction. Shall we call it a night?\nIndeed we shall!"
  },
  {
    "objectID": "posts/socatic-dialogue-part-01/index.html#footnotes",
    "href": "posts/socatic-dialogue-part-01/index.html#footnotes",
    "title": "Nerdy Dialogues on Life and Death",
    "section": "Footnotes",
    "text": "Footnotes\n\n\ni.e. two people talking about an idea, one of whom thinks they know more than the other one, though, very annoyingly, insists they don’t.↩︎\nExisting starts with a boss fight.↩︎\nMy Socrates doesn’t intend this to sound condescending, but it still does a bit.↩︎"
  },
  {
    "objectID": "posts/tidy-tuesday-dr-who/index.html",
    "href": "posts/tidy-tuesday-dr-who/index.html",
    "title": "Tidy Tuesday on Dr Who",
    "section": "",
    "text": "First we load the packages\nThe tidyverse equivalent of pacman is now pak.\nThe latest dataset is here, and the specific files to work.\n\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndta_list &lt;- tidytuesdayR::tt_load(x = \"2023-11-28\")\n\n--- Compiling #TidyTuesday Information for 2023-11-28 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `drwho_episodes.csv`\n    Downloading file 2 of 3: `drwho_directors.csv`\n    Downloading file 3 of 3: `drwho_writers.csv`\n\n\n--- Download complete ---\n\ndta_eps &lt;- dta_list[[\"drwho_episodes\"]]\ndta_wrt &lt;- dta_list[[\"drwho_writers\"]]\n\nLet’s see how the viewship changed over time\n\ndta_eps_season &lt;- \n  dta_eps |&gt; \n  group_by(season_number) |&gt; \n  mutate(\n    mean_viewers = mean(uk_viewers),\n    mean_date = mean(first_aired)\n    ) |&gt; \n  ungroup()\n\ndta_eps_season |&gt; \n  ggplot(aes(x = first_aired, y = uk_viewers)) + \n  geom_point(colour = \"grey\") +\n  geom_point(aes(x = mean_date, y = mean_viewers), size = 2.5) + \n  scale_x_date(breaks = \"2 years\", labels = \\(x) format(x, \"%Y\")) +\n  labs(\n    x = \"First aired\",\n    y = \"UK Viewers (millions)\",\n    title = \"Viewers over time for Dr Who\",\n    subtitle = \"People don't watch TV like they used to...\"\n  ) +\n  annotate(\"text\", x = lubridate::make_date(2015), y = 10, label = \"What happened here?!\") +\n  annotate(\"text\", x = lubridate::make_date(2014), y= 8, label = \"Smartphone strangling the TV from now\", hjust = 0) + \n  stat_smooth(colour = \"blue\", se = FALSE) \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nLet’s now look at writers by season\n\ndta_eps_wrt &lt;- \n  dta_eps |&gt; \n    left_join(dta_wrt, by = \"story_number\") \n\nHow many episodes by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  summarise(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  arrange(desc(n_written))\n\n# A tibble: 40 × 2\n   writer           n_written\n   &lt;chr&gt;                &lt;int&gt;\n 1 Steven Moffat           45\n 2 Russell T Davies        31\n 3 Chris Chibnall          29\n 4 Mark Gatiss              9\n 5 Toby Whithouse           7\n 6 Gareth Roberts           5\n 7 Helen Raynor             4\n 8 Jamie Mathieson          4\n 9 Peter Harness            4\n10 Matthew Graham           3\n# ℹ 30 more rows\n\n\nSo Moffat wrote most episodes, then Davies, then Chibnall\nAnd what about popularity by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  mutate(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  filter(n_written &gt;= 5) |&gt; \n  ggplot(aes(x = fct_reorder(writer, rating), y= rating)) + \n  geom_boxplot() + \n  coord_flip() + \n  labs(\n    x = \"Distribution of ratings\",\n    y = \"Writer\", \n    title = \"Rating distribution by writer\",\n    subtitle = \"Writers who wrote at least five episodes\"\n  )\n\n\n\n\nWhen were the different writers active?\n\nmajor_writers_active &lt;- \n  dta_eps_wrt |&gt; \n    group_by(writer) |&gt;  \n    mutate(\n      n_written = n()\n    ) |&gt; \n    ungroup() |&gt; \n    filter(n_written &gt;= 5) |&gt; \n    group_by(writer) |&gt; \n    summarise(\n      started_writing = min(first_aired),\n      finished_writing = max(first_aired),\n      n_written = n_written[1]\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(\n      yr_start = year(started_writing),\n      yr_end = year(finished_writing)\n    )\n\nmajor_writers_active |&gt; \n  arrange(started_writing)\n\n# A tibble: 6 × 6\n  writer           started_writing finished_writing n_written yr_start yr_end\n  &lt;chr&gt;            &lt;date&gt;          &lt;date&gt;               &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Russell T Davies 2005-03-26      2010-01-01              31     2005   2010\n2 Mark Gatiss      2005-04-09      2017-06-10               9     2005   2017\n3 Steven Moffat    2005-05-21      2017-12-25              45     2005   2017\n4 Toby Whithouse   2006-04-29      2017-06-03               7     2006   2017\n5 Gareth Roberts   2007-04-07      2011-09-24               5     2007   2011\n6 Chris Chibnall   2007-05-19      2022-10-23              29     2007   2022\n\n\nHere we see the tenure of different major writers. Russell T Davies and Steven Moffatt are the major players."
  },
  {
    "objectID": "posts/tidy-tuesday-dr-who/index.html#tidy-tuesday-challenge",
    "href": "posts/tidy-tuesday-dr-who/index.html#tidy-tuesday-challenge",
    "title": "Tidy Tuesday on Dr Who",
    "section": "",
    "text": "First we load the packages\nThe tidyverse equivalent of pacman is now pak.\nThe latest dataset is here, and the specific files to work.\n\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\ndta_list &lt;- tidytuesdayR::tt_load(x = \"2023-11-28\")\n\n--- Compiling #TidyTuesday Information for 2023-11-28 ----\n--- There are 3 files available ---\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 3: `drwho_episodes.csv`\n    Downloading file 2 of 3: `drwho_directors.csv`\n    Downloading file 3 of 3: `drwho_writers.csv`\n\n\n--- Download complete ---\n\ndta_eps &lt;- dta_list[[\"drwho_episodes\"]]\ndta_wrt &lt;- dta_list[[\"drwho_writers\"]]\n\nLet’s see how the viewship changed over time\n\ndta_eps_season &lt;- \n  dta_eps |&gt; \n  group_by(season_number) |&gt; \n  mutate(\n    mean_viewers = mean(uk_viewers),\n    mean_date = mean(first_aired)\n    ) |&gt; \n  ungroup()\n\ndta_eps_season |&gt; \n  ggplot(aes(x = first_aired, y = uk_viewers)) + \n  geom_point(colour = \"grey\") +\n  geom_point(aes(x = mean_date, y = mean_viewers), size = 2.5) + \n  scale_x_date(breaks = \"2 years\", labels = \\(x) format(x, \"%Y\")) +\n  labs(\n    x = \"First aired\",\n    y = \"UK Viewers (millions)\",\n    title = \"Viewers over time for Dr Who\",\n    subtitle = \"People don't watch TV like they used to...\"\n  ) +\n  annotate(\"text\", x = lubridate::make_date(2015), y = 10, label = \"What happened here?!\") +\n  annotate(\"text\", x = lubridate::make_date(2014), y= 8, label = \"Smartphone strangling the TV from now\", hjust = 0) + \n  stat_smooth(colour = \"blue\", se = FALSE) \n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nLet’s now look at writers by season\n\ndta_eps_wrt &lt;- \n  dta_eps |&gt; \n    left_join(dta_wrt, by = \"story_number\") \n\nHow many episodes by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  summarise(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  arrange(desc(n_written))\n\n# A tibble: 40 × 2\n   writer           n_written\n   &lt;chr&gt;                &lt;int&gt;\n 1 Steven Moffat           45\n 2 Russell T Davies        31\n 3 Chris Chibnall          29\n 4 Mark Gatiss              9\n 5 Toby Whithouse           7\n 6 Gareth Roberts           5\n 7 Helen Raynor             4\n 8 Jamie Mathieson          4\n 9 Peter Harness            4\n10 Matthew Graham           3\n# ℹ 30 more rows\n\n\nSo Moffat wrote most episodes, then Davies, then Chibnall\nAnd what about popularity by writer?\n\ndta_eps_wrt |&gt; \n  group_by(writer) |&gt;  \n  mutate(\n    n_written = n()\n  ) |&gt; \n  ungroup() |&gt; \n  filter(n_written &gt;= 5) |&gt; \n  ggplot(aes(x = fct_reorder(writer, rating), y= rating)) + \n  geom_boxplot() + \n  coord_flip() + \n  labs(\n    x = \"Distribution of ratings\",\n    y = \"Writer\", \n    title = \"Rating distribution by writer\",\n    subtitle = \"Writers who wrote at least five episodes\"\n  )\n\n\n\n\nWhen were the different writers active?\n\nmajor_writers_active &lt;- \n  dta_eps_wrt |&gt; \n    group_by(writer) |&gt;  \n    mutate(\n      n_written = n()\n    ) |&gt; \n    ungroup() |&gt; \n    filter(n_written &gt;= 5) |&gt; \n    group_by(writer) |&gt; \n    summarise(\n      started_writing = min(first_aired),\n      finished_writing = max(first_aired),\n      n_written = n_written[1]\n    ) |&gt; \n    ungroup() |&gt; \n    mutate(\n      yr_start = year(started_writing),\n      yr_end = year(finished_writing)\n    )\n\nmajor_writers_active |&gt; \n  arrange(started_writing)\n\n# A tibble: 6 × 6\n  writer           started_writing finished_writing n_written yr_start yr_end\n  &lt;chr&gt;            &lt;date&gt;          &lt;date&gt;               &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 Russell T Davies 2005-03-26      2010-01-01              31     2005   2010\n2 Mark Gatiss      2005-04-09      2017-06-10               9     2005   2017\n3 Steven Moffat    2005-05-21      2017-12-25              45     2005   2017\n4 Toby Whithouse   2006-04-29      2017-06-03               7     2006   2017\n5 Gareth Roberts   2007-04-07      2011-09-24               5     2007   2011\n6 Chris Chibnall   2007-05-19      2022-10-23              29     2007   2022\n\n\nHere we see the tenure of different major writers. Russell T Davies and Steven Moffatt are the major players."
  },
  {
    "objectID": "posts/tidy-tuesday-dr-who/index.html#coda",
    "href": "posts/tidy-tuesday-dr-who/index.html#coda",
    "title": "Tidy Tuesday on Dr Who",
    "section": "Coda",
    "text": "Coda\nNeither of us know much about Dr Who!\nBut hopefully we now know a bit more!"
  },
  {
    "objectID": "posts/tidy-tuesday-more-life-expectancy/index.html",
    "href": "posts/tidy-tuesday-more-life-expectancy/index.html",
    "title": "Tidy Tuesday on Life Expectancy - Part Two",
    "section": "",
    "text": "In the previous week’s TidyTuesday session, we looked at Life Expectancy.\nFor this week, the Tidy Tuesday dataset of the week was of Christmas films. However, as public health folks we felt more interested in continuing to look at life expectancy, so continued with the previous week’s dataset.\nThis session was led by Andrew Saul."
  },
  {
    "objectID": "posts/tidy-tuesday-more-life-expectancy/index.html#introduction",
    "href": "posts/tidy-tuesday-more-life-expectancy/index.html#introduction",
    "title": "Tidy Tuesday on Life Expectancy - Part Two",
    "section": "",
    "text": "In the previous week’s TidyTuesday session, we looked at Life Expectancy.\nFor this week, the Tidy Tuesday dataset of the week was of Christmas films. However, as public health folks we felt more interested in continuing to look at life expectancy, so continued with the previous week’s dataset.\nThis session was led by Andrew Saul."
  },
  {
    "objectID": "posts/tidy-tuesday-more-life-expectancy/index.html#script-and-outputs-from-session",
    "href": "posts/tidy-tuesday-more-life-expectancy/index.html#script-and-outputs-from-session",
    "title": "Tidy Tuesday on Life Expectancy - Part Two",
    "section": "Script and outputs from session",
    "text": "Script and outputs from session\nLoading some packages\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\n\n\nUse the tidytuesdayR package to load the data (rather than a direct link):\n\n\nCode\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-05')\n\n\n\n    Downloading file 1 of 3: `life_expectancy.csv`\n    Downloading file 2 of 3: `life_expectancy_different_ages.csv`\n    Downloading file 3 of 3: `life_expectancy_female_male.csv`\n\n\nPopulate the content of the list above into three separate datasets:\n\n\nCode\nle &lt;- tuesdata[[1]]\nle_diff &lt;- tuesdata[[2]]\nle_gender &lt;- tuesdata[[3]]\n\n\nHave a quick look at the data\n\n\nCode\nglimpse(le)\n\n\nRows: 20,755\nColumns: 4\n$ Entity         &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanist…\n$ Code           &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\",…\n$ Year           &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1…\n$ LifeExpectancy &lt;dbl&gt; 27.7275, 27.9634, 28.4456, 28.9304, 29.2258, 29.9206, 3…\n\n\nCode\nglimpse(le_diff)\n\n\nRows: 20,755\nColumns: 9\n$ Entity           &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghani…\n$ Code             &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG…\n$ Year             &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958,…\n$ LifeExpectancy0  &lt;dbl&gt; 27.7275, 27.9634, 28.4456, 28.9304, 29.2258, 29.9206,…\n$ LifeExpectancy10 &lt;dbl&gt; 49.1459, 49.2941, 49.5822, 49.8634, 49.9306, 50.4315,…\n$ LifeExpectancy25 &lt;dbl&gt; 54.4422, 54.5644, 54.7998, 55.0286, 55.1165, 55.4902,…\n$ LifeExpectancy45 &lt;dbl&gt; 63.4225, 63.5006, 63.6476, 63.7889, 63.8481, 64.0732,…\n$ LifeExpectancy65 &lt;dbl&gt; 73.4901, 73.5289, 73.6018, 73.6706, 73.7041, 73.8087,…\n$ LifeExpectancy80 &lt;dbl&gt; 83.7259, 83.7448, 83.7796, 83.8118, 83.8334, 83.8760,…\n\n\nCode\nglimpse(le_gender)\n\n\nRows: 19,922\nColumns: 4\n$ Entity               &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afg…\n$ Code                 &lt;chr&gt; \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", …\n$ Year                 &lt;dbl&gt; 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1…\n$ LifeExpectancyDiffFM &lt;dbl&gt; 1.261900, 1.270601, 1.288300, 1.306601, 1.276501,…\n\n\nThere are fields code and entity, where entity tends to be more verbose/descriptive. Entities include geographic regions, countries, economic groupings etc. (So fairly messy, definitely not mutally exclusive and exhaustive)\n\n\nCode\nle_diff %&gt;% \n  count(Entity) %&gt;% \n  pull(Entity)\n\n\nWe decided to look at a series of countries from across the world.\n\n\nCode\ncountries &lt;- c(\"Germany\", \"United Kingdom\", \"Saudi Arabia\", \"South Africa\",\n               \"South Korea\", \"Japan\", \"Vietnam\", \"Argentina\", \"Venezuela\", \"France\")\n\n\nToday we looked at life expectency in a selection of countries from 1900\n\n\nCode\nle1900 &lt;- le %&gt;% \n  filter(Entity %in% countries,\n         Year&gt;=1900) \n\nle1900 %&gt;% \n  ggplot(aes(x=Year, y=LifeExpectancy))+\n  geom_line()+\n  facet_wrap(vars(Entity))\n\n\n\n\n\nWe then looked at the change in life expectency per year\n\n\nCode\nle1900 %&gt;% \n  group_by(Entity) %&gt;% \n  mutate(lag_diff = LifeExpectancy - lag(LifeExpectancy, order_by = Year),\n         sign = lag_diff&gt;0) %&gt;% \n  ggplot(aes(x=Year, y=lag_diff))+\n  geom_point(aes(colour = sign))+\n  geom_hline(yintercept = 0)+\n  facet_wrap(vars(Entity))\n\n\n\n\n\nWe changed the axis magnification of each country, so that the changes were more readily observable\n\n\nCode\nle1900lag &lt;- le1900 %&gt;% \n  group_by(Entity) %&gt;% \n  mutate(lag_diff = LifeExpectancy - lag(LifeExpectancy, order_by = Year),\n         sign = lag_diff&gt;0)\n\n le1900lag %&gt;% \n  ggplot(aes(x=Year, y=lag_diff))+\n  geom_point(aes(colour = sign))+\n  geom_hline(yintercept = 0)+\n  facet_wrap(vars(Entity), scales = \"free_y\")\n\n\n\n\n\nFinally, we examined variability in the change of life expectency altered for UK, France and Germany. Here is can be seen that variability in life expectancy dramatically increased around the First and Second World Wars. Data for Germany was incomplete for this period.\nTo do this we made use of the slider package, and within this the slide_index function, to produce a rolling standard deviation of annual changes.\n\n\nCode\nlibrary(slider)\nle1900lag %&gt;% \n  arrange(Year) %&gt;% \n  filter(Entity %in% c(\"United Kingdom\", \"France\", \"Germany\")) %&gt;% \n  mutate(roll_sd = slide_index_dbl(lag_diff, Year, .before = 4, .after = 4, .f = sd, .complete = T)) %&gt;% \n  ggplot(aes(x=Year, y=roll_sd, color = Entity))+\n  geom_line()"
  },
  {
    "objectID": "posts/unattended-deaths/index.html",
    "href": "posts/unattended-deaths/index.html",
    "title": "A Deathly Silence",
    "section": "",
    "text": "Trends in R98/R99 deaths since 1990\n\n\nWhat does it mean when someone dies, and no one notices for days, weeks, or months on end?\nThe bodies, once found, will be decomposed to such an extent that no effective autopsy can be performed, and so no cause of death can be identified. Such deaths are then likely to be coded either as R98 (‘Unattended death’) or R99 (‘Other ill-defined and unknown causes of mortality’). Far from being ‘junk codes’, wouldn’t a sudden and sustained change in deaths coded this way (absent an obvious explanation, such as a change in coding practice) signal that something broader is afoot?\nWorking with Lu Hiam, an Oxford PhD student and former GP, and Theodore Estrin-Serlui, a histopathologist, I analysed trends in deaths with these codes, as compared with mortality trends overall in England & Wales.\nSuch codes are rarely used, but in England & Wales they sadly became many times more common over the 1990s and 2000s. Standardised mortality rates in the R98/R99 category became more than three and a half times a common between 1990 and 2010, even as general standardised mortality rates fell by around a third.\nFor every body found so decomposed that the R98/R99 category had to be used, there are usually many more that have been unattended for a few days, have started to decompose, but for which autopsy can still be successfully performed. If these deaths are the tip of the iceberg, the base of this iceberg may be a growing epidemic of loneliness and social isolation, of ever more people with connections to friends and family, with no one to turn to in times of crisis.\nOur paper, A Deathly Silence, has been published in the Journal of the Royal Society of Medicine, and received press coverage from a number of outlets."
  },
  {
    "objectID": "posts/tidy-tuesday-education-townsize/index.html",
    "href": "posts/tidy-tuesday-education-townsize/index.html",
    "title": "Tidytuesday 2024-01-23",
    "section": "",
    "text": "This week’s TidyTuesday used data from the UK ONS which was explored in the 2023 article ’Why do children and young people in smaller towns do better academically than those in larger towns?’."
  },
  {
    "objectID": "posts/tidy-tuesday-education-townsize/index.html#background",
    "href": "posts/tidy-tuesday-education-townsize/index.html#background",
    "title": "Tidytuesday 2024-01-23",
    "section": "",
    "text": "This week’s TidyTuesday used data from the UK ONS which was explored in the 2023 article ’Why do children and young people in smaller towns do better academically than those in larger towns?’."
  },
  {
    "objectID": "posts/tidy-tuesday-education-townsize/index.html#aims",
    "href": "posts/tidy-tuesday-education-townsize/index.html#aims",
    "title": "Tidytuesday 2024-01-23",
    "section": "Aims",
    "text": "Aims\nOur first aim was to try to replicate the headline finding from the article above: that children in smaller towns have better average educational outcomes than in larger towns. We also sought to replicate and improve on the ‘beeswarm’ plot used in the original article, and to look at other factors which may explain differences in educational qualifications."
  },
  {
    "objectID": "posts/tidy-tuesday-education-townsize/index.html#package-loading",
    "href": "posts/tidy-tuesday-education-townsize/index.html#package-loading",
    "title": "Tidytuesday 2024-01-23",
    "section": "Package loading",
    "text": "Package loading\n\nlibrary(tidyverse)\nlibrary(ggbeeswarm) # for the beeswarm plot"
  },
  {
    "objectID": "posts/tidy-tuesday-education-townsize/index.html#data",
    "href": "posts/tidy-tuesday-education-townsize/index.html#data",
    "title": "Tidytuesday 2024-01-23",
    "section": "Data",
    "text": "Data\n\nee &lt;- tidytuesdayR::tt_load('2024-01-23') |&gt;\n  purrr::pluck(1)\n\n\n    Downloading file 1 of 1: `english_education.csv`\n\n\npurrr::pluck(1) was used because the data contained only a single dataset, but by default the tt_load function returns a list. So, the pluck(1) function takes the first element of the list, which in this case is in effect turning the data into a dataframe."
  },
  {
    "objectID": "posts/tidy-tuesday-education-townsize/index.html#counting-towns-in-data",
    "href": "posts/tidy-tuesday-education-townsize/index.html#counting-towns-in-data",
    "title": "Tidytuesday 2024-01-23",
    "section": "Counting towns in data",
    "text": "Counting towns in data\n\nee |&gt;\n  count(size_flag, sort=T) |&gt;\n  knitr::kable(caption = \"Counts of small/med/city class\")\n\n\nCounts of small/med/city class\n\n\nsize_flag\nn\n\n\n\n\nSmall Towns\n662\n\n\nMedium Towns\n331\n\n\nLarge Towns\n89\n\n\nCity\n18\n\n\nInner London BUA\n1\n\n\nNot BUA\n1\n\n\nOther Small BUAs\n1\n\n\nOuter london BUA\n1\n\n\n\n\n\nThere are 662 small towns, 331 medium towns, and 89 large towns"
  },
  {
    "objectID": "posts/tidy-tuesday-education-townsize/index.html#removing-oddball-locations-and-londons-and-factoring",
    "href": "posts/tidy-tuesday-education-townsize/index.html#removing-oddball-locations-and-londons-and-factoring",
    "title": "Tidytuesday 2024-01-23",
    "section": "Removing oddball locations and Londons and factoring",
    "text": "Removing oddball locations and Londons and factoring\n\nee |&gt;\n  mutate(town_size = factor(size_flag, levels = c(\"Small Towns\", \"Medium Towns\", \"Large Towns\"), ordered=T)) |&gt;\n  filter(!is.na(town_size)) -&gt; ee_fact"
  },
  {
    "objectID": "posts/tidy-tuesday-education-townsize/index.html#summary-by-group",
    "href": "posts/tidy-tuesday-education-townsize/index.html#summary-by-group",
    "title": "Tidytuesday 2024-01-23",
    "section": "Summary by group",
    "text": "Summary by group\n\nee_fact |&gt;\n  group_by(town_size) |&gt;\n  summarise(count = n(),\n            `mean ed score` = mean(education_score),\n            `sd ed score` = sd(education_score),\n            se = `sd ed score`/count^0.5,\n            `total population` = sum(population_2011)) |&gt;\n  mutate(across(where(is.numeric), round, 3)) |&gt;\n  knitr::kable()\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 3)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\n\n\n\n\n\n\n\n\n\ntown_size\ncount\nmean ed score\nsd ed score\nse\ntotal population\n\n\n\n\nSmall Towns\n662\n0.297\n3.887\n0.151\n6880216\n\n\nMedium Towns\n331\n-0.253\n3.324\n0.183\n12213733\n\n\nLarge Towns\n89\n-0.811\n2.298\n0.244\n10466343"
  },
  {
    "objectID": "posts/tidy-tuesday-education-townsize/index.html#anova-for-smallmedlarge-towns",
    "href": "posts/tidy-tuesday-education-townsize/index.html#anova-for-smallmedlarge-towns",
    "title": "Tidytuesday 2024-01-23",
    "section": "ANOVA for small/med/large towns",
    "text": "ANOVA for small/med/large towns\nWe built a series of linear regression models, and used ANOVA to compare between them. A low p-value from ANOVA, when comparing two or more models that are ‘nested’, can be taken as a signal that the more complex/unrestricted of the models should be used.\n\nmod_base &lt;- lm(education_score ~ town_size, data = ee_fact)\nsummary(mod_base)\n\n\nCall:\nlm(formula = education_score ~ town_size, data = ee_fact)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.3246  -2.5270  -0.1996   2.3052  11.5749 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) -0.255991   0.151293  -1.692  0.09093 . \ntown_size.L -0.783553   0.288560  -2.715  0.00673 **\ntown_size.Q -0.003547   0.232530  -0.015  0.98783   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.615 on 1079 degrees of freedom\nMultiple R-squared:  0.009593,  Adjusted R-squared:  0.007758 \nF-statistic: 5.226 on 2 and 1079 DF,  p-value: 0.005513\n\nmod_dep &lt;- lm(education_score ~ town_size + income_flag, data = ee_fact)\nsummary(mod_dep)\n\n\nCall:\nlm(formula = education_score ~ town_size + income_flag, data = ee_fact)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.4402 -1.8983 -0.0131  1.8447  9.2254 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        -2.29448    0.14533 -15.788  &lt; 2e-16 ***\ntown_size.L                         0.51810    0.22501   2.303   0.0215 *  \ntown_size.Q                        -0.01572    0.17726  -0.089   0.9293    \nincome_flagLower deprivation towns  5.31339    0.19207  27.664  &lt; 2e-16 ***\nincome_flagMid deprivation towns    1.82601    0.23489   7.774 1.77e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.754 on 1077 degrees of freedom\nMultiple R-squared:  0.4261,    Adjusted R-squared:  0.424 \nF-statistic: 199.9 on 4 and 1077 DF,  p-value: &lt; 2.2e-16\n\nmod_dep2 &lt;- lm(education_score ~ town_size * income_flag, data = ee_fact)\nsummary(mod_dep2)\n\n\nCall:\nlm(formula = education_score ~ town_size * income_flag, data = ee_fact)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.576 -1.868 -0.049  1.788  9.090 \n\nCoefficients:\n                                               Estimate Std. Error t value\n(Intercept)                                    -2.19244    0.15155 -14.466\ntown_size.L                                     0.94843    0.28240   3.359\ntown_size.Q                                    -0.10774    0.24096  -0.447\nincome_flagLower deprivation towns              4.77144    0.30602  15.592\nincome_flagMid deprivation towns                1.59836    0.31953   5.002\ntown_size.L:income_flagLower deprivation towns -1.36508    0.60094  -2.272\ntown_size.Q:income_flagLower deprivation towns -0.11774    0.44806  -0.263\ntown_size.L:income_flagMid deprivation towns   -0.76520    0.61668  -1.241\ntown_size.Q:income_flagMid deprivation towns   -0.04965    0.48199  -0.103\n                                               Pr(&gt;|t|)    \n(Intercept)                                     &lt; 2e-16 ***\ntown_size.L                                    0.000811 ***\ntown_size.Q                                    0.654869    \nincome_flagLower deprivation towns              &lt; 2e-16 ***\nincome_flagMid deprivation towns               6.62e-07 ***\ntown_size.L:income_flagLower deprivation towns 0.023309 *  \ntown_size.Q:income_flagLower deprivation towns 0.792769    \ntown_size.L:income_flagMid deprivation towns   0.214935    \ntown_size.Q:income_flagMid deprivation towns   0.917974    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.749 on 1073 degrees of freedom\nMultiple R-squared:  0.4305,    Adjusted R-squared:  0.4262 \nF-statistic: 101.4 on 8 and 1073 DF,  p-value: &lt; 2.2e-16\n\nanova(mod_base, mod_dep, mod_dep2)\n\nAnalysis of Variance Table\n\nModel 1: education_score ~ town_size\nModel 2: education_score ~ town_size + income_flag\nModel 3: education_score ~ town_size * income_flag\n  Res.Df     RSS Df Sum of Sq        F  Pr(&gt;F)    \n1   1079 14097.2                                  \n2   1077  8168.3  2    5928.9 392.3729 &lt; 2e-16 ***\n3   1073  8106.8  4      61.5   2.0343 0.08749 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(mod_dep, mod_dep2)\n\nAnalysis of Variance Table\n\nModel 1: education_score ~ town_size + income_flag\nModel 2: education_score ~ town_size * income_flag\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1   1077 8168.3                              \n2   1073 8106.8  4    61.479 2.0343 0.08749 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe summary from mod_dep indicates that deprivation tertile, using the IMD income domain, may have more of an effect than town size, and in the opposite direction."
  },
  {
    "objectID": "posts/tidy-tuesday-education-townsize/index.html#beeswarm-plot",
    "href": "posts/tidy-tuesday-education-townsize/index.html#beeswarm-plot",
    "title": "Tidytuesday 2024-01-23",
    "section": "Beeswarm plot",
    "text": "Beeswarm plot\nWe reproduce the beeswarm plot from the original article, but colouring areas by income tertile:\n\nee_fact |&gt;\n  mutate(income_flag = factor(income_flag, levels = c(\"Lower deprivation towns\", \"Mid deprivation towns\", \"Higher deprivation towns\"))) |&gt;\n  ggplot(aes(x = town_size, y = education_score, color = income_flag)) +\n  geom_beeswarm() +\n  coord_flip() +\n  theme(legend.position = \"bottom\") +\n  scale_color_manual(values = c(\"#73b8fd\", \"#0068c6\", \"#003b7c\"))"
  },
  {
    "objectID": "posts/tidy-tuesday-education-townsize/index.html#conclusion",
    "href": "posts/tidy-tuesday-education-townsize/index.html#conclusion",
    "title": "Tidytuesday 2024-01-23",
    "section": "Conclusion",
    "text": "Conclusion\nWe were able to replicate the headline finding from the article, and the type of visualisation used. But we also identified area deprivation as an important (and likely a more important) determinant of education scores."
  },
  {
    "objectID": "posts/on-sweary-soaps/index.html",
    "href": "posts/on-sweary-soaps/index.html",
    "title": "On Sweary Soap Operas: A Concealed Television Genre",
    "section": "",
    "text": "This is not a doll\n\n\nBack in the 1980s I used to play with He-Man dolls. My favourite came pre-decomposed: Modulok and Multi-bot. Both were collections of interchangeable heads, torsos and limbs - alien parts for Modulok; robot parts for Multi-bot - that could joined up in more ways that a young boy could count. The two characters could be combined, creating oddly cute orgosynthetic monstrosities: HR Giger by Mattel circling Lego’s IP, as it were.\nNevertheless, if my father asked me if I was enjoying playing with my He-Man dolls, I’d be quick to correct him. “They’re not dolls!”, I’d tell him. “They’re action figures!” If my father then asked, “What’s the difference?”, I think I’d be ready to confabulate a distinction, usually related to the functionality - “press this button and it can talk or punch, or talk to who it’s punching” - or its durability. Given MultiModulokbot starts off broken up, and is intended to be torn limb from torso from head from limb, I may have had a point on the latter, but more through luck than judgement.\nI was thinking about the doll/action figure distinction recently after realising that, over the last few weeks, I’d been tricked into watching a couple of soap operas, and worse still occasionally even enjoying doing so. The first of these was Billions, which I initially watched hoping it would be to Hedge Funds what The Wire was to Street Gangs. With a top investigative journalist as one of its executive producers, and an assault course of legal and financial jargon to contend with from the first minute, I thought maybe it might at least reach the navel of the Wire’s mantel. But by around the fourth series I realised fictionalised socioethnographic investigation was never what Billions wanted to be, and if it had ever played with the idea of saying something meaningful about the ultrawealthy, it had no interest in this any more. No. Billions, I’d realised, was quite happy being something like Dynasty with F-bombs, and C-bombs, and BDSM, strung together mainly around a kind of baroque storyline involving a preposterous love triangle, or maybe a love chevron, or maybe a love human centipede. In Billions, various ludicrous hypermasculine archetypes, sometimes played by women or non-binary actors, act as if every decision they make is life-or-death, that they’re one step away from destroying each other, vanquishing their foes, and achieving ultimate victory. Yet there they are, two years later, three years later, all in one piece, none-the-worse, still acting as if they’re still dancing on an existential precipice, and that maybe this scheme is the one that will finally seal the deal. (It doesn’t.)\nMore recently, I’ve found myself watching Loudermilk, about a former music journalist and recovering alcoholic who’s also a straight-talking foul-mouthed misanthropist with a heart of gold. Secondary characters call Loudermilk Loundermilk repeatedly, as if to remind viewers what show they’re watching, and attractive young women seem to find him appealing for no obvious reason. Much of the show involves men in recovery talking to each other in a room, hiding their love and concern for each other inside superficially cruel and callous insults. Plot twists abound - an affair here, a visit from a long-lost relative there - but ultimately it’s still the same set of characters, sitting in a room, loving to hate each other.\n\n\n\n\n\n\nA Drama, not a soap opera\n\n\n\n\n\n\n\nA Comedy Drama, not a soap opera\n\n\n\n\n\nSeries like Billions and Loudermilk would never admit to being soap operas because, just as the young boy who played with action figures would never have played with dolls, so many viewers of sweary soap operas would never watch soap operas."
  },
  {
    "objectID": "posts/talk-at-edinbr/index.html",
    "href": "posts/talk-at-edinbr/index.html",
    "title": "EdinbR talk on modelling economic (in)activity transitions",
    "section": "",
    "text": "Yesterday I had the great privilege of being one of two speakers at the Edinburgh R Users group, called EdinbR. (Difficult to say without sounding like a pirate.)\nI spoke through some of the modelling and conceptual challenges involved in trying to model the effect that various drivers/factors/exposures have on how many people in the UK become economically inactive, especially economically inactive for reasons of long-term sickness.\nThe talk seemed to go well (though perhaps the speaker’s always the last person qualified to judge), even though some of the algebra didn’t render correctly. (Which unfortunately means I also used algebra.)\nLike this blog, the presentation also made use of Quarto, but in the presentation’s case using reveal.js.\nThe presentation is available, for those intrepid souls interested in seeing something with R code and algebra, here."
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html",
    "href": "posts/robocop-is-wonderfully-childish/index.html",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "",
    "text": "Robocop (1987): Wonderfully Childish"
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html#preamble",
    "href": "posts/robocop-is-wonderfully-childish/index.html#preamble",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Preamble",
    "text": "Preamble\nHere’s something that’s differently nerdy. Some thoughts on the enduring and childish appeal of Robocop as a character and concept, lifted largely from some notes I made on Obsidian (which also uses Markdown)."
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html#notes-on-robocop",
    "href": "posts/robocop-is-wonderfully-childish/index.html#notes-on-robocop",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Notes on Robocop",
    "text": "Notes on Robocop\nWent through a Robocop phase. First film is good. Second film is adequate fan service (by Frank Miller), though unpleasant in terms of how it makes a boy a main antagonist. Reboot is terrible.\nI think this was inspired by seeing footage for the Robocop game Rogue City. Also remembered discussion about how in the original film Robocop’s death and resurrection is modelled on Jesus. It’s definitely true his death is a ‘passion’ (modern equivalent: torture porn?), and that his suffering at the hands of sadistic tormenters fits this pattern.\nIn the first film there’s also the sense of the protagonist reclaiming his humanity. Murphy’s memory is wiped, but through force of will he brings himself to remember who he was, and to identify as Murphy. That’s the last line. He’s thanked and addressed by the Old Man as a person, not as property That’s the arc: becoming human again.\nBy contrast the reboot started with Murphy knowing who he was, though the scientists can modify the extent to which his feelings or programming are in charge. There’s a plot about police corruption and selling weapons illegally, and plenty of exposition from scientists where they try to shoehorn in cod-philosophy on personhood and free will, but this felt like ‘filler’ between a series of action sequences, which were much faster but also much less weighty and visceral than those in the original.\nIn the original Robocop is a bullet sponge. He was slow, apparently unfeeling. Though this might be partly a function of Peter Weller not having a great deal of mobility when wearing the suit, it also feeds into the central character arc: robots don’t feel; people do.1 Robots aren’t vulnerable as people are. As Robocop becomes damaged, he becomes more vulnerable, and his face becomes visible. Vulnerability is necessary for humanity to be restored. Robocop’s suit makes him a superhero, an adolescent boy fantasy of massive strength and power, but it also makes him a prisoner, trapped and entombed.\n\n\n\nRegaining Humanity: But at what cost?\n\n\nAgain, by contrast, in the remake Murphy has lost less. He still has a human hand, still has his memories and sense of self, and still has all the speed and mobility he had as a person, only more so. He doesn’t have an arc, he has perturbations and wobbles.\nLet’s think some more about why Robocop is an adolescent, or even pre-adolescent, fantasy. Firstly, it appeals to a kind of crude creativity of combinatorials: take two things that are familiar, combine them, and make something unfamiliar. With only a limited number of schemas, even a young child can wonder what happens when they are combined, and feel excited and proud about bootstrapping from everyday experience to pure fantasy. Like the distinction between animal, vegetable and mineral, ‘man’ and ‘machine’ are different primary colours, and seeing the concept of Robocop may be for a young child like seeing red and blue make purple for the first time.\n\n\n\nTeenage Mutant Ninja Turtles: Another contemporaneous example of the ‘primary colour chimera’ attractive to children\n\n\nSecond, there’s the power fantasy. Within this, there’s the sense of seeing someone who can play an action game and be allowed to ‘cheat’. The roles of an action game, involving shooting, must include that, once shot, the player must ‘play dead’. But in Robocop is a fantasy of a character who’s still allowed to shoot others whilst being able to ignore when others shoot him. Robocop presents a fantasy for a young child of playing a game where you can do things that no-one else can, because you’re more special than everyone else.\nI suspect that’s why, even though Robocop was clearly unsuitable for children, the concept of Robocop appeared to have so much appeal to children. There’s something inherently childish about seeing such a pure chimera rendered on screen, and the possibilities and affordances of this chimera are signalled so brightly from the images alone that the (adult) source material does not need to be consulted.\nAgain, this seems to be yet another reason why the remake did not work: 2014’s Murphy was not machine enough. The suit did not make him clearly ‘other’ enough. Even though the graphic violence was toned down so that in theory children could watch, the lack of schematic purity in the ideal types being mixed meant the pleasure of seeing primary colours being mixed, in the way the original so effectively managed, was muddied and diluted."
  },
  {
    "objectID": "posts/robocop-is-wonderfully-childish/index.html#footnotes",
    "href": "posts/robocop-is-wonderfully-childish/index.html#footnotes",
    "title": "Robocop (1987) is wonderfully childish",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlistair Gray’s Lanark played with a similar idea, in the form of Dragonhide, a magical realist representation of dermatitis, emotional distance, and lack of outlet for artistic expression. Too much Dragonhide, the book warns, causes sufferers to become unable to move, and unable to vent, their heat and energy, without expression, eventually causing them to boil to death!↩︎"
  },
  {
    "objectID": "posts/unrepentent-confessional/index.html",
    "href": "posts/unrepentent-confessional/index.html",
    "title": "David Sederis: Humourists as Unrepentent Observational Confessionals",
    "section": "",
    "text": "Whereas I devoted much of the 2022 Christmas break to (re?)learning Bayesian modelling, and over 2023 took four months off to learn software development, over the 2023 Christmas break I vowed to do nothing useful, nothing that involves thinking or effort or learning, in order to make the break as restful and uneventful as I could.\nHowever, as anyone who’s tried sitting with their own thoughts for more than a couple of minutes realises, it’s not really possible to avoid thinking about things, and attending to one’s own thoughts is often far from restful. Attempting to sit mindfully for more than a minute puts the lie to the very idea that I have thoughts. Instead the reverse is true: thoughts have us.\nAttempting to sit passively with one’s own thoughts is to attempt to recognise the eternal turbulence of what Rational Mystic Sam Harris keeps referring to as the ‘field of consciousness’. It’s like standing on a worn wet stone, surrounded by eddies and currents, and being invited to dip a toe into the waters without falling in. Failing at this task is the default state of cognition, it seems, especially (possibly uniquely?) for humans. Even if we stay mindful enough to notice that thoughts just appear to us, unbidden, without some kind of humuncular avatar of the self causing or driving them, the temptation of following a thought, of engaging with it, is something that is near impossible to resist. Drop into a thought, and it takes us to another thought, then another, then another, and before long we find ourselves thinking about cheese when we meant to think about spreadsheets, or Nazis when we meant to think about buying Christmas cards, or stuck in a ruminatory cycle where a complex of three or more thoughts cycle back onto each other in some kind of unstoppable mental vortex.\nInvariably, the thoughts we have, or rather the thoughts that have us, lead us into some dark, surreal or smelly places. We seldom report to others the mental journeys we went on to retrieve information that’s actionable or useful to others. We self-curate, and self-censor, displaying the pearl of wisdom we’ve found, without recounting the sadistic joy we felt ripping the oyster that contained it apart and ending its innocent life. We wipe off the stink and the gore accrued in the journeys we’ve been on inside our own heads, and instead try to present to others a pristine edit of these journeys, and we pretend these curated edits are our ‘true self’. And almost everyone else feels compelled to do the same, because we do, and we do, because everyone else does.\nAll this is a rambling, (mindful?) way of saying, I read David Sedaris’ latest book Happy-Go-Lucky over the Christmas break, and like his other books and stories found it humourous (as intended), engaging and refreshing as an example of how to tell stories that don’t aim to build and sell a pristine curated self to others. Though the term ‘humourist’ (or rather ‘humorist’) has been used to describe Sedaris’ outputs and the genre he dominates so effectively, a more accurate term may be “unrepentant observational confessional”.\nSedaris’ stories are finely crafted, polished, and I’m sure edited and reedited many times, often using, like most stand-up comedians, the involuntary and unvarnished feedback from audiences (such as whether people laugh) as a guide on how to improve his texts further. But they are not stories in service to the promotion of a pristine and virtuous self, a paragon to live by. No, they’re stylised records of ‘things that happen’, inside and out, and the relationship between occurrances, thoughts, feelings, and actions. And importantly, though stylised, they have a sense of verisimilitude to the inner world in which thoughts and associations are often far from linear or pristine, whether it be noting that crowds shouting “Black Lives Matter” do so with a quality and cadence of a fishmonger selling “Fresh-Caught Haddock”, the private vilification of those who move too slowly in queues, or Sedaris’ recognition that the deaths of his mentally ill sister, or homophobic father, did not evoke in him the quality of grief that a ‘good person’ ought to experience.\nHumour often comes from distance: the distance in time needed to sublimate tragedy into comedy (‘too soon?’), but also the distance between expectation and reality. The Pristine Self creates this expectation, whereas careful observation and honest accounting reveals a reality that near invariably falls short of this expectation. Sedaris’ stories work as humour because he knows how much most people edit the stories they tell themselves and others about themselves in order to maintain the Pristine Self, but he doesn’t. Instead he edits to make the blacks darker, the smells skinkier, and the circles his mind wanders in more eccentric."
  },
  {
    "objectID": "posts/circular-reasoning/index.html",
    "href": "posts/circular-reasoning/index.html",
    "title": "I got permanently banned from a politics forum for mentioning how circles work",
    "section": "",
    "text": "A month or so back, I was skimming a far-left-wing online politics forum, where a visual metaphor was presented to illustrate why it’s important that nominally left-wing parties and institutions be genuinely left-wing, and not the insidious pretenders to the left disparagingly known as ‘liberals’. Verbally, the argument was something as follows:\nSo, in this story, both the right and the liberal faux left are complicit in the horrible direction that modern society is taking. The liberals are at least as much to blame as the media, and so we hate them both.\nBut let’s first try to draw this visual metaphor, starting with just a unit circle:\nCode\nlibrary(tidyverse)\norigin &lt;- c(x= 0, y = 0)\n\nradius &lt;- 1\n\nangles &lt;- seq(0, 2*pi, length.out = 200)\n\nxpos &lt;- cos(angles)\nypos &lt;- sin(angles)\n\ndf &lt;- tibble(angle = angles, x = xpos, y = ypos)\n\ndf |&gt;\n    ggplot(aes(x=x, y = y)) + \n        geom_path(linewidth = 1) + \n        coord_equal() +\n        theme_minimal()\nNow let’s think about how to draw the teeth:\nCode\n# For a sawtooth circle, the radius will be made of two components: \n# r_main, for the inner part of the circle, which does not change\n# r_saw, the amount the circle extends beyond r_main at different angles\n\ngen_coords &lt;- function(angles, r_main = 1, r_saw_max = 1, n_teeth = 36) {\n\n    gen_saw_protrusion &lt;- function(r_saw_max, angles, n_teeth) {\n        r_saw_max  * angles %% (2*pi / n_teeth)\n    }\n\n    x &lt;- (r_main + gen_saw_protrusion(r_saw_max, angles, n_teeth) ) * cos(angles)\n    y &lt;- (r_main + gen_saw_protrusion(r_saw_max, angles, n_teeth) ) * sin(angles)\n\n    tibble(angle = angles, x = x, y = y)\n}\n\nangles &lt;- seq(0, 2*pi, length.out = 2000)\n\nsawcircle_coords &lt;- gen_coords(angles)\n\n\nsawcircle_coords |&gt;\n    ggplot(aes(x=x, y = y)) + \n        geom_path(linewidth = 1) + \n        coord_equal() +\n        theme_minimal()\nNow let’s label it:\nCode\nrw_angles &lt;- seq(20 * pi / 180, 70 * pi / 180, length.out = 100) \nrw_curve &lt;- tibble(\n    x = 1.1 * cos(rw_angles),\n    y = 1.1 * sin(rw_angles)\n)\n\nsawcircle_coords |&gt;\n    ggplot(aes(x=x, y = y)) + \n        geom_path(linewidth = 1) + \n        coord_equal() +\n        theme_minimal() + \n        annotate('rect', xmin = -1.2, xmax=0, ymin=0, ymax=1.2, fill = 'red', alpha = 0.1) + \n        annotate('rect', xmin=0, xmax = 1.2, ymin=0, ymax=1.2, fill = 'blue', alpha = 0.1) + \n        annotate('rect', xmin=0, xmax=1.2, ymin = -1.2, ymax = 0, fill = 'blue', alpha = 0.2) + \n        annotate('rect', xmin=-1.2, xmax=0, ymin=-1.2, ymax=0, fill = 'red', alpha = 0.2 ) + \n        annotate('text', x = -0.5, y = 0.25, label = \"centre left\") + \n        annotate('text', x = 0.5, y = 0.25, label = 'centre right') + \n        annotate('text', x = 0.5, y = -0.25, label = 'far right') + \n        annotate('text', x = -0.5, y = -0.25, label = 'far left') + \n        annotate('rect', xmin=-0.70, xmax=-0.95, ymin=0.70, ymax = 0.95, fill = 'red') + \n        annotate('text', x = -0.45, y = 0.58, colour = 'red', label = 'LIBERAL\\n BLOCKERS!') + \n        geomtextpath::geom_labelpath(aes(x = x, y = y), inherit.aes = FALSE, \n            data = rw_curve, \n            label = \"RW Media!\",\n            color = \"blue\", \n            arrow = arrow(ends = \"first\")\n\n        )\nYes. That’s pretty much how the image looked."
  },
  {
    "objectID": "posts/circular-reasoning/index.html#banning-offence",
    "href": "posts/circular-reasoning/index.html#banning-offence",
    "title": "I got permanently banned from a politics forum for mentioning how circles work",
    "section": "Banning offence…",
    "text": "Banning offence…\nSo, why did I get permanently banned from the forum that promoted this visual metaphor?\nWell, I tend to take metaphors very seriously. So I asked something like the following:\n\nDoesn’t this model suggest that some on the far right will become far left?\n\nWhich, if this political circle works like any other kind of circle, would seem to be the case…\n\n\nCode\nor_angles &lt;- seq(240 * pi / 180, 300 * pi / 180, length.out = 100) \nor_curve &lt;- tibble(\n    x = 0.95 * cos(or_angles),\n    y = 0.95 * sin(or_angles)\n)\n\n\nsawcircle_coords |&gt;\n    ggplot(aes(x=x, y = y)) + \n        geom_path(linewidth = 1) + \n        coord_equal() +\n        theme_minimal() + \n        annotate('rect', xmin = -1.2, xmax=0, ymin=0, ymax=1.2, fill = 'red', alpha = 0.1) + \n        annotate('rect', xmin=0, xmax = 1.2, ymin=0, ymax=1.2, fill = 'blue', alpha = 0.1) + \n        annotate('rect', xmin=0, xmax=1.2, ymin = -1.2, ymax = 0, fill = 'blue', alpha = 0.2) + \n        annotate('rect', xmin=-1.2, xmax=0, ymin=-1.2, ymax=0, fill = 'red', alpha = 0.2 ) + \n        annotate('text', x = -0.5, y = 0.25, label = \"centre left\") + \n        annotate('text', x = 0.5, y = 0.25, label = 'centre right') + \n        annotate('text', x = 0.5, y = -0.25, label = 'far right') + \n        annotate('text', x = -0.5, y = -0.25, label = 'far left') + \n        annotate('rect', xmin=-0.70, xmax=-0.95, ymin=0.70, ymax = 0.95, fill = 'red') + \n        annotate('text', x = -0.45, y = 0.58, colour = 'red', label = 'LIBERAL\\n BLOCKERS!') + \n        geomtextpath::geom_labelpath(aes(x = x, y = y), inherit.aes = FALSE, \n            data = or_curve, \n            label = \"Radicalised!\",\n            color = \"black\", \n            arrow = arrow(ends = \"first\")\n        ) + \n        geomtextpath::geom_labelpath(aes(x = x, y = y), inherit.aes = FALSE, \n            data = rw_curve, \n            label = \"RW Media!\",\n            color = \"blue\", \n            arrow = arrow(ends = \"first\")\n        ) + \n        geom_point(aes(x = x, y = y), inherit.aes= FALSE, \n        data = or_curve[1,], colour = \"darkred\", shape = 15, size = 3) + \n        geom_point(aes(x = x, y = y), inherit.aes= FALSE, \n        data = or_curve[nrow(or_curve),], colour = \"darkblue\", shape = 16, size = 3)\n\n\n\n\n\nYup. That’s how circles work.\nBut apparently mentioning this gets you banned!"
  },
  {
    "objectID": "posts/utterences-comments/index.html",
    "href": "posts/utterences-comments/index.html",
    "title": "New blog feature: comments",
    "section": "",
    "text": "I think I’ve managed to set up a blog comment feature for each of the posts.\nAs usual, the quarto documentation is great, though information on comments is in the HTML basics section rather than the website or blog section, so takes a bit of hunting. Albert Rapp’s very comprehensive blogpost is a great resource, covering this and much else.\nThe Quarto documentation gives three options for comments:\n\nHypothes.is, which allows comments and annotations to be provided line-by-line, a bit like non-editable tracked changes.\nutterances, which is a lightweight interface based on the discussion feature in github.\ngiscus, which seems to be built on utterances, but a bit more heavy-weight/opinionated.\n\nI’ve attempted, and think I’ve managed to implement, utterances.\nIn order to make a comment on a post, you need to have a Github username, and log in.\nFrom my end, I needed to do the following:\n\nSet up a public Github repo for blog comments. I unimaginately called this BlogComments\nInstall utterances on github and associate it with this repo\nWithin the file posts/_metadata.yml, add the following declaration\n\ncomments: \n  utterances: \n    repo:  JonMinton/BlogComments\nParameters in posts/_metadata.yml are applied to all posts within the posts subdirectory. This should mean that each post will now contain a comment box at the bottom.\nWhen a comment is added by a registered Github user, metadata from the specific post being commented on should be appended to an issue/discussion post within the JonMinton/BlogComments directory. And whenever a post is rendered, all associated discussion/issue items in the BlogComments repo should be fetched and shown at the bottom of the post.\nI’ve said should because I’ve only just set this up, and there are currently no comments.\nWhy not try to add a comment and see what happens?!"
  },
  {
    "objectID": "posts/glm-series/index.html",
    "href": "posts/glm-series/index.html",
    "title": "GLMs: My first series",
    "section": "",
    "text": "I now have four fairly technical posts that form part of a series on understanding statistical modelling from a generalised linear regression (GLM) perspective. This series is far from complete, but is complete enough that it should be fairly useful to intrepid readers.\nTo make it easier to find this series, I’ve now created a page for links just to entries in this series. To see this, just look on the top left of this site, and click on ‘generalised linear models’."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "First Post",
    "section": "",
    "text": "Hi, this is my first blog post. I’m making this using Quarto, starting off by slavishly following the tutorial, then incrementally adapting it to suit my preferences.\n\nI’m even keeping the default image of the first blog post. It’s not dissimilar to what I’m actually looking at!"
  },
  {
    "objectID": "posts/wrapping-guide/index.html",
    "href": "posts/wrapping-guide/index.html",
    "title": "How to wrap presents",
    "section": "",
    "text": "Here’s the key information I’ve learned from watching too many Youtube videos on how to wrap presents over the years.\n\n\n\n\nflowchart TB\n\nstart[Presents to Wrap]\ndecision{Are they cuboid?}\nbox(Put them in a cuboid)\naction(Wrap them)\nfinish[Presents are wrapped]\n\nstart --&gt; decision\ndecision --&gt;|yes| action\ndecision --&gt;|no| box\nbox --&gt; action\naction --&gt; finish\n\n\n\n\n\n\nYou’re welcome!"
  },
  {
    "objectID": "posts/economic-inactivity-modelling-package-readme/index.html",
    "href": "posts/economic-inactivity-modelling-package-readme/index.html",
    "title": "My Economic Inactivity Modelling Package: Informative Readme File!",
    "section": "",
    "text": "A few weeks ago, as I wasn’t using any personal or sensitive data, I decided to make the main repository where I keep my economic inactivity modelling work public, meaning in theory anyone could take a look.\nHowever (much like this blog), I didn’t tell anyone about it.\nThe repo is a bit of a mess, but it works. It’s both an R package, containing various convenience functions and lookup files, and a series of notebooks, presentations and now draft papers which make use of such functions and files through quarto. In due course, it may be a good idea to separate the package side of things from the ‘working’ repo which makes use of the package. Any suggestions how best to do this are welcome.\nThe main thing I’ve changed recently is the readme.md file. The economic inactivity project makes extensive use of Understanding Society, in order to populate the models with information on transitions from one wave to the next between the seven mutually exclusive economic inactivity states. Now, the readme.md contains information about how and where to add the relevant Understanding Society1 dataset to a local clone of the repo in order to try out the functions and package.\nTo reiterate, caveat emptor, the repo is what it is. But if you’re interested in taking a look, creating your own fork of it, cloning it, and adding the requisite data, then it’s available from this link"
  },
  {
    "objectID": "posts/economic-inactivity-modelling-package-readme/index.html#footnotes",
    "href": "posts/economic-inactivity-modelling-package-readme/index.html#footnotes",
    "title": "My Economic Inactivity Modelling Package: Informative Readme File!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe particular version of the dataset I plumped for includes British Household Panel Survey data as well, the predecessor to Understanding Society, which began in 1991. So there’s the potential to use the package to explore changes in transition probabilities and drivers thereof between states for a much longer period than I’m using to calibrate the model and explore the trends.↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html",
    "href": "posts/glms/lms-are-glms-part-05/index.html",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "",
    "text": "In the first part of this series, I stated that statistical model fitting, within the generalised model framework presented in King, Tomz, and Wittenberg (2000), involves adjusting candidate values for elements of \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_K \\}\\) such that the difference between what the model predicts given some predictor values, \\(Y_i | X_i\\), and what has been observed alongside the predictors, \\(y_i\\), is minimised on average1 in some way.\nThe aim of this post is to show how this process is typically implemented in GLMs, using likelihood theory."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-05/index.html#aim",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "",
    "text": "In the first part of this series, I stated that statistical model fitting, within the generalised model framework presented in King, Tomz, and Wittenberg (2000), involves adjusting candidate values for elements of \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_K \\}\\) such that the difference between what the model predicts given some predictor values, \\(Y_i | X_i\\), and what has been observed alongside the predictors, \\(y_i\\), is minimised on average1 in some way.\nThe aim of this post is to show how this process is typically implemented in GLMs, using likelihood theory."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html#bayes-rule-and-likelihood",
    "href": "posts/glms/lms-are-glms-part-05/index.html#bayes-rule-and-likelihood",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Bayes’ Rule and Likelihood",
    "text": "Bayes’ Rule and Likelihood\nStatisticians and more advanced users of statistical models often divide themselves into ‘frequentists’ and ‘Bayesians’. To some extent the distinction is really between ‘improper Bayesians’ and ‘proper Bayesians’, however, as Bayes’ Rule is at the root of both approaches. Bayes’ Rule is:\n\\[\nP(A|B) = \\frac{P(B|A)P(A)}{P(B)}\n\\]\nNote in the above the left hand side of the equation is \\(P(A|B)\\) and the right hand side of the equation includes \\(P(B|A)\\). To write it out as awkward prose, therefore, Bayes’ Rule is a way of expressing that given this in terms of this given that.\nAs with much of algebra, \\(A\\) and \\(B\\) are just placeholders. We could instead use different symbols instead, such as:\n\\[\nP(\\tilde{\\theta} | y) = \\frac{P(y | \\tilde{\\theta})P(\\tilde{\\theta})}{P(y)}\n\\]\nLikelihood theory offers a way of thinking about how good a model is in terms of its relationship to the data. According to King (1998) (p. 59), it can be expressed as:\n\\[\nL(\\tilde{\\theta}| y) = k(y) P(y | \\tilde{\\theta})\n\\]\nOr\n\\[\nL(\\tilde{\\theta} | y) \\propto P(y | \\tilde{\\theta})\n\\]\nWhere \\(\\tilde{\\theta}\\) is a proposed parameter or parameter combination for the model, and \\(y\\) is the observed outcome.2\nThe important thing to note is that both Bayes’ Rule and Likelihood Theory are ways of expressing this given that as a function of that given this. Specifically, the model given the data, as a function of the data given the model. 3"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html#likelihood-for-linear-regression",
    "href": "posts/glms/lms-are-glms-part-05/index.html#likelihood-for-linear-regression",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Likelihood for linear regression",
    "text": "Likelihood for linear regression\nWhen, many years ago, I completed the course from this modelling framework is most associated, a hazing ritual employed near the start of the course was to require participants to derive the likelihood of different model specifications. However, I don’t feel like hazing myself right now, so instead we can use the derivation shown on slide 8 of these slides:\n\\[\nL(\\beta, \\sigma^2 | y) = \\prod{L(y_i | \\mu_i, \\sigma^2)}\n\\]\nWhere \\(\\mu = X \\beta\\), \\(i\\) indicates an observation in the data (a row of \\(X\\) when \\(X\\) is in matrix form), and \\(\\prod\\) indicates the likelihoods from each observation should be multiplied with each other to derive the overall likelihood for all observed data.\nIn practice the log Likelihood, rather than the likelihood itself, is used, because this allows calculation of a sum of terms (\\(\\sum\\)) rather than product of terms (\\(\\prod\\)), and the latter tends to be computationally easier to calculate.\nAs we are interested only in how likelihood varies as a function of those model parameters we wish to estimate, \\(\\theta = \\{\\beta, \\sigma^2\\}\\), some of the terms in the log likelihood expression can be omitted, leaving us with:\n\\[\n\\log{L(\\beta, \\sigma^2 | y)} \\doteq \\sum{-\\frac{1}{2}[\\log{\\sigma^2} + \\frac{(y_i - X_i\\beta)^2}{\\sigma^2}]}\n\\]\nFor all the complexity of the above expression, at heart it takes three inputs:\n\n\\(\\theta = \\{\\beta, \\sigma^2\\}\\) : The candidate parameters for the model.\n\\(y\\) : the observed response value from the dataset \\(D\\)\n\\(X\\) : the observed predictor values from the dataset \\(D\\)\n\nAnd returns one value, the log likelihood \\(\\log{L(.)}\\).\nTo reiterate, we can’t change the data, but we can keep changing the candidate parameters \\(\\theta\\). Each time we do so, \\(\\log{L(.)}\\) will change too.\nThe aim of model calibration, in the Likelihood framework, is to maximise the Likelihood. The parameter set that maximises the likelihood is also the parameter set that maximises the log likelihood.\nTo continue the example from the slides, we can write out a function for calculating the log likelihood of standard linear regression as follows:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\nIn the above, pars is (almost but not quite) \\(\\theta\\), the parameters to estimate. For standard linear regression \\(\\theta = \\{\\beta, \\sigma^2\\}\\), where \\(\\beta = \\{\\beta_0, \\beta_1, ..., \\beta_k\\}\\), i.e. a vector of beta parameters, one for each column (variable) in \\(X\\), the predictor matrix of observations; this is why \\(beta\\) is selected from the first K values in pars where K is the number of columns in \\(X\\).\nThe last value in pars is used to derive the proposed \\(\\sigma^2\\). If we call this last value eta (\\(\\eta\\)), then we can say \\(\\sigma^2 = e^{\\eta}\\). So, whereas \\(\\theta\\) is a vector that ‘packs’ \\(\\beta\\) and \\(\\sigma^2\\) into a single ordered series of values, pars packs eta in place of \\(\\sigma^2\\). This substitution of eta for \\(\\sigma^2\\) is done to make it easier for standard parameter fitting algorithms to work, as they tend to operate over the full real number range, rather than just over positive values.\nIn order to illustrate how the log likelihood function llNormal works in practice, let’s construct a simple toy dataset \\(D\\), and decompose \\(D = \\{y, X\\}\\), the two types of data input that go into the llNormal function.\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nIn the code above we have created \\(y\\), a vector of nine observed responses; and \\(X\\), a matrix of predictors with two columns (the number of variables for which \\(beta\\) terms need to be estimated) and nine rows (the number of observations).\nGraphically, the relationship between x and y looks as follows:\n\n\nCode\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()\n\n\n\n\n\nIn this toy example, but almost never in reality, we know the correct parameters for the model. These are \\({\\beta_0 = 2.5, \\beta_1 = 1.4}\\) and \\(\\sigma^2 = 0.25\\). 4 Soon, we will see how effectively we can use optimisation algorithms to recover these true model parameters. But first, let’s see how the log likelihood varies as a function jointly of different candidate values of \\(\\beta_0\\) (the intercept) and \\(\\beta_1\\) (the slope parameter), if we already set \\(\\sigma^2\\) to 0.25.\n\n\nCode\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-5, 5, by = 0.1),\n    beta_1 = seq(-5, 5, by = 0.1)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\n\n\n\nCode\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nLooking at this joint surface of values, we can see a ‘hotspot’ where \\(\\beta_0\\) is around 2.5, and \\(\\beta_1\\) is around 1.4, just as we should expect. We can check this further by filtering candidate_param_values on the highest observed values of ll.\n\n\nCode\ncandidate_param_values |&gt; \n    filter(ll == max(ll))\n\n\n# A tibble: 1 × 3\n  beta_0 beta_1    ll\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    2.4    1.4  1.41"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html#summary",
    "href": "posts/glms/lms-are-glms-part-05/index.html#summary",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Summary",
    "text": "Summary\nWe have now introduced the concepts of Bayes Rule, Likelihood, and log likelihood, then derived the log likelihood for standard linear regression. We then built a toy dataset where we know the true parameters, and looked at how the log likelihood varies as different \\(\\beta\\) parameters are proposed. We identified a ‘hot spot’ when the \\(\\beta\\) parameters proposed are close to the ‘true values’."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-05/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-05/index.html#footnotes",
    "title": "Part Five: Traversing the Likelihood Landscape",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf \\(Y_i\\) is what the model predicts given observations \\(X_i\\), and \\(y_i\\) is the outcome observed to have occurred alongside \\(X_i\\), then we can call \\(\\delta_i = h(y_i, Y_i)\\) the difference, or error, between predicted and observed value. The function \\(h(.,.)\\) is typically the squared difference between predicted and observed values, \\((Y_i - y_i)^2\\), but could also in principle be the absolute difference \\(|Y_i - y_i|\\). Term-fitting algorithms usually compare not any individual \\(\\delta_i\\), but a sum of these error terms \\(\\delta\\). The aim of the algorithm is to find the set of \\(\\beta\\) terms that is least wrong for the whole dataset \\(D\\), rather than any specific row in the dataset \\(D_i\\).↩︎\nAs King (1998) (p. 59) describes it, “\\(k(y)\\) is an unknown fuction of the data. Whereas traditional probability is a measure of absolute uncertainty … the constant \\(k(y)\\) means that likelihood is only a relative measure of uncertainty”↩︎\nFrequentist approaches can thus be considered a kind of ‘improper Bayesian’ approach by considering \\(k(y)\\) in the Likelihood formula as a stand-in for \\(\\frac{P(\\tilde{\\theta})}{P(y)}\\) in Bayes’ Rule. Roughly speaking, it’s because of the improperness of treating the two terms as equivalent, and the relativeness of \\(k(y)\\), that mean frequentist probability statements can’t be interpreted as Bayesian probability statements. But thinking of the two terms as equivalent can be helpful for spotting the similarity between the two formulae.↩︎\ni.e. the square of the sd passed to rnorm() of 0.5↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-03/index.html",
    "href": "posts/glms/lms-are-glms-part-03/index.html",
    "title": "Part Three: glm is just fancy lm",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-03/index.html#tldr",
    "href": "posts/glms/lms-are-glms-part-03/index.html#tldr",
    "title": "Part Three: glm is just fancy lm",
    "section": "",
    "text": "This is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in King, Tomz, and Wittenberg (2000)."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-03/index.html#part-3-how-to-express-a-linear-model-as-a-generalised-linear-model",
    "href": "posts/glms/lms-are-glms-part-03/index.html#part-3-how-to-express-a-linear-model-as-a-generalised-linear-model",
    "title": "Part Three: glm is just fancy lm",
    "section": "Part 3: How to express a linear model as a generalised linear model",
    "text": "Part 3: How to express a linear model as a generalised linear model\nIn the last part, we introduced two types of generalised linear models, with two types of transformation for the systematic component of the model, g(.), the logit transformation, and the identity transformation. This post will show how this framework is implemented in practice in R.\nIn R, there’s the lm function for linear models, and the glm function for generalised linear models.\nI’ve argued previously that the standard linear regression is just a specific type of generalised linear model, one that makes use of an identity transformation I(.) for its systematic component g(.). Let’s now demonstrate that by producing the same model specification using both lm and glm.\nWe can start by being painfully unimaginative and picking using one of R’s standard datasets\n\nlibrary(tidyverse)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\niris |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\nIt looks like, where the petal length is over 2.5, the relationship with sepal length is fairly linear\n\niris |&gt; \n  filter(Petal.Length &gt; 2.5) |&gt; \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n\n\n\n\nSo, let’s make a linear regression just of this subset\n\niris_ss &lt;- \n  iris |&gt; \n  filter(Petal.Length &gt; 2.5) \n\nWe can produce the regression using lm as follows:\n\nmod_lm &lt;- lm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\nAnd we can use the summary function (which checks the type of mod_lm and evokes summary.lm implicitly) to get the following:\n\nsummary(mod_lm)\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09194 -0.26570  0.00761  0.21902  0.87502 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3731 on 98 degrees of freedom\nMultiple R-squared:  0.6864,    Adjusted R-squared:  0.6832 \nF-statistic: 214.5 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nWoohoo! Three stars next to the Petal.Length coefficient! Definitely publishable!\nTo do the same using glm.\n\nmod_glm &lt;- glm(Sepal.Length ~ Petal.Length, data = iris_ss)\n\nAnd we can use the summary function for this data too. In this case, summary evokes summary.glm because it knows the class of mod_glm contains glm.\n\nsummary(mod_glm)\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.09194  -0.26570   0.00761   0.21902   0.87502  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nSo, the coefficients are exactly the same. But there’s also some additional information in the summary, including on the type of ‘family’ used. Why is this?\nIf we look at the help for glm we can see that, by default, the family argument is set to gaussian.\nAnd if we delve a bit further into the help file, in the details about the family argument, it links to the family help page. The usage statement of the family help file is as follows:\nfamily(object, ...)\n\nbinomial(link = \"logit\")\ngaussian(link = \"identity\")\nGamma(link = \"inverse\")\ninverse.gaussian(link = \"1/mu^2\")\npoisson(link = \"log\")\nquasi(link = \"identity\", variance = \"constant\")\nquasibinomial(link = \"logit\")\nquasipoisson(link = \"log\")\nEach family has a default link argument, and for this gaussian family, this link is the identity function.\nWe can also see that, for both the binomial and quasibinomial family, the default link is logit, which transforms all predictors onto a 0-1 scale, as shown in the last post.\nSo, by using the default family, the Gaussian family is selected, and by using the default Gaussian family member, the identity link is selected.\nWe can confirm this by setting the family and link explicitly, showing that we get the same results\n\nmod_glm2 &lt;- glm(Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), data = iris_ss)\nsummary(mod_glm2)\n\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), \n    data = iris_ss)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.09194  -0.26570   0.00761   0.21902   0.87502  \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.99871    0.22593   13.27   &lt;2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n\n\nIt’s the same!\nHow do these terms used in the glm function, family and link, relate to the general framework in King, Tomz, and Wittenberg (2000)?\n\nfamily is the stochastic component, f(.)\nlink is the systematic component, g(.)\n\nThey’re different terms, but it’s the same broad framework.\nLinear models are just one type of general linear model!\n\nComing up\nIn the next part of this series, we will delve into the differences between linear regression models and logistic regression models, with a focus on how to get meaningful effect estimates from both types of model."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-08/index.html",
    "href": "posts/glms/lms-are-glms-part-08/index.html",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "",
    "text": "The previous post, perhaps the toughest of the series, showed how some special settings within R’s numerical optimisation optim() function can be used to estimate how much uncertainty there is in our estimates of the the model parameters \\(\\beta\\). We covered the concept that information and uncertainty are inversely related: the more information we have, the less uncertain we are, and vice versa. We estimated parameter uncertainty around the point that maximised (log) likelihood by asking the algorithm to take small steps from this highest point in different directions (dimensions, in effect variables), and report how steep the fall is in different directions. Steeper falls along a dimension imply less uncertainty and so more more information and narrower confidence intervals; as usual, the converse is also true. The component returned by optim() which reports the results of this ‘stepping out’ is a square matrix called the Hessian, which can be inverted to produce estimates of the variances and covarainces of each of the parameters being estimated in our model."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-08/index.html#recap",
    "href": "posts/glms/lms-are-glms-part-08/index.html#recap",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "",
    "text": "The previous post, perhaps the toughest of the series, showed how some special settings within R’s numerical optimisation optim() function can be used to estimate how much uncertainty there is in our estimates of the the model parameters \\(\\beta\\). We covered the concept that information and uncertainty are inversely related: the more information we have, the less uncertain we are, and vice versa. We estimated parameter uncertainty around the point that maximised (log) likelihood by asking the algorithm to take small steps from this highest point in different directions (dimensions, in effect variables), and report how steep the fall is in different directions. Steeper falls along a dimension imply less uncertainty and so more more information and narrower confidence intervals; as usual, the converse is also true. The component returned by optim() which reports the results of this ‘stepping out’ is a square matrix called the Hessian, which can be inverted to produce estimates of the variances and covarainces of each of the parameters being estimated in our model."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-08/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-08/index.html#aim",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Aim",
    "text": "Aim\nThe aims of this post are to show how estimates of uncertainty around the point estimates produced from the Hessian, based around the curvature measured around the point of maximum likelihood, are similar to those produced using a much more extensive (and computationally intensive) interrogation of the likelihood surface using a grid-search approach. It will also show how representations of joint uncertainty for parameter values can be generated using the multivariate normal distribution."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-08/index.html#comparing-inferred-and-observed-likelihood-surfaces",
    "href": "posts/glms/lms-are-glms-part-08/index.html#comparing-inferred-and-observed-likelihood-surfaces",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Comparing inferred and observed likelihood surfaces",
    "text": "Comparing inferred and observed likelihood surfaces\nLet’s return once again to the toy dataset used in the last two posts, whose true parameters we know because we made them up; and also the log likelihood function:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nTo estract estimates of uncertainty about the uncertainty of each of these parameters, we used optim() with the options shown below, and then inverted the matrix to go from information to uncertainty.\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\nWe were especially interested in the first two rows and columns of this matrix, as they correspond to uncertainty in \\(\\beta = \\{ \\beta_0, \\beta_1 \\}\\).\n\n\nCode\ninv_hess_betas &lt;- inv_hess[1:2, 1:2]\n\ninv_hess_betas\n\n\n             [,1]         [,2]\n[1,]  0.033577455 -0.004379668\n[2,] -0.004379668  0.004379668\n\n\nBack in part five, we used this same dataset to show how the log likelihood varies for various, equally spaced, candidate values for \\(\\beta_0\\) and \\(\\beta_1\\) (having fixed \\(\\eta = \\exp({\\sigma^2})\\) at its true value). This led to the followng map of the landscape1\n\n\nCode\nlibrary(tidyverse)\ncandidate_param_values &lt;- expand_grid(\n    beta_0 = seq(-15, 15, by = 0.05),\n    beta_1 = seq(-15, 15, by = 0.05)\n)\n\nfeed_to_ll &lt;- function(b0, b1){\n    pars &lt;- c(b0, b1, log(0.25))\n    llNormal(pars, y, X)\n}\n\ncandidate_param_values &lt;- candidate_param_values |&gt;\n    mutate(\n        ll = map2_dbl(beta_0, beta_1, feed_to_ll)\n    )\n\ncandidate_param_values |&gt;\n    ggplot(aes(beta_0, beta_1, z = ll)) + \n    geom_contour_filled() + \n    geom_vline(xintercept = 0) +\n    geom_hline(yintercept = 0) +\n    labs(\n        title = \"Log likelihood as a function of possible values of beta_0 and beta_1\",\n        x = \"beta0 (the intercept)\",\n        y = \"beta1 (the slope)\"\n    )\n\n\n\n\n\nWithin the above we can see that the log likelihood landscape for these two parameters looks like a bivariate normal distribution, we can also see a bit of a slant in this normal distribution. This implies a correlation between the two candidate values. The direction of the slant is downwards from left to right, implying the correlation is negative.\nFirstly let’s check that the correlation between \\(\\beta_0\\) and \\(\\beta_1\\) implied by the Hessian is negative. These are the off-diagonal elements, either first row, second column, or second row, first column:\n\n\nCode\ninv_hess_betas[1,2]\n\n\n[1] -0.004379668\n\n\nCode\ninv_hess_betas[2,1]\n\n\n[1] -0.004379668\n\n\nYes they are!\nAs mentioned previously, the likelihood surface produced by the gridsearch method involves a lot of computations, so a lot of steps, and likely a lot of trial and error, if it were to be used to try to find the maximum likelihood value for the parameters. By contrast, the optim() algorithm typically involves far fewer steps, ‘feeling’ its way up the hill until it reaches a point where there’s nowhere higher. 2 When it then reaches this highest point, it then ‘feels’ the curvature around this point in multiple directions, producing the Hessian. The algorithm doesn’t see the likelihood surface, because it hasn’t travelled along most of it. But the Hessian can be used to infer the likelihood surface, subject to subject (usually) reasonable assumptions.\nWhat are these (usually) reasonable assumptions? Well, that the likelihood surface can be approximated by a multivariate normal distribution, which is a generalisation of the standard Normal distribution over more than one dimensions.3\nWe can use the mvrnorm function from the MASS package, alongside the point estimates and Hessian from optim, in order to produce estimates of \\(\\theta = \\{ \\beta_0, \\beta_1, \\eta \\}\\) which represent reasonable uncertainty about the true values of each of these parameters. Algebraically, this can be expressed as something like the following:\n\\[\n\\tilde{\\theta} \\sim Multivariate Normal(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n\\]\nWhere \\(\\dot{\\theta}\\) are the point estimates from optim() and \\(\\Sigma\\) is the implied variance-covariance matrix recovered from the Hessian.\nLet’s create this MVN model and see what kinds of outputs it produces.\n\n\nCode\nlibrary(MASS)\n\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\nhead(param_draws)\n\n\n        beta0    beta1         eta\n[1,] 2.564978 1.375636 -0.30407255\n[2,] 2.440111 1.367774 -1.16815288\n[3,] 2.775332 1.338583 -0.05574937\n[4,] 2.283011 1.481799 -0.26095101\n[5,] 2.695635 1.228565 -1.18369341\n[6,] 2.686818 1.483601 -0.44262363\n\n\nWe can see that mvrnorm(), with these inputs from optim() produces three columns: one for each parameter being estimated \\(\\{ \\beta_0, \\beta_1, \\eta \\}\\). The n argumment indicates the number of draws to take; in this case, 10000. This number of draws makes it easier to see how much variation there is in each of the estimates.\n\n\nCode\ndf_param_draws &lt;- \nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    mutate(\n        sig2 = exp(eta)\n    ) |&gt;\n    pivot_longer(\n        -draw, \n        names_to = \"param\",\n        values_to = \"value\"\n    ) \n    \ndf_param_draws |&gt;\n    ggplot(aes(x = value)) + \n    geom_density() + \n    facet_grid(param ~ .) + \n    geom_vline(xintercept=0)\n\n\n\n\n\nThere are a number of things to note here: firstly, that the average of the \\(\\beta_0\\) and \\(\\beta_1\\) values appear close to their known ‘true’ values of 2.5 and 1.4 respectively. Secondly, that whereas the \\(\\eta\\) values are normally distributed, the \\(\\sigma^2\\) values derived from them are not, and are never below zero; this is the effect of the exponential link between quantities. Thirdly, that the implied values of \\(\\sigma^2\\) do appear to be centred around 0.25, as they should be as \\(\\sigma\\) was set to 0.50 in the model.\nAnd forthly, that the density around \\(\\beta_1\\) is more peaked than around \\(\\beta_0\\). This concords with what we saw previously in the filled contour map: both the horizontal beta0 axis and vertical beta1 axis are on the same scale, but the oval is broader along the horizontal axis than the vertical axis. This in effect implies that we have more information about the true value of \\(\\beta_1\\), the slope, than about the true value of \\(\\beta_0\\), the intercept.\nWe can also use these draws to reproduce something similar to, but not identical to, 4 the previous filled contour map:\n\n\nCode\n# param_draws |&gt;\n#     as_tibble(\n#         rownames = 'draw'\n#     ) |&gt;\n#     ggplot(aes(x = beta0, y = beta1)) + \n#     geom_point(alpha = 0.1) + \n#     coord_cartesian(xlim = c(-10, 10), ylim = c(-10, 10))\n\nparam_draws |&gt;\n    as_tibble(\n        rownames = 'draw'\n    ) |&gt;\n    ggplot(aes(x = beta0, y = beta1)) + \n    geom_density_2d_filled() + \n    coord_equal()\n\n\n\n\n\nOnce again, we see the same qualities as the contour map produced by interrogating the likelihood surface exhaustively: the distribution appears bivariate normal; there is a greater range in the distribution along the beta0 than the beta1 axis; and there is evidence of some negative correlation between the two parameters."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-08/index.html#summary",
    "href": "posts/glms/lms-are-glms-part-08/index.html#summary",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Summary",
    "text": "Summary\nThis post has shown how optim(), which in its vanilla state only returns point estimates, can be configured to also calculater and report the Hessian, a record of instantaneous curvature around the point estimates. Even without a fine-grained and exhausive search throughout the likelihood surface, this measure of curvature can be used to produce similar measures of uncertainty to the more exhausive approach, in a fraction of the number of computations.\nMore importantly, it can be used to generate draws of plausible combinations of parameter values, something denoted as \\(\\tilde{\\theta}\\) earlier. This is something especially useful for producing honest quantities of interest, which both tell users of models something they want to know, while also representing how uncertain we are in this knowledge.\nWe’ll cover that in the next post… 5"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-08/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-08/index.html#footnotes",
    "title": "Part Eight: Guessing what a landscape looks like by feeling the curves beneath our feet",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’ve narrowed the space between values slightly, and increased the range of permutations of values to search through, for an even more precise recovery of the likelihood landscape.↩︎\nIn practice, the algorithm seeks to minimise the value returned by the function, not maximise it, hence the negative being applied through the argument fnscale = -1 in the control argument. But the principle is identical.↩︎\nThis means that, whereas the standard Normal returns a single output, the Multivariate Normal returns a vector of outputs, one for each parameter in \\(\\theta\\), which should also be the length of the diagonal (or alternatively either the number of rows or columns) of \\(\\Sigma\\).↩︎\nThe values will not be identical because the values for \\(\\eta\\), and so \\(\\sigma^2\\), have not been fixed at the true value in this example.↩︎\nI was expecting to cover it in the current post, but this is probably enough content for now!↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html",
    "href": "posts/glms/lms-are-glms-part-06/index.html",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "",
    "text": "In the previous post in this series, I presented a function for calculating the log likelihood of a standard linear regression with a Normal error term. I then built a very simple dataset, ten data points linking \\(x\\) to \\(y\\), and showed how the log likelihood varied as a combination of different candidate values for the model’s intercept and slope terms (\\(\\beta_0\\) and \\(\\beta_1\\) respectively).\nThe aim of this this post is to show how the best parameter combinations tend to be estimated from a model’s log likelihood in practice, using an optimisation algorithm that iteratively tries out new parameter values, and keeps trying and trying until some kind of condition is met. This is what the last figure in the first post is trying to illustrate."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-06/index.html#aim",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "",
    "text": "In the previous post in this series, I presented a function for calculating the log likelihood of a standard linear regression with a Normal error term. I then built a very simple dataset, ten data points linking \\(x\\) to \\(y\\), and showed how the log likelihood varied as a combination of different candidate values for the model’s intercept and slope terms (\\(\\beta_0\\) and \\(\\beta_1\\) respectively).\nThe aim of this this post is to show how the best parameter combinations tend to be estimated from a model’s log likelihood in practice, using an optimisation algorithm that iteratively tries out new parameter values, and keeps trying and trying until some kind of condition is met. This is what the last figure in the first post is trying to illustrate."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#optimisation-algorithms-getting-there-faster",
    "href": "posts/glms/lms-are-glms-part-06/index.html#optimisation-algorithms-getting-there-faster",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Optimisation algorithms: getting there faster",
    "text": "Optimisation algorithms: getting there faster\nIn the previous post, we ‘cheated’ a bit when using the log likelihood function, fixing the value for one of the parameters \\(\\sigma^2\\) to the value we used when we generated the data, so we could instead look at how the log likelihood surface varied as different combinations of \\(\\beta_0\\) and \\(\\beta_1\\) were plugged into the formula. \\(\\beta_0\\) and \\(\\beta_1\\) values ranging from -5 to 5, and at steps of 0.1, were considered: 101 values of \\(\\beta_0\\), 101 values of \\(\\beta_1\\), and so over 10,0001 unique \\(\\{\\beta_0, \\beta_1\\}\\) combinations were stepped through. This approach is known as grid search, and seldom used in practice (except for illustration purposes) because the number of calculations involved can very easily get out of hand. For example, if we were to use it to explore as many distinct values of \\(\\sigma^2\\) as we considered for \\(\\beta_0\\) and \\(\\beta_1\\), the total number of \\(\\{\\beta_0, \\beta_1, \\sigma^2 \\}\\) combinations we would crawl through would be over 100,000 2 rather than over 10,000.\nOne feature we noticed with the likelihood surface over \\(\\beta_0\\) and \\(\\beta_1\\) in the previous post is that it appears to look like a hill, with a clearly defined highest point (the region of maximum likelihood) and descent in all directions from this highest point. Where likelihood surfaces have this feature of being single-peaked in this way (known as ‘unimodal’), then a class of algorithms known as ‘hill climbing algorithms’ can be applied to find the top of such peaks in a way that tends to be both quicker (fewer steps) and more precise than the grid search approach used for illustration in the previous post."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#code-recap",
    "href": "posts/glms/lms-are-glms-part-06/index.html#code-recap",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Code recap",
    "text": "Code recap\nLet’s copy over the code we used in the previous post for:\n\n\nCalculating log likelihood\n\n\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\nAnd\n\n\nGenerating our tame toy dataset of 10 data points\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\nTo recap, the toy dataset looks as follows:\n\n\nCode\nlibrary(tidyverse)\ntibble(x=x, y=y) |&gt;\n    ggplot(aes(x, y)) + \n    geom_point()"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#optim-our-robo-chauffeur",
    "href": "posts/glms/lms-are-glms-part-06/index.html#optim-our-robo-chauffeur",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "optim: our Robo-Chauffeur",
    "text": "optim: our Robo-Chauffeur\nNote how the llNormal function takes a single argument, pars, which packages up all the specific candidate parameter values we want to try out. In our previous post, we also had a ‘feeder function’, feed_to_ll, which takes the various \\(\\beta\\) candidate values from the grid and packages them into pars. In our previous post, we had to specify the candidate values to try to feed to llNormal packages inside pars.\nBut we don’t have to do this. We can instead use an algorithm to take candidate parameters, try them out, then make new candidate parameters and try them out, for us. Much as a taxi driver needs to know where to meet a passenger, but doesn’t want the passenger to tell them exactly which route to take, we just need to specify a starting set of values for the parameters to optimise. R’s standard way of doing this is with the optim function. Here’s it in action:\n\n\nCode\noptim_results &lt;-  optim(\n    # par contains our initial guesses for the three parameters to estimate\n    par = c(0, 0, 0), \n\n    # by default, most optim algorithms prefer to search for a minima (lowest point) rather than maxima \n    # (highest point). So, I'm making a function to call which simply inverts the log likelihood by multiplying \n    # what it returns by -1\n    fn = function(par, y, X) {-llNormal(par, y, X)}, \n\n    # in addition to the par vector, our function also needs the observed output (y)\n    # and the observed predictors (X). These have to be specified as additional arguments.\n    y = y, X = X\n    )\n\noptim_results\n\n\n$par\n[1]  2.460571  1.375421 -1.336209\n\n$value\n[1] -1.51397\n\n$counts\nfunction gradient \n     216       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nThe optim function returns a fairly complex output structure, with the following components:\n\npar: the values for the parameters (in our case \\(\\{\\beta_0, \\beta_1, \\eta \\}\\)) which the optimisation algorithm ended up with.\nvalue: the value returned by the function fn when the optim routine was stopped.\ncounts: the number of times the function fn was repeatedly called by optim before optim decided it had had enough\nconvergence: whether the algorithm used by optim completed successfully (i.e. reached what it considers a good set of parameter estimates in par), or not.\n\nIn this case, convergence is 0, which (perhaps counterintuitively) indicates a successful completion. counts indicates that optim called the log likelihood function 216 times before stopping, and par indicates values of \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\eta = -1.34\\}\\) were arrived at. As \\(\\sigma^2 = e^\\eta\\), this means \\(\\theta = \\{\\beta_0 = 2.46, \\beta_1 = 1.38, \\sigma^2 = 0.26 \\}\\). As a reminder, the ‘true’ values are \\(\\{\\beta_0 = 2.50, \\beta_1 = 1.40, \\sigma^2 = 0.25\\}\\).\nSo, the optim algorithm has arrived at pretty much the correct answers for all three parameters, in 216 calls to the log likelihood function, whereas for the grid search approach in the last post we made over 10,000 calls to the log likelihood function for just two of the three parameters.\nLet’s see if we can get more information on exactly what kind of path optim took to get to this set of parameter estimates. We should be able to do this by specifying a value in the trace component in the control argument slot…"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#comparisons",
    "href": "posts/glms/lms-are-glms-part-06/index.html#comparisons",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Comparisons",
    "text": "Comparisons\nFor comparison let’s see what lm and glm produce.\nFirst lm:\n\n\nCode\ntoy_df &lt;- tibble(\n    x = x, \n    y = y\n)\n\n\nmod_lm &lt;- lm(y ~ x, data = toy_df)\nsummary(mod_lm)\n\n\n\nCall:\nlm(formula = y ~ x, data = toy_df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6082 -0.3852 -0.1668  0.2385  1.1092 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5813 on 7 degrees of freedom\nMultiple R-squared:  0.9796,    Adjusted R-squared:  0.9767 \nF-statistic:   336 on 1 and 7 DF,  p-value: 3.564e-07\n\n\n\\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\), i.e. the same to 2 decimal places.\nAnd now with glm:\n\n\nCode\nmod_glm &lt;- glm(y ~ x, data = toy_df, family = gaussian(link = \"identity\"))\n\nsummary(mod_glm)\n\n\n\nCall:\nglm(formula = y ~ x, family = gaussian(link = \"identity\"), data = toy_df)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.6082  -0.3852  -0.1668   0.2385   1.1092  \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.46067    0.20778   11.84 6.95e-06 ***\nx            1.37542    0.07504   18.33 3.56e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.3378601)\n\n    Null deviance: 115.873  on 8  degrees of freedom\nResidual deviance:   2.365  on 7  degrees of freedom\nAIC: 19.513\n\nNumber of Fisher Scoring iterations: 2\n\n\nOnce again, \\(\\{\\beta_0 = 2.46, \\beta_1 = 1.38\\}\\)"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#discussion",
    "href": "posts/glms/lms-are-glms-part-06/index.html#discussion",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Discussion",
    "text": "Discussion\nIn the above, we’ve successfully used optim, our Robo-Chauffeur, to arrive very quickly at some good estimates for our parameters of interest, \\(\\beta_0\\) and \\(\\beta_1\\), which are in effect identical to those produced by the lm and glm functions.\nThis isn’t a coincidence. What we’ve done the hard way is what the glm function (in particular) largely does ‘under the hood’."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#coming-up",
    "href": "posts/glms/lms-are-glms-part-06/index.html#coming-up",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Coming up",
    "text": "Coming up\nIn the next part of this series, we’ll see how other outputs available from optim can be used to estimate uncertainty in the parameters of interest, how this information can be used to produce the kinds of estimates of standard errors around coefficients which are summarised in glm and lm summary() functions, and which many (ab)users of statistical models obsess about when star-gazing, and how information about uncertainty in parameter estimates allows for more honest model-based predictions."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-06/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-06/index.html#footnotes",
    "title": "Part Six: The Robo-Chauffeur",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n\\(101^2 = 10201\\)↩︎\n\\(101^3 = 1030301\\)↩︎"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html",
    "href": "posts/glms/lms-are-glms-part-09/index.html",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "",
    "text": "The last eight posts in this series have taken us into some fairly arcane territory, including concepts like the use of link functions and statistical families within GLM, the likelihood theory of inference and its relation to Bayes’ Rule, and how models are fit in practice using optimisation algorithms. In the last couple of posts we showed how optim(), R’s standard optimisation function, can be used to recover not just the maximum likelihood (point) estimates of a series of parameters to be estimated, but also estimates of how much uncertainty there is about these estimates: both singularly, which gives rise to measures like standard errors, Z scores and P-values - the place where sadly all too many statistical analyses stop at; and jointly, through the calculation of the Hessian and corresponding variance-covariance matrix of uncertainty about the parameter vector.\nIn the last post, we showed how known uncertainty about the parameter values in the statistical model can be represented by using the point estimates \\(\\dot{\\theta}\\) and variance-covariance measure of uncertainty \\(\\Sigma\\) can be used to produce a long series of plausible joint estimates of \\(\\tilde{\\theta}\\) (the parameter estimates with uncertainty) by passing the above as parameters to the multivariate normal distribution and taking repeated draws."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#recap",
    "href": "posts/glms/lms-are-glms-part-09/index.html#recap",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "",
    "text": "The last eight posts in this series have taken us into some fairly arcane territory, including concepts like the use of link functions and statistical families within GLM, the likelihood theory of inference and its relation to Bayes’ Rule, and how models are fit in practice using optimisation algorithms. In the last couple of posts we showed how optim(), R’s standard optimisation function, can be used to recover not just the maximum likelihood (point) estimates of a series of parameters to be estimated, but also estimates of how much uncertainty there is about these estimates: both singularly, which gives rise to measures like standard errors, Z scores and P-values - the place where sadly all too many statistical analyses stop at; and jointly, through the calculation of the Hessian and corresponding variance-covariance matrix of uncertainty about the parameter vector.\nIn the last post, we showed how known uncertainty about the parameter values in the statistical model can be represented by using the point estimates \\(\\dot{\\theta}\\) and variance-covariance measure of uncertainty \\(\\Sigma\\) can be used to produce a long series of plausible joint estimates of \\(\\tilde{\\theta}\\) (the parameter estimates with uncertainty) by passing the above as parameters to the multivariate normal distribution and taking repeated draws."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#aim",
    "href": "posts/glms/lms-are-glms-part-09/index.html#aim",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Aim",
    "text": "Aim\nIn this post, we’ll now, finally, show how this knowledge can be applied to do something with statistical models that ought to be done far more often: report on what King, Tomz, and Wittenberg (2000) calls quantities of interest, including predicted values, expected values, and first differences. Quantities of interest are not the direction and statistical significance (P-values) that many users of statistical models convince themselves matter, leading to the kind of mindless stargazing summaries of model outputs described in post four. Instead, they’re the kind of questions that someone, not trained to think that stargazing is satisfactory, might reasonably want answers to. These might include:\n\nWhat is the expected income of someone who completes course X in the five years after graduation? (Expected values)\nWhat is the expected range of incomes of someone who completes course X in the five years after graduation? (Predicted values)\nWhat is the expected difference in incomes between someone who completes course X, compared to course Y, in the five years after graduation? (First Differences)\n\nIn post four, we showed how to answer some of the questions of this form, for both standard linear regression and logistic regression. We showed that for linear regression such answers tend to come directly from the summary of coefficients, but that for logistic regression such answers tend to be both more ambiguous and dependent on other factors (such as gender of graduate, degree, ethnicity, age and so on), and require more processing in order to produce estimates for.\nHowever, we previously produced only point estimates for these questions, and so in a sense misled the questioner with the apparent certainty of our estimates. We now know, from post eight, that we can use information about parameter uncertainty to produce parameter estimates \\(\\tilde{\\theta}\\) that do convey parameter uncertainty, and so we can do better than the point estimates alone to answer such questions in way that takes into account such uncertainty, with a range of values rather than a single value."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#method",
    "href": "posts/glms/lms-are-glms-part-09/index.html#method",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Method",
    "text": "Method\nLet’s make use of our toy dataset one last time, and go through the motions to produce the \\(\\tilde{\\theta}\\) draws we ended with on the last post:\n\n\nCode\nllNormal &lt;- function(pars, y, X){\n    beta &lt;- pars[1:ncol(X)]\n    sigma2 &lt;- exp(pars[ncol(X)+1])\n    -1/2 * (sum(log(sigma2) + (y - (X%*%beta))^2 / sigma2))\n}\n\n\n\n\nCode\n# set a seed so runs are identical\nset.seed(7)\n# create a main predictor variable vector: -3 to 5 in increments of 1\nx &lt;- (-3):5\n# Record the number of observations in x\nN &lt;- length(x)\n# Create a response variable with variability\ny &lt;- 2.5 + 1.4 * x  + rnorm(N, mean = 0, sd = 0.5)\n\n# bind x into a two column matrix whose first column is a vector of 1s (for the intercept)\n\nX &lt;- cbind(rep(1, N), x)\n# Clean up names\ncolnames(X) &lt;- NULL\n\n\n\n\nCode\nfuller_optim_output &lt;- optim(\n    par = c(0, 0, 0), \n    fn = llNormal,\n    method = \"BFGS\",\n    control = list(fnscale = -1),\n    hessian = TRUE,\n    y = y, \n    X = X\n)\n\nfuller_optim_output\n\n\n$par\n[1]  2.460675  1.375424 -1.336438\n\n$value\n[1] 1.51397\n\n$counts\nfunction gradient \n      80       36 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n              [,1]          [,2]          [,3]\n[1,] -3.424917e+01 -3.424917e+01  2.727840e-05\n[2,] -3.424917e+01 -2.625770e+02 -2.818257e-05\n[3,]  2.727840e-05 -2.818257e-05 -4.500002e+00\n\n\nCode\nhess &lt;- fuller_optim_output$hessian\ninv_hess &lt;- solve(-hess)\ninv_hess\n\n\n              [,1]          [,2]          [,3]\n[1,]  3.357745e-02 -4.379668e-03  2.309709e-07\n[2,] -4.379668e-03  4.379668e-03 -5.397790e-08\n[3,]  2.309709e-07 -5.397790e-08  2.222221e-01\n\n\n\n\nCode\npoint_estimates &lt;- fuller_optim_output$par\n\nvcov &lt;- -solve(fuller_optim_output$hessian)\nparam_draws &lt;- MASS::mvrnorm(\n    n = 10000, \n    mu = point_estimates, \n    Sigma = vcov\n)\n\ncolnames(param_draws) &lt;- c(\n    \"beta0\", \"beta1\", \"eta\"\n)\n\n\nLet’s now look at our toy data again, and decide on some specific questions to answer:\n\n\nCode\nlibrary(tidyverse)\ntoy_df &lt;- tibble(x = x, y = y)\n\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() \n\n\n\n\n\nWithin the data itself, we have only supplied x and y values for whole numbers of x between -3 and 5. But we can use the model to produce estimates for non-integer values of x. Let’s try 2.5. For this single value of x, we can produce both predicted values and expected values, by passing the same value of x to each of the plausible estimates of \\(\\theta\\) returned by the multivariate normal function above.\n\n\nCode\ncandidate_x &lt;- 2.5"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#expected-values",
    "href": "posts/glms/lms-are-glms-part-09/index.html#expected-values",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Expected values",
    "text": "Expected values\nHere’s an example of estimating the expected value of y for x = 2.5 using loops and standard algebra:\n\n\nCode\n# Using standard algebra and loops\nN &lt;- nrow(param_draws)\nexpected_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    expected_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"]\n}\n\nhead(expected_y_simpler)\n\n\n[1] 6.004068 5.859547 6.121791 5.987509 5.767047 6.395820\n\n\nWe can see just from the first few values that each estimate is slightly different. Let’s order the values from lowest to highest, and find the range where 95% of values sit:\n\n\nCode\nev_range &lt;- quantile(expected_y_simpler,  probs = c(0.025, 0.500, 0.975)) \n\nev_range\n\n\n    2.5%      50%    97.5% \n5.505104 5.898148 6.291150 \n\n\nThe 95% interval is therefore between 5.51 and 6.29, with the median (similar but not quite the point estimate) being 5.90. Let’s plot this against the data:\n\n\nCode\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  median(expected_y_simpler), size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = ev_range[1], yend = ev_range[3], colour = \"blue\")\n\n\n\n\n\nThe vertical blue line therefore shows the range of estimates for \\(Y|x=2.5\\) that contain 95% of the expected values given the draws of \\(\\beta = \\{\\beta_0, \\beta_1\\}\\) which we produced from the Multivariate Normal given the point estimates and Hessian from optim(). This is our estimated range for the expected value, not predicted value. What’s the difference?"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#predicted-values",
    "href": "posts/glms/lms-are-glms-part-09/index.html#predicted-values",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Predicted values",
    "text": "Predicted values\nOne clue about the difference between expected value lies in the parameters from optim() we did and did not use: Whereas we have both point estimates and uncertainty estimates for the parameters \\(\\{\\beta_0, \\beta_1, \\sigma^2\\}\\),1 we only made use of the the two \\(\\beta\\) parameters when producing this estimate.\nNow let’s recall the general model formula, from the start of King, Tomz, and Wittenberg (2000), which we repeated for the first few posts in the series:\nStochastic Component\n\\[\nY_i \\sim f(\\theta_i, \\alpha)\n\\]\nSystematic Component\n\\[\n\\theta_i = g(X_i, \\beta)\n\\]\nThe manual for Zelig, the (now defunct) R package that used to support analysis using this approach, states that for Normal Linear Regression these two components are resolved as follows:\nStochastic Component\n\\[\nY_i \\sim Normal(\\mu_i, \\sigma^2)\n\\]\nSystematic Component\n\\[\n\\mu_i = x_i \\beta\n\\]\nThe page then goes onto state that the expected value, \\(E(Y)\\), is :\n\\[\nE(Y) = \\mu_i = x_i \\beta\n\\]\nSo, in this case, the expected value is the systematic component only, and does not involve the dispersion parameter in the stochastic component, which for normal linear regression is the \\(\\sigma^2\\) term. That’s why we didn’t use estimates of \\(\\sigma^2\\) when simulating the expected values.\nBut why is this? Well, it comes from the expectation operator, \\(E(.)\\). This operator means something like, return to me the value that would be expected if this experiment were performed an infinite number of times.\nThere are two types of uncertainty which give rise to variation in the predicted estimate: sampling uncertainty, and stochastic variation. In the expected value condition, this second source of variation falls to zero,2 leaving only the influence of sampling uncertainty, as in uncertainty about the true value of the \\(\\beta\\) parameters, remaining on uncertainty on the predicted outputs.\nFor predicted values, we therefore need to reintroduce stochastic variation as a source of variation in the range of estimates produced. Each \\(\\eta\\) value we have implies a different \\(\\sigma^2\\) value in the stochastic part of the equation, which we can then add onto the variation caused by parameter uncertainty alone:\n\n\nCode\nN &lt;- nrow(param_draws)\npredicted_y_simpler &lt;- vector(\"numeric\", N)\nfor (i in 1:N){\n    predicted_y_simpler[i] &lt;- param_draws[i, \"beta0\"] + candidate_x * param_draws[i, \"beta1\"] + \n        rnorm(\n            1, mean = 0, \n            sd = sqrt(exp(param_draws[i, \"eta\"]))\n        )\n}\n\nhead(predicted_y_simpler)\n\n\n[1] 4.802092 6.706397 7.073450 6.118750 6.757717 7.461254\n\n\nLet’s now get the 95% prediction interval for the predicted values, and compare them with the expected values predicted interval earlier\n\n\nCode\npv_range &lt;- \n    quantile(\n        predicted_y_simpler, \n        probs = c(0.025, 0.500, 0.975)\n    )\n\npv_range\n\n\n    2.5%      50%    97.5% \n4.766300 5.895763 7.055408 \n\n\nSo, whereas the median is similar to before, 5.90, the 95% interval is now from 4.77 to 7.063. This compares with the 5.51 to 6.29 range for the expected values. Let’s now plot this predicted value range just as we did with the expected values:\n\n\nCode\ntoy_df |&gt; \n    ggplot(aes(x = x, y = y)) + \n    geom_point() + \n    annotate(\"point\", x = candidate_x, y =  pv_range[2], size = 1.2, shape = 2, colour = \"blue\") + \n    annotate(\"segment\", x = candidate_x, xend=candidate_x, y = pv_range[1], yend = pv_range[3], colour = \"red\")\n\n\n\n\n\nClearly considerably wider."
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#summary",
    "href": "posts/glms/lms-are-glms-part-09/index.html#summary",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Summary",
    "text": "Summary\nThis post is hopefully where our toy dataset, which we’ve been hauling with us since post five, can finally retire, happy in the knowledge that it’s taken us through some of the toughest parts of this blog series. The ideas developed over the last few posts can now finally be applied to answering some questions that are actually (or arguably) interesting!"
  },
  {
    "objectID": "posts/glms/lms-are-glms-part-09/index.html#footnotes",
    "href": "posts/glms/lms-are-glms-part-09/index.html#footnotes",
    "title": "Part Nine: Answering questions with honest uncertainty: Expected values and Predicted values",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhere \\(\\sigma^2\\) is from \\(\\eta\\) and we defined \\(e^{\\eta} = \\sigma^2\\), a transformation which allowed optim() to search over an unbounded rather than bounded real number line↩︎\nIt can be easier to see this by using the more conventional way of expressing Normal linear regression: \\(Y_i = x_i \\beta + \\epsilon\\), where \\(\\epsilon \\sim Normal(0, \\sigma^2)\\). The expectation is therefore \\(E(Y_i) = E( x_i \\beta + \\epsilon ) = E(x_i \\beta) + E(\\epsilon)\\). For the first part of this equation, \\(E(x_i \\beta) = x_i \\beta\\), because the systematic component is always the same value, no matter how many times a draw is taken from the model. And for the second part, \\(E(\\epsilon) = 0\\), because Normal distributions are symmetrical around their central value over the long term: on average, every large positive value drawn from this distribution will become cancelled out by an equally large negative value, meaning the expected value returned by the distribution is zero. Hence, \\(E(Y) = x_i \\beta\\).↩︎\nBecause these estimates depend on random variation, these intervals may be slightly different to two decimal places than the values I’m quoting here.↩︎"
  },
  {
    "objectID": "posts/interactive-sliders/index.html",
    "href": "posts/interactive-sliders/index.html",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "",
    "text": "Below is an example of creating a plotly chart with an interactive slider using crosstalk.\nBy default, the plot shows the proportion of datazones in a local authority that are in the 15% most deprived datazones in Scotland. (Using the 2020 SIMD).\nThe slider allows different thresholds than the 15% default to be selected."
  },
  {
    "objectID": "posts/interactive-sliders/index.html#introduction",
    "href": "posts/interactive-sliders/index.html#introduction",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "",
    "text": "Below is an example of creating a plotly chart with an interactive slider using crosstalk.\nBy default, the plot shows the proportion of datazones in a local authority that are in the 15% most deprived datazones in Scotland. (Using the 2020 SIMD).\nThe slider allows different thresholds than the 15% default to be selected."
  },
  {
    "objectID": "posts/interactive-sliders/index.html#data-downloading-and-preparation",
    "href": "posts/interactive-sliders/index.html#data-downloading-and-preparation",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "Data Downloading and Preparation",
    "text": "Data Downloading and Preparation\nTo see the code itself, just click on the word ‘code’ to open up the block’.\n\n\nCode\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(crosstalk)\nlibrary(here)\n\n\n\n\nCode\nif(!file.exists(here(\"simd_data.xlsx\"))){\n  download.file(\n    url = \"https://www.gov.scot/binaries/content/documents/govscot/publications/statistics/2020/01/scottish-index-of-multiple-deprivation-2020-data-zone-look-up-file/documents/scottish-index-of-multiple-deprivation-data-zone-look-up/scottish-index-of-multiple-deprivation-data-zone-look-up/govscot%3Adocument/SIMD%2B2020v2%2B-%2Bdatazone%2Blookup.xlsx\",\n    destfile = here(\"simd_data.xlsx\"),\n    mode = \"wb\"\n  )\n}\n\ndta &lt;- openxlsx::readWorkbook(here(\"simd_data.xlsx\"), sheet = \"SIMD 2020v2 DZ lookup data\")\n\n\nThe code for the figure itself is below. It’s quite a convoluted process. There’s almost certaintly neater ways of doing this. The main thing to keep in mind is all the figures exist; just only one is visible at a time.\n\n\nCode\n# So let's construct a new aval containing the different x-y tuples given the threshold selected\n\ncalc_prop_deprived &lt;- function(q, dta){\n    dta %&gt;% \n      group_by(HBname) %&gt;% \n      summarise(prop_deprived = mean(pct_rank &lt; q)) %&gt;% \n      ungroup()\n}\n\ndf_rank &lt;- \n  dta %&gt;% \n    select(HBname, SIMD2020v2_Rank) %&gt;% \n    mutate(pct_rank = SIMD2020v2_Rank / max(SIMD2020v2_Rank))\n\n\nshared_df &lt;- tibble(\n  dep_quants = seq(0.05, 0.95, by = 0.05)\n) %&gt;% \n  mutate(derived_props = map(dep_quants, calc_prop_deprived, dta = df_rank)) %&gt;% \n  unnest(derived_props) %&gt;% \n  mutate(undep_quants = 1 - dep_quants) \n\n\n# Now to put it in the structure, and set active for `dep_quants = 0.15`\n\n\nunique_dep_quants &lt;- unique(shared_df$dep_quants)\nn_steps &lt;- length(unique_dep_quants)\n\ndep_vals &lt;- list()\nfor (step in 1:n_steps){\n  tmp &lt;- \n    shared_df %&gt;% \n      filter(dep_quants == unique_dep_quants[step]) %&gt;% \n      select(HBname, prop_deprived) %&gt;% \n      mutate(HBname = reorder(HBname, prop_deprived))\n  \n  dep_vals[[step]] &lt;- list(\n    visible = FALSE,\n    name = paste0('Quantile: ', unique_dep_quants[step]),\n    x=tmp$prop_deprived,\n    y=tmp$HBname\n    \n  ) \n}\n\n# 15% is the third list object \n\ndep_vals[3][[1]]$visible = TRUE\n\n# Now visualise \n\n# create steps and plot all traces\ndep_steps &lt;- list()\nfig &lt;- plot_ly() \nfor (i in c(3, 1, 2, 4:n_steps)) { # Start with 3 as this is 15% and this should determine the default HB order \n fig &lt;- add_bars(fig,x=dep_vals[i][[1]]$x,  y=dep_vals[i][[1]]$y, visible = dep_vals[i][[1]]$visible, \n                 name = dep_vals[i][[1]]$name, orientation = 'h', hoverinfo = 'x+y', color = I(\"gray\"),\n                 showlegend = FALSE) %&gt;% \n   layout(\n      title = list(\n        text = glue::glue(\"Proportion of datazones in Health Boards at least this deprived\")\n      ),\n      xaxis = list(\n        title = \"Proportion this deprived in Health Board\",\n        range = list(0, 1)\n      ),\n      yaxis = list(\n        title = \"Health Board\"\n      )\n   )\n\n  step &lt;- list(args = list('visible', rep(FALSE, length(dep_vals))),\n               method = 'restyle')\n  step$args[[2]][i] = TRUE  \n  step$label = unique_dep_quants[i]\n  dep_steps[[i]] = step \n}  \n#names(dep_steps) &lt;- unique_dep_quants\n\nfig &lt;- fig %&gt;%\n  layout(sliders = list(list(active = 2,\n                             currentvalue = list(prefix = \"Deprivation: \"),\n                             steps = dep_steps)))\n\nfig\n\n\n\n\n\n\nAs you can see, there’s still some work to do regarding formatting. But it works!"
  },
  {
    "objectID": "posts/interactive-sliders/index.html#static",
    "href": "posts/interactive-sliders/index.html#static",
    "title": "Interactive Sliders with Crosstalk and Plotly",
    "section": "Static",
    "text": "Static\nFor comparison, here’s the same data used to produce a static plot\n\n\nCode\n# Now to put it in the structure, and set active for `dep_quants = 0.15`\n\ndf_15pc &lt;- shared_df |&gt; \n  filter(between(dep_quants, 0.149, 0.151)) |&gt; \n  select(-dep_quants, -undep_quants)\n\ndf_15pc |&gt;\n  mutate(pct_deprived = 100 * prop_deprived) |&gt; \n  ggplot(aes(y= pct_deprived, x = fct_reorder(HBname, pct_deprived))) + \n  geom_bar(stat = \"identity\") +\n  geom_text(\n    aes(\n      label = ifelse(df_15pc$prop_deprived &gt; 0, sprintf(\"%.1f\", pct_deprived), \"\")\n    ), \n    color = \"white\",\n    hjust = 1, \n    nudge_y = -0.5\n  ) + \n  coord_flip() + \n  labs(\n    x = \"Health Board\",\n    y = \"Percent of datazones in 15% most deprived proportion of Scotland\",\n    title = \"Percent of datazones in Health Board in 15% most deprived areas of Scotland\",\n    subtitle = \"SIMD 2020\"\n  ) + \n  geom_hline(yintercept = 0)"
  },
  {
    "objectID": "posts/optimised-for-twitter/index.html",
    "href": "posts/optimised-for-twitter/index.html",
    "title": "Optimised for Twitter?",
    "section": "",
    "text": "I’ve finally got around to ‘optimising the Quarto blog for Twitter’, by following the guidance in this section of Quarto’s website and adding the following declaration to the _quarto.yml file in the project root.\nwebsite:\n  twitter-card:\n   site: \"@jonminton\"\nHowever, is Twitter optimised for Twitter, these days? Is Twitter even Twitter!?"
  },
  {
    "objectID": "posts/tidy-tuesday-life-expectancy/index.html",
    "href": "posts/tidy-tuesday-life-expectancy/index.html",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "",
    "text": "This week’s Tidy Tuesday compares life expectancy across the globe and is available here:"
  },
  {
    "objectID": "posts/tidy-tuesday-life-expectancy/index.html#loading-the-data",
    "href": "posts/tidy-tuesday-life-expectancy/index.html#loading-the-data",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Loading the data",
    "text": "Loading the data\n\n\nCode\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\ndata_url &lt;- \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-12-05/life_expectancy_different_ages.csv\"\n \ndta &lt;- read_csv(data_url)\n\n# Alternatively\ntuesdata &lt;- tidytuesdayR::tt_load('2023-12-05')\n\n\n\n    Downloading file 1 of 3: `life_expectancy.csv`\n    Downloading file 2 of 3: `life_expectancy_different_ages.csv`\n    Downloading file 3 of 3: `life_expectancy_female_male.csv`\n\n\n\n\nCode\nnames(tuesdata)\n\n\n[1] \"life_expectancy\"                \"life_expectancy_different_ages\"\n[3] \"life_expectancy_female_male\"   \n\n\nCode\nlife_expectancy &lt;- tuesdata$life_expectancy\n\nn_distinct(life_expectancy$Entity)\n\n\n[1] 261"
  },
  {
    "objectID": "posts/tidy-tuesday-life-expectancy/index.html#setting-a-global-plot-theme",
    "href": "posts/tidy-tuesday-life-expectancy/index.html#setting-a-global-plot-theme",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Setting a global plot theme",
    "text": "Setting a global plot theme\n\n\nCode\ntheme_set(theme_minimal())"
  },
  {
    "objectID": "posts/tidy-tuesday-life-expectancy/index.html#comparing-life-expectancy-across-regions",
    "href": "posts/tidy-tuesday-life-expectancy/index.html#comparing-life-expectancy-across-regions",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Comparing life expectancy across regions",
    "text": "Comparing life expectancy across regions\n\n\nCode\nregions &lt;- life_expectancy %&gt;%\n  filter(str_detect(Entity, \"region\"))\n\nregions %&gt;%\n  ggplot(aes(x = Year, y = LifeExpectancy)) +\n  geom_line(aes(col = Entity)) +\n  theme(legend.position = \"top\") +\n  annotate(geom = \"text\",\n           x = 1960, y = 50,\n           label = \"What happened here?\") +\n  geom_vline(xintercept = 2019, linetype = 2) +\n  annotate(geom = \"text\", x = 2019, y = 75,\n           label = \"Start of COVID pandemic\",\n           hjust = 1)"
  },
  {
    "objectID": "posts/tidy-tuesday-life-expectancy/index.html#difference-in-life-expectancy-between-more-and-less-developed-regions",
    "href": "posts/tidy-tuesday-life-expectancy/index.html#difference-in-life-expectancy-between-more-and-less-developed-regions",
    "title": "Tidy Tuesday on Life Expectancy",
    "section": "Difference in life expectancy between more and less developed regions",
    "text": "Difference in life expectancy between more and less developed regions\n\n\nCode\nregions %&gt;%\n  filter(Entity %in% c(\"More developed regions\",\n                       \"Less developed regions\"))  %&gt;%\n  arrange(Year, Entity) %&gt;%\n  group_by(Year) %&gt;%\n  mutate(difference = LifeExpectancy - lag(LifeExpectancy)) %&gt;%\n  filter(!is.na(difference)) %&gt;%\n  ungroup() %&gt;%\n  ggplot(aes(x = Year, y = difference)) +\n  geom_area(alpha = 0.5) +\n  expand_limits(y = 0) +\n  labs(title = \"Difference in life expectancy between more developed and less developed regions\",\n       y = \"Difference in life expectancy (years)\")\n\n\n\n\n\nWe look at life expectancy at different ages in three specific countries.\n\n\nCode\ndata_tidy &lt;-\n  dta |&gt;\n    pivot_longer(\n      cols = LifeExpectancy0:LifeExpectancy80\n    ) |&gt;\n    mutate(\n      starting_age = str_remove(name, \"LifeExpectancy\") %&gt;%\n        as.numeric()\n    ) |&gt;\n    select(-name) |&gt;\n    rename(e_x = value)\n\n\n\n\nCode\ndata_tidy |&gt;\n  filter(\n    Entity %in% c(\n      \"Nigeria\", \"Iran\",\n      \"South Africa\"\n    )\n  ) |&gt;\n  arrange(Year)  |&gt;\n  ggplot(aes(Year, e_x, group = factor(starting_age), colour = factor(starting_age))) +\n  geom_line() +\n  facet_wrap(~Entity)\n\n\n\n\n\nSandra Nwobi, who suggested the three countries above, provides the following summary:\n\nOf the three developing countries—Iran, South Africa, and Nigeria—Nigeria has a significantly higher zero-age death rate in the late 50s and early 60s. This can be attributed to a number of factors, including socioeconomic instability, political unrest, malnutrition, and limited access to healthcare. Comparing this result to South Africa and Iran, it is comparatively higher. However, there have been noticeable improvements in Nigeria during the 1980s, with a steady increase. Nevertheless, much work needs to be done to combat this in Nigeria, as it performs significantly worse than the other two countries.\n\nThere was a noticeable decline in data in the early 2000s, particularly in South Africa. Health crises like HIV/AIDS, which may have affected people between the ages of 0 and 25, as well as a number of social and economic problems may have contributed to this decline.\n\nIran’s data indicates consistent growth across all age groups over the years, with the exception of a general decline in 2020 that was likely caused by the COVID-19 virus. Out of the three countries, South Africa is the most affected, maybe as a result of a much older demography compared to Nigeria."
  },
  {
    "objectID": "posts/beavis-and-butthead-is-dumbface/index.html",
    "href": "posts/beavis-and-butthead-is-dumbface/index.html",
    "title": "Beavis and Butt-Head: When a physics graduate dons Dumbface",
    "section": "",
    "text": "Mike Judge’s Beavis and Butt-Head\n\n\nA second season of “Mike Judge’s Beavis & Butt-Head” is now available on Paramount+, continuing a series that began on MTV in the early 1990s. I’ve been watching it, generally enjoying it, but feeling a gnawing sense of discomfort while doing so. Here’s why.\nMike Judge is a physics graduate, whose other credits include: Silicon Valley, a sitcom about tech startups; King of the Hill, a surprisingly gentle and sympathetic animated sitcom about a lower middle class social conservative family; and Idiocracy, a feature length science fiction comedy whose premise is that, “People are getting dumber; society’s getting dumber; at this rate someone who’s average now will be considered a genius a few generations from now.”\nIn Idiocracy, the proposed mechanism for the world’s dumbing down is a kind of dysgenic selective breeding. Whereas smarter people, with their careful planning and fantastic career opportunities, equivocate and defer the decision to have children, dumber and more feckless people, who don’t tend to do much thinking or planning, and wouldn’t be giving up on any great opportunities, continue to breed like rabbits, or even viruses. Dumber people have a higher R number, so will outbreed smarter people until almost everyone in society’s dumb. From an evolutionary perspective, dumb is the winning strategy.\nIf this sounds like the kind of plotline a eugenicist might come up with, I think you might be right. The alternative is that Judge is a black pilled cynic, a wannabe eugenicist, who just wishes, like Marxism, it would only work in practice. Watching a Judge film or TV show is being invited to judge, to find others inferior and wanting, and so feel superior. But that short-term feeling of superiority is fleeting; what lingers is the sense of loneliness, of being ‘the only adult in the room’, the hell of other people, when the other people are idiots.\nThe intellectual elitism, and sneering at the dumb, that finds most full expression in Idiocracy, has always been present in Beavis & Butt-Head. To an extent that’s the entire plot. Beavis & Butt-Head’s lack of intellect is extrapolated to such an extent it becomes grotesquely surreal. And they combine this lack of intelligence with a lack of almost any detectable virtues or redeeming qualities, with the possible exception of Beavis’ sense of loyalty to Butt-Head, a loyalty that is often presented as misplaced, enabling the pair’s co-dependence and Butt-Head’s constant physical and emotional abuse of Beavis, his only friend in the world.\nOther targets of Beavis & Butt-Head’s humour are those characters who overestimate the two title characters, treat them with kindness, and try to help them. This includes their hippy teacher, Mr Van Driessen, who is frequently seen to permit delinquency and disruption from the titular pair, apparently to the detriment both of the pair themselves, and the rest of the class. Other recent episodes feature a kindly middle-aged couple, who happily provide the pair with provisions with which their own home will be attacked; someone who heroically rescues them from a sewer (which the pair mistakenly believe is Hell, as in their illiteracy they misread ‘Department of Sanitation’ as ‘De Apartment of Satan’); and their ever forgiving and kindly neighbour (and Hank Hill prototype?) Mr Anderson, whose property the pair damage and steal without apparent repercussion (except of the karmic variety).\nPerhaps the most depressing segments in the recent Beavis and Butt-Head are those titled Old Beavis and Butt-Head. Breaking a forth wall in long-run cartoon series, the premise of these segments is that we might expect that someone who was a teenager in the early 1990s might be middle aged (or ‘old’, from their former teenage self’s perspective) in the 2020s. And so these age-appropriate versions of the characters are presented. By now, the segments suggest, Butt-Head is jobless, obese, and living off disability payments. Beavis is wrinkly and crag-toothed, but has at least managed, after decades of (not) trying, to get a job, working as Butt-Head’s full time (taxdollar funded) carer.\nMike Judge, as well as creating the series, also voices both characters. In doing so, and in the context of his own academic achievements, just how wretchedly they are drawn, and his other outputs, I think he does the vocal equivalent of ‘donning Dumbface’. Beavis and Butt-Head aren’t just incapable and inferior along narrowly intellectual or academic lines, but in every conceivable way. Every thing they say (with the exception of some of their commentary segments), every thing they do, every scrape and escapade they put themselves in, is yet more evidence of their incorrigible worthlessness, and every attempt to help them as coming from a well-intentioned but misplaced belief that they could ever be better than they are. If we can’t get rid of people like them, the show seems to be suggesting, the best we can do is laugh at them mercilessly. (Maybe behind their backs, just to be safe.)\nAs mentioned, I’ve been watching the new series, and against my better nature enjoying it. It’s a guilty pleasure. Hopefully the above goes to illustrate just how guilty."
  },
  {
    "objectID": "posts/new-frasiers-eerie-familiarity/index.html",
    "href": "posts/new-frasiers-eerie-familiarity/index.html",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "",
    "text": "New Frasier.. like the Old Frasier\nAs with new Beavis and Butt-Head, which I’ve written about previously, Paramount+ includes access to new Frasier, a return of the vainglorious pratfalling public psychiatrist to the small screen after the original series ended in 2004.\nThough Frasier himself has returned, none of the supporting characters have. Instead he’s surrounded by an entirely (apparently) new cast of supporting characters, and the series is set in a brand new (but also very old) location. Given this, we might expect Frasier (2023) to feel very different to Frasier (1993)…\nBut it doesn’t. It feels eerily familiar. Despite almost everything, apart from the title character, being different, Frasier (2023) somehow feels largely the same as Frasier (1993).\nThe aim of this post is to try to think through why, with New Frasier, despite almost everything being different, almost everything is also the same."
  },
  {
    "objectID": "posts/new-frasiers-eerie-familiarity/index.html#characters-the-situation-in-situation-comedies",
    "href": "posts/new-frasiers-eerie-familiarity/index.html#characters-the-situation-in-situation-comedies",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "Characters: The Situation in Situation Comedies",
    "text": "Characters: The Situation in Situation Comedies\nCommercially successful TV series, back in the 1990s, weren’t really meant to go anywhere in terms of character and characterisation. Whereas Joseph Campbell’s Monomyth, which countless films use as their narrative template, focuses on how a sequence of events and experiences lead to irreconcilable change in the central character, the formula for a successful TV series depends on the central character not changing. Things happen to the central character, but like a boulder in a stream, the central character ultimately remains largely unchanged and unmoved as a result. The reason for this was largely due to the value to an audience of familiarity, which brings a sense of warmth to characters, and also not to burn through an ultimately finite supply of Heroic Fuel: There’s only so many times a character can face adversity, the call to adventure, the descent into the Underworld, look one’s Adversary in the eyes, almost die (literally or figuratively), ultimately triumph, and return to the light wounded, wiser but ultimately stronger. For an episodic series, if a character is shown to be broken and remoulded every week, before too long the audience will start to feel they’re made more of clay than flesh and bone.\nSo if the Monomyth can’t be used as the main narrative engine of a TV series, what can? For sitcoms, the clue’s partially in the name: the situation. And for most successful sitcoms, including Frasier, much of the situation comes from the interplay between characters.\nHow did this work in Frasier (1993)? Well, in the original Frasier the following supporting characters were introduced:\n\nMarty. Frasier’s father, a retired police officer. Whereas Frasier is booksmart, Marty is streetsmart. Frasier and Marty are both smart, but orientated towards fundamentally different forms of knowledge and competence. They might be related, but they’ve always swum in different waters.\nNiles. Frasier’s brother. Whereas Marty is too dissimilar to Frasier, Niles is too similar. Frasier’s knowledge and interests are esoteric, high culture not mainstream, and so there are few people in the world who will understand him. Niles does. But their world of high culture is so exclusive it’s also small. And it’s competitive, their academic and professional overachievement fueled by unquensionable egotism and self doubt. So, as well as Niles being Frasier’s closest friend, he’s also his closest rival.\nRos. Frasier’s Radio producer. Like Marty she’s streetsmart (albeit in the ‘streets’ of media production). And like Frasier she’s competitive. Because she’s more worldly wise than Frasier, and his boss, she exploits and manipulates Frasier to achieve her own ambitions, which often don’t align exactly with his own. She is, in a platonic sense, Frasier’s pimp.\nDaphne. The Help. Marty’s live-in carer, launderer and folder of clothes, cooker of foods, provider of basic needs. Daphne is economically dependent on Frasier’s largesse, and appears to be somewhat naive. However this appearance of naivity is sometimes shown to be an illusion.\n\nWhereas in Frasier (2023) there are the following supporting characters:\n\nFreddy. Frasier’s son, a working firefighter. Whereas Frasier is only booksmart, Freddy is also streetsmart. Freddy actively rejected the path to high culture that Frasier set him on. They might be related, but they swim in different waters.\nAlan. Frasier’s university buddy. Whereas Freddy is too dissimilar to Frasier, Alan is too similar. Frasier’s knowledge and interests are estoteric, high culture not mainstream, and so there are few people in the world who will understand him. Alan does. But their world of high culture is so exclusive it’s also small. And it’s competitive. Alan, however, has tenure, something Frasier covets, so in this sense, as well as being Fraiser’s closest friend Alan is, if not exactly a rival, someone Frasier finds himself measuring himself against, and finding wanting.1\nOlivia. Frasier’s boss. As well as being an academic, she’s also a manager of academics, and so a practitioner of the Dark Arts of academic self promotion. Like Frasier she’s competitive, especially with her sister, who’s also a senior academic. She is, in a platonic sense, Frasier’s pimp, and calls him her ‘dancing bear’.\nEve. Not The Help, but a single mother Freddy helped, and so Frasier must support too. Eve is somewhat economically dependent on Frasier’s largesse, living rent-free in one of his apartments. She has good social instincts and works at a bar, but considers herself an actor, though has a naively delusional sense of her own abilities in this field.\n\nIf the second series of descriptions seems similar to the first, this may help explain how and why Frasier (2023) is eerily familiar to viewers of Frasier (1993). Despite some differences, there is almost a one-to-one mapping between the main supporting characters in both Frasiers.2 Though it might not declare itself as such, Frasier (2023) is not just a sitcom, but a sci-fi sitcom, as it appears, like an inversion of Dr Who, that everyone except the lead character has regenerated into a new body."
  },
  {
    "objectID": "posts/new-frasiers-eerie-familiarity/index.html#character-based-situational-combinations",
    "href": "posts/new-frasiers-eerie-familiarity/index.html#character-based-situational-combinations",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "Character-based situational combinations",
    "text": "Character-based situational combinations\nSo, Frasier (2023) takes the same supporting character archetypes as Frasier (1993) and regenerates them. Why? I think this is because of the way even a small number of supporting characters can generate a large number of character-based situational combinations, and so a great deal of fuel for episodic stories, each based on how the main character interacts with one, two, or possibly three of the other characters. The Wikipedia article on Combination goes into a painful amount of detail on this.\nSay there are four primary characters supporting characters. 3 There are then four ways (F, A, O and E) that a single supporting character can interact with the main character. This comes intuitively, but also from the Binomial Coefficient \\(C(n, k) = \\frac{n!}{k!(n-k)!}\\), where \\(!\\) indicates factorial. We can work out the total number of combinations of four characters as follows:\n\n\nCode\nlibrary(tidyverse)\nmy_binomial &lt;- function(n,k) {factorial(n) / (factorial(k) * factorial(n-k))}\n\nn_characters &lt;- 4\n\ndf &lt;- tibble(\n    n_characters_interacting = 0:4\n    ) |&gt;\n    mutate(\n        n_comb_with_this_many_chars = map_int(n_characters_interacting, function(x) my_binomial(n_characters, x))\n    ) |&gt;\n    mutate(\n        cumulative_combinations = cumsum(n_comb_with_this_many_chars)\n    )\n\ndf \n\n\n# A tibble: 5 × 3\n  n_characters_interacting n_comb_with_this_many_chars cumulative_combinations\n                     &lt;int&gt;                       &lt;int&gt;                   &lt;int&gt;\n1                        0                           1                       1\n2                        1                           4                       5\n3                        2                           6                      11\n4                        3                           4                      15\n5                        4                           1                      16\n\n\nSo, with 4 supporting characters, there are 16 combinations of interactions with Frasier (where 0 characters interacting would be Frasier soliloquizing, say if he gets stuck in a lift). This isn’t a huge number of situations, but more than a modern season. But this is just combinations, not permutations: a situation in which Alan verbs Olivia, for example, would be different to one in which Olivia verbs Alan, but in combinatorials counted as the same.4 It also excludes any B plots not involving Frasier. For this we simply need to change the n in the above from 4 to 5, and exclude k=0 from the option, as a story involving no characters probably wouldn’t work…\n\n\nCode\nn_characters &lt;- 5\n\ndf &lt;- tibble(\n    n_characters_interacting = 1:5\n    ) |&gt;\n    mutate(\n        n_comb_with_this_many_chars = map_int(n_characters_interacting, function(x) my_binomial(n_characters, x))\n    ) |&gt;\n    mutate(\n        cumulative_combinations = cumsum(n_comb_with_this_many_chars)\n    )\n\ndf \n\n\n# A tibble: 5 × 3\n  n_characters_interacting n_comb_with_this_many_chars cumulative_combinations\n                     &lt;int&gt;                       &lt;int&gt;                   &lt;int&gt;\n1                        1                           5                       5\n2                        2                          10                      15\n3                        3                          10                      25\n4                        4                           5                      30\n5                        5                           1                      31\n\n\nAllowing Frasier not to be in every story, the number of character-based combinations increases to 31, which seems plenty of basic story types from which between 5 (one scene) and 30 (one show) minutes of content could be derived. And as mentioned, this is just combinations, not permutations, where order matters. If looking at permutations, then the number of sequences with five characters is \\(5!\\),5 or 120, but we also need to include four, three, and two character sequences too, ie. \\(5! + 4! + 3! + 2!\\), which brings up the number of permutations to 152.6"
  },
  {
    "objectID": "posts/new-frasiers-eerie-familiarity/index.html#so-what",
    "href": "posts/new-frasiers-eerie-familiarity/index.html#so-what",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "So what?",
    "text": "So what?\nOkay, I’ve gone into the technical details a bit more than I was expecting to. The point is that even with a fairly small number of characters, the number of possible situations and circumstances that derive entirely from placing characters in a room together, and thinking how they might interact with each other, quickly becomes large enough to avoid being repetitive despite being familiar. Of course, additional supporting characters, guest stars, and scenarios all help increase the number of stories even further, but just having a small number of well defined characters, and imagining the narrative molecules and compounds these character elements may form when forced to mix, appears to do the bulk of the storytelling. With a sitcom, with interesting and well defined characters, in a sense it seems the stories write themselves.\nAnd why almost the same characters, rather than just the same number of characters? I think this was because over a decade of Frasier provides plenty of experience about what these character combinations produce. So, why start from scratch?7"
  },
  {
    "objectID": "posts/new-frasiers-eerie-familiarity/index.html#conclusion",
    "href": "posts/new-frasiers-eerie-familiarity/index.html#conclusion",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "Conclusion",
    "text": "Conclusion\nI think a second season is likely."
  },
  {
    "objectID": "posts/new-frasiers-eerie-familiarity/index.html#footnotes",
    "href": "posts/new-frasiers-eerie-familiarity/index.html#footnotes",
    "title": "The Eerie Familiarity of Frasier (2023)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nBeing an English character played by an English actor (Unlike Freddy and Olivia, who are both American characters played by English actors), Alan’s character is also a poorly dressed high functioning alcoholic.↩︎\nThe one exception to this is David Crane, son of Niles and Daphne. Initially it appeared the ‘Niles’ archetype from Frasier-1 had undergone some kind of Narrative Fission Event and been split into both Alan and David. On further viewing, however, it becomes apparent David is instead an intruder from another show, being effectively a toned down version of Sheldon Cooper from the Big Bang Theory, and being more a caricuture than a character.↩︎\nI’m excluding David as a primary supporting character as he appears to be a caricature, but he may become more fleshed out over time.↩︎\nSituations best described using intransitive verbs are probably closer to combinatorials not permutations. A pizza with ham and mushroom would be much like a pizza with mushroom and ham. Similarly a plot in which Alan and Olivia eat dinner together would be much like a plot in which Oliva and Alan eat dinner together. However a situation in which Alan invites Olivia to dinner would be different to a situation in which Olivia invites Alan to dinner!↩︎\nfactorial(5) in R↩︎\nfactorial(5) + factorial(4) + factorial(3) + factorial(2) in R↩︎\nAs with the phrase “History doesn’t repeat, but it does rhyme”, we probably shouldn’t assume exactly the same plots will occur in Frasier 2023 as with Frasier 1993. If it were to, we would expect Alan, who’s in his sixties, to become emphatuated with Eve, who’s in her twenties. I suspect this won’t happen, unless there’s a reboot of the reboot in 20 more years featuring Leonardo Dicaprio↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Jon. I’m interested in epidemiology, data science, cities, population structure, software development, R, javascript, python, the two cultures, cats, pop culture, storytelling and ideology, irrational rationality, taking dumb things seriously (and vice versa), cooking, and cats. Welcome to my Quarto blog, which I started in late 2023."
  }
]