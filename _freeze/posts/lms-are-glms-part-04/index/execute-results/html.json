{
  "hash": "461f5a01804ae500f4a1601ed2ece566",
  "result": {
    "markdown": "---\ntitle: \"Linear Models are General Linear Models\"\nsubtitle: \"Part Four: why only betas just look at beta\"\nauthor: \"Jon Minton\"\ndate: \"2023-12-19\"\ncode-fold: true\nwarning: false\nmessage: false\nheader-includes: \n    - \\usepackage{amsmath}\ncategories: [statistics, R]\nbibliography: references.bib\n---\n\n\n## tl;dr\n\nThis is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in @KinTomWit00.\n\n## Part 4: Why overuse of linear regression lead people to look at models in the wrong way\n\nIn previous posts in this series I've reintroduced standard linear regression and logistic regression as both being special versions of the same generalised model formula\n\n**Stochastic Component**\n\n$$\nY_i \\sim f(\\theta_i, \\alpha)\n$$\n\n**Systematic Component**\n\n$$\n\\theta_i = g(X_i, \\beta)\n$$\n\nWith standard linear regression the link function $g(.)$ is $I(.)$, i.e. the identity function, meaning *what goes in, is what comes out*. By contrast for logistic regression $g(.)$ is the logistic function, which squishes and squashes any real number as an input into a value between 0 and 1 as an output.\n\nThough it's not always phrased this way, a motivating question behind the construction of most statistical models is, \"What influence does a single input to the model, $x_j$, have on the output, $Y$?\"[^1] For a single variable $x_j$ which is either present (`1`) or absent (`0`), this is in effect asking what is $E(Y | x_j = 1) - E(Y | x_j = 0)$ ?[^2]\n\n[^1]: Note here I'm using $x_j$, not $x_i$, and that $X\\beta$ is shorthand for $\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3$ and so on. In using the $j$ suffix, I'm referring to just one of the specific $x$ values, $x_1$, $x_2$, $x_3$, which is equivalent to selecting one of the *columns in* $X$. By contrast $i$ should be considered shorthand for selection of one of the *rows of* $X$, i.e. one of the series of observations that goes into the dataset $D$.\n\n[^2]: $E(.)$ is the expectation operator, and $|$ indicates a condition. So, the two terms mean, respectively, *what is the expected value of the outcome if the variable of interest is 'switched on'?*, and *what is the expected value of the outcome if the variable of interest is 'switched off'?*\n\nLet's look at a linear regression case, then a logistic regression case.\n\n**Linear Regression example**\n\nUsing the iris dataset, let's try to predict Sepal Width (a continuous variable) on Sepal Length (a continuous variable) and whether the species setosa or not. As a reminder the data relating these three variables look as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\niris |>\n    ggplot(aes(Sepal.Length, Sepal.Width, group = Species, colour = Species, shape = Species)) + \n    geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nLet's now build the model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndf <- iris |> mutate(is_setosa = Species == 'setosa')\n\nmod_lm <- lm(Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nmod_lm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length + is_setosa, data = df)\n\nCoefficients:\n  (Intercept)   Sepal.Length  is_setosaTRUE  \n       0.7307         0.3420         0.9855  \n```\n:::\n:::\n\n\nThe coefficients $\\boldsymbol{\\beta}  = \\{\\beta_0, \\beta_1, \\beta_2\\}$ are $\\{0.73, 0.34, 0.99\\}$, and refer to the intercept, Sepal Length and is_setosa respectively. \n\nIf we assume a Sepel Length of 6, for example, then the expected Sepal Width (the thing we are predicting) is `0.73 + 6 * 0.34 + 0.99` or about `3.77` in the case where `is_setosa` is true, and `0.73 + 6 * 0.34`  or about `2.78` where `is_setosa` is false. \n\nThe difference between these two values, `3.77` and `2.78`, i.e. the 'influence of setosa' on the outcome, is `0.99`, i.e. the $\\beta_2$ coefficient shown before. In fact, for any conceivable (and non-conceivable, i.e. negative) value of Sepal Length, the difference is still `0.99`. \n\nThis is the $\\beta_2$ coefficient, and the reason why, for linear regression, and almost exclusively linear regression, looking at the coefficients themselves provides substantively meaningful information (something @KinTomWit00 calls a 'quantity of interest') about the size of influence that a predictor has on a response. \n\n\nNow let's look at an equivalent (swapped) example using logistic regression, in which we try to understand how the *probability* of an observation being setosa depends jointly on Sepal length and Sepal width, and we are especially interested in the influence that a 1 unit change in Sepal length has on this probability\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_logistic <- glm(\n    is_setosa ~ Sepal.Length + Sepal.Width, \n    data = df, \n    family = binomial(link = \"logit\")\n    )\n\nmod_logistic\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:  glm(formula = is_setosa ~ Sepal.Length + Sepal.Width, family = binomial(link = \"logit\"), \n    data = df)\n\nCoefficients:\n (Intercept)  Sepal.Length   Sepal.Width  \n       437.2        -163.4         137.9  \n\nDegrees of Freedom: 149 Total (i.e. Null);  147 Residual\nNull Deviance:\t    191 \nResidual Deviance: 2.706e-08 \tAIC: 6\n```\n:::\n:::\n\n\nHere the coefficients $\\boldsymbol{\\beta}  = \\{\\beta_0, \\beta_1, \\beta_2\\}$ are $\\{0.73, 0.34, 0.99\\}$, and refer to the intercept, Sepal Length and is_setosa respectively.\n\nBut what does this actually mean, substantively? \n\nA very common approach to trying to answer this question is to look at the statistical significance of the coefficients, which we can do with the `summary()` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod_logistic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = is_setosa ~ Sepal.Length + Sepal.Width, family = binomial(link = \"logit\"), \n    data = df)\n\nDeviance Residuals: \n       Min          1Q      Median          3Q         Max  \n-1.249e-04  -2.100e-08  -2.100e-08   2.100e-08   1.003e-04  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept)     437.2   128737.9   0.003    0.997\nSepal.Length   -163.4    45394.8  -0.004    0.997\nSepal.Width     137.9    44846.1   0.003    0.998\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1.9095e+02  on 149  degrees of freedom\nResidual deviance: 2.7060e-08  on 147  degrees of freedom\nAIC: 6\n\nNumber of Fisher Scoring iterations: 25\n```\n:::\n:::\n\n\nThis indicates that the standard errors on the coefficients is very large in comparison with the coefficient point estimates. The ratio of these two quantities is the z value, where heuristically 'larger is better'. The column `Pr(>|z|)` indicates the probability that the coefficient estimates include zero (where, heuristically, smaller is better). \n\nThe model above is terrible. To better try to illustrate my point let's look at a variant including an interaction term between the `Sepal.Length` and `Sepal.width`\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Note the * instead of the + in the formula line below\nmod_logistic2 <- glm(\n    is_setosa ~ Sepal.Length * Sepal.Width, \n    data = df, \n    family = binomial(link = \"logit\")\n    )\n\nsummary(mod_logistic2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = is_setosa ~ Sepal.Length * Sepal.Width, family = binomial(link = \"logit\"), \n    data = df)\n\nDeviance Residuals: \n       Min          1Q      Median          3Q         Max  \n-8.167e-05  -2.100e-08  -2.100e-08   2.100e-08   6.280e-05  \n\nCoefficients:\n                          Estimate Std. Error z value Pr(>|z|)\n(Intercept)                1144.40  777433.66   0.001    0.999\nSepal.Length               -288.93  166374.92  -0.002    0.999\nSepal.Width                -194.66  240635.52  -0.001    0.999\nSepal.Length:Sepal.Width     60.29   50045.48   0.001    0.999\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1.9095e+02  on 149  degrees of freedom\nResidual deviance: 1.4661e-08  on 146  degrees of freedom\nAIC: 8\n\nNumber of Fisher Scoring iterations: 25\n```\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}