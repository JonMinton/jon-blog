{
  "hash": "7b474ae509c43d6bf1ad981ea14b1059",
  "result": {
    "markdown": "---\ntitle: \"Part Twelve: Honest Predictions the slightly-less easier way\"\nauthor: \"Jon Minton\"\ndate: \"2024-02-04\"\ncode-fold: true\nwarning: false\nmessage: false\ncategories: [statistics, R]\nbibliography: references.bib\n---\n\n\n## Aim \n\nThe [last post](../lms-are-glms-part-11/index.qmd) ended by showing how the `predict` function can be used to show point estimates and uncertainty intervals for expected values and predicted values for a model based on a toothsome dataset. In this post we will start with that model and look at other information that can be recovered from it, information that will allow the effects of joint parameter uncertainty to be propagated through to prediction.\n\n\n## Recap of core concepts \n\nBack in [part 8](../lms-are-glms-part-08/index.qmd) we stated that estimates of the *cloud of uncertainty* in model parameters, that results from having limited numbers of observations in the data, can be represented as:\n\n$$\n\\tilde{\\theta} \\sim MVN(\\mu = \\dot{\\theta}, \\sigma^2 = \\Sigma)\n$$\n\nWhere `MVN` means multivariate normal, and needs the two quantities $\\dot{\\theta}$ and $\\Sigma$ as parameters. \n\nPreviously we showed how to extract (estimates of) these two quantities from `optim()`, where the first quantity, $\\dot{\\theta}$, was taken from the converged parameter point estimate slot `par`, and the second quantity, $\\Sigma$, was derived from the `hessian` slot. \n\nBut we don't *need* to use `optim()` directly in order to recover these quantities. Instead we can get them from the standard model objects produced by either `lm()` or `glm()`. Let's check this out...\n\n\n## Building our model \n\nLet's load the data and model we arrived at previously\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\ndf <- ToothGrowth |> tibble()\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 60 × 3\n     len supp   dose\n   <dbl> <fct> <dbl>\n 1   4.2 VC      0.5\n 2  11.5 VC      0.5\n 3   7.3 VC      0.5\n 4   5.8 VC      0.5\n 5   6.4 VC      0.5\n 6  10   VC      0.5\n 7  11.2 VC      0.5\n 8  11.2 VC      0.5\n 9   5.2 VC      0.5\n10   7   VC      0.5\n# ℹ 50 more rows\n```\n:::\n\n```{.r .cell-code}\nbest_model <- lm(len ~ log(dose) * supp, data = df)\n\nsummary(best_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = len ~ log(dose) * supp, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.5433 -2.4921 -0.5033  2.7117  7.8567 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       20.6633     0.6791  30.425  < 2e-16 ***\nlog(dose)          9.2549     1.2000   7.712  2.3e-10 ***\nsuppVC            -3.7000     0.9605  -3.852 0.000303 ***\nlog(dose):suppVC   3.8448     1.6971   2.266 0.027366 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.72 on 56 degrees of freedom\nMultiple R-squared:  0.7755,\tAdjusted R-squared:  0.7635 \nF-statistic:  64.5 on 3 and 56 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nLet's now look at some convenience functions, other than just summary, that work with `lm()` and `glm()` objects, and recover the quantities required from MVN to represent the uncertainty cloud. \n\n## Extracting quantities for modelling uncertainty\n\nFirstly, for the point estimates $\\dot{\\theta}$, we can use the `coefficients()` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef <- coefficients(best_model)\n\ncoef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     (Intercept)        log(dose)           suppVC log(dose):suppVC \n       20.663333         9.254889        -3.700000         3.844782 \n```\n:::\n:::\n\n\nAnd for the variance-covariance matrix, for representing joint uncertainty about the above estimates, we can use the `vcov` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSig <- vcov(best_model)\n\nSig\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                   (Intercept)     log(dose)        suppVC log(dose):suppVC\n(Intercept)       4.612422e-01 -8.768056e-17 -4.612422e-01    -7.224251e-17\nlog(dose)        -8.768056e-17  1.440023e+00  1.753611e-16    -1.440023e+00\nsuppVC           -4.612422e-01  1.753611e-16  9.224843e-01     1.748938e-16\nlog(dose):suppVC -7.224251e-17 -1.440023e+00  1.748938e-16     2.880045e+00\n```\n:::\n:::\n\n\nFinally, we can extract *the point estimate* for stochastic variation in the model, i.e. variation assumed by the model even if parameter uncertainty were minimised, using the `sigma` function:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsig <- sigma(best_model)\n\nsig\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.719847\n```\n:::\n:::\n\n\nWe now have three quantities, `coef`, `Sig` and `sig` (note the upper and lower case `s` in the above). These provide something *almost but not exactly equivalent* to the contents of `par` and that derived from `hessian` when using `optim()` previously. The section below explains this distinction in more detail. \n\n### Back to the weeds (potentially skippable)\n\nRecall the 'grandmother formulae', from @KinTomWit00, which the first few posts in this series started with: \n\n**Stochastic Component**\n\n$$\nY_i \\sim f(\\theta_i, \\alpha)\n$$\n\n**Systematic Component**\n\n$$\n\\theta_i = g(X_i, \\beta)\n$$\n\nFor standard linear regression this becomes:\n\n**Stochastic Component**\n\n$$\nY_i \\sim Norm(\\theta_i, \\sigma^2)\n$$\n\n**Systematic Component**\n\n$$\n\\theta_i =X_i \\beta\n$$\n\nOur main parameters are $\\theta$, which combined our predictors $X_i$ and our model parameter estimates $\\beta$. Of these two components we *know* the data - they are what they are - but are merely *estimating* our model parameters $\\beta$. So, any estimation uncertainty in this part of the equation results from $\\beta$ alone. \n\nOur ancillary parameter is $\\sigma^2$. This is our estimate of how much fundamental variation there is in how the data (the response variables $Y$) is drawn from the stochastic data generating process. \n\nWhen we used `optim()` directly, we estimated $\\sigma^2$ along with the other $\\beta$ parameters, via the $\\eta$ parameter `eta`, defined as $\\sigma^2 = e^{\\eta}$ to allow `optim()` to search over an unbounded real number range. If there are `k` $\\beta$ parameters, therefore, `optim()`'s `par` vector contained `k + 1` values, with this last value being the point estimate for the `eta` parameter. Similarly, the number of rows, columns, and length of diagonal elements in the variance-covariance matrix recoverable through optim's `hessian` slot was also `k + 1` rather than `k`, with the last row, last column, and last diagonal element being measures of covariance between $\\eta$ and the $\\beta$ elements, and variance in $\\eta$ itself. \n\nBy contrast, the length of coefficients returned by `coefficients(best_model)` is `k`, the number of $\\beta$ parameters being estimated, and the dimensions of `vcov(best_model)` returned are also `k by k`. \n\n**This means there is one fewer piece/type of information about model parameters returned by `coefficients(model)`, `vcov(model)` and `sigma(model)` than was potentially recoverable by `optim()`'s `par` and `hessian` parameter slots: namely, uncertainty about the true value of the ancillary parameter $\\sigma^2$.** The following table summarises this difference:\n\nInformation type | via optim | via lm and glm\n---------------- | --------- | --------------\nMain parameters: point | first k elements of `par` | `coefficients()` function\nMain parameters: uncertainty | first k rows and columns of `hessian` | `vcov()` function\nAncillary parameters: point | k+1th through to last element of `par` | `sigma()` function or equivalent for `glm()`\nAncillary parameters: uncertainty | last columns and rows of `hessian` (after rows and columns k)|  ---  \n\n\nSo long as capturing uncertainty about the fundamental variability in the stochastic part of the model isn't critical to our predictions then omission of a measure of uncertainty in the ancillary parameters $\\alpha$ is likely a price worth paying for the additional convenience of being able to use the model objects directly. However we should be aware that, whereas with optim we potentially have both $\\tilde{\\beta}$ and $\\tilde{\\alpha}$ to represent model uncertainty, when using the three convenience functions `coefficients()`, `vcov()` and `sigma()` we technically 'only' have $\\tilde{\\beta}$ and $\\dot{\\alpha}$ (i.e. point estimates alone for the ancillary parameters).\n\nWith the above caveat in mind, let's now look at using the results of `coefficients()`, `vcov()` and `sigma()` to generate (mostly) honest representations of expected values, predicted values, and first differences \n\n\n## Model predictions \n\nAs covered in section two, we can use the `mvrnorm` function from the `MASS` package to create $\\tilde{\\beta}$, our parameter estimates with uncertainty:\n\n\n### Parameter simulation\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_tilde <- MASS::mvrnorm(\n    n = 10000, \n    mu = coef, \n    Sigma = Sig\n)\n\nhead(beta_tilde)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     (Intercept) log(dose)    suppVC log(dose):suppVC\n[1,]    20.87538  9.689866 -2.836542         1.723545\n[2,]    19.43247  8.859462 -2.908695         4.902105\n[3,]    20.04414 10.951369 -2.337728         1.553765\n[4,]    20.08595  9.444338 -3.747288         4.100071\n[5,]    20.38512  7.122832 -2.729296         4.390524\n[6,]    20.26225 10.424484 -2.907097         1.458138\n```\n:::\n:::\n\n\nLet's first look at each of these parameters individually:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_tilde |> \n    as_tibble() |>\n    pivot_longer(everything(), names_to = \"coefficient\", values_to = \"value\") |> \n    ggplot(aes(x = value)) + \n    facet_grid(coefficient ~ .) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nNow let's look at a couple of coefficients jointly, to see how they're correlated. Firstly the association between the intercept and the log dosage:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_tilde |> \n    as_tibble() |>\n    ggplot(aes(x = `(Intercept)`, y = `log(dose)`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nHere the covariance between the two parameters appears very low. Now let's look at how log dosage and Vitamin C supplement factor are associated: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_tilde |> \n    as_tibble() |>\n    ggplot(aes(x = `log(dose)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nAgain, the covariance appears low. Finally, the association between log dose and the interaction term \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_tilde |> \n    as_tibble() |>\n    ggplot(aes(x = `log(dose)`, y = `log(dose):suppVC`)) + \n    geom_density_2d_filled() + \n    coord_equal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nHere we have a much stronger negative covariance between the two coefficients. Let's look at the variance-covariance extracted from the model previously to confirm this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nknitr::kable(Sig)\n```\n\n::: {.cell-output-display}\n|                 | (Intercept)| log(dose)|     suppVC| log(dose):suppVC|\n|:----------------|-----------:|---------:|----------:|----------------:|\n|(Intercept)      |   0.4612422|  0.000000| -0.4612422|         0.000000|\n|log(dose)        |   0.0000000|  1.440023|  0.0000000|        -1.440023|\n|suppVC           |  -0.4612422|  0.000000|  0.9224843|         0.000000|\n|log(dose):suppVC |   0.0000000| -1.440023|  0.0000000|         2.880045|\n:::\n:::\n\n\nHere we can see that the covariance between intercept and log dose is effectively zero, as is the covariance between the intercept and the interaction term, and the covariance between the log(dose) and suppVC factor. However, there is a negative covariance between log dose and the interaction term, i.e. what we have plotted above, and also between the intercept and the VC factor. For completeness, let's look at this last assocation, which we expect to show negative association: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_tilde |> \n    as_tibble() |>\n    ggplot(aes(x = `(Intercept)`, y = suppVC)) + \n    geom_density_2d_filled() + \n    coord_equal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nYes it is! The parameter estimates follow the covariance provided by Sigma, as we would expect. \n\n### Expected values\n\nLet's stay we are initially interested in the expected values for a dosage of 1.25mg, with the OJ (rather than VC) supplement: \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# first element is 1 due to intercept\npredictor <- c(1, log(1.25), 0, 0) \n\npredictions_ev <- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor %*% this_beta\n)\n\nhead(predictions_ev)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 23.03761 21.40940 22.48786 22.19340 21.97454 22.58840\n```\n:::\n:::\n\n\nLet's now get a 95% credible interval: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(predictions_ev, probs = c(0.025, 0.500, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2.5%      50%    97.5% \n21.31398 22.71939 24.14458 \n```\n:::\n:::\n\n\nSo, the 95% interval for the expected value is between `21.31` and `24.14`, with a middle (median) estimate of `22.73`.[^1] Let's check this against estimates from the `predict()` function:\n\n[^1]: These are the values produced the first time I ran the simulation. They are likely to be a little different each time, so may not be identical to the number of decimal places reported when I next render this document. These estimates are *approximations*. \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'confidence')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      fit      lwr      upr\n1 22.7285 21.26607 24.19093\n```\n:::\n:::\n\n\nThe expected values using the predict function give a 95% confidence interval of `21.27`  to `24.19`, with a point estimate of `22.73`. These are not identical, as the methods employed are not identical,[^2] but they are hopefully similar enough to demonstrate they are attempts at getting at the same quantities of interest. \n\n\n\n[^2]: Because the simulation approach relies on random numbers, the draws will never be the same unless the same random number seed is using using `set.seed()`. However with more simulations, using the `n` parameter from `mvrnorm`, the distributions of estimates should become ever closer to each other. \n\n### Predicted values\n\nPredicted values also include inherent stochastic variation from the ancillary parameters $\\alpha$, which for linear regression is $\\sigma^2$. We can simply add these only the expected values above to produce predicted values: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- length(predictions_ev)\n\nshoogliness <- rnorm(n=n, mean = 0, sd = sig)\n\npredictions_pv <- predictions_ev + shoogliness\n\n\nhead(predictions_pv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 25.66925 28.20725 28.10618 18.29734 22.02755 24.62414\n```\n:::\n:::\n\n\nLet's get the 95% interval from the above using quantile\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(predictions_pv, probs = c(0.025, 0.5000, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2.5%      50%    97.5% \n15.35227 22.72828 30.31997 \n```\n:::\n:::\n\n\nAs expected, the interval is now much wider, with a 95% interval from `15.34` to `30.11`. The central estimate should in theory, with an infinite number of runs, be the same, however because of random variation it will never be *exactly* the same to an arbitrary number of decimal places. In this case, the middle estimate is `22.75`, not identical to the central estimate from the expected values distribution of `22.72`. The number of simulations can *always* be increased to produce greater precision if needed. \n\nLet's now compare this with the prediction interval produce by the predict function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(best_model, newdata = data.frame(dose = 1.25, supp = \"OJ\"), interval = 'prediction')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      fit      lwr     upr\n1 22.7285 15.13461 30.3224\n```\n:::\n:::\n\n\nAgain, the interval estimates are not exactly the same, but they are very similar. \n\n### First differences \n\nIt's in the production of estimates of **first differences** - this, compared to that, holding all else constant - that the simulation approach shines for producing estimates with credible uncertainty. In our case, let's say we are interested in asking: \n\n> What is the expected effect of using the VC supplement, rather than the OJ supplement, where the dose is 1.25mg? \n\nSo, the first difference is from switching from OJ to VC, holding the other factor constant. \n\nWe can answer this question by using the same selection of $\\tilde{\\beta}$ draws, but passing two different scenarios: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#scenario 0: supplement is OJ\npredictor_x0 <- c(1, log(1.25), 0, 0) \n\n#scenario 1: supplement is VC\npredictor_x1 <- c(1, log(1.25), 1, 1 * log(1.25)) \n\n\npredictions_ev_x0 <- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x0 %*% this_beta\n)\n\npredictions_ev_x1 <- apply(\n    beta_tilde, \n    1, \n    FUN = function(this_beta) predictor_x1 %*% this_beta\n)\n\npredictions_df <- \n    tibble(\n        x0 = predictions_ev_x0,\n        x1 = predictions_ev_x1\n    ) |>\n    mutate(\n        fd = x1 - x0\n    )\n\npredictions_df\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10,000 × 3\n      x0    x1    fd\n   <dbl> <dbl> <dbl>\n 1  23.0  20.6 -2.45\n 2  21.4  19.6 -1.81\n 3  22.5  20.5 -1.99\n 4  22.2  19.4 -2.83\n 5  22.0  20.2 -1.75\n 6  22.6  20.0 -2.58\n 7  23.8  19.1 -4.74\n 8  23.1  19.8 -3.32\n 9  22.6  20.2 -2.40\n10  22.8  20.6 -2.21\n# ℹ 9,990 more rows\n```\n:::\n:::\n\n\n\nLet's look at the distribution of both scenarios individually: \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions_df |>\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |>\n    filter(scenario != \"fd\") |>\n    ggplot(aes(x = estimate)) + \n    facet_wrap(~scenario, ncol = 1) + \n    geom_histogram(bins = 100)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nAnd the distribution of the pairwise differences between them: \n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions_df |>\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |>\n    filter(scenario == \"fd\") |>\n    ggplot(aes(x = estimate)) + \n    geom_histogram(bins = 100) + \n    geom_vline(xintercept = 0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nIt's this last distribution which shows our first differences, i.e. our answer, hedged with an appropriate dose of uncertainty, to the specific question shown above. We can get a 95% interval of the first difference as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions_df |>\n    pivot_longer(everything(), names_to = \"scenario\", values_to = \"estimate\") |>\n    filter(scenario == \"fd\") |> \n    pull('estimate') |>\n    quantile(probs = c(0.025, 0.500, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      2.5%        50%      97.5% \n-4.8391048 -2.8440436 -0.8101429 \n```\n:::\n:::\n\n\nSo, 95% of estimates of the first difference are between `-4.85` and `-0.81`, with the middle of this distribution (on this occasion) being `-2.83`. \n\nUnlike with the expected values and predicted values, the `predict()` function does not return first differences with honest uncertainty in this way. What we have above is something new. \n\n\n## Summary\n\nIn this post we've finally combined all the learning we've developed over the 11 previous posts to answer three specific 'what if?' questions: one on expected values, one on predicted values, and one on first differences. These are what @KinTomWit00 refer to as *quantities of interest*, and I hope you agree these are more organic and reasonable types of question to ask of data and statistical models than simply looking at coefficients and p-values and reporting which ones are 'statistically significant'. \n\nIf you've been able to follow everything in these posts, and can generalise the approach shown above to other types of statistical model, then **congratulations!** You've learned the framework for answering meaningful questions using statistical models which is at the heart of [one of the toughest methods courses for social scientists offered by one of the most prestigious universities in the world](https://projects.iq.harvard.edu/gov2001/home).[^3]\n\n\n[^3]: I took this course via the Harvard extension school while doing my PhD in York quite a few years ago. I took it as a non-credit option - as what's the value of a fraction of a degree when I had two already? - but was told by the tutors that I'd completed it to a 'Grade A-' level. So, not perfect, but good enough...",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}