{
  "hash": "3bd88eb67ec677dee9829eb0a9f3f903",
  "result": {
    "markdown": "---\ntitle: \"Part Two: Systematic components and link functions\"\nauthor: \"Jon Minton\"\ndate: \"2023-12-1\"\ncategories: [statistics]\nbibliography: references.bib\n---\n\n\n## tl;dr\n\nThis is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in @KinTomWit00 .\n\n## Part 2: Systematic components and link functions\n\nIn [part 1 of this series](../lms-are-glms-part-01) we introduced the following general framework for thinking about statistical models and what they contain.\n\n**Stochastic Component**\n\n$$\nY_i \\sim f(\\theta_i, \\alpha)\n$$\n\n**Systematic Component**\n\n$$\n\\theta_i = g(X_i, \\beta)\n$$ The terminology are as described previously.\n\nThese equations are too broad and abstract to be implemented directly. Instead, specific choices about the $f(.)$ and $g(.)$ need to be made. @KinTomWit00 gives the following examples:\n\n**Logistic Regression**\n\n$$\nY_i \\sim Bernoulli(\\pi_i) \n$$\n\n$$\n\\pi_i = \\frac{1}{1 + e^{-X_i\\beta}}\n$$\n\n**Linear Regression**\n\n$$\nY_i \\sim N(\\mu_i, \\sigma^2) \n$$ $$\n \\mu_i = X_i\\beta\n$$\n\nSo, what's so special about linear regression, in this framework?\n\nIn one sense, not so much. It's got a systematic component, and it's got a stochastic component. But so do other models. But in another sense, quite a lot. It's a rare case where the systematic component, $g(.)$, *doesn't* transform its inputs in some weird and wonderful way. We can say that $g(.)$ is the identity transform, $I(.)$, which in words means *take what you're given, do nothing to it, and pass it on*.\n\nBy contrast, the systematic component for logistic regression is known as the logistic function. $logistic(x) := \\frac{1}{1 + e^{-x}}$ It transforms inputs that could be anywhere on the real number line to values that lay somewhere between 0 and 1. Why 0 to 1? Because what logistic regression models produce aren't predicted values, but predicted *probabilities*, and nothing can be more probable than certain (1) or less probable than impossible (0).\n\nWe can compare the transformations used in linear and logistic regression as follows:[^1]\n\n[^1]: Using some base R graphics functions as I'm feeling masochistic\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# Define transformations\nident <- function(x) {x}\nlgt <- function(x) {1 / (1 + exp(-x))}\n\n\n# Draw the associations\ncurve(ident, -6, 6,\n      xlab = \"x (before transform)\",\n      ylab = \"z (after transform)\",\n      main = \"The Identity 'Transformation'\"\n      )\n```\n\n::: {.cell-output-display}\n![Identity Transformation](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncurve(lgt, -6, 6, \n      xlab = \"x (before transform)\", \n      ylab = \"z (after transform)\",\n      main = \"The Logistic Transformation\"\n      )\n```\n\n::: {.cell-output-display}\n![Logistic Transformation](index_files/figure-html/unnamed-chunk-1-2.png){width=672}\n:::\n:::\n\n\nThe usual input to the transformation function $g(.)$ is a sum of products. For three variables, for example, this could be $\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$. In matrix algebra this generalises to $\\boldsymbol{X\\beta}$ , where $\\boldsymbol{X}$ is the predictor data whose rows are observations, columns are variables, and whose first column is a vector of 1s (for the intercept term). The $\\boldsymbol{\\beta}$ term is a row-wise vector comprising each specific $\\beta$ term, such as $\\boldsymbol{\\beta} = \\{ \\beta_0, \\beta_1, \\beta_2 \\}$ in the three variable example above.\n\nWhat's special about the identity transformation, and so linear regression, is that there is a fairly clear correspondence between a $\\beta_j$ term and the estimated influence of changing a predictor variable $x_j$ on the predicted outcome $Y$, i.e. the 'effect of $x_j$ on $Y$'. For other transformations this tends to not be the case.\n\nWe'll delve into how this is implemented in practice in [part 3](../lms-are-glms-part-03).",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}