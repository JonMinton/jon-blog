{
  "hash": "08fcd1340eb9791e2021d786b33a2a90",
  "result": {
    "markdown": "---\ntitle: \"Part Three: glm is just fancy lm\"\nauthor: \"Jon Minton\"\ndate: \"2023-12-2\"\ncategories: [statistics, R]\npart-id: 3\nbibliography: references.bib\n---\n\n\n## tl;dr\n\nThis is part of a series of posts which introduce and discuss the implications of a general framework for thinking about statistical modelling. This framework is most clearly expressed in @KinTomWit00.\n\n## Part 3: How to express a linear model as a generalised linear model\n\nIn [the last part](../lms-are-glms-part-02/index.qmd), we introduced two types of generalised linear models, with two types of transformation for the systematic component of the model, `g(.)`, the logit transformation, and the identity transformation. This post will show how this framework is implemented in practice in R.\n\nIn R, there's the `lm` function for linear models, and the `glm` function for generalised linear models.\n\nI've argued previously that the standard linear regression is just a specific type of generalised linear model, one that makes use of an identity transformation `I(.)` for its systematic component `g(.)`. Let's now demonstrate that by producing the same model specification using both `lm` and `glm`.\n\nWe can start by being painfully unimaginative and picking using one of R's standard datasets\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyverse' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.2     ✔ purrr   1.0.1\n✔ tibble  3.2.1     ✔ dplyr   1.1.1\n✔ tidyr   1.3.0     ✔ stringr 1.5.0\n✔ readr   2.1.4     ✔ forcats 1.0.0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'ggplot2' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tibble' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'readr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'purrr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'dplyr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'stringr' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'forcats' was built under R version 4.1.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\niris |> \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nIt looks like, where the petal length is over 2.5, the relationship with sepal length is fairly linear\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris |> \n  filter(Petal.Length > 2.5) |> \n  ggplot(aes(Petal.Length, Sepal.Length)) + \n  geom_point() + \n  labs(\n    title = \"The Iris dataset *Yawn*\",\n    x = \"Petal Length\",\n    y = \"Sepal Length\"\n  ) + \n  expand_limits(x = 0, y = 0)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nSo, let's make a linear regression just of this subset\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris_ss <- \n  iris |> \n  filter(Petal.Length > 2.5) \n```\n:::\n\n\nWe can produce the regression using `lm` as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_lm <- lm(Sepal.Length ~ Petal.Length, data = iris_ss)\n```\n:::\n\n\nAnd we can use the `summary` function (which checks the type of `mod_lm` and evokes `summary.lm` implicitly) to get the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09194 -0.26570  0.00761  0.21902  0.87502 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.99871    0.22593   13.27   <2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3731 on 98 degrees of freedom\nMultiple R-squared:  0.6864,\tAdjusted R-squared:  0.6832 \nF-statistic: 214.5 on 1 and 98 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nWoohoo! Three stars next to the `Petal.Length` coefficient! Definitely publishable!\n\nTo do the same using `glm`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_glm <- glm(Sepal.Length ~ Petal.Length, data = iris_ss)\n```\n:::\n\n\nAnd we can use the `summary` function for this data too. In this case, `summary` evokes `summary.glm` because it knows the class of `mod_glm` contains `glm`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mod_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, data = iris_ss)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.09194  -0.26570   0.00761   0.21902   0.87502  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.99871    0.22593   13.27   <2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n:::\n\n\nSo, the coefficients are exactly the same. But there's also some additional information in the summary, including on the type of 'family' used. Why is this?\n\nIf we look at the help for `glm` we can see that, by default, the `family` argument is set to `gaussian`.\n\nAnd if we delve a bit further into the help file, in the details about the family argument, it links to the `family` help page. The usage statement of the `family` help file is as follows:\n\n```         \nfamily(object, ...)\n\nbinomial(link = \"logit\")\ngaussian(link = \"identity\")\nGamma(link = \"inverse\")\ninverse.gaussian(link = \"1/mu^2\")\npoisson(link = \"log\")\nquasi(link = \"identity\", variance = \"constant\")\nquasibinomial(link = \"logit\")\nquasipoisson(link = \"log\")\n```\n\nEach family has a default `link` argument, and for this `gaussian` family, this link is the identity function.\n\nWe can also see that, for both the `binomial` and `quasibinomial` family, the default link is `logit`, which transforms all predictors onto a 0-1 scale, as shown in the last post.\n\nSo, by using the default family, the Gaussian family is selected, and by using the default Gaussian family member, the identity link is selected.\n\nWe can confirm this by setting the family and link explicitly, showing that we get the same results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_glm2 <- glm(Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), data = iris_ss)\nsummary(mod_glm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Sepal.Length ~ Petal.Length, family = gaussian(link = \"identity\"), \n    data = iris_ss)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-1.09194  -0.26570   0.00761   0.21902   0.87502  \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.99871    0.22593   13.27   <2e-16 ***\nPetal.Length  0.66516    0.04542   14.64   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1391962)\n\n    Null deviance: 43.496  on 99  degrees of freedom\nResidual deviance: 13.641  on 98  degrees of freedom\nAIC: 90.58\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n:::\n\n\nIt's the same!\n\nHow do these terms used in the `glm` function, `family` and `link`, relate to the general framework in @KinTomWit00?\n\n-   `family` is the stochastic component, `f(.)`\n-   `link` is the systematic component, `g(.)`\n\nThey're different terms, but it's the same broad framework.\n\nLinear models are just one type of general linear model![^claude-python]\n\n### Coming up\n\nIn [the next part](../lms-are-glms-part-04/index.qmd) of this series, we will delve into the differences between linear regression models and logistic regression models, with a focus on how to get meaningful effect estimates from both types of model.\n\n[^claude-python]: **Note from Claude:** For Python users, the equivalent functionality is available through multiple libraries. The statsmodels library most closely mirrors R's approach: `statsmodels.api.GLM(y, X, family=sm.families.Gaussian())` is equivalent to R's `glm(y ~ x, family=gaussian())`. For production ML workflows, scikit-learn provides `LinearRegression()` and `LogisticRegression()` with simpler APIs but less statistical detail. The fast.ai course \"Practical Deep Learning for Coders\" demonstrates how these GLM concepts scale to deep learning, showing that even complex neural networks follow the same fit-predict-evaluate loop described in this series.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}